Using TensorFlow backend.
[2019-04-27 17:50:11,080] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='part3_v1', activation='relu', agent_num=5, check_args_only=False, clip_norm=5.0, debug_log_prob=0.0005, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part3-NA-Bej-Train-v1', eval_act_func='part3_bej_det_v1', eval_env_res_max_keep=100, eval_epi_num=1, eval_freq=25000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_add_time_to_state=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_r_term_zero=True, is_warm_start=False, job_mode='Train', learning_rate=0.0001, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=2500000, metric_func='part3_v1', model_dir='None', model_param=[256, 2], model_type='nn', num_threads=16, output='./Part3-NA-Bej-Train-v1-res1', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part3_v3', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=1.0, rwd_p_para=1.0, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=17, test_env=['Part3-NA-Bej-Test-v1', 'Part3-NA-Bej-Test-v2', 'Part3-NA-Bej-Test-v3', 'Part3-NA-Bej-Test-v4'], test_mode='Multiple', train_act_func='part3_bej_sto_v1', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=50.0, weight_initer='glorot_uniform', window_len=26)
[2019-04-27 17:50:11,080] A3C_AGENT_MAIN INFO:Start compiling...
2019-04-27 17:50:11.124018: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-04-27 17:50:27,351] A3C_AGENT_MAIN INFO:Start the learning...
[2019-04-27 17:50:27,351] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part3-NA-Bej-Train-v1', 'Part3-NA-Bej-Test-v1', 'Part3-NA-Bej-Test-v2', 'Part3-NA-Bej-Test-v3', 'Part3-NA-Bej-Test-v4'] ...
[2019-04-27 17:50:27,362] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation worker starts!
[2019-04-27 17:50:27,365] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation worker starts!
[2019-04-27 17:50:27,368] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation worker starts!
[2019-04-27 17:50:27,371] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation worker starts!
[2019-04-27 17:50:27,377] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation worker starts!
[2019-04-27 17:50:27,377] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 17:50:27,378] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-04-27 17:50:27,437] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:50:27,437] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res2/Eplus-env-sub_run1
[2019-04-27 17:50:28,378] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 17:50:28,380] A3C_AGENT_WORKER-Thread-3 INFO:Local worker starts!
[2019-04-27 17:50:28,478] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:50:28,478] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res3/Eplus-env-sub_run1
[2019-04-27 17:50:28,623] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-27 17:50:28,624] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 17:50:28,624] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 17:50:28,624] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:50:28,625] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:50:28,625] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 17:50:28,625] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 17:50:28,626] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:50:28,626] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 17:50:28,626] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:50:28,627] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:50:28,630] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run1
[2019-04-27 17:50:28,639] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run1
[2019-04-27 17:50:28,641] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run1
[2019-04-27 17:50:28,658] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run1
[2019-04-27 17:50:28,659] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run1
[2019-04-27 17:50:29,381] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 17:50:29,382] A3C_AGENT_WORKER-Thread-9 INFO:Local worker starts!
[2019-04-27 17:50:29,467] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:50:29,468] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res4/Eplus-env-sub_run1
[2019-04-27 17:50:30,383] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 17:50:30,385] A3C_AGENT_WORKER-Thread-10 INFO:Local worker starts!
[2019-04-27 17:50:30,439] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:50:30,440] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res5/Eplus-env-sub_run1
[2019-04-27 17:50:31,386] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 17:50:31,387] A3C_AGENT_WORKER-Thread-11 INFO:Local worker starts!
[2019-04-27 17:50:31,468] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:50:31,469] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res6/Eplus-env-sub_run1
[2019-04-27 17:50:32,389] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 17:50:32,394] A3C_AGENT_WORKER-Thread-12 INFO:Local worker starts!
[2019-04-27 17:50:32,507] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:50:32,508] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res7/Eplus-env-sub_run1
[2019-04-27 17:50:33,393] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 17:50:33,396] A3C_AGENT_WORKER-Thread-13 INFO:Local worker starts!
[2019-04-27 17:50:33,460] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:50:33,461] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res8/Eplus-env-sub_run1
[2019-04-27 17:50:34,395] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 17:50:34,396] A3C_AGENT_WORKER-Thread-14 INFO:Local worker starts!
[2019-04-27 17:50:34,500] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:50:34,500] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res9/Eplus-env-sub_run1
[2019-04-27 17:50:35,397] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 17:50:35,403] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-04-27 17:50:35,468] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:50:35,468] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res10/Eplus-env-sub_run1
[2019-04-27 17:50:36,401] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 17:50:36,402] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-04-27 17:50:36,510] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:50:36,510] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res11/Eplus-env-sub_run1
[2019-04-27 17:50:37,403] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 17:50:37,409] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-04-27 17:50:37,474] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:50:37,476] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res12/Eplus-env-sub_run1
[2019-04-27 17:50:38,408] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 17:50:38,410] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-04-27 17:50:38,468] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:50:38,469] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res13/Eplus-env-sub_run1
[2019-04-27 17:50:39,410] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 17:50:39,411] A3C_AGENT_WORKER-Thread-19 INFO:Local worker starts!
[2019-04-27 17:50:39,481] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:50:39,483] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res14/Eplus-env-sub_run1
[2019-04-27 17:50:40,412] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 17:50:40,414] A3C_AGENT_WORKER-Thread-20 INFO:Local worker starts!
[2019-04-27 17:50:40,522] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:50:40,522] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res15/Eplus-env-sub_run1
[2019-04-27 17:50:41,415] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 17:50:41,419] A3C_AGENT_WORKER-Thread-21 INFO:Local worker starts!
[2019-04-27 17:50:41,481] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:50:41,481] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res16/Eplus-env-sub_run1
[2019-04-27 17:50:42,420] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 17:50:42,425] A3C_AGENT_WORKER-Thread-22 INFO:Local worker starts!
[2019-04-27 17:50:42,532] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:50:42,533] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res17/Eplus-env-sub_run1
[2019-04-27 17:50:47,069] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-27 17:50:47,070] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [23.76226052333334, 67.40892919166667, 1.0, 2.0, 0.2002126034353939, 1.0, 1.0, 0.2002126034353939, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 491955.7258121201, 491955.7258121201, 159842.816708527]
[2019-04-27 17:50:47,070] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 17:50:47,071] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0.16431694 0.1747278  0.19205414 0.20236556 0.26653546], sampled 0.6520356887833653
[2019-04-27 17:50:58,370] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-27 17:50:58,373] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [36.0, 22.0, 1.0, 2.0, 0.6100642768951491, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260425882001, 747982.8463527348, 747982.8463527353, 161930.5698640035]
[2019-04-27 17:50:58,374] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 17:50:58,378] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0.17734447 0.15081333 0.1551593  0.22596698 0.29071593], sampled 0.6172723303289989
[2019-04-27 17:51:07,095] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-27 17:51:07,096] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [25.53333333333333, 80.0, 1.0, 2.0, 0.5741245451700511, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 664954.1329801794, 664954.1329801794, 154194.8293505398]
[2019-04-27 17:51:07,097] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 17:51:07,099] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0.18009879 0.17507054 0.16969861 0.19536717 0.27976492], sampled 0.4185581273534321
[2019-04-27 17:51:24,377] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-27 17:51:24,377] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [24.53333333333333, 93.33333333333334, 1.0, 2.0, 0.6452732863042522, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 736745.1900953584, 736745.1900953584, 166107.8704997956]
[2019-04-27 17:51:24,378] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 17:51:24,381] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0.1506714  0.17558171 0.20168084 0.16380712 0.30825892], sampled 0.2663022939240791
[2019-04-27 17:51:44,320] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-27 17:51:44,321] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [26.06323793, 79.84675350666667, 1.0, 2.0, 0.6096818856124058, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 697254.1052785332, 697254.1052785327, 159870.1406904026]
[2019-04-27 17:51:44,323] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 17:51:44,326] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0.13818616 0.14512509 0.17284258 0.2417222  0.302124  ], sampled 0.3562436917329125
[2019-04-27 17:52:22,673] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 3584.5489 2591661114.1332 246.0000
[2019-04-27 17:52:22,999] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 3782.2785 2532015936.3275 219.0000
[2019-04-27 17:52:23,040] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 3706.1580 2769673468.7054 317.0000
[2019-04-27 17:52:23,055] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 3749.1210 2560336137.0152 260.0000
[2019-04-27 17:52:23,103] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 3839.9409 2498642196.2405 194.0000
[2019-04-27 17:52:24,120] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 3706.1579578632623, 2769673468.7053604, 317.0, 3782.27845750457, 2532015936.3275313, 219.0, 3839.940898196313, 2498642196.2405176, 194.0, 3584.548919323077, 2591661114.133206, 246.0, 3749.1210058722377, 2560336137.0152197, 260.0]
[2019-04-27 17:52:35,259] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.1900283e-07 1.8585314e-06 2.2822294e-04 9.9436498e-01 5.4043015e-03], sum to 1.0000
[2019-04-27 17:52:35,267] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2226
[2019-04-27 17:52:35,380] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.8, 14.0, 1.0, 2.0, 0.1821896024288543, 1.0, 2.0, 0.1821896024288543, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 469586.1291278343, 469586.1291278343, 156444.5343104658], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 162000.0000, 
sim time next is 162600.0000, 
raw observation next is [31.71666666666667, 13.66666666666667, 1.0, 2.0, 0.1826121946558124, 1.0, 2.0, 0.1826121946558124, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 471109.7231940877, 471109.7231940881, 155748.5745051675], 
processed observation next is [1.0, 0.9130434782608695, 0.730246913580247, 0.1366666666666667, 1.0, 1.0, 0.026919279352157627, 1.0, 1.0, 0.026919279352157627, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.16825347256931703, 0.16825347256931716, 0.2995164894330144], 
reward next is 0.7005, 
noisyNet noise sample is [array([0.9169517], dtype=float32), 1.3482323]. 
=============================================
[2019-04-27 17:52:35,450] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8843330e-07 3.1004165e-06 1.1677652e-05 9.9922037e-01 7.6460215e-04], sum to 1.0000
[2019-04-27 17:52:35,461] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6528
[2019-04-27 17:52:35,566] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.46666666666667, 12.66666666666667, 1.0, 2.0, 0.1789536762133091, 1.0, 2.0, 0.1789536762133091, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 461711.591951347, 461711.591951347, 149391.1534391553], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 164400.0000, 
sim time next is 165000.0000, 
raw observation next is [31.38333333333333, 12.33333333333333, 1.0, 2.0, 0.1783870759528293, 1.0, 2.0, 0.1783870759528293, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 460249.2894012477, 460249.2894012482, 147839.3240565051], 
processed observation next is [1.0, 0.9130434782608695, 0.7179012345679011, 0.12333333333333331, 1.0, 1.0, 0.021889376134320584, 1.0, 1.0, 0.021889376134320584, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1643747462147313, 0.1643747462147315, 0.28430639241635597], 
reward next is 0.7157, 
noisyNet noise sample is [array([0.33897218], dtype=float32), -1.5853276]. 
=============================================
[2019-04-27 17:52:35,585] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[50.025898]
 [50.036747]
 [50.04552 ]
 [50.198547]
 [50.488663]], R is [[50.2336998 ]
 [50.44406891]
 [50.64897537]
 [50.84790802]
 [51.03991318]].
[2019-04-27 17:52:35,659] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.0216585e-07 9.9447419e-08 1.4232754e-04 9.9968481e-01 1.7261936e-04], sum to 1.0000
[2019-04-27 17:52:35,668] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9987
[2019-04-27 17:52:35,782] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.03333333333333, 10.66666666666667, 1.0, 2.0, 0.176897516989053, 1.0, 2.0, 0.176897516989053, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 456404.9947787616, 456404.9947787621, 142304.9873040653], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 168000.0000, 
sim time next is 168600.0000, 
raw observation next is [30.96666666666667, 10.33333333333333, 1.0, 2.0, 0.1766266265268594, 1.0, 2.0, 0.1766266265268594, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 455705.8753163369, 455705.8753163374, 141435.0937938669], 
processed observation next is [1.0, 0.9565217391304348, 0.7024691358024692, 0.1033333333333333, 1.0, 1.0, 0.019793603008165947, 1.0, 1.0, 0.019793603008165947, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1627520983272632, 0.16275209832726337, 0.27199056498820556], 
reward next is 0.7280, 
noisyNet noise sample is [array([0.10846959], dtype=float32), 0.25857964]. 
=============================================
[2019-04-27 17:52:42,964] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 7917: loss 8.8274
[2019-04-27 17:52:43,022] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 7919: learning rate 0.0001
[2019-04-27 17:52:43,060] A3C_AGENT_WORKER-Thread-3 INFO:Local step 500, global step 7944: loss 1.3747
[2019-04-27 17:52:43,063] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 500, global step 7945: learning rate 0.0001
[2019-04-27 17:52:43,074] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 7951: loss 3.1327
[2019-04-27 17:52:43,076] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 7951: learning rate 0.0001
[2019-04-27 17:52:43,093] A3C_AGENT_WORKER-Thread-10 INFO:Local step 500, global step 7960: loss 4.5532
[2019-04-27 17:52:43,095] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 500, global step 7961: learning rate 0.0001
[2019-04-27 17:52:43,107] A3C_AGENT_WORKER-Thread-9 INFO:Local step 500, global step 7967: loss 3.5381
[2019-04-27 17:52:43,110] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 500, global step 7967: learning rate 0.0001
[2019-04-27 17:52:43,118] A3C_AGENT_WORKER-Thread-22 INFO:Local step 500, global step 7972: loss -9.7593
[2019-04-27 17:52:43,120] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 500, global step 7972: learning rate 0.0001
[2019-04-27 17:52:43,130] A3C_AGENT_WORKER-Thread-19 INFO:Local step 500, global step 7975: loss 4.9400
[2019-04-27 17:52:43,132] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 500, global step 7975: learning rate 0.0001
[2019-04-27 17:52:43,148] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 7983: loss 6.2616
[2019-04-27 17:52:43,149] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 7983: learning rate 0.0001
[2019-04-27 17:52:43,149] A3C_AGENT_WORKER-Thread-13 INFO:Local step 500, global step 7983: loss 3.1880
[2019-04-27 17:52:43,150] A3C_AGENT_WORKER-Thread-12 INFO:Local step 500, global step 7983: loss 4.0318
[2019-04-27 17:52:43,153] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 500, global step 7984: learning rate 0.0001
[2019-04-27 17:52:43,154] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 500, global step 7984: learning rate 0.0001
[2019-04-27 17:52:43,189] A3C_AGENT_WORKER-Thread-21 INFO:Local step 500, global step 7999: loss 4.6493
[2019-04-27 17:52:43,192] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 500, global step 7999: learning rate 0.0001
[2019-04-27 17:52:43,210] A3C_AGENT_WORKER-Thread-11 INFO:Local step 500, global step 8008: loss 6.0141
[2019-04-27 17:52:43,213] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 500, global step 8009: learning rate 0.0001
[2019-04-27 17:52:43,222] A3C_AGENT_WORKER-Thread-20 INFO:Local step 500, global step 8013: loss -6.8077
[2019-04-27 17:52:43,223] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 500, global step 8015: learning rate 0.0001
[2019-04-27 17:52:43,255] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 8029: loss 5.0632
[2019-04-27 17:52:43,257] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 8030: learning rate 0.0001
[2019-04-27 17:52:43,289] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 8047: loss 3.3067
[2019-04-27 17:52:43,292] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 8048: learning rate 0.0001
[2019-04-27 17:52:43,315] A3C_AGENT_WORKER-Thread-14 INFO:Local step 500, global step 8060: loss 0.1215
[2019-04-27 17:52:43,317] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 500, global step 8060: learning rate 0.0001
[2019-04-27 17:52:59,652] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1000, global step 15886: loss 2.7160
[2019-04-27 17:52:59,655] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1000, global step 15887: learning rate 0.0001
[2019-04-27 17:52:59,734] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1000, global step 15930: loss 2.7661
[2019-04-27 17:52:59,737] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1000, global step 15930: learning rate 0.0001
[2019-04-27 17:52:59,782] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 15949: loss 2.6394
[2019-04-27 17:52:59,786] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 15951: learning rate 0.0001
[2019-04-27 17:52:59,790] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1000, global step 15956: loss 2.6813
[2019-04-27 17:52:59,792] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1000, global step 15957: learning rate 0.0001
[2019-04-27 17:52:59,814] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 15965: loss 2.6861
[2019-04-27 17:52:59,817] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1000, global step 15966: loss 2.3294
[2019-04-27 17:52:59,819] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 15965: learning rate 0.0001
[2019-04-27 17:52:59,820] A3C_AGENT_WORKER-Thread-22 INFO:Local step 1000, global step 15966: loss 2.2364
[2019-04-27 17:52:59,822] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1000, global step 15966: learning rate 0.0001
[2019-04-27 17:52:59,823] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 1000, global step 15967: learning rate 0.0001
[2019-04-27 17:52:59,850] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 15981: loss 2.2469
[2019-04-27 17:52:59,852] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1000, global step 15982: loss 2.1056
[2019-04-27 17:52:59,853] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 15982: learning rate 0.0001
[2019-04-27 17:52:59,853] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1000, global step 15982: learning rate 0.0001
[2019-04-27 17:52:59,870] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1000, global step 15990: loss 1.9844
[2019-04-27 17:52:59,872] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1000, global step 15990: learning rate 0.0001
[2019-04-27 17:52:59,920] A3C_AGENT_WORKER-Thread-21 INFO:Local step 1000, global step 16011: loss 1.7944
[2019-04-27 17:52:59,923] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 1000, global step 16011: learning rate 0.0001
[2019-04-27 17:52:59,937] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1000, global step 16018: loss 1.7840
[2019-04-27 17:52:59,940] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1000, global step 16019: learning rate 0.0001
[2019-04-27 17:52:59,943] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 16020: loss 1.8281
[2019-04-27 17:52:59,944] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1000, global step 16020: loss 1.5661
[2019-04-27 17:52:59,944] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 16020: learning rate 0.0001
[2019-04-27 17:52:59,949] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1000, global step 16021: learning rate 0.0001
[2019-04-27 17:53:00,033] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1000, global step 16069: loss 1.0765
[2019-04-27 17:53:00,035] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1000, global step 16069: learning rate 0.0001
[2019-04-27 17:53:00,120] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 16106: loss 0.5888
[2019-04-27 17:53:00,122] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 16107: learning rate 0.0001
[2019-04-27 17:53:15,160] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.3282925e-15 1.0000000e+00 1.5225088e-17 3.9100815e-18 7.4412215e-21], sum to 1.0000
[2019-04-27 17:53:15,167] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6895
[2019-04-27 17:53:15,278] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.76666666666667, 66.33333333333334, 1.0, 2.0, 0.3379137662042673, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 428373.8733191731, 428373.8733191731, 120371.727495494], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 877200.0000, 
sim time next is 877800.0000, 
raw observation next is [21.68333333333334, 66.16666666666666, 1.0, 2.0, 0.3349065986664459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 425028.9059156064, 425028.9059156064, 119984.6153110949], 
processed observation next is [0.0, 0.13043478260869565, 0.3586419753086422, 0.6616666666666666, 1.0, 1.0, 0.20822214126957844, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1517960378270023, 0.1517960378270023, 0.23073964482902865], 
reward next is 0.7693, 
noisyNet noise sample is [array([1.3286244], dtype=float32), -1.3934903]. 
=============================================
[2019-04-27 17:53:15,459] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.2866867e-12 1.0000000e+00 3.9967263e-14 1.3000287e-15 1.0800030e-18], sum to 1.0000
[2019-04-27 17:53:15,472] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9189
[2019-04-27 17:53:15,574] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.3, 65.5, 1.0, 2.0, 0.3207513605293399, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 408958.342293521, 408958.342293521, 118176.7525903198], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 880200.0000, 
sim time next is 880800.0000, 
raw observation next is [21.2, 65.33333333333333, 1.0, 2.0, 0.3161350322033156, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 403542.6595819312, 403542.6595819317, 117592.6313066157], 
processed observation next is [0.0, 0.17391304347826086, 0.34074074074074073, 0.6533333333333333, 1.0, 1.0, 0.1858750383372805, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.14412237842211828, 0.14412237842211847, 0.22613967558964557], 
reward next is 0.7739, 
noisyNet noise sample is [array([0.9384405], dtype=float32), 0.6506054]. 
=============================================
[2019-04-27 17:53:16,399] A3C_AGENT_WORKER-Thread-22 INFO:Local step 1500, global step 23869: loss 0.0211
[2019-04-27 17:53:16,400] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 1500, global step 23869: learning rate 0.0001
[2019-04-27 17:53:16,490] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1500, global step 23910: loss 0.0866
[2019-04-27 17:53:16,497] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1500, global step 23911: learning rate 0.0001
[2019-04-27 17:53:16,551] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1500, global step 23943: loss 0.1028
[2019-04-27 17:53:16,554] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1500, global step 23943: learning rate 0.0001
[2019-04-27 17:53:16,556] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1500, global step 23944: loss 0.0444
[2019-04-27 17:53:16,560] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1500, global step 23945: learning rate 0.0001
[2019-04-27 17:53:16,560] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 23945: loss 0.1592
[2019-04-27 17:53:16,564] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 23945: learning rate 0.0001
[2019-04-27 17:53:16,600] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1500, global step 23968: loss 0.0042
[2019-04-27 17:53:16,601] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1500, global step 23968: learning rate 0.0001
[2019-04-27 17:53:16,621] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 23977: loss 0.0090
[2019-04-27 17:53:16,624] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 23977: learning rate 0.0001
[2019-04-27 17:53:16,637] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1500, global step 23986: loss 0.0098
[2019-04-27 17:53:16,638] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 23986: loss 0.0012
[2019-04-27 17:53:16,639] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1500, global step 23986: learning rate 0.0001
[2019-04-27 17:53:16,642] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 23988: learning rate 0.0001
[2019-04-27 17:53:16,668] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1500, global step 23998: loss 0.0021
[2019-04-27 17:53:16,672] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1500, global step 23998: learning rate 0.0001
[2019-04-27 17:53:16,675] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1500, global step 24000: loss 0.0065
[2019-04-27 17:53:16,679] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1500, global step 24000: learning rate 0.0001
[2019-04-27 17:53:16,695] A3C_AGENT_WORKER-Thread-21 INFO:Local step 1500, global step 24010: loss 0.0004
[2019-04-27 17:53:16,696] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 1500, global step 24010: learning rate 0.0001
[2019-04-27 17:53:16,737] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 24031: loss 0.0115
[2019-04-27 17:53:16,741] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 24031: learning rate 0.0001
[2019-04-27 17:53:16,782] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1500, global step 24052: loss 0.1542
[2019-04-27 17:53:16,783] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1500, global step 24052: learning rate 0.0001
[2019-04-27 17:53:16,809] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 24062: loss 0.0741
[2019-04-27 17:53:16,810] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 24062: learning rate 0.0001
[2019-04-27 17:53:16,924] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1500, global step 24120: loss 0.0078
[2019-04-27 17:53:16,926] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1500, global step 24120: learning rate 0.0001
[2019-04-27 17:53:18,742] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-27 17:53:18,743] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 17:53:18,744] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 17:53:18,745] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:53:18,747] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:53:18,747] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 17:53:18,750] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 17:53:18,752] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 17:53:18,750] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:53:18,754] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:53:18,755] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:53:18,764] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run2
[2019-04-27 17:53:18,780] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run2
[2019-04-27 17:53:18,798] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run2
[2019-04-27 17:53:18,798] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run2
[2019-04-27 17:53:18,829] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run2
[2019-04-27 17:53:40,201] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0065635], dtype=float32), 0.01904461]
[2019-04-27 17:53:40,202] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [32.2, 44.5, 1.0, 2.0, 0.569288338865246, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 665300.2775014894, 665300.2775014889, 153648.2543167291]
[2019-04-27 17:53:40,203] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 17:53:40,205] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.3118396e-16 1.0000000e+00 4.1053633e-19 6.6484248e-20 5.1777879e-24], sampled 0.580985047388353
[2019-04-27 17:53:55,084] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0065635], dtype=float32), 0.01904461]
[2019-04-27 17:53:55,085] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [28.0, 64.0, 1.0, 2.0, 0.5533893376709146, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 646490.451996122, 646490.451996122, 150986.7453822285]
[2019-04-27 17:53:55,086] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 17:53:55,089] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [3.94959982e-17 1.00000000e+00 1.07080915e-19 1.75005617e-20
 9.79136128e-25], sampled 0.37130827942473266
[2019-04-27 17:54:00,560] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0065635], dtype=float32), 0.01904461]
[2019-04-27 17:54:00,562] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [26.16666666666667, 74.83333333333334, 1.0, 2.0, 0.5796350760077945, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 675639.7203346051, 675639.7203346051, 155320.5163890673]
[2019-04-27 17:54:00,564] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 17:54:00,566] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.0436573e-17 1.0000000e+00 2.4516383e-20 3.8693898e-21 1.4669551e-25], sampled 0.25172229300769744
[2019-04-27 17:54:09,908] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0065635], dtype=float32), 0.01904461]
[2019-04-27 17:54:09,909] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [26.11666666666667, 88.16666666666667, 1.0, 2.0, 0.8874071027441528, 1.0, 2.0, 0.8874071027441528, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 122.4015943271048, 2024312.568649953, 2024312.568649953, 381265.0125751177]
[2019-04-27 17:54:09,910] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 17:54:09,914] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [8.4578993e-16 1.0000000e+00 3.9695661e-18 9.2583576e-19 7.1956736e-23], sampled 0.0341924817573368
[2019-04-27 17:54:09,915] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2024312.568649953 W.
[2019-04-27 17:54:13,509] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0065635], dtype=float32), 0.01904461]
[2019-04-27 17:54:13,511] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [22.725237255, 81.00287681, 1.0, 2.0, 0.4302586071101255, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 524477.730509116, 524477.730509116, 132739.1418206584]
[2019-04-27 17:54:13,512] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 17:54:13,516] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.3365198e-16 1.0000000e+00 4.5304972e-19 7.1676040e-20 5.5201642e-24], sampled 0.7551511994176656
[2019-04-27 17:54:30,127] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0065635], dtype=float32), 0.01904461]
[2019-04-27 17:54:30,129] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [23.55, 97.0, 1.0, 2.0, 0.5920586512344038, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 684329.7000618353, 684329.7000618353, 157180.8904051606]
[2019-04-27 17:54:30,130] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 17:54:30,133] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.4960541e-17 1.0000000e+00 3.8628436e-20 6.1199782e-21 2.7696931e-25], sampled 0.08884890578250682
[2019-04-27 17:55:01,727] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8771.9095 2170601483.0346 493.0000
[2019-04-27 17:55:02,099] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8700.8855 2195072372.2615 572.0000
[2019-04-27 17:55:02,152] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8583.7006 2248699398.1120 553.0000
[2019-04-27 17:55:02,171] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8924.2933 2120398140.2949 430.0000
[2019-04-27 17:55:02,252] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8099.9424 2445324199.3218 746.0000
[2019-04-27 17:55:03,266] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 25000, evaluation results [25000.0, 8099.942407935641, 2445324199.321786, 746.0, 8771.90946536216, 2170601483.03465, 493.0, 8924.293283520115, 2120398140.2948759, 430.0, 8583.700649532175, 2248699398.111969, 553.0, 8700.885519567202, 2195072372.261538, 572.0]
[2019-04-27 17:55:03,312] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.6009583e-18 1.0000000e+00 3.7385712e-21 1.1249709e-20 1.5350878e-25], sum to 1.0000
[2019-04-27 17:55:03,320] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6328
[2019-04-27 17:55:03,448] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 51.83333333333334, 1.0, 2.0, 0.3424459142590393, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 432593.5612691618, 432593.5612691618, 120948.7085488188], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 940200.0000, 
sim time next is 940800.0000, 
raw observation next is [24.5, 51.66666666666667, 1.0, 2.0, 0.3383808494579916, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 428010.3358941993, 428010.3358941993, 120423.4811291616], 
processed observation next is [0.0, 0.9130434782608695, 0.46296296296296297, 0.5166666666666667, 1.0, 1.0, 0.21235815411665665, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.15286083424792832, 0.15286083424792832, 0.23158361755608], 
reward next is 0.7684, 
noisyNet noise sample is [array([-0.6027329], dtype=float32), -1.8472902]. 
=============================================
[2019-04-27 17:55:05,221] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.1381166e-16 1.0000000e+00 1.0941894e-18 2.5565193e-18 1.8300847e-23], sum to 1.0000
[2019-04-27 17:55:05,229] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1492
[2019-04-27 17:55:05,237] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.23333333333333, 59.83333333333333, 1.0, 2.0, 0.3709213540838068, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 477056.3607776678, 477056.3607776678, 124782.7736748369], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 972600.0000, 
sim time next is 973200.0000, 
raw observation next is [21.36666666666667, 59.66666666666667, 1.0, 2.0, 0.3296747860436336, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 423602.8250593706, 423602.8250593701, 119318.3496032123], 
processed observation next is [1.0, 0.2608695652173913, 0.3469135802469137, 0.5966666666666667, 1.0, 1.0, 0.20199379290908762, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1512867232354895, 0.1512867232354893, 0.2294583646215621], 
reward next is 0.7705, 
noisyNet noise sample is [array([-2.0662823], dtype=float32), -0.7306857]. 
=============================================
[2019-04-27 17:55:07,910] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.7693658e-08 1.0000000e+00 6.9616268e-10 2.2081654e-11 6.3154896e-13], sum to 1.0000
[2019-04-27 17:55:07,921] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3738
[2019-04-27 17:55:08,037] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.93333333333334, 45.33333333333333, 1.0, 2.0, 0.2846802675432002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 365840.4827311359, 365840.4827311363, 113700.6534565991], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1024800.0000, 
sim time next is 1025400.0000, 
raw observation next is [23.86666666666667, 45.66666666666667, 1.0, 2.0, 0.283599328486754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 364448.2163075392, 364448.2163075387, 113570.1640885162], 
processed observation next is [1.0, 0.8695652173913043, 0.4395061728395063, 0.4566666666666667, 1.0, 1.0, 0.1471420577223262, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.13016007725269257, 0.13016007725269238, 0.218404161708685], 
reward next is 0.7816, 
noisyNet noise sample is [array([-0.12090956], dtype=float32), 1.1890136]. 
=============================================
[2019-04-27 17:55:12,941] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.3925791e-18 1.0000000e+00 1.4926308e-21 1.4290947e-21 3.7022034e-25], sum to 1.0000
[2019-04-27 17:55:12,953] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7930
[2019-04-27 17:55:12,959] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 67.0, 1.0, 2.0, 0.3296954799109905, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 417457.0829316771, 417457.0829316771, 119302.6114286103], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1108800.0000, 
sim time next is 1109400.0000, 
raw observation next is [21.66666666666667, 67.33333333333334, 1.0, 2.0, 0.3263826580007151, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 413571.8897500925, 413571.8897500925, 118879.7606055034], 
processed observation next is [1.0, 0.8695652173913043, 0.3580246913580249, 0.6733333333333335, 1.0, 1.0, 0.19807459285799414, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.14770424633931875, 0.14770424633931875, 0.22861492424135268], 
reward next is 0.7714, 
noisyNet noise sample is [array([-0.45254168], dtype=float32), 0.6750417]. 
=============================================
[2019-04-27 17:55:17,649] A3C_AGENT_WORKER-Thread-22 INFO:Local step 2000, global step 31816: loss 0.0791
[2019-04-27 17:55:17,651] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 2000, global step 31817: learning rate 0.0001
[2019-04-27 17:55:17,875] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 31925: loss 0.0391
[2019-04-27 17:55:17,879] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 31926: learning rate 0.0001
[2019-04-27 17:55:17,885] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2000, global step 31927: loss 0.0541
[2019-04-27 17:55:17,888] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2000, global step 31929: learning rate 0.0001
[2019-04-27 17:55:17,918] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2000, global step 31941: loss 0.1049
[2019-04-27 17:55:17,922] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2000, global step 31943: learning rate 0.0001
[2019-04-27 17:55:17,939] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2000, global step 31954: loss 0.2092
[2019-04-27 17:55:17,940] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2000, global step 31955: learning rate 0.0001
[2019-04-27 17:55:17,961] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2000, global step 31965: loss 0.1941
[2019-04-27 17:55:17,964] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2000, global step 31965: learning rate 0.0001
[2019-04-27 17:55:17,969] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2000, global step 31968: loss 0.3023
[2019-04-27 17:55:17,970] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2000, global step 31968: learning rate 0.0001
[2019-04-27 17:55:17,974] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2000, global step 31969: loss 0.2354
[2019-04-27 17:55:17,976] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2000, global step 31971: learning rate 0.0001
[2019-04-27 17:55:17,984] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 31973: loss 0.3207
[2019-04-27 17:55:17,989] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 31974: learning rate 0.0001
[2019-04-27 17:55:18,045] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2000, global step 31999: loss 0.1955
[2019-04-27 17:55:18,050] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2000, global step 32000: learning rate 0.0001
[2019-04-27 17:55:18,068] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 32009: loss 0.2019
[2019-04-27 17:55:18,071] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 32009: learning rate 0.0001
[2019-04-27 17:55:18,081] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 32018: loss 0.1348
[2019-04-27 17:55:18,084] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 32019: learning rate 0.0001
[2019-04-27 17:55:18,100] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 32027: loss 0.0811
[2019-04-27 17:55:18,102] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 32027: learning rate 0.0001
[2019-04-27 17:55:18,188] A3C_AGENT_WORKER-Thread-21 INFO:Local step 2000, global step 32067: loss 0.0191
[2019-04-27 17:55:18,192] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 2000, global step 32069: learning rate 0.0001
[2019-04-27 17:55:18,279] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2000, global step 32113: loss 0.0240
[2019-04-27 17:55:18,280] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2000, global step 32113: learning rate 0.0001
[2019-04-27 17:55:18,328] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.4997869e-20 1.0000000e+00 9.4950375e-25 1.0292425e-24 2.1894501e-28], sum to 1.0000
[2019-04-27 17:55:18,337] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5558
[2019-04-27 17:55:18,344] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2000, global step 32141: loss 0.0566
[2019-04-27 17:55:18,345] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.45, 94.0, 1.0, 2.0, 0.3370533766749014, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 426047.5908334068, 426047.5908334068, 120247.6722039335], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1211400.0000, 
sim time next is 1212000.0000, 
raw observation next is [18.4, 94.0, 1.0, 2.0, 0.3350839064631392, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 423803.88257905, 423803.88257905, 119994.4335267545], 
processed observation next is [1.0, 0.0, 0.237037037037037, 0.94, 1.0, 1.0, 0.2084332219799276, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.15135852949251788, 0.15135852949251788, 0.2307585260129894], 
reward next is 0.7692, 
noisyNet noise sample is [array([-0.5567462], dtype=float32), -0.72920465]. 
=============================================
[2019-04-27 17:55:18,346] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2000, global step 32141: learning rate 0.0001
[2019-04-27 17:55:18,360] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[92.82132 ]
 [93.04334 ]
 [93.448204]
 [93.80037 ]
 [94.41695 ]], R is [[92.3039093 ]
 [92.14962769]
 [91.99643707]
 [91.8443985 ]
 [91.69355011]].
[2019-04-27 17:55:19,464] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.2581993e-14 1.0000000e+00 1.6142175e-17 1.4604896e-17 1.5587834e-21], sum to 1.0000
[2019-04-27 17:55:19,474] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5307
[2019-04-27 17:55:19,594] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.86666666666667, 93.0, 1.0, 2.0, 0.3064748978975573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 390432.0411450056, 390432.0411450056, 116376.8982450002], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1225200.0000, 
sim time next is 1225800.0000, 
raw observation next is [17.85, 93.0, 1.0, 2.0, 0.3051760254477649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 388848.9943779768, 388848.9943779773, 116215.3801098533], 
processed observation next is [1.0, 0.17391304347826086, 0.21666666666666673, 0.93, 1.0, 1.0, 0.17282860172352965, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.13887464084927742, 0.1388746408492776, 0.22349111559587173], 
reward next is 0.7765, 
noisyNet noise sample is [array([0.46602032], dtype=float32), 0.7329138]. 
=============================================
[2019-04-27 17:55:21,765] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.9234896e-15 1.0000000e+00 2.8626104e-17 1.7535948e-16 1.5426872e-21], sum to 1.0000
[2019-04-27 17:55:21,771] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6738
[2019-04-27 17:55:21,887] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.33333333333334, 54.66666666666667, 1.0, 2.0, 0.7956646103705036, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260425127729, 975577.6562586419, 975577.6562586419, 197987.1383850665], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1267800.0000, 
sim time next is 1268400.0000, 
raw observation next is [26.36666666666667, 54.33333333333334, 1.0, 2.0, 0.8967673515456569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.94963667106222, 6.9112, 121.9259060753789, 1123556.100475961, 1103873.111503675, 220331.3474297454], 
processed observation next is [1.0, 0.6956521739130435, 0.5320987654320989, 0.5433333333333334, 1.0, 1.0, 0.8771039899353058, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.003843667106221993, 0.0, 0.8094612223346495, 0.40127003588427185, 0.39424039696559826, 0.4237141296725873], 
reward next is 0.3841, 
noisyNet noise sample is [array([0.12180447], dtype=float32), -0.96818495]. 
=============================================
[2019-04-27 17:55:23,939] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.0812724e-14 1.0000000e+00 2.5201904e-17 2.2410859e-16 4.6921352e-21], sum to 1.0000
[2019-04-27 17:55:23,948] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0959
[2019-04-27 17:55:23,954] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.76666666666667, 86.0, 1.0, 2.0, 0.3507572022679769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 441391.2680962301, 441391.2680962301, 122022.5261088725], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1300800.0000, 
sim time next is 1301400.0000, 
raw observation next is [19.7, 86.0, 1.0, 2.0, 0.347122980868833, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 437165.7858235738, 437165.7858235738, 121547.057884526], 
processed observation next is [1.0, 0.043478260869565216, 0.28518518518518515, 0.86, 1.0, 1.0, 0.22276545341527734, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1561306377941335, 0.1561306377941335, 0.23374434208562692], 
reward next is 0.7663, 
noisyNet noise sample is [array([0.00054274], dtype=float32), -1.6278399]. 
=============================================
[2019-04-27 17:55:25,772] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.4253965e-11 1.0000000e+00 1.8809704e-12 3.0441486e-13 1.4984480e-16], sum to 1.0000
[2019-04-27 17:55:25,782] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5865
[2019-04-27 17:55:25,788] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 48.0, 1.0, 2.0, 0.7382086456913569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 922078.287030925, 922078.287030925, 186598.3964357549], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1333800.0000, 
sim time next is 1334400.0000, 
raw observation next is [26.66666666666667, 46.33333333333333, 1.0, 2.0, 0.7639077486601982, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 955065.4809056662, 955065.4809056658, 191847.9429434658], 
processed observation next is [1.0, 0.43478260869565216, 0.5432098765432101, 0.46333333333333326, 1.0, 1.0, 0.7189377960240454, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.3410948146091665, 0.34109481460916635, 0.3689383518143573], 
reward next is 0.6311, 
noisyNet noise sample is [array([-0.6208421], dtype=float32), 1.6417373]. 
=============================================
[2019-04-27 17:55:25,811] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.9563587e-12 1.0000000e+00 5.4361646e-14 7.9510441e-15 5.7880388e-17], sum to 1.0000
[2019-04-27 17:55:25,822] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1859
[2019-04-27 17:55:25,835] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1328770.782633936 W.
[2019-04-27 17:55:25,839] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.2, 40.5, 1.0, 2.0, 0.9322960258421175, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.243794790272164, 6.9112, 121.9246271771142, 1328770.782633936, 1158454.500253004, 228870.8033698536], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1338600.0000, 
sim time next is 1339200.0000, 
raw observation next is [28.4, 40.0, 1.0, 2.0, 0.3408353455406806, 1.0, 1.0, 0.3408353455406806, 1.0, 1.0, 0.5556298572679268, 6.9112, 6.9112, 121.94756008, 1244889.850464122, 1244889.850464122, 271057.6259859356], 
processed observation next is [1.0, 0.5217391304347826, 0.6074074074074074, 0.4, 1.0, 1.0, 0.21528017326271504, 1.0, 0.5, 0.21528017326271504, 1.0, 0.5, 0.44453732158490844, 0.0, 0.0, 0.8096049824067558, 0.44460351802290066, 0.44460351802290066, 0.5212646653575685], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5014345], dtype=float32), -0.24387015]. 
=============================================
[2019-04-27 17:55:26,212] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.5553643e-12 1.0000000e+00 6.2259639e-14 1.5178256e-14 8.2815900e-18], sum to 1.0000
[2019-04-27 17:55:26,219] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1576
[2019-04-27 17:55:26,225] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1336011.400667408 W.
[2019-04-27 17:55:26,231] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.7, 35.5, 1.0, 2.0, 0.9310964865450887, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.25417438090506, 6.9112, 121.9248242813817, 1336011.400667408, 1160379.617942975, 228672.6210176687], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1343400.0000, 
sim time next is 1344000.0000, 
raw observation next is [29.9, 35.0, 1.0, 2.0, 0.5668363112195, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9322337963438273, 6.911200000000001, 6.9112, 121.925836082506, 1394092.019479352, 1394092.019479352, 277581.039313159], 
processed observation next is [1.0, 0.5652173913043478, 0.6629629629629629, 0.35, 1.0, 1.0, 0.4843289419279761, 0.0, 1.0, -0.1904761904761905, 1.0, 0.5, 0.915292245429784, 8.881784197001253e-17, 0.0, 0.8094607576547608, 0.49789000695691144, 0.49789000695691144, 0.5338096909868443], 
reward next is 0.4662, 
noisyNet noise sample is [array([-0.5181986], dtype=float32), 1.9744663]. 
=============================================
[2019-04-27 17:55:26,248] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[52.841232]
 [52.71987 ]
 [52.63228 ]
 [52.055424]
 [51.817688]], R is [[52.07320786]
 [51.55247498]
 [51.66447067]
 [51.78902054]
 [51.91618347]].
[2019-04-27 17:55:29,807] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.3141550e-14 1.0000000e+00 7.8962661e-17 4.2725541e-18 1.4904808e-20], sum to 1.0000
[2019-04-27 17:55:29,814] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6403
[2019-04-27 17:55:29,928] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.33333333333334, 52.00000000000001, 1.0, 2.0, 0.2884618891004749, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 368960.6981618685, 368960.6981618685, 114163.2259482839], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1405200.0000, 
sim time next is 1405800.0000, 
raw observation next is [23.7, 51.0, 1.0, 2.0, 0.2927123233612687, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 373828.4581916349, 373828.4581916344, 114680.6400026384], 
processed observation next is [0.0, 0.2608695652173913, 0.4333333333333333, 0.51, 1.0, 1.0, 0.15799086114436747, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.13351016363986962, 0.13351016363986942, 0.22053969231276616], 
reward next is 0.7795, 
noisyNet noise sample is [array([1.3372103], dtype=float32), 1.7440852]. 
=============================================
[2019-04-27 17:55:30,187] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.7789318e-14 1.0000000e+00 3.4245542e-17 1.3755501e-18 2.0253657e-21], sum to 1.0000
[2019-04-27 17:55:30,196] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8084
[2019-04-27 17:55:30,208] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 35.0, 1.0, 2.0, 0.3435660408223578, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 432083.2274576823, 432083.2274576823, 121070.9620652359], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1414800.0000, 
sim time next is 1415400.0000, 
raw observation next is [29.16666666666667, 34.33333333333334, 1.0, 2.0, 0.3431121560900645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 431581.6773342358, 431581.6773342353, 121012.4289902141], 
processed observation next is [0.0, 0.391304347826087, 0.6358024691358026, 0.34333333333333343, 1.0, 1.0, 0.21799066201198156, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.15413631333365566, 0.15413631333365546, 0.23271620959656558], 
reward next is 0.7673, 
noisyNet noise sample is [array([-0.7629009], dtype=float32), -0.38306245]. 
=============================================
[2019-04-27 17:55:34,741] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2500, global step 39873: loss 0.0412
[2019-04-27 17:55:34,743] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2500, global step 39873: learning rate 0.0001
[2019-04-27 17:55:34,764] A3C_AGENT_WORKER-Thread-22 INFO:Local step 2500, global step 39883: loss 0.0045
[2019-04-27 17:55:34,770] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 2500, global step 39883: learning rate 0.0001
[2019-04-27 17:55:34,823] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 39910: loss 0.0002
[2019-04-27 17:55:34,825] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 39910: learning rate 0.0001
[2019-04-27 17:55:34,851] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2500, global step 39922: loss 0.0002
[2019-04-27 17:55:34,854] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2500, global step 39924: learning rate 0.0001
[2019-04-27 17:55:34,899] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2500, global step 39942: loss 0.0005
[2019-04-27 17:55:34,904] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2500, global step 39942: learning rate 0.0001
[2019-04-27 17:55:34,937] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2500, global step 39964: loss 0.0629
[2019-04-27 17:55:34,940] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2500, global step 39964: learning rate 0.0001
[2019-04-27 17:55:34,960] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2500, global step 39974: loss 0.0203
[2019-04-27 17:55:34,964] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2500, global step 39976: learning rate 0.0001
[2019-04-27 17:55:34,978] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2500, global step 39980: loss 0.0408
[2019-04-27 17:55:34,984] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2500, global step 39983: learning rate 0.0001
[2019-04-27 17:55:34,984] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 39984: loss 0.0397
[2019-04-27 17:55:34,990] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 39988: learning rate 0.0001
[2019-04-27 17:55:34,996] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2500, global step 39990: loss 0.0695
[2019-04-27 17:55:34,997] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2500, global step 39990: learning rate 0.0001
[2019-04-27 17:55:35,027] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 40000: loss 0.0143
[2019-04-27 17:55:35,028] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 40000: learning rate 0.0001
[2019-04-27 17:55:35,045] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 40010: loss 0.0205
[2019-04-27 17:55:35,047] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 40010: learning rate 0.0001
[2019-04-27 17:55:35,092] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 40031: loss 0.0060
[2019-04-27 17:55:35,093] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 40031: learning rate 0.0001
[2019-04-27 17:55:35,147] A3C_AGENT_WORKER-Thread-21 INFO:Local step 2500, global step 40057: loss 0.0082
[2019-04-27 17:55:35,150] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 2500, global step 40057: learning rate 0.0001
[2019-04-27 17:55:35,289] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2500, global step 40122: loss 0.0943
[2019-04-27 17:55:35,292] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2500, global step 40123: learning rate 0.0001
[2019-04-27 17:55:35,380] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2500, global step 40164: loss 0.0052
[2019-04-27 17:55:35,382] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2500, global step 40165: learning rate 0.0001
[2019-04-27 17:55:36,256] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.3166646e-13 1.0000000e+00 4.6802774e-16 1.9773733e-17 6.2063785e-21], sum to 1.0000
[2019-04-27 17:55:36,262] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4657
[2019-04-27 17:55:36,267] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.96666666666667, 25.83333333333333, 1.0, 2.0, 0.4153908341900356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 507391.4184692099, 507391.4184692099, 130622.4885984796], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1522200.0000, 
sim time next is 1522800.0000, 
raw observation next is [34.8, 27.0, 1.0, 2.0, 0.422723749255348, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 514721.5200659206, 514721.5200659206, 131629.8186525689], 
processed observation next is [0.0, 0.6521739130434783, 0.8444444444444443, 0.27, 1.0, 1.0, 0.3127663681611286, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.18382911430925736, 0.18382911430925736, 0.25313426663955557], 
reward next is 0.7469, 
noisyNet noise sample is [array([-1.6316711], dtype=float32), -1.0419257]. 
=============================================
[2019-04-27 17:55:36,950] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.5128368e-15 1.0000000e+00 1.9331902e-16 5.9840591e-18 3.8330691e-23], sum to 1.0000
[2019-04-27 17:55:36,958] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0947
[2019-04-27 17:55:36,964] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 69.0, 1.0, 2.0, 0.5081287035775254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 608206.1064550175, 608206.106455017, 144256.5027971114], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1538400.0000, 
sim time next is 1539000.0000, 
raw observation next is [25.2, 71.5, 1.0, 2.0, 0.5048353941747977, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 605144.8280509411, 605144.8280509406, 143766.1228044478], 
processed observation next is [0.0, 0.8260869565217391, 0.4888888888888889, 0.715, 1.0, 1.0, 0.4105183263985687, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.21612315287533612, 0.21612315287533596, 0.27647331308547657], 
reward next is 0.7235, 
noisyNet noise sample is [array([-1.5847812], dtype=float32), -1.0054975]. 
=============================================
[2019-04-27 17:55:36,983] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[76.27863 ]
 [76.16209 ]
 [76.0353  ]
 [75.87947 ]
 [75.757225]], R is [[76.35002899]
 [76.30911255]
 [76.26791382]
 [76.22650146]
 [76.18444824]].
[2019-04-27 17:55:46,547] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.6536967e-12 1.0000000e+00 5.2413053e-11 1.1662227e-12 2.7687537e-17], sum to 1.0000
[2019-04-27 17:55:46,554] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2541
[2019-04-27 17:55:46,558] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 77.66666666666667, 1.0, 2.0, 0.4357198472843529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 532347.2901269754, 532347.2901269749, 133572.8381295796], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1712400.0000, 
sim time next is 1713000.0000, 
raw observation next is [23.05, 78.33333333333334, 1.0, 2.0, 0.4385905490575081, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 535488.0416661269, 535488.0416661269, 133984.4678966954], 
processed observation next is [1.0, 0.8260869565217391, 0.40925925925925927, 0.7833333333333334, 1.0, 1.0, 0.33165541554465255, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1912457291664739, 0.1912457291664739, 0.25766243826287577], 
reward next is 0.7423, 
noisyNet noise sample is [array([-0.12632725], dtype=float32), 0.34361115]. 
=============================================
[2019-04-27 17:55:46,569] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[72.575905]
 [72.43885 ]
 [72.95369 ]
 [73.11492 ]
 [73.51327 ]], R is [[72.30567169]
 [72.32574463]
 [72.34590912]
 [72.36643982]
 [72.38768768]].
[2019-04-27 17:55:51,393] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3000, global step 47834: loss 3.7785
[2019-04-27 17:55:51,397] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3000, global step 47835: learning rate 0.0001
[2019-04-27 17:55:51,532] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3000, global step 47898: loss 5.6814
[2019-04-27 17:55:51,536] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3000, global step 47901: loss 0.6173
[2019-04-27 17:55:51,537] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3000, global step 47899: learning rate 0.0001
[2019-04-27 17:55:51,541] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3000, global step 47902: learning rate 0.0001
[2019-04-27 17:55:51,595] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3000, global step 47927: loss 1.6151
[2019-04-27 17:55:51,596] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3000, global step 47927: learning rate 0.0001
[2019-04-27 17:55:51,613] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3000, global step 47936: loss 1.7357
[2019-04-27 17:55:51,614] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3000, global step 47937: learning rate 0.0001
[2019-04-27 17:55:51,616] A3C_AGENT_WORKER-Thread-22 INFO:Local step 3000, global step 47937: loss 0.1268
[2019-04-27 17:55:51,617] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 3000, global step 47937: learning rate 0.0001
[2019-04-27 17:55:51,686] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3000, global step 47969: loss 1.3246
[2019-04-27 17:55:51,689] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3000, global step 47970: learning rate 0.0001
[2019-04-27 17:55:51,702] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3000, global step 47974: loss 1.7426
[2019-04-27 17:55:51,705] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3000, global step 47974: learning rate 0.0001
[2019-04-27 17:55:51,728] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3000, global step 47988: loss 1.3133
[2019-04-27 17:55:51,730] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3000, global step 47989: learning rate 0.0001
[2019-04-27 17:55:51,759] A3C_AGENT_WORKER-Thread-21 INFO:Local step 3000, global step 48004: loss 0.0683
[2019-04-27 17:55:51,762] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 3000, global step 48005: learning rate 0.0001
[2019-04-27 17:55:51,777] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3000, global step 48011: loss 0.3905
[2019-04-27 17:55:51,778] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3000, global step 48012: loss 1.2640
[2019-04-27 17:55:51,779] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3000, global step 48012: learning rate 0.0001
[2019-04-27 17:55:51,781] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3000, global step 48014: learning rate 0.0001
[2019-04-27 17:55:51,818] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3000, global step 48035: loss 0.3765
[2019-04-27 17:55:51,819] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3000, global step 48035: learning rate 0.0001
[2019-04-27 17:55:51,823] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3000, global step 48037: loss 0.0367
[2019-04-27 17:55:51,825] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3000, global step 48037: learning rate 0.0001
[2019-04-27 17:55:52,095] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3000, global step 48171: loss 0.0495
[2019-04-27 17:55:52,096] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3000, global step 48171: learning rate 0.0001
[2019-04-27 17:55:52,157] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3000, global step 48197: loss 0.0473
[2019-04-27 17:55:52,159] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3000, global step 48197: learning rate 0.0001
[2019-04-27 17:55:55,915] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-27 17:55:55,916] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 17:55:55,917] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 17:55:55,917] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:55:55,918] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:55:55,919] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 17:55:55,921] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 17:55:55,925] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:55:55,925] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 17:55:55,927] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:55:55,929] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:55:55,944] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run3
[2019-04-27 17:55:55,964] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run3
[2019-04-27 17:55:55,986] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run3
[2019-04-27 17:55:55,987] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run3
[2019-04-27 17:55:56,026] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run3
[2019-04-27 17:56:00,505] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00962554], dtype=float32), 0.026337791]
[2019-04-27 17:56:00,506] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [21.08333333333333, 40.0, 1.0, 2.0, 0.2391617906949852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 308495.3215080231, 308495.3215080231, 85945.01200074836]
[2019-04-27 17:56:00,508] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 17:56:00,510] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [5.1986254e-10 1.0000000e+00 2.1130853e-09 1.8607709e-11 3.0718521e-14], sampled 0.7107903695065004
[2019-04-27 17:56:17,255] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00962554], dtype=float32), 0.026337791]
[2019-04-27 17:56:17,256] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [33.33333333333334, 31.16666666666667, 1.0, 2.0, 0.4233856809730401, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 515046.3057843704, 515046.3057843704, 131710.8601218332]
[2019-04-27 17:56:17,259] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 17:56:17,261] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [8.7397839e-10 1.0000000e+00 3.5239069e-09 3.2478086e-11 5.6784682e-14], sampled 0.4667161670430593
[2019-04-27 17:56:30,103] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00962554], dtype=float32), 0.026337791]
[2019-04-27 17:56:30,104] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [26.58333333333333, 68.33333333333334, 1.0, 2.0, 0.5612948315855556, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 663425.6755748364, 663425.6755748364, 152621.5296164386]
[2019-04-27 17:56:30,105] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 17:56:30,106] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [7.0783268e-10 1.0000000e+00 3.4371976e-09 3.0231938e-11 4.5215825e-14], sampled 0.21458933757789878
[2019-04-27 17:57:35,798] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00962554], dtype=float32), 0.026337791]
[2019-04-27 17:57:35,798] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [20.23870855, 79.27816488500001, 1.0, 2.0, 0.3005544654605629, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 380245.667809858, 380245.667809858, 115622.1469421398]
[2019-04-27 17:57:35,799] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 17:57:35,802] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [5.5544125e-10 1.0000000e+00 2.2016617e-09 1.9604986e-11 3.0472085e-14], sampled 0.39516682314546236
[2019-04-27 17:57:39,187] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8771.9095 2170601483.0346 493.0000
[2019-04-27 17:57:39,243] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8583.7006 2248699398.1120 553.0000
[2019-04-27 17:57:39,260] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8701.7435 2195019782.1641 572.0000
[2019-04-27 17:57:39,269] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8924.2933 2120398140.2949 430.0000
[2019-04-27 17:57:39,358] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8099.2011 2445377682.4312 746.0000
[2019-04-27 17:57:40,374] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 50000, evaluation results [50000.0, 8099.201055540698, 2445377682.4311814, 746.0, 8771.90946536216, 2170601483.03465, 493.0, 8924.293283520115, 2120398140.2948759, 430.0, 8583.700649532175, 2248699398.111969, 553.0, 8701.74352259033, 2195019782.1641326, 572.0]
[2019-04-27 17:57:52,500] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3500, global step 55859: loss 0.0031
[2019-04-27 17:57:52,505] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3500, global step 55860: learning rate 0.0001
[2019-04-27 17:57:52,506] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3500, global step 55861: loss 0.0034
[2019-04-27 17:57:52,507] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3500, global step 55861: learning rate 0.0001
[2019-04-27 17:57:52,558] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3500, global step 55884: loss 0.0117
[2019-04-27 17:57:52,560] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3500, global step 55884: learning rate 0.0001
[2019-04-27 17:57:52,587] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3500, global step 55899: loss 0.0484
[2019-04-27 17:57:52,590] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3500, global step 55900: learning rate 0.0001
[2019-04-27 17:57:52,630] A3C_AGENT_WORKER-Thread-22 INFO:Local step 3500, global step 55919: loss 0.0556
[2019-04-27 17:57:52,631] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 3500, global step 55919: learning rate 0.0001
[2019-04-27 17:57:52,679] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3500, global step 55944: loss 0.1355
[2019-04-27 17:57:52,682] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3500, global step 55945: learning rate 0.0001
[2019-04-27 17:57:52,735] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3500, global step 55972: loss 0.1240
[2019-04-27 17:57:52,738] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3500, global step 55972: learning rate 0.0001
[2019-04-27 17:57:52,760] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3500, global step 55985: loss 0.0431
[2019-04-27 17:57:52,761] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3500, global step 55985: learning rate 0.0001
[2019-04-27 17:57:52,765] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3500, global step 55985: loss 0.1005
[2019-04-27 17:57:52,768] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3500, global step 55985: learning rate 0.0001
[2019-04-27 17:57:52,778] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3500, global step 55990: loss 0.0627
[2019-04-27 17:57:52,779] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3500, global step 55990: loss 0.0367
[2019-04-27 17:57:52,780] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3500, global step 55990: learning rate 0.0001
[2019-04-27 17:57:52,782] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3500, global step 55991: learning rate 0.0001
[2019-04-27 17:57:52,828] A3C_AGENT_WORKER-Thread-21 INFO:Local step 3500, global step 56019: loss 0.0001
[2019-04-27 17:57:52,830] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 3500, global step 56019: learning rate 0.0001
[2019-04-27 17:57:52,858] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3500, global step 56033: loss 0.0109
[2019-04-27 17:57:52,860] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3500, global step 56035: learning rate 0.0001
[2019-04-27 17:57:52,869] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3500, global step 56037: loss 0.0002
[2019-04-27 17:57:52,871] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3500, global step 56037: learning rate 0.0001
[2019-04-27 17:57:53,180] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3500, global step 56188: loss 0.0009
[2019-04-27 17:57:53,182] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3500, global step 56188: learning rate 0.0001
[2019-04-27 17:57:53,288] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3500, global step 56235: loss 0.0213
[2019-04-27 17:57:53,290] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3500, global step 56236: learning rate 0.0001
[2019-04-27 17:57:56,616] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.26750665e-11 9.99984622e-01 1.53917572e-05 1.15784354e-10
 3.34546057e-13], sum to 1.0000
[2019-04-27 17:57:56,630] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5762
[2019-04-27 17:57:56,753] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.26666666666667, 89.0, 1.0, 2.0, 0.9026244953062114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 122.3761217902864, 1050041.336084004, 1050041.336084004, 219242.8654959249], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2167800.0000, 
sim time next is 2168400.0000, 
raw observation next is [24.23333333333333, 89.0, 1.0, 2.0, 0.7623787226478497, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 887235.4145293842, 887235.4145293842, 189204.8082051947], 
processed observation next is [1.0, 0.08695652173913043, 0.45308641975308633, 0.89, 1.0, 1.0, 0.7171175269617258, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3168697909033515, 0.3168697909033515, 0.36385540039460523], 
reward next is 0.6361, 
noisyNet noise sample is [array([0.7534646], dtype=float32), -1.8270981]. 
=============================================
[2019-04-27 17:57:57,605] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0272385e-11 9.9999952e-01 4.8738076e-07 4.6209493e-13 7.8891232e-14], sum to 1.0000
[2019-04-27 17:57:57,617] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0190
[2019-04-27 17:57:57,621] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.06666666666667, 89.0, 1.0, 2.0, 0.600877259537118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 702135.375749168, 702135.375749168, 159044.5051388009], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2182800.0000, 
sim time next is 2183400.0000, 
raw observation next is [24.1, 89.0, 1.0, 2.0, 0.5851769999182485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 683388.0095935089, 683388.0095935084, 156321.5314949953], 
processed observation next is [1.0, 0.2608695652173913, 0.4481481481481482, 0.89, 1.0, 1.0, 0.506163095140772, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.24406714628339604, 0.24406714628339587, 0.30061832979806785], 
reward next is 0.6994, 
noisyNet noise sample is [array([0.6106589], dtype=float32), 1.2010388]. 
=============================================
[2019-04-27 17:58:04,408] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.5448822e-05 9.4256711e-01 5.7221182e-02 1.6994446e-06 1.6465757e-04], sum to 1.0000
[2019-04-27 17:58:04,417] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1088
[2019-04-27 17:58:04,421] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.71666666666667, 69.5, 1.0, 2.0, 0.4951069650557366, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 591961.3499538577, 591961.3499538577, 142181.0757166152], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2310600.0000, 
sim time next is 2311200.0000, 
raw observation next is [25.6, 70.0, 1.0, 2.0, 0.4982982303517338, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 596105.1368709797, 596105.1368709797, 142692.8257425904], 
processed observation next is [1.0, 0.782608695652174, 0.5037037037037038, 0.7, 1.0, 1.0, 0.4027359885139688, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2128946917396356, 0.2128946917396356, 0.2744092802742123], 
reward next is 0.7256, 
noisyNet noise sample is [array([0.91008985], dtype=float32), 0.47932068]. 
=============================================
[2019-04-27 17:58:09,138] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4000, global step 63799: loss 0.0696
[2019-04-27 17:58:09,138] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4000, global step 63799: learning rate 0.0001
[2019-04-27 17:58:09,274] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4000, global step 63863: loss 2.3831
[2019-04-27 17:58:09,276] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4000, global step 63863: learning rate 0.0001
[2019-04-27 17:58:09,302] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4000, global step 63874: loss 0.7438
[2019-04-27 17:58:09,305] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4000, global step 63875: learning rate 0.0001
[2019-04-27 17:58:09,386] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4000, global step 63918: loss 0.9650
[2019-04-27 17:58:09,387] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4000, global step 63919: learning rate 0.0001
[2019-04-27 17:58:09,431] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4000, global step 63939: loss 3.2951
[2019-04-27 17:58:09,435] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4000, global step 63939: learning rate 0.0001
[2019-04-27 17:58:09,449] A3C_AGENT_WORKER-Thread-22 INFO:Local step 4000, global step 63947: loss 0.7685
[2019-04-27 17:58:09,454] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 4000, global step 63949: learning rate 0.0001
[2019-04-27 17:58:09,455] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4000, global step 63949: loss 0.5034
[2019-04-27 17:58:09,458] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4000, global step 63950: learning rate 0.0001
[2019-04-27 17:58:09,518] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4000, global step 63978: loss 2.2307
[2019-04-27 17:58:09,525] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4000, global step 63979: learning rate 0.0001
[2019-04-27 17:58:09,558] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4000, global step 63996: loss 2.7719
[2019-04-27 17:58:09,561] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4000, global step 63997: learning rate 0.0001
[2019-04-27 17:58:09,577] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4000, global step 64006: loss 1.5443
[2019-04-27 17:58:09,579] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4000, global step 64006: learning rate 0.0001
[2019-04-27 17:58:09,584] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4000, global step 64010: loss 1.7184
[2019-04-27 17:58:09,586] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4000, global step 64010: learning rate 0.0001
[2019-04-27 17:58:09,597] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4000, global step 64017: loss 1.7991
[2019-04-27 17:58:09,599] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4000, global step 64017: learning rate 0.0001
[2019-04-27 17:58:09,719] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4000, global step 64071: loss 0.3130
[2019-04-27 17:58:09,722] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4000, global step 64071: learning rate 0.0001
[2019-04-27 17:58:09,741] A3C_AGENT_WORKER-Thread-21 INFO:Local step 4000, global step 64081: loss 1.0942
[2019-04-27 17:58:09,746] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 4000, global step 64083: learning rate 0.0001
[2019-04-27 17:58:09,834] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4000, global step 64123: loss 0.4327
[2019-04-27 17:58:09,836] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4000, global step 64123: learning rate 0.0001
[2019-04-27 17:58:10,038] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4000, global step 64218: loss 0.4290
[2019-04-27 17:58:10,042] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4000, global step 64218: learning rate 0.0001
[2019-04-27 17:58:10,570] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8871570e-14 1.1177142e-05 9.9998879e-01 8.2624828e-15 2.8854089e-10], sum to 1.0000
[2019-04-27 17:58:10,578] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6409
[2019-04-27 17:58:10,695] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.78333333333333, 58.16666666666666, 1.0, 2.0, 0.1919209878864529, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3183192953959583, 6.911199999999999, 6.9112, 121.9260426156618, 475410.2209468252, 475410.2209468256, 165699.5867085247], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2412600.0000, 
sim time next is 2413200.0000, 
raw observation next is [24.56666666666667, 59.33333333333334, 1.0, 2.0, 0.1915951127474858, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3178173549124612, 6.911199999999999, 6.9112, 121.9260426156618, 474651.1184127104, 474651.1184127109, 165620.2745857769], 
processed observation next is [1.0, 0.9565217391304348, 0.46543209876543223, 0.5933333333333334, 1.0, 1.0, 0.03761322946129263, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.1472716936405765, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.169518256575968, 0.16951825657596817, 0.31850052804957096], 
reward next is 0.6815, 
noisyNet noise sample is [array([-0.17320031], dtype=float32), 0.99465936]. 
=============================================
[2019-04-27 17:58:13,684] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1809017e-11 2.1530337e-05 9.9997842e-01 6.2457255e-11 8.9259391e-09], sum to 1.0000
[2019-04-27 17:58:13,691] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5237
[2019-04-27 17:58:13,693] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [34.4, 23.66666666666667, 1.0, 2.0, 0.6722430316297093, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9569058759514307, 6.9112, 6.9112, 121.9260426156618, 1532455.680824084, 1532455.680824084, 300964.3739571419], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2475600.0000, 
sim time next is 2476200.0000, 
raw observation next is [34.34999999999999, 23.83333333333333, 1.0, 2.0, 0.6843389200283461, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9570052218226119, 6.911200000000001, 6.9112, 121.9260426156618, 1546807.468734442, 1546807.468734442, 303258.1706380729], 
processed observation next is [1.0, 0.6521739130434783, 0.8277777777777773, 0.23833333333333329, 1.0, 1.0, 0.6242130000337452, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.9462565272782649, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.5524312388337292, 0.5524312388337292, 0.5831887896886017], 
reward next is 0.4168, 
noisyNet noise sample is [array([-1.8077475], dtype=float32), 0.56312716]. 
=============================================
[2019-04-27 17:58:14,568] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.55826320e-13 5.93812592e-06 9.99994040e-01 1.48544818e-13
 1.02846856e-10], sum to 1.0000
[2019-04-27 17:58:14,575] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8423
[2019-04-27 17:58:14,580] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.43333333333333, 35.33333333333333, 1.0, 2.0, 0.1796052946959089, 0.0, 2.0, 0.0, 1.0, 2.0, 0.299886182493213, 6.9112, 6.9112, 121.9260426156618, 447246.7299423311, 447246.7299423311, 162604.7477370438], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2493600.0000, 
sim time next is 2494200.0000, 
raw observation next is [29.26666666666667, 36.16666666666667, 1.0, 2.0, 0.1816038331555987, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3029536701214984, 6.911199999999999, 6.9112, 121.9260426156618, 451923.5057644167, 451923.5057644172, 163091.5722930374], 
processed observation next is [1.0, 0.8695652173913043, 0.6395061728395063, 0.3616666666666667, 1.0, 1.0, 0.025718848994760346, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.12869208765187296, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.16140125205872025, 0.1614012520587204, 0.3136376390250719], 
reward next is 0.6864, 
noisyNet noise sample is [array([1.2620813], dtype=float32), -1.4559684]. 
=============================================
[2019-04-27 17:58:15,637] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.5072095e-10 1.8972003e-05 9.9998093e-01 4.2123566e-12 7.9349221e-08], sum to 1.0000
[2019-04-27 17:58:15,644] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2200
[2019-04-27 17:58:15,651] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.33333333333334, 58.66666666666667, 1.0, 2.0, 0.2079464865724075, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3463079080635058, 6.911199999999999, 6.9112, 121.9260426156618, 516823.0813496343, 516823.0813496348, 169056.8956693387], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2521200.0000, 
sim time next is 2521800.0000, 
raw observation next is [24.25, 59.0, 1.0, 2.0, 0.2044436934664954, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3407125764427226, 6.911199999999999, 6.9112, 121.9260426156618, 508389.6151657411, 508389.6151657415, 168221.0560813294], 
processed observation next is [1.0, 0.17391304347826086, 0.4537037037037037, 0.59, 1.0, 1.0, 0.052909158888685005, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.17589072055340327, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1815677197020504, 0.18156771970205052, 0.3235020309256335], 
reward next is 0.6765, 
noisyNet noise sample is [array([-2.1068218], dtype=float32), 1.1516197]. 
=============================================
[2019-04-27 17:58:25,301] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4500, global step 71752: loss 0.2432
[2019-04-27 17:58:25,303] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4500, global step 71753: learning rate 0.0001
[2019-04-27 17:58:25,442] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4500, global step 71818: loss 0.0292
[2019-04-27 17:58:25,444] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4500, global step 71818: learning rate 0.0001
[2019-04-27 17:58:25,654] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4500, global step 71919: loss 0.0001
[2019-04-27 17:58:25,657] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4500, global step 71919: learning rate 0.0001
[2019-04-27 17:58:25,686] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4500, global step 71935: loss 0.0003
[2019-04-27 17:58:25,689] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4500, global step 71935: learning rate 0.0001
[2019-04-27 17:58:25,712] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4500, global step 71946: loss 0.0032
[2019-04-27 17:58:25,715] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4500, global step 71948: learning rate 0.0001
[2019-04-27 17:58:25,723] A3C_AGENT_WORKER-Thread-22 INFO:Local step 4500, global step 71951: loss 0.0009
[2019-04-27 17:58:25,728] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 4500, global step 71951: learning rate 0.0001
[2019-04-27 17:58:25,736] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4500, global step 71956: loss 0.0113
[2019-04-27 17:58:25,737] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4500, global step 71957: learning rate 0.0001
[2019-04-27 17:58:25,777] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4500, global step 71974: loss 0.0003
[2019-04-27 17:58:25,781] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4500, global step 71976: learning rate 0.0001
[2019-04-27 17:58:25,809] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4500, global step 71991: loss 0.0013
[2019-04-27 17:58:25,811] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4500, global step 71992: learning rate 0.0001
[2019-04-27 17:58:25,850] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4500, global step 72011: loss 0.0049
[2019-04-27 17:58:25,851] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4500, global step 72011: loss 0.0051
[2019-04-27 17:58:25,852] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4500, global step 72011: learning rate 0.0001
[2019-04-27 17:58:25,854] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4500, global step 72011: learning rate 0.0001
[2019-04-27 17:58:25,946] A3C_AGENT_WORKER-Thread-21 INFO:Local step 4500, global step 72053: loss 0.0003
[2019-04-27 17:58:25,952] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 4500, global step 72054: learning rate 0.0001
[2019-04-27 17:58:25,974] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4500, global step 72065: loss 0.0006
[2019-04-27 17:58:25,978] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4500, global step 72065: learning rate 0.0001
[2019-04-27 17:58:26,046] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4500, global step 72102: loss 0.0009
[2019-04-27 17:58:26,050] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4500, global step 72103: learning rate 0.0001
[2019-04-27 17:58:26,058] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4500, global step 72104: loss 0.0041
[2019-04-27 17:58:26,062] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4500, global step 72106: learning rate 0.0001
[2019-04-27 17:58:26,227] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4500, global step 72185: loss 0.0355
[2019-04-27 17:58:26,229] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4500, global step 72185: learning rate 0.0001
[2019-04-27 17:58:29,519] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.1324523e-13 1.9026096e-06 9.9999809e-01 1.1679924e-12 1.2174511e-09], sum to 1.0000
[2019-04-27 17:58:29,526] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4782
[2019-04-27 17:58:29,648] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.16666666666666, 94.0, 1.0, 2.0, 0.3169706038405961, 0.0, 2.0, 0.0, 1.0, 2.0, 0.504627524326109, 6.911199999999999, 6.9112, 121.9260426156618, 722478.1603080729, 722478.1603080734, 199859.8131090902], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2767800.0000, 
sim time next is 2768400.0000, 
raw observation next is [24.0, 94.0, 1.0, 2.0, 0.3107686451411213, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4948293099323742, 6.911199999999999, 6.9112, 121.9260426156618, 710386.3876154477, 710386.3876154481, 198127.8258722392], 
processed observation next is [1.0, 0.043478260869565216, 0.4444444444444444, 0.94, 1.0, 1.0, 0.17948648231085867, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.36853663741546766, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2537094241483742, 0.25370942414837433, 0.3810150497543061], 
reward next is 0.6190, 
noisyNet noise sample is [array([0.1512233], dtype=float32), 0.4091916]. 
=============================================
[2019-04-27 17:58:32,209] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-04-27 17:58:32,211] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 17:58:32,212] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:58:32,212] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 17:58:32,213] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 17:58:32,214] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:58:32,214] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 17:58:32,215] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 17:58:32,218] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:58:32,219] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:58:32,216] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 17:58:32,228] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run4
[2019-04-27 17:58:32,245] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run4
[2019-04-27 17:58:32,264] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run4
[2019-04-27 17:58:32,265] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run4
[2019-04-27 17:58:32,301] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run4
[2019-04-27 17:58:43,872] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0252527], dtype=float32), 0.029238038]
[2019-04-27 17:58:43,874] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [19.5, 43.66666666666667, 1.0, 2.0, 0.16, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2177925841865549, 6.911199999999999, 6.9112, 121.9260426156618, 311002.7030865806, 311002.7030865811, 119382.3052847164]
[2019-04-27 17:58:43,874] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 17:58:43,877] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [9.9069524e-08 6.3498295e-04 9.9936253e-01 6.2077593e-10 2.3870452e-06], sampled 0.3960924793666655
[2019-04-27 17:58:54,182] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0252527], dtype=float32), 0.029238038]
[2019-04-27 17:58:54,183] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [27.51279933, 38.35699239666667, 1.0, 2.0, 0.1661767427649598, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2817958274678805, 6.911199999999999, 6.9112, 121.9260426156618, 418031.1062804601, 418031.1062804606, 158878.9647776812]
[2019-04-27 17:58:54,183] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 17:58:54,187] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [6.4906274e-08 4.9179798e-04 9.9950624e-01 3.8247910e-10 1.7752939e-06], sampled 0.6404309572938556
[2019-04-27 17:58:57,709] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0252527], dtype=float32), 0.029238038]
[2019-04-27 17:58:57,710] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [26.74007836, 74.42522921333332, 1.0, 2.0, 0.5349873450317901, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8517172766120521, 6.911199999999997, 6.9112, 121.9260426156618, 1219804.08192045, 1219804.081920451, 269416.2652707035]
[2019-04-27 17:58:57,710] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 17:58:57,712] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [6.1828175e-08 3.3302090e-04 9.9966514e-01 3.1765374e-10 1.6168408e-06], sampled 0.3583001731150274
[2019-04-27 17:59:15,772] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0252527], dtype=float32), 0.029238038]
[2019-04-27 17:59:15,773] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [31.16666666666666, 62.33333333333333, 1.0, 2.0, 0.3561045994691164, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5669301198719733, 6.911199999999999, 6.9112, 121.9260426156618, 811724.3787429582, 811724.3787429587, 210944.4297398427]
[2019-04-27 17:59:15,774] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 17:59:15,778] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [6.4730727e-08 3.3756346e-04 9.9966073e-01 2.3270680e-10 1.5675940e-06], sampled 0.5897248318780981
[2019-04-27 17:59:17,610] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0252527], dtype=float32), 0.029238038]
[2019-04-27 17:59:17,611] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [22.83333333333334, 79.66666666666667, 1.0, 2.0, 0.2914006064990157, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4732442698946286, 6.9112, 6.9112, 121.9260426156618, 705718.6812608029, 705718.6812608029, 191181.481521812]
[2019-04-27 17:59:17,612] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 17:59:17,615] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [7.0456679e-08 4.0860253e-04 9.9958962e-01 3.2481501e-10 1.7186197e-06], sampled 0.16798287503350995
[2019-04-27 17:59:33,033] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0252527], dtype=float32), 0.029238038]
[2019-04-27 17:59:33,035] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [27.0, 84.0, 1.0, 2.0, 0.3397445741578193, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5408844267675612, 6.911199999999999, 6.9112, 121.9260426156618, 774413.599848501, 774413.5998485015, 206236.660967292]
[2019-04-27 17:59:33,037] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 17:59:33,040] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.1897362e-07 4.6415432e-04 9.9953330e-01 3.7565778e-10 2.5449524e-06], sampled 0.8212612131062966
[2019-04-27 17:59:48,456] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0252527], dtype=float32), 0.029238038]
[2019-04-27 17:59:48,459] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [20.6, 83.0, 1.0, 2.0, 0.1859509041822711, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3102288737432851, 6.911199999999999, 6.9112, 121.9260426156618, 462770.8396582573, 462770.8396582577, 164042.614007202]
[2019-04-27 17:59:48,461] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 17:59:48,463] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [5.2056365e-08 3.4560476e-04 9.9965298e-01 2.3943331e-10 1.4847768e-06], sampled 0.39900548624590126
[2019-04-27 18:00:15,534] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 6807.3062 2600483528.6373 61.0000
[2019-04-27 18:00:16,062] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 7159.8931 2623645302.4020 97.0000
[2019-04-27 18:00:16,069] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 6579.6283 2661387980.4769 110.0000
[2019-04-27 18:00:16,227] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 7488.3980 2565785418.4053 47.0000
[2019-04-27 18:00:16,313] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7154.3832 2831167129.9144 210.0000
[2019-04-27 18:00:17,329] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 75000, evaluation results [75000.0, 7154.3831651707305, 2831167129.914354, 210.0, 6807.306221209087, 2600483528.637283, 61.0, 7488.397974216505, 2565785418.4053335, 47.0, 6579.628267316844, 2661387980.4768515, 110.0, 7159.893119858908, 2623645302.401994, 97.0]
[2019-04-27 18:00:24,153] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.3594681e-17 4.4994497e-09 1.0000000e+00 3.8902710e-22 1.4788982e-13], sum to 1.0000
[2019-04-27 18:00:24,160] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9026
[2019-04-27 18:00:24,165] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.0, 94.0, 1.0, 2.0, 0.3304834530269651, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5261404203135828, 6.9112, 6.9112, 121.9260426156618, 753293.4324818657, 753293.4324818657, 203618.2609248789], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2941200.0000, 
sim time next is 2941800.0000, 
raw observation next is [25.08333333333334, 92.66666666666667, 1.0, 2.0, 0.3294830858321834, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5245478031598053, 6.911199999999999, 6.9112, 121.9260426156618, 751012.1104071923, 751012.1104071927, 203337.4421956641], 
processed observation next is [1.0, 0.043478260869565216, 0.4845679012345681, 0.9266666666666667, 1.0, 1.0, 0.20176557837164688, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.4056847539497566, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.26821861085971155, 0.2682186108597117, 0.39103354268396945], 
reward next is 0.6090, 
noisyNet noise sample is [array([-1.1879147], dtype=float32), 0.09002287]. 
=============================================
[2019-04-27 18:00:27,274] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5000, global step 79744: loss 2.8848
[2019-04-27 18:00:27,278] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5000, global step 79744: learning rate 0.0001
[2019-04-27 18:00:27,527] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5000, global step 79830: loss 2.4355
[2019-04-27 18:00:27,530] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5000, global step 79830: learning rate 0.0001
[2019-04-27 18:00:27,727] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5000, global step 79892: loss 2.1450
[2019-04-27 18:00:27,732] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5000, global step 79893: learning rate 0.0001
[2019-04-27 18:00:27,877] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5000, global step 79929: loss 2.1267
[2019-04-27 18:00:27,879] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5000, global step 79929: learning rate 0.0001
[2019-04-27 18:00:27,973] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5000, global step 79939: loss 2.2004
[2019-04-27 18:00:27,974] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5000, global step 79940: learning rate 0.0001
[2019-04-27 18:00:27,975] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5000, global step 79940: loss 2.0386
[2019-04-27 18:00:28,057] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5000, global step 79940: learning rate 0.0001
[2019-04-27 18:00:28,156] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5000, global step 79959: loss 1.9338
[2019-04-27 18:00:28,159] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5000, global step 79959: learning rate 0.0001
[2019-04-27 18:00:28,159] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5000, global step 79959: loss 2.0645
[2019-04-27 18:00:28,162] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5000, global step 79960: learning rate 0.0001
[2019-04-27 18:00:28,340] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5000, global step 79973: loss 2.0539
[2019-04-27 18:00:28,343] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5000, global step 79973: learning rate 0.0001
[2019-04-27 18:00:28,447] A3C_AGENT_WORKER-Thread-22 INFO:Local step 5000, global step 79986: loss 2.0101
[2019-04-27 18:00:28,449] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 5000, global step 79987: learning rate 0.0001
[2019-04-27 18:00:28,453] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5000, global step 79989: loss 1.9587
[2019-04-27 18:00:28,531] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5000, global step 79989: learning rate 0.0001
[2019-04-27 18:00:28,750] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5000, global step 80066: loss 1.5406
[2019-04-27 18:00:28,753] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5000, global step 80067: learning rate 0.0001
[2019-04-27 18:00:28,857] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5000, global step 80077: loss 1.3767
[2019-04-27 18:00:28,859] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5000, global step 80077: learning rate 0.0001
[2019-04-27 18:00:28,977] A3C_AGENT_WORKER-Thread-21 INFO:Local step 5000, global step 80102: loss 1.2789
[2019-04-27 18:00:28,980] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 5000, global step 80103: learning rate 0.0001
[2019-04-27 18:00:29,079] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5000, global step 80117: loss 1.5050
[2019-04-27 18:00:29,080] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5000, global step 80117: learning rate 0.0001
[2019-04-27 18:00:29,214] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.6735298e-15 2.7524416e-07 9.9999976e-01 8.9472844e-20 6.5829607e-12], sum to 1.0000
[2019-04-27 18:00:29,223] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2087
[2019-04-27 18:00:29,228] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.0, 94.0, 1.0, 2.0, 0.3590473281616921, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5716150397885364, 6.911199999999998, 6.9112, 121.9260426156618, 818435.7774468975, 818435.7774468984, 211802.4389205852], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3006000.0000, 
sim time next is 3006600.0000, 
raw observation next is [26.08333333333334, 92.33333333333334, 1.0, 2.0, 0.3573564390844934, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5689230893092233, 6.911199999999999, 6.9112, 121.9260426156618, 814579.4070225364, 814579.4070225368, 211308.5988602485], 
processed observation next is [1.0, 0.8260869565217391, 0.5216049382716051, 0.9233333333333335, 1.0, 1.0, 0.23494814176725404, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.4611538616365291, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.290921216793763, 0.29092121679376315, 0.40636269011586246], 
reward next is 0.5936, 
noisyNet noise sample is [array([-0.83095646], dtype=float32), -0.9667385]. 
=============================================
[2019-04-27 18:00:29,324] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5000, global step 80201: loss 1.0734
[2019-04-27 18:00:29,327] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5000, global step 80202: learning rate 0.0001
[2019-04-27 18:00:33,653] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.1467033e-11 2.6485848e-08 1.0000000e+00 3.3917285e-16 1.7203756e-09], sum to 1.0000
[2019-04-27 18:00:33,664] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4284
[2019-04-27 18:00:33,669] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.2, 86.0, 1.0, 2.0, 0.9137657522710574, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.911199999999998, 6.9112, 121.9260426156618, 1756798.405688888, 1756798.405688889, 359961.2893953195], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3085200.0000, 
sim time next is 3085800.0000, 
raw observation next is [28.5, 84.16666666666667, 1.0, 2.0, 0.4488279975949239, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7145487894785132, 6.911199999999999, 6.9112, 121.9260426156618, 1023224.28430351, 1023224.28430351, 239653.9579018799], 
processed observation next is [1.0, 0.7391304347826086, 0.6111111111111112, 0.8416666666666667, 1.0, 1.0, 0.3438428542796714, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6431859868481415, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.3654372443941107, 0.3654372443941107, 0.4608729959651537], 
reward next is 0.5391, 
noisyNet noise sample is [array([-0.48988605], dtype=float32), -1.7915696]. 
=============================================
[2019-04-27 18:00:35,579] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8423124e-14 2.2748494e-09 1.0000000e+00 1.7807393e-18 9.8561393e-13], sum to 1.0000
[2019-04-27 18:00:35,588] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9570
[2019-04-27 18:00:35,595] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.0, 55.0, 1.0, 2.0, 0.323959019769663, 0.0, 2.0, 0.0, 1.0, 2.0, 0.521386317938107, 6.911200000000001, 6.9112, 121.9260426156618, 772601.5126746276, 772601.5126746271, 200681.1053830365], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3121200.0000, 
sim time next is 3121800.0000, 
raw observation next is [27.83333333333334, 56.16666666666667, 1.0, 2.0, 0.4308221257782767, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6921752621872058, 6.911199999999999, 6.9112, 121.9260426156618, 1023720.017678543, 1023720.017678544, 232701.290708121], 
processed observation next is [1.0, 0.13043478260869565, 0.58641975308642, 0.5616666666666668, 1.0, 1.0, 0.32240729259318657, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6152190777340072, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.36561429202805107, 0.36561429202805146, 0.4475024821310019], 
reward next is 0.5525, 
noisyNet noise sample is [array([-1.3495535], dtype=float32), 0.4381771]. 
=============================================
[2019-04-27 18:00:38,867] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.7574419e-13 3.0158784e-08 1.0000000e+00 7.2498086e-17 2.2333457e-12], sum to 1.0000
[2019-04-27 18:00:38,874] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7189
[2019-04-27 18:00:38,879] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [33.33333333333334, 36.0, 1.0, 2.0, 0.2440265744677825, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3915151107790514, 6.911199999999999, 6.9112, 121.9260426156618, 577719.2620113569, 577719.2620113574, 180091.6099871162], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3177600.0000, 
sim time next is 3178200.0000, 
raw observation next is [33.16666666666666, 37.0, 1.0, 2.0, 0.2468048588429065, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3956671524185962, 6.911199999999999, 6.9112, 121.9260426156618, 583110.6988813381, 583110.6988813386, 180837.1175877646], 
processed observation next is [1.0, 0.782608695652174, 0.7839506172839502, 0.37, 1.0, 1.0, 0.1033391176701268, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.2445839405232452, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.20825382102904932, 0.2082538210290495, 0.3477636876687781], 
reward next is 0.6522, 
noisyNet noise sample is [array([-0.42754585], dtype=float32), 1.9122453]. 
=============================================
[2019-04-27 18:00:43,156] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.1501775e-21 7.9847865e-14 1.0000000e+00 8.0822767e-27 1.3339971e-18], sum to 1.0000
[2019-04-27 18:00:43,163] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8104
[2019-04-27 18:00:43,170] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.5, 44.5, 1.0, 2.0, 0.2674090858149594, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4276019009263025, 6.911199999999999, 6.9112, 121.9260426156618, 626846.2067352082, 626846.2067352086, 186219.0514248969], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3249000.0000, 
sim time next is 3249600.0000, 
raw observation next is [31.66666666666666, 43.33333333333333, 1.0, 2.0, 0.2649504780560902, 0.0, 2.0, 0.0, 1.0, 2.0, 0.423935877015623, 6.9112, 6.9112, 121.9260426156618, 622402.9999361525, 622402.9999361525, 185537.7543774844], 
processed observation next is [0.0, 0.6086956521739131, 0.7283950617283949, 0.4333333333333333, 1.0, 1.0, 0.12494104530486926, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.27991984626952876, 0.0, 0.0, 0.8094621288201359, 0.22228678569148302, 0.22228678569148302, 0.35680337380285465], 
reward next is 0.6432, 
noisyNet noise sample is [array([-0.52302206], dtype=float32), -0.09706848]. 
=============================================
[2019-04-27 18:00:44,795] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.5918192e-19 1.0612920e-10 1.0000000e+00 3.6139220e-23 1.0947208e-16], sum to 1.0000
[2019-04-27 18:00:44,805] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0464
[2019-04-27 18:00:44,917] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [23.0, 90.66666666666666, 1.0, 2.0, 0.2713331204999385, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4344040643707389, 6.911199999999999, 6.9112, 121.9260426156618, 638594.4954840726, 638594.4954840731, 187112.5405415109], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3279000.0000, 
sim time next is 3279600.0000, 
raw observation next is [22.8, 90.0, 1.0, 2.0, 0.2641079076312784, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4236553022441314, 6.911199999999999, 6.9112, 121.9260426156618, 624982.5690339754, 624982.5690339758, 185107.3373834549], 
processed observation next is [0.0, 1.0, 0.4, 0.9, 1.0, 1.0, 0.12393798527533142, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.2795691278051642, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.22320806036927693, 0.2232080603692771, 0.35597564881433635], 
reward next is 0.6440, 
noisyNet noise sample is [array([0.42820394], dtype=float32), 0.93035334]. 
=============================================
[2019-04-27 18:00:45,225] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5500, global step 87744: loss 0.1039
[2019-04-27 18:00:45,230] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5500, global step 87747: learning rate 0.0001
[2019-04-27 18:00:45,301] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5500, global step 87782: loss 0.0824
[2019-04-27 18:00:45,305] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5500, global step 87782: learning rate 0.0001
[2019-04-27 18:00:45,372] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5500, global step 87815: loss 0.0439
[2019-04-27 18:00:45,374] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5500, global step 87815: learning rate 0.0001
[2019-04-27 18:00:45,590] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5500, global step 87914: loss 0.0127
[2019-04-27 18:00:45,594] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5500, global step 87915: learning rate 0.0001
[2019-04-27 18:00:45,649] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5500, global step 87943: loss 0.0221
[2019-04-27 18:00:45,656] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5500, global step 87945: learning rate 0.0001
[2019-04-27 18:00:45,661] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5500, global step 87948: loss 0.0161
[2019-04-27 18:00:45,663] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5500, global step 87948: loss 0.0379
[2019-04-27 18:00:45,665] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5500, global step 87948: learning rate 0.0001
[2019-04-27 18:00:45,666] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5500, global step 87950: learning rate 0.0001
[2019-04-27 18:00:45,715] A3C_AGENT_WORKER-Thread-22 INFO:Local step 5500, global step 87974: loss 0.0358
[2019-04-27 18:00:45,717] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5500, global step 87974: loss 0.0235
[2019-04-27 18:00:45,717] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 5500, global step 87974: learning rate 0.0001
[2019-04-27 18:00:45,719] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5500, global step 87974: learning rate 0.0001
[2019-04-27 18:00:45,793] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5500, global step 88011: loss 0.0283
[2019-04-27 18:00:45,798] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5500, global step 88012: learning rate 0.0001
[2019-04-27 18:00:45,848] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5500, global step 88040: loss 0.0109
[2019-04-27 18:00:45,850] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5500, global step 88040: learning rate 0.0001
[2019-04-27 18:00:45,923] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5500, global step 88072: loss 0.0086
[2019-04-27 18:00:45,924] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5500, global step 88072: learning rate 0.0001
[2019-04-27 18:00:46,050] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5500, global step 88133: loss 0.0178
[2019-04-27 18:00:46,052] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5500, global step 88134: learning rate 0.0001
[2019-04-27 18:00:46,083] A3C_AGENT_WORKER-Thread-21 INFO:Local step 5500, global step 88148: loss 0.0222
[2019-04-27 18:00:46,085] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 5500, global step 88148: learning rate 0.0001
[2019-04-27 18:00:46,088] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5500, global step 88149: loss 0.0218
[2019-04-27 18:00:46,089] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5500, global step 88149: learning rate 0.0001
[2019-04-27 18:00:46,207] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5500, global step 88205: loss 0.0543
[2019-04-27 18:00:46,209] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5500, global step 88205: learning rate 0.0001
[2019-04-27 18:00:48,811] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.1747041e-19 1.1962295e-11 1.0000000e+00 1.6990522e-24 3.4883675e-18], sum to 1.0000
[2019-04-27 18:00:48,819] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2138
[2019-04-27 18:00:48,828] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.9, 81.5, 1.0, 2.0, 0.336349282618318, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5354790120597495, 6.911199999999999, 6.9112, 121.9260426156618, 766670.5037058372, 766670.5037058376, 205272.0152404672], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3353400.0000, 
sim time next is 3354000.0000, 
raw observation next is [26.86666666666667, 80.66666666666667, 1.0, 2.0, 0.3370179867359613, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5365436107932102, 6.9112, 6.9112, 121.9260426156618, 768195.5036099172, 768195.5036099172, 205460.9582656797], 
processed observation next is [0.0, 0.8260869565217391, 0.5506172839506175, 0.8066666666666668, 1.0, 1.0, 0.210735698495192, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.4206795134915127, 0.0, 0.0, 0.8094621288201359, 0.27435553700354187, 0.27435553700354187, 0.39511722743399946], 
reward next is 0.6049, 
noisyNet noise sample is [array([1.048657], dtype=float32), 0.4777416]. 
=============================================
[2019-04-27 18:00:48,840] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[72.625206]
 [72.55672 ]
 [72.49684 ]
 [72.40704 ]
 [72.30626 ]], R is [[72.5633316 ]
 [72.44294739]
 [72.31916809]
 [72.1968689 ]
 [72.07096863]].
[2019-04-27 18:00:58,474] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.3574337e-13 7.7039664e-10 1.0000000e+00 4.7584442e-16 2.6787221e-08], sum to 1.0000
[2019-04-27 18:00:58,483] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9656
[2019-04-27 18:00:58,488] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.3218077227815358, 0.0, 2.0, 0.0, 1.0, 2.0, 0.512328375214051, 6.911199999999999, 6.9112, 121.9260426156618, 733508.7883471654, 733508.7883471659, 201197.8485881078], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3522000.0000, 
sim time next is 3522600.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.3239487066617671, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5157368912783563, 6.911199999999999, 6.9112, 121.9260426156618, 738391.1661097357, 738391.1661097362, 201792.2614621826], 
processed observation next is [1.0, 0.782608695652174, 0.5555555555555556, 0.79, 1.0, 1.0, 0.19517703174019896, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.39467111409794536, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.263711130753477, 0.2637111307534772, 0.3880620412734281], 
reward next is 0.6119, 
noisyNet noise sample is [array([0.34770408], dtype=float32), -0.43149912]. 
=============================================
[2019-04-27 18:00:59,537] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.4447125e-15 6.4285584e-11 1.0000000e+00 7.8221507e-18 1.4672692e-09], sum to 1.0000
[2019-04-27 18:00:59,546] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6405
[2019-04-27 18:00:59,554] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [22.16666666666667, 93.16666666666667, 1.0, 2.0, 0.2488391069898268, 0.0, 2.0, 0.0, 1.0, 2.0, 0.400106048668655, 6.911199999999999, 6.9112, 121.9260426156618, 592168.9973603486, 592168.997360349, 181100.9418141476], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3545400.0000, 
sim time next is 3546000.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.2470843823006208, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3975014291279378, 6.9112, 6.9112, 121.9260426156618, 588689.4994227361, 588689.4994227361, 180625.9718443151], 
processed observation next is [1.0, 0.043478260869565216, 0.37037037037037035, 0.94, 1.0, 1.0, 0.10367188369121523, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.24687678640992222, 0.0, 0.0, 0.8094621288201359, 0.21024624979383433, 0.21024624979383433, 0.34735763816214443], 
reward next is 0.6526, 
noisyNet noise sample is [array([-0.07812349], dtype=float32), -0.28171104]. 
=============================================
[2019-04-27 18:00:59,572] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[60.638916]
 [60.801037]
 [61.045685]
 [61.300064]
 [61.76504 ]], R is [[60.56272125]
 [60.60882568]
 [60.65363312]
 [60.69736862]
 [60.74030685]].
[2019-04-27 18:01:02,031] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6000, global step 95751: loss 0.0066
[2019-04-27 18:01:02,035] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6000, global step 95752: learning rate 0.0001
[2019-04-27 18:01:02,080] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6000, global step 95778: loss 0.0066
[2019-04-27 18:01:02,083] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6000, global step 95778: learning rate 0.0001
[2019-04-27 18:01:02,097] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.6074618e-10 2.5749528e-06 9.9997807e-01 1.0170928e-10 1.9354535e-05], sum to 1.0000
[2019-04-27 18:01:02,106] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6735
[2019-04-27 18:01:02,216] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.0, 85.0, 1.0, 2.0, 0.4447438967810897, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7102656632363158, 6.911199999999999, 6.9112, 121.9260426156618, 1037723.427808288, 1037723.427808288, 237835.8407806616], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3586800.0000, 
sim time next is 3587400.0000, 
raw observation next is [24.0, 86.0, 1.0, 2.0, 0.4260433192153609, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6802075069833498, 6.911199999999999, 6.9112, 121.9260426156618, 992834.9753227871, 992834.9753227875, 231844.0167973224], 
processed observation next is [1.0, 0.5217391304347826, 0.4444444444444444, 0.86, 1.0, 1.0, 0.316718237161144, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6002593837291872, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.35458391975813824, 0.3545839197581384, 0.44585387845638924], 
reward next is 0.5541, 
noisyNet noise sample is [array([-0.9289811], dtype=float32), 0.1895235]. 
=============================================
[2019-04-27 18:01:02,233] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6000, global step 95807: loss 0.0082
[2019-04-27 18:01:02,235] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6000, global step 95808: learning rate 0.0001
[2019-04-27 18:01:02,276] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0484310e-09 6.1889688e-07 9.9998951e-01 2.0632027e-11 9.8578384e-06], sum to 1.0000
[2019-04-27 18:01:02,287] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4551
[2019-04-27 18:01:02,290] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.86666666666667, 77.66666666666667, 1.0, 2.0, 0.6363490775993096, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9790633598001063, 6.911199999999999, 6.9112, 121.9260426156618, 1456101.709645902, 1456101.709645902, 302445.448509596], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3591600.0000, 
sim time next is 3592200.0000, 
raw observation next is [25.08333333333334, 74.83333333333333, 1.0, 2.0, 0.625706624462268, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9762981357542021, 6.911199999999999, 6.9112, 121.9260426156618, 1446407.867202881, 1446407.867202882, 299987.4194679093], 
processed observation next is [1.0, 0.5652173913043478, 0.4845679012345681, 0.7483333333333333, 1.0, 1.0, 0.5544126481693666, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.9703726696927525, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.5165742382867432, 0.5165742382867435, 0.5768988835921333], 
reward next is 0.4231, 
noisyNet noise sample is [array([1.146049], dtype=float32), -1.2031635]. 
=============================================
[2019-04-27 18:01:02,411] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6000, global step 95895: loss 0.0057
[2019-04-27 18:01:02,416] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6000, global step 95895: learning rate 0.0001
[2019-04-27 18:01:02,445] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6000, global step 95909: loss 0.0058
[2019-04-27 18:01:02,446] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6000, global step 95909: learning rate 0.0001
[2019-04-27 18:01:02,485] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6000, global step 95923: loss 0.0036
[2019-04-27 18:01:02,487] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6000, global step 95923: learning rate 0.0001
[2019-04-27 18:01:02,528] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6000, global step 95950: loss 0.0050
[2019-04-27 18:01:02,532] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6000, global step 95950: learning rate 0.0001
[2019-04-27 18:01:02,536] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6000, global step 95952: loss 0.0045
[2019-04-27 18:01:02,543] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6000, global step 95954: learning rate 0.0001
[2019-04-27 18:01:02,728] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6000, global step 96037: loss 0.0030
[2019-04-27 18:01:02,729] A3C_AGENT_WORKER-Thread-22 INFO:Local step 6000, global step 96037: loss 0.0079
[2019-04-27 18:01:02,733] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6000, global step 96037: learning rate 0.0001
[2019-04-27 18:01:02,734] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 6000, global step 96038: learning rate 0.0001
[2019-04-27 18:01:02,763] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6000, global step 96054: loss 0.0048
[2019-04-27 18:01:02,766] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6000, global step 96055: learning rate 0.0001
[2019-04-27 18:01:02,798] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6000, global step 96072: loss 0.0082
[2019-04-27 18:01:02,800] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6000, global step 96072: learning rate 0.0001
[2019-04-27 18:01:02,921] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6000, global step 96130: loss 0.0237
[2019-04-27 18:01:02,925] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6000, global step 96130: learning rate 0.0001
[2019-04-27 18:01:02,949] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6000, global step 96142: loss 0.0430
[2019-04-27 18:01:02,951] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6000, global step 96143: learning rate 0.0001
[2019-04-27 18:01:02,989] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6000, global step 96158: loss 0.0347
[2019-04-27 18:01:02,991] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6000, global step 96160: learning rate 0.0001
[2019-04-27 18:01:03,068] A3C_AGENT_WORKER-Thread-21 INFO:Local step 6000, global step 96197: loss 0.0729
[2019-04-27 18:01:03,070] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 6000, global step 96197: learning rate 0.0001
[2019-04-27 18:01:08,738] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.0920490e-17 6.9719842e-12 1.0000000e+00 8.5726476e-19 9.5449620e-12], sum to 1.0000
[2019-04-27 18:01:08,749] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5248
[2019-04-27 18:01:08,754] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.0, 94.0, 1.0, 2.0, 0.3321673241728513, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5288211980176882, 6.911199999999999, 6.9112, 121.9260426156618, 757133.4901874531, 757133.4901874536, 204091.4638045172], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3715200.0000, 
sim time next is 3715800.0000, 
raw observation next is [25.0, 94.00000000000001, 1.0, 2.0, 0.3322519049732189, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5289558534064779, 6.911199999999999, 6.9112, 121.9260426156618, 757326.3766855808, 757326.3766855813, 204115.2612365067], 
processed observation next is [1.0, 0.0, 0.48148148148148145, 0.9400000000000002, 1.0, 1.0, 0.20506179163478438, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.4111948167580973, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.270473705959136, 0.27047370595913617, 0.39252934853174365], 
reward next is 0.6075, 
noisyNet noise sample is [array([-0.36694762], dtype=float32), 0.2369511]. 
=============================================
[2019-04-27 18:01:10,135] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6931397e-15 2.7171470e-11 1.0000000e+00 3.6441812e-17 1.6069067e-09], sum to 1.0000
[2019-04-27 18:01:10,141] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7123
[2019-04-27 18:01:10,145] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.4, 98.0, 1.0, 2.0, 0.3280819017470606, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5223170724629099, 6.9112, 6.9112, 121.9260426156618, 747816.7429647115, 747816.7429647115, 202945.0951816084], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3733200.0000, 
sim time next is 3733800.0000, 
raw observation next is [24.33333333333333, 98.33333333333334, 1.0, 2.0, 0.4053361785818995, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6453083971801191, 6.911199999999999, 6.9112, 121.9260426156618, 924013.1925869095, 924013.19258691, 225752.5799662697], 
processed observation next is [1.0, 0.21739130434782608, 0.45679012345678993, 0.9833333333333334, 1.0, 1.0, 0.2920668792641661, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5566354964751489, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.33000471163818196, 0.3300047116381821, 0.434139576858211], 
reward next is 0.5659, 
noisyNet noise sample is [array([-0.6380608], dtype=float32), 0.4307224]. 
=============================================
[2019-04-27 18:01:10,655] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.0022154e-14 1.5976490e-10 1.0000000e+00 8.8647782e-17 1.3653022e-09], sum to 1.0000
[2019-04-27 18:01:10,664] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8058
[2019-04-27 18:01:10,669] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.0, 100.0, 1.0, 2.0, 0.3580889833901008, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5700893237010528, 6.9112, 6.9112, 121.9260426156618, 816250.1009816426, 816250.1009816426, 211519.030908808], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3738000.0000, 
sim time next is 3738600.0000, 
raw observation next is [24.0, 100.0, 1.0, 2.0, 0.3557605458367259, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5663823752841629, 6.911199999999999, 6.9112, 121.9260426156618, 810939.7091208796, 810939.7091208801, 210840.747711076], 
processed observation next is [1.0, 0.2608695652173913, 0.4444444444444444, 1.0, 1.0, 1.0, 0.23304826885324512, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.4579779691052036, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.28962132468602847, 0.28962132468602864, 0.4054629763674538], 
reward next is 0.5945, 
noisyNet noise sample is [array([0.58047724], dtype=float32), -0.46964478]. 
=============================================
[2019-04-27 18:01:10,996] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-04-27 18:01:10,998] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 18:01:10,999] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:01:11,001] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 18:01:11,003] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 18:01:11,003] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:01:11,004] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 18:01:11,004] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:01:11,006] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:01:11,006] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 18:01:11,009] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:01:11,022] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run5
[2019-04-27 18:01:11,042] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run5
[2019-04-27 18:01:11,043] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run5
[2019-04-27 18:01:11,044] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run5
[2019-04-27 18:01:11,101] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run5
[2019-04-27 18:02:01,628] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0252527], dtype=float32), 0.034409206]
[2019-04-27 18:02:01,629] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [32.9, 56.66666666666667, 1.0, 2.0, 0.3560606657919185, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5668601760270542, 6.9112, 6.9112, 121.9260426156618, 811624.1808841632, 811624.1808841632, 210933.0525230345]
[2019-04-27 18:02:01,631] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 18:02:01,633] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [2.8694406e-11 4.0091827e-08 9.9999976e-01 6.3655185e-13 2.4909008e-07], sampled 0.3277422405604402
[2019-04-27 18:02:20,685] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0252527], dtype=float32), 0.034409206]
[2019-04-27 18:02:20,686] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [27.16666666666666, 80.16666666666667, 1.0, 2.0, 0.999842284754093, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.9260426156618, 1855063.012390577, 1855063.012390577, 379393.9431682077]
[2019-04-27 18:02:20,687] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 18:02:20,689] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [9.6203601e-10 3.6457169e-07 9.9999368e-01 2.4908211e-11 5.9578692e-06], sampled 0.976498895520498
[2019-04-27 18:02:26,620] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0252527], dtype=float32), 0.034409206]
[2019-04-27 18:02:26,621] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [21.18333333333333, 82.16666666666667, 1.0, 2.0, 0.1951356006350125, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3229771256478979, 6.9112, 6.9112, 121.9260426156618, 482511.3769824877, 482511.3769824877, 166537.7435204096]
[2019-04-27 18:02:26,621] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 18:02:26,622] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [8.69632404e-12 1.74993655e-08 9.99999881e-01 1.72216760e-13
 1.03979396e-07], sampled 0.05685890821617412
[2019-04-27 18:02:34,773] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0252527], dtype=float32), 0.034409206]
[2019-04-27 18:02:34,773] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [28.43035349, 86.61988154000001, 1.0, 2.0, 0.407816637722608, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6492573689150416, 6.911199999999999, 6.9112, 121.9260426156618, 929671.131116859, 929671.1311168595, 226532.2356679315]
[2019-04-27 18:02:34,774] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:02:34,778] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [2.8741180e-11 3.4962241e-08 9.9999964e-01 5.3547135e-13 3.4812641e-07], sampled 0.12038552427262206
[2019-04-27 18:02:54,017] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 7161.4663 2623811787.5594 97.0000
[2019-04-27 18:02:54,372] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 7490.5507 2565897169.4708 47.0000
[2019-04-27 18:02:54,470] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 6810.3314 2600748146.3363 61.0000
[2019-04-27 18:02:54,524] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7156.5872 2831348127.0571 210.0000
[2019-04-27 18:02:54,581] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 6580.7547 2661623244.5523 110.0000
[2019-04-27 18:02:55,597] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 100000, evaluation results [100000.0, 7156.587240130192, 2831348127.0570793, 210.0, 6810.331391877612, 2600748146.336314, 61.0, 7490.550701547243, 2565897169.4708056, 47.0, 6580.754708850709, 2661623244.5523167, 110.0, 7161.466343444668, 2623811787.5593724, 97.0]
[2019-04-27 18:02:56,432] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.7713191e-08 2.0213623e-05 5.1983410e-01 5.2896191e-08 4.8014548e-01], sum to 1.0000
[2019-04-27 18:02:56,442] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8355
[2019-04-27 18:02:56,449] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.66666666666667, 76.33333333333334, 1.0, 2.0, 0.4718305838617918, 1.0, 2.0, 0.4718305838617918, 1.0, 2.0, 0.7511696559573013, 6.911199999999999, 6.9112, 121.94756008, 1614114.124533927, 1614114.124533927, 329975.6661198189], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3759600.0000, 
sim time next is 3760200.0000, 
raw observation next is [31.0, 75.0, 1.0, 2.0, 0.4355177391624536, 1.0, 2.0, 0.4355177391624536, 1.0, 2.0, 0.6933584245691665, 6.911200000000001, 6.9112, 121.94756008, 1489772.928488533, 1489772.928488533, 312990.1986778394], 
processed observation next is [1.0, 0.5217391304347826, 0.7037037037037037, 0.75, 1.0, 1.0, 0.3279973085267305, 1.0, 1.0, 0.3279973085267305, 1.0, 1.0, 0.6166980307114581, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.532061760174476, 0.532061760174476, 0.6019042282266143], 
reward next is 0.3981, 
noisyNet noise sample is [array([-0.62118924], dtype=float32), 0.7546986]. 
=============================================
[2019-04-27 18:02:57,774] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3695105e-10 1.7182877e-07 4.1642152e-02 1.0416983e-09 9.5835763e-01], sum to 1.0000
[2019-04-27 18:02:57,784] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4872
[2019-04-27 18:02:57,790] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.5, 65.0, 1.0, 2.0, 0.2167212046889842, 1.0, 2.0, 0.2167212046889842, 1.0, 2.0, 0.3450272159817467, 6.911199999999999, 6.9112, 121.94756008, 740975.0789353659, 740975.0789353664, 226134.5579327196], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3795600.0000, 
sim time next is 3796200.0000, 
raw observation next is [29.25, 68.0, 1.0, 2.0, 0.2186831301530166, 1.0, 2.0, 0.2186831301530166, 1.0, 2.0, 0.348150665215938, 6.9112, 6.9112, 121.94756008, 747686.2207838773, 747686.2207838773, 226794.8169653241], 
processed observation next is [1.0, 0.9565217391304348, 0.6388888888888888, 0.68, 1.0, 1.0, 0.06986086922978167, 1.0, 1.0, 0.06986086922978167, 1.0, 1.0, 0.1851883315199225, 0.0, 0.0, 0.8096049824067558, 0.26703079313709904, 0.26703079313709904, 0.4361438787794694], 
reward next is 0.5639, 
noisyNet noise sample is [array([0.7088313], dtype=float32), -1.4413201]. 
=============================================
[2019-04-27 18:02:58,810] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.0544534e-12 2.7859949e-08 3.1492496e-03 2.1839563e-11 9.9685067e-01], sum to 1.0000
[2019-04-27 18:02:58,820] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3016
[2019-04-27 18:02:58,827] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.7, 72.0, 1.0, 2.0, 0.197220458610895, 1.0, 2.0, 0.197220458610895, 1.0, 2.0, 0.3140023848531492, 6.911200000000001, 6.9112, 121.94756008, 675226.8089619093, 675226.8089619088, 219692.7716204867], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3808800.0000, 
sim time next is 3809400.0000, 
raw observation next is [26.58333333333333, 72.33333333333333, 1.0, 2.0, 0.1949232682141008, 1.0, 2.0, 0.1949232682141008, 1.0, 2.0, 0.3103864500468206, 6.9112, 6.9112, 121.94756008, 668827.987141539, 668827.987141539, 218951.153354872], 
processed observation next is [0.0, 0.08695652173913043, 0.5401234567901233, 0.7233333333333333, 1.0, 1.0, 0.041575319302500954, 1.0, 1.0, 0.041575319302500954, 1.0, 1.0, 0.13798306255852572, 0.0, 0.0, 0.8096049824067558, 0.23886713826483535, 0.23886713826483535, 0.42105991029783074], 
reward next is 0.5789, 
noisyNet noise sample is [array([0.82292295], dtype=float32), 0.2915062]. 
=============================================
[2019-04-27 18:03:00,829] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0012872e-12 1.2840422e-09 4.6671458e-04 8.8070447e-13 9.9953330e-01], sum to 1.0000
[2019-04-27 18:03:00,835] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0945
[2019-04-27 18:03:00,838] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.0, 57.5, 1.0, 2.0, 0.2450292798910612, 1.0, 2.0, 0.2450292798910612, 1.0, 2.0, 0.3900945936330995, 6.9112, 6.9112, 121.94756008, 837813.9645243904, 837813.9645243904, 235869.7755264495], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3846600.0000, 
sim time next is 3847200.0000, 
raw observation next is [33.0, 57.0, 1.0, 2.0, 0.24890876750617, 1.0, 2.0, 0.24890876750617, 1.0, 2.0, 0.3962708642624435, 6.911200000000001, 6.9112, 121.94756008, 851086.2282369123, 851086.2282369118, 237238.8375881765], 
processed observation next is [0.0, 0.5217391304347826, 0.7777777777777778, 0.57, 1.0, 1.0, 0.10584377084067857, 1.0, 1.0, 0.10584377084067857, 1.0, 1.0, 0.24533858032805436, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.3039593672274687, 0.3039593672274685, 0.4562285338234164], 
reward next is 0.5438, 
noisyNet noise sample is [array([-0.36736897], dtype=float32), 0.25575867]. 
=============================================
[2019-04-27 18:03:03,495] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6500, global step 103772: loss 0.0073
[2019-04-27 18:03:03,500] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6500, global step 103774: learning rate 0.0001
[2019-04-27 18:03:03,545] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6500, global step 103797: loss 0.0053
[2019-04-27 18:03:03,548] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6500, global step 103797: learning rate 0.0001
[2019-04-27 18:03:03,645] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6500, global step 103844: loss 0.0517
[2019-04-27 18:03:03,647] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6500, global step 103844: learning rate 0.0001
[2019-04-27 18:03:03,769] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6500, global step 103902: loss 0.0189
[2019-04-27 18:03:03,770] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6500, global step 103903: learning rate 0.0001
[2019-04-27 18:03:03,852] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6500, global step 103940: loss 0.0290
[2019-04-27 18:03:03,858] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6500, global step 103944: learning rate 0.0001
[2019-04-27 18:03:03,869] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6500, global step 103949: loss 0.0305
[2019-04-27 18:03:03,871] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6500, global step 103949: learning rate 0.0001
[2019-04-27 18:03:03,875] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6500, global step 103951: loss 0.0278
[2019-04-27 18:03:03,877] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6500, global step 103951: learning rate 0.0001
[2019-04-27 18:03:03,906] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6500, global step 103966: loss 0.0069
[2019-04-27 18:03:03,912] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6500, global step 103967: learning rate 0.0001
[2019-04-27 18:03:03,937] A3C_AGENT_WORKER-Thread-22 INFO:Local step 6500, global step 103981: loss 0.0077
[2019-04-27 18:03:03,940] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 6500, global step 103981: learning rate 0.0001
[2019-04-27 18:03:04,100] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6500, global step 104059: loss 0.0181
[2019-04-27 18:03:04,102] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6500, global step 104059: learning rate 0.0001
[2019-04-27 18:03:04,130] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6500, global step 104072: loss 0.0394
[2019-04-27 18:03:04,132] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6500, global step 104072: learning rate 0.0001
[2019-04-27 18:03:04,144] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6500, global step 104078: loss 0.0367
[2019-04-27 18:03:04,147] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6500, global step 104079: learning rate 0.0001
[2019-04-27 18:03:04,151] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6500, global step 104081: loss 0.0478
[2019-04-27 18:03:04,154] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6500, global step 104082: learning rate 0.0001
[2019-04-27 18:03:04,196] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6500, global step 104104: loss 0.1189
[2019-04-27 18:03:04,199] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6500, global step 104104: learning rate 0.0001
[2019-04-27 18:03:04,340] A3C_AGENT_WORKER-Thread-21 INFO:Local step 6500, global step 104174: loss 0.0494
[2019-04-27 18:03:04,346] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 6500, global step 104176: learning rate 0.0001
[2019-04-27 18:03:04,358] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6500, global step 104181: loss 0.0549
[2019-04-27 18:03:04,364] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6500, global step 104183: learning rate 0.0001
[2019-04-27 18:03:08,911] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.7342195e-10 2.6398544e-07 2.5127598e-05 6.5262157e-11 9.9997461e-01], sum to 1.0000
[2019-04-27 18:03:08,920] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6170
[2019-04-27 18:03:08,924] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.4, 87.66666666666667, 1.0, 2.0, 0.5035347090404269, 1.0, 2.0, 0.5035347090404269, 1.0, 2.0, 0.8016436557729606, 6.9112, 6.9112, 122.6498136504825, 1722667.750873302, 1722667.750873302, 345484.4889289874], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3982800.0000, 
sim time next is 3983400.0000, 
raw observation next is [25.35, 88.0, 1.0, 2.0, 0.4683016202171954, 1.0, 2.0, 0.4683016202171954, 1.0, 2.0, 0.7455514309047816, 6.911199999999999, 6.9112, 121.94756008, 1602030.854878468, 1602030.854878468, 328293.0300512896], 
processed observation next is [1.0, 0.08695652173913043, 0.4944444444444445, 0.88, 1.0, 1.0, 0.3670257383538041, 1.0, 1.0, 0.3670257383538041, 1.0, 1.0, 0.6819392886309769, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.57215387674231, 0.57215387674231, 0.6313327500986339], 
reward next is 0.3687, 
noisyNet noise sample is [array([-0.66135275], dtype=float32), 0.3103998]. 
=============================================
[2019-04-27 18:03:10,102] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.6447290e-12 1.0063486e-08 3.5207322e-06 2.5464457e-11 9.9999642e-01], sum to 1.0000
[2019-04-27 18:03:10,110] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4911
[2019-04-27 18:03:10,115] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.36666666666667, 94.0, 1.0, 2.0, 0.2977006654204879, 1.0, 2.0, 0.2977006654204879, 1.0, 2.0, 0.473949154783216, 6.911199999999999, 6.9112, 121.94756008, 1018029.683199678, 1018029.683199679, 255174.6379195419], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4000800.0000, 
sim time next is 4001400.0000, 
raw observation next is [24.35, 94.0, 1.0, 2.0, 0.3016102138200531, 1.0, 2.0, 0.3016102138200531, 1.0, 2.0, 0.4801732831604261, 6.911200000000001, 6.9112, 121.94756008, 1031407.935080592, 1031407.935080592, 256669.2300969764], 
processed observation next is [1.0, 0.30434782608695654, 0.4574074074074075, 0.94, 1.0, 1.0, 0.1685835878810156, 1.0, 1.0, 0.1685835878810156, 1.0, 1.0, 0.3502166039505326, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.36835997681449717, 0.36835997681449717, 0.4935946732634161], 
reward next is 0.5064, 
noisyNet noise sample is [array([0.5081165], dtype=float32), -0.34844846]. 
=============================================
[2019-04-27 18:03:10,431] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.8336771e-09 1.5055379e-07 3.9029946e-05 3.2228291e-09 9.9996078e-01], sum to 1.0000
[2019-04-27 18:03:10,438] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7189
[2019-04-27 18:03:10,443] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.6, 94.0, 1.0, 2.0, 0.4369762049170162, 1.0, 2.0, 0.4369762049170162, 1.0, 2.0, 0.6956803495493438, 6.911200000000001, 6.9112, 121.94756008, 1494766.761792128, 1494766.761792127, 313658.3545795695], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4006800.0000, 
sim time next is 4007400.0000, 
raw observation next is [24.66666666666667, 94.00000000000001, 1.0, 2.0, 0.4519567307866141, 1.0, 2.0, 0.4519567307866141, 1.0, 2.0, 0.7195298346154104, 6.9112, 6.9112, 121.94756008, 1546062.445530677, 1546062.445530677, 320589.3455577784], 
processed observation next is [1.0, 0.391304347826087, 0.469135802469136, 0.9400000000000002, 1.0, 1.0, 0.3475675366507311, 1.0, 1.0, 0.3475675366507311, 1.0, 1.0, 0.6494122932692631, 0.0, 0.0, 0.8096049824067558, 0.5521651591180989, 0.5521651591180989, 0.6165179722264968], 
reward next is 0.3835, 
noisyNet noise sample is [array([-1.1325399], dtype=float32), 0.49347854]. 
=============================================
[2019-04-27 18:03:16,693] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.2739252e-08 1.6826878e-06 6.6839534e-06 1.4696046e-08 9.9999154e-01], sum to 1.0000
[2019-04-27 18:03:16,705] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6554
[2019-04-27 18:03:16,710] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.1, 68.0, 1.0, 2.0, 0.6253684306686172, 1.0, 2.0, 0.6253684306686172, 1.0, 2.0, 0.9956069084523432, 6.9112, 6.9112, 121.94756008, 2139990.278132071, 2139990.278132071, 409370.7688264329], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4120200.0000, 
sim time next is 4120800.0000, 
raw observation next is [29.13333333333333, 67.33333333333334, 1.0, 2.0, 0.6197753297178898, 1.0, 2.0, 0.6197753297178898, 1.0, 2.0, 0.9867025095842041, 6.911199999999999, 6.9112, 121.94756008, 2120828.143791061, 2120828.143791062, 406255.4428409364], 
processed observation next is [1.0, 0.6956521739130435, 0.6345679012345677, 0.6733333333333335, 1.0, 1.0, 0.5473515829974879, 1.0, 1.0, 0.5473515829974879, 1.0, 1.0, 0.9833781369802552, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.7574386227825217, 0.757438622782522, 0.7812604670018009], 
reward next is 0.2187, 
noisyNet noise sample is [array([0.23952875], dtype=float32), -0.46218207]. 
=============================================
[2019-04-27 18:03:20,239] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7000, global step 111746: loss 0.1904
[2019-04-27 18:03:20,241] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7000, global step 111746: learning rate 0.0001
[2019-04-27 18:03:20,282] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7000, global step 111766: loss 0.1515
[2019-04-27 18:03:20,283] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7000, global step 111766: learning rate 0.0001
[2019-04-27 18:03:20,416] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7000, global step 111840: loss 0.0594
[2019-04-27 18:03:20,417] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7000, global step 111840: learning rate 0.0001
[2019-04-27 18:03:20,487] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7000, global step 111872: loss 0.0278
[2019-04-27 18:03:20,490] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7000, global step 111873: learning rate 0.0001
[2019-04-27 18:03:20,593] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7000, global step 111922: loss 0.0194
[2019-04-27 18:03:20,597] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7000, global step 111923: learning rate 0.0001
[2019-04-27 18:03:20,624] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7000, global step 111938: loss 0.0050
[2019-04-27 18:03:20,626] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7000, global step 111938: learning rate 0.0001
[2019-04-27 18:03:20,654] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7000, global step 111954: loss 0.0155
[2019-04-27 18:03:20,655] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7000, global step 111954: learning rate 0.0001
[2019-04-27 18:03:20,702] A3C_AGENT_WORKER-Thread-22 INFO:Local step 7000, global step 111974: loss 0.0304
[2019-04-27 18:03:20,704] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 7000, global step 111975: learning rate 0.0001
[2019-04-27 18:03:20,737] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7000, global step 111991: loss 0.0221
[2019-04-27 18:03:20,739] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7000, global step 111991: learning rate 0.0001
[2019-04-27 18:03:20,821] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7000, global step 112025: loss 0.0141
[2019-04-27 18:03:20,824] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7000, global step 112026: learning rate 0.0001
[2019-04-27 18:03:20,889] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7000, global step 112059: loss 0.0239
[2019-04-27 18:03:20,893] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7000, global step 112059: loss 0.0192
[2019-04-27 18:03:20,895] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7000, global step 112059: learning rate 0.0001
[2019-04-27 18:03:20,900] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7000, global step 112063: learning rate 0.0001
[2019-04-27 18:03:20,997] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7000, global step 112111: loss 0.0006
[2019-04-27 18:03:21,000] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7000, global step 112111: learning rate 0.0001
[2019-04-27 18:03:21,079] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7000, global step 112151: loss 0.0006
[2019-04-27 18:03:21,081] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7000, global step 112152: learning rate 0.0001
[2019-04-27 18:03:21,142] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7000, global step 112179: loss 0.0006
[2019-04-27 18:03:21,144] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7000, global step 112180: learning rate 0.0001
[2019-04-27 18:03:21,165] A3C_AGENT_WORKER-Thread-21 INFO:Local step 7000, global step 112190: loss 0.0007
[2019-04-27 18:03:21,167] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 7000, global step 112190: learning rate 0.0001
[2019-04-27 18:03:25,154] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.2263419e-12 3.2059684e-09 5.0623530e-06 4.2381320e-12 9.9999499e-01], sum to 1.0000
[2019-04-27 18:03:25,164] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6379
[2019-04-27 18:03:25,170] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 38.0, 1.0, 2.0, 0.3652319067852214, 1.0, 2.0, 0.3652319067852214, 1.0, 2.0, 0.5829009901566546, 6.911199999999999, 6.9112, 121.94756008, 1274722.284523525, 1274722.284523525, 282230.8364995438], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4279200.0000, 
sim time next is 4279800.0000, 
raw observation next is [32.0, 38.0, 1.0, 2.0, 0.3650963614896705, 1.0, 2.0, 0.3650963614896705, 1.0, 2.0, 0.582732459864436, 6.911199999999999, 6.9112, 121.94756008, 1274746.986546693, 1274746.986546693, 282172.0655833394], 
processed observation next is [1.0, 0.5217391304347826, 0.7407407407407407, 0.38, 1.0, 1.0, 0.24416233510675062, 1.0, 1.0, 0.24416233510675062, 1.0, 1.0, 0.478415574830545, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.45526678090953315, 0.45526678090953315, 0.5426385876602682], 
reward next is 0.4574, 
noisyNet noise sample is [array([-0.60466105], dtype=float32), -0.46873826]. 
=============================================
[2019-04-27 18:03:36,558] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.7594107e-14 1.3551411e-10 1.3865972e-05 9.6841372e-13 9.9998617e-01], sum to 1.0000
[2019-04-27 18:03:36,566] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2706
[2019-04-27 18:03:36,570] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.2359119178510163, 1.0, 2.0, 0.2359119178510163, 1.0, 2.0, 0.3755794563335962, 6.911199999999998, 6.9112, 121.94756008, 806623.112517842, 806623.1125178429, 232685.3850073506], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4482000.0000, 
sim time next is 4482600.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.23184990241094, 1.0, 2.0, 0.23184990241094, 1.0, 2.0, 0.3691125954623875, 6.9112, 6.9112, 121.94756008, 792727.2075226003, 792727.2075226003, 231281.6188613019], 
processed observation next is [0.0, 0.9130434782608695, 0.5555555555555556, 0.84, 1.0, 1.0, 0.0855355981082619, 1.0, 1.0, 0.0855355981082619, 1.0, 1.0, 0.21139074432798435, 0.0, 0.0, 0.8096049824067558, 0.28311685982950013, 0.28311685982950013, 0.4447723439640421], 
reward next is 0.5552, 
noisyNet noise sample is [array([0.8464366], dtype=float32), -0.012956556]. 
=============================================
[2019-04-27 18:03:36,969] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7500, global step 119749: loss 0.0043
[2019-04-27 18:03:36,971] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7500, global step 119749: learning rate 0.0001
[2019-04-27 18:03:37,063] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7500, global step 119792: loss 0.1085
[2019-04-27 18:03:37,064] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7500, global step 119792: learning rate 0.0001
[2019-04-27 18:03:37,095] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7500, global step 119803: loss 0.2296
[2019-04-27 18:03:37,097] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7500, global step 119803: learning rate 0.0001
[2019-04-27 18:03:37,183] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7500, global step 119852: loss 0.1544
[2019-04-27 18:03:37,186] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7500, global step 119853: learning rate 0.0001
[2019-04-27 18:03:37,259] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7500, global step 119882: loss 0.0586
[2019-04-27 18:03:37,261] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7500, global step 119883: learning rate 0.0001
[2019-04-27 18:03:37,387] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7500, global step 119946: loss 0.0150
[2019-04-27 18:03:37,390] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7500, global step 119946: learning rate 0.0001
[2019-04-27 18:03:37,405] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7500, global step 119954: loss 0.0099
[2019-04-27 18:03:37,408] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7500, global step 119954: learning rate 0.0001
[2019-04-27 18:03:37,517] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7500, global step 120013: loss 0.0037
[2019-04-27 18:03:37,520] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7500, global step 120013: learning rate 0.0001
[2019-04-27 18:03:37,521] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7500, global step 120014: loss 0.0081
[2019-04-27 18:03:37,525] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7500, global step 120016: learning rate 0.0001
[2019-04-27 18:03:37,532] A3C_AGENT_WORKER-Thread-22 INFO:Local step 7500, global step 120022: loss 0.0147
[2019-04-27 18:03:37,533] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 7500, global step 120022: learning rate 0.0001
[2019-04-27 18:03:37,556] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7500, global step 120031: loss 0.0318
[2019-04-27 18:03:37,558] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7500, global step 120032: learning rate 0.0001
[2019-04-27 18:03:37,663] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7500, global step 120087: loss 0.1021
[2019-04-27 18:03:37,667] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7500, global step 120087: learning rate 0.0001
[2019-04-27 18:03:37,725] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7500, global step 120116: loss 0.0127
[2019-04-27 18:03:37,726] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7500, global step 120116: learning rate 0.0001
[2019-04-27 18:03:37,739] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7500, global step 120121: loss 0.0104
[2019-04-27 18:03:37,742] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7500, global step 120122: learning rate 0.0001
[2019-04-27 18:03:37,828] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7500, global step 120163: loss 0.0063
[2019-04-27 18:03:37,829] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7500, global step 120163: learning rate 0.0001
[2019-04-27 18:03:37,975] A3C_AGENT_WORKER-Thread-21 INFO:Local step 7500, global step 120231: loss 0.0614
[2019-04-27 18:03:37,976] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 7500, global step 120231: learning rate 0.0001
[2019-04-27 18:03:47,910] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-27 18:03:47,911] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 18:03:47,911] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 18:03:47,912] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:03:47,912] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 18:03:47,913] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 18:03:47,913] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 18:03:47,912] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:03:47,913] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:03:47,916] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:03:47,918] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:03:47,936] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run6
[2019-04-27 18:03:47,957] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run6
[2019-04-27 18:03:47,980] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run6
[2019-04-27 18:03:47,981] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run6
[2019-04-27 18:03:47,981] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run6
[2019-04-27 18:03:52,430] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.0252527], dtype=float32), 0.036294956]
[2019-04-27 18:03:52,430] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [26.26725587, 37.01022017, 1.0, 2.0, 0.16, 1.0, 2.0, 0.16, 1.0, 2.0, 0.2, 6.9112, 6.9112, 121.94756008, 398978.0441685641, 398978.0441685641, 175944.8943352512]
[2019-04-27 18:03:52,431] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 18:03:52,434] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [8.2468282e-13 3.2813507e-09 5.1339098e-06 5.5018273e-12 9.9999487e-01], sampled 0.7280402351910024
[2019-04-27 18:04:48,102] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0252527], dtype=float32), 0.036294956]
[2019-04-27 18:04:48,103] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [25.43333333333333, 86.0, 1.0, 2.0, 0.2027340755117083, 1.0, 2.0, 0.2027340755117083, 1.0, 2.0, 0.3227592508025283, 6.9112, 6.9112, 121.94756008, 693131.126659295, 693131.126659295, 221489.7957183024]
[2019-04-27 18:04:48,104] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 18:04:48,107] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [6.3759236e-13 2.6035460e-09 3.4756122e-06 4.2098438e-12 9.9999654e-01], sampled 0.20291497520979518
[2019-04-27 18:05:14,147] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0252527], dtype=float32), 0.036294956]
[2019-04-27 18:05:14,147] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [27.58333333333334, 66.0, 1.0, 2.0, 0.1912748021647263, 1.0, 2.0, 0.1912748021647263, 1.0, 2.0, 0.3046162252070834, 6.9112, 6.9112, 121.94756008, 657426.4955538262, 657426.4955538262, 217774.2257462171]
[2019-04-27 18:05:14,148] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 18:05:14,152] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [5.7909710e-13 2.1800723e-09 2.8667680e-06 3.6516250e-12 9.9999714e-01], sampled 0.034938264970737376
[2019-04-27 18:05:30,575] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.0252527], dtype=float32), 0.036294956]
[2019-04-27 18:05:30,576] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [19.6, 88.0, 1.0, 2.0, 0.16, 1.0, 2.0, 0.16, 1.0, 2.0, 0.2, 6.9112, 6.9112, 121.94756008, 429646.9592272844, 429646.9592272844, 183623.9435012844]
[2019-04-27 18:05:30,577] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 18:05:30,579] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [2.3338302e-12 8.9125214e-09 9.6714848e-06 1.7035054e-11 9.9999034e-01], sampled 0.060576698745776314
[2019-04-27 18:05:31,046] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 4609.0871 2894632381.2833 12.0000
[2019-04-27 18:05:31,150] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 4512.3165 3107438565.8250 0.0000
[2019-04-27 18:05:31,240] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 4392.4157 2875782614.0950 8.0000
[2019-04-27 18:05:31,296] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 4489.4114 2940759151.0289 28.0000
[2019-04-27 18:05:31,384] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 4275.9573 2920152679.8633 33.0000
[2019-04-27 18:05:32,400] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 125000, evaluation results [125000.0, 4512.316459283687, 3107438565.8249564, 0.0, 4609.087071503921, 2894632381.283263, 12.0, 4392.415652321868, 2875782614.094957, 8.0, 4489.411365254514, 2940759151.0289044, 28.0, 4275.9572757857495, 2920152679.8633223, 33.0]
[2019-04-27 18:05:32,658] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.628613e-13 5.672010e-10 1.324198e-06 8.660704e-13 9.999987e-01], sum to 1.0000
[2019-04-27 18:05:32,663] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4674
[2019-04-27 18:05:32,666] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.0, 100.0, 1.0, 2.0, 0.2051441753578555, 1.0, 2.0, 0.2051441753578555, 1.0, 2.0, 0.3265962082490669, 6.911199999999999, 6.9112, 121.94756008, 701374.8286200545, 701374.828620055, 222282.3192597977], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4690800.0000, 
sim time next is 4691400.0000, 
raw observation next is [23.3, 98.33333333333334, 1.0, 2.0, 0.2218200579681915, 1.0, 2.0, 0.2218200579681915, 1.0, 2.0, 0.3531447564603031, 6.911200000000001, 6.9112, 121.94756008, 758416.8053016441, 758416.8053016437, 227854.9805575551], 
processed observation next is [1.0, 0.30434782608695654, 0.41851851851851857, 0.9833333333333334, 1.0, 1.0, 0.07359530710498986, 1.0, 1.0, 0.07359530710498986, 1.0, 1.0, 0.19143094557537882, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.2708631447505872, 0.27086314475058704, 0.4381826549183752], 
reward next is 0.5618, 
noisyNet noise sample is [array([-0.16193666], dtype=float32), 0.16970485]. 
=============================================
[2019-04-27 18:05:38,040] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8000, global step 127697: loss 0.1618
[2019-04-27 18:05:38,042] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8000, global step 127697: learning rate 0.0001
[2019-04-27 18:05:38,180] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8000, global step 127765: loss 0.1476
[2019-04-27 18:05:38,186] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8000, global step 127768: learning rate 0.0001
[2019-04-27 18:05:38,243] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8000, global step 127790: loss 0.1200
[2019-04-27 18:05:38,249] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8000, global step 127791: learning rate 0.0001
[2019-04-27 18:05:38,271] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8000, global step 127804: loss 0.1541
[2019-04-27 18:05:38,276] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8000, global step 127806: learning rate 0.0001
[2019-04-27 18:05:38,410] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8000, global step 127870: loss 0.1094
[2019-04-27 18:05:38,411] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8000, global step 127870: learning rate 0.0001
[2019-04-27 18:05:38,536] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8000, global step 127942: loss 0.0948
[2019-04-27 18:05:38,537] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8000, global step 127942: learning rate 0.0001
[2019-04-27 18:05:38,633] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8000, global step 127990: loss 0.0910
[2019-04-27 18:05:38,635] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8000, global step 127990: learning rate 0.0001
[2019-04-27 18:05:38,642] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8000, global step 127994: loss 0.0857
[2019-04-27 18:05:38,646] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8000, global step 127995: learning rate 0.0001
[2019-04-27 18:05:38,712] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8000, global step 128025: loss 0.0874
[2019-04-27 18:05:38,713] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8000, global step 128025: learning rate 0.0001
[2019-04-27 18:05:38,738] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8000, global step 128032: loss 0.0826
[2019-04-27 18:05:38,740] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8000, global step 128032: learning rate 0.0001
[2019-04-27 18:05:38,873] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8000, global step 128096: loss 0.0788
[2019-04-27 18:05:38,877] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8000, global step 128098: learning rate 0.0001
[2019-04-27 18:05:38,882] A3C_AGENT_WORKER-Thread-22 INFO:Local step 8000, global step 128100: loss 0.0801
[2019-04-27 18:05:38,886] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 8000, global step 128100: learning rate 0.0001
[2019-04-27 18:05:38,892] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8000, global step 128102: loss 0.0824
[2019-04-27 18:05:38,896] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8000, global step 128102: learning rate 0.0001
[2019-04-27 18:05:39,006] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8000, global step 128161: loss 0.0842
[2019-04-27 18:05:39,013] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8000, global step 128164: learning rate 0.0001
[2019-04-27 18:05:39,083] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8000, global step 128193: loss 0.0807
[2019-04-27 18:05:39,086] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8000, global step 128193: learning rate 0.0001
[2019-04-27 18:05:39,252] A3C_AGENT_WORKER-Thread-21 INFO:Local step 8000, global step 128275: loss 0.0773
[2019-04-27 18:05:39,255] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 8000, global step 128277: learning rate 0.0001
[2019-04-27 18:05:43,276] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.5306074e-08 1.3444303e-06 6.5304989e-06 3.0887573e-08 9.9999201e-01], sum to 1.0000
[2019-04-27 18:05:43,283] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5068
[2019-04-27 18:05:43,288] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.16666666666667, 89.0, 1.0, 2.0, 0.557475535782572, 1.0, 2.0, 0.557475535782572, 1.0, 2.0, 0.887519208676539, 6.9112, 6.9112, 121.94756008, 1907414.940063538, 1907414.940063538, 372693.3763082104], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4882200.0000, 
sim time next is 4882800.0000, 
raw observation next is [28.33333333333334, 89.0, 1.0, 2.0, 0.4624108395297605, 1.0, 2.0, 0.4624108395297605, 1.0, 2.0, 0.736173116201052, 6.9112, 6.9112, 121.94756008, 1581860.998421497, 1581860.998421497, 325499.5776998006], 
processed observation next is [1.0, 0.5217391304347826, 0.6049382716049385, 0.89, 1.0, 1.0, 0.3600129042020958, 1.0, 1.0, 0.3600129042020958, 1.0, 1.0, 0.6702163952513148, 0.0, 0.0, 0.8096049824067558, 0.5649503565791061, 0.5649503565791061, 0.6259607263457704], 
reward next is 0.3740, 
noisyNet noise sample is [array([0.22380511], dtype=float32), -1.2673308]. 
=============================================
[2019-04-27 18:05:45,070] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.7777452e-11 9.8502762e-09 6.3092652e-05 1.7422739e-11 9.9993694e-01], sum to 1.0000
[2019-04-27 18:05:45,078] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6718
[2019-04-27 18:05:45,083] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.0, 94.0, 1.0, 2.0, 0.2936980750765338, 1.0, 2.0, 0.2936980750765338, 1.0, 2.0, 0.4675769006003741, 6.911199999999999, 6.9112, 121.94756008, 1004333.289015863, 1004333.289015863, 253653.3001928301], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4914000.0000, 
sim time next is 4914600.0000, 
raw observation next is [27.83333333333334, 94.00000000000001, 1.0, 2.0, 0.2911744822613531, 1.0, 2.0, 0.2911744822613531, 1.0, 2.0, 0.4635592586509262, 6.9112, 6.9112, 121.94756008, 995697.9765249535, 995697.9765249535, 252698.7016800008], 
processed observation next is [1.0, 0.9130434782608695, 0.58641975308642, 0.9400000000000002, 1.0, 1.0, 0.1561600979301823, 1.0, 1.0, 0.1561600979301823, 1.0, 1.0, 0.3294490733136577, 0.0, 0.0, 0.8096049824067558, 0.3556064201874834, 0.3556064201874834, 0.48595904169230925], 
reward next is 0.5140, 
noisyNet noise sample is [array([-0.47472626], dtype=float32), -1.0937569]. 
=============================================
[2019-04-27 18:05:45,866] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.42197383e-13 1.04663389e-08 1.37021025e-05 4.41569142e-11
 9.99986291e-01], sum to 1.0000
[2019-04-27 18:05:45,878] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1495
[2019-04-27 18:05:45,991] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.95, 77.0, 1.0, 2.0, 0.2134825290193069, 1.0, 2.0, 0.2134825290193069, 1.0, 2.0, 0.3398711388393176, 6.9112, 6.9112, 121.94756008, 729896.6969814841, 729896.6969814841, 225049.3377429527], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4930200.0000, 
sim time next is 4930800.0000, 
raw observation next is [26.93333333333333, 76.33333333333334, 1.0, 2.0, 0.2108379952534101, 1.0, 2.0, 0.2108379952534101, 1.0, 2.0, 0.335660954957558, 6.9112, 6.9112, 121.94756008, 720850.7863574657, 720850.7863574657, 224167.5562124696], 
processed observation next is [1.0, 0.043478260869565216, 0.5530864197530863, 0.7633333333333334, 1.0, 1.0, 0.060521422920726306, 1.0, 1.0, 0.060521422920726306, 1.0, 1.0, 0.16957619369694746, 0.0, 0.0, 0.8096049824067558, 0.2574467094133806, 0.2574467094133806, 0.4310914542547492], 
reward next is 0.5689, 
noisyNet noise sample is [array([-0.8431105], dtype=float32), -0.15194063]. 
=============================================
[2019-04-27 18:05:52,711] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.8162863e-15 2.1474489e-10 3.5239430e-07 1.6329429e-13 9.9999964e-01], sum to 1.0000
[2019-04-27 18:05:52,723] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2397
[2019-04-27 18:05:52,734] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.25, 74.0, 1.0, 2.0, 0.2347974643267543, 1.0, 2.0, 0.2347974643267543, 1.0, 2.0, 0.3738052100277537, 6.9112, 6.9112, 121.94756008, 802810.6104801185, 802810.6104801185, 232299.3299583682], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5051400.0000, 
sim time next is 5052000.0000, 
raw observation next is [29.4, 75.0, 1.0, 2.0, 0.2393748830356985, 1.0, 2.0, 0.2393748830356985, 1.0, 2.0, 0.3810926096885126, 6.911200000000001, 6.9112, 121.94756008, 818469.902338496, 818469.9023384955, 233889.4119886528], 
processed observation next is [0.0, 0.4782608695652174, 0.6444444444444444, 0.75, 1.0, 1.0, 0.09449390837583156, 1.0, 1.0, 0.09449390837583156, 1.0, 1.0, 0.22636576211064074, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.2923106794066057, 0.29231067940660554, 0.44978733074740923], 
reward next is 0.5502, 
noisyNet noise sample is [array([-0.16355796], dtype=float32), -0.07622029]. 
=============================================
[2019-04-27 18:05:52,753] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[60.018414]
 [59.953655]
 [59.883034]
 [59.84965 ]
 [59.81611 ]], R is [[60.00371933]
 [59.95695496]
 [59.90966797]
 [59.86283875]
 [59.81649399]].
[2019-04-27 18:05:53,918] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.0715641e-14 4.6831478e-10 2.4721695e-07 4.2530642e-14 9.9999976e-01], sum to 1.0000
[2019-04-27 18:05:53,926] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2310
[2019-04-27 18:05:53,931] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.81666666666667, 70.16666666666667, 1.0, 2.0, 0.2721346504580001, 1.0, 2.0, 0.2721346504580001, 1.0, 2.0, 0.4332472263359567, 6.911199999999999, 6.9112, 121.94756008, 930549.9928193301, 930549.9928193305, 245610.973614909], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5064600.0000, 
sim time next is 5065200.0000, 
raw observation next is [32.0, 71.0, 1.0, 2.0, 0.2877719636304494, 1.0, 2.0, 0.2877719636304494, 1.0, 2.0, 0.458142338178232, 6.9112, 6.9112, 121.94756008, 984055.2809338773, 984055.2809338773, 251417.2522316525], 
processed observation next is [0.0, 0.6521739130434783, 0.7407407407407407, 0.71, 1.0, 1.0, 0.15210948051243975, 1.0, 1.0, 0.15210948051243975, 1.0, 1.0, 0.32267792272279, 0.0, 0.0, 0.8096049824067558, 0.3514483146192419, 0.3514483146192419, 0.48349471583010095], 
reward next is 0.5165, 
noisyNet noise sample is [array([3.0013301], dtype=float32), -1.1213623]. 
=============================================
[2019-04-27 18:05:54,747] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8500, global step 135654: loss 1.3604
[2019-04-27 18:05:54,750] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8500, global step 135655: learning rate 0.0001
[2019-04-27 18:05:55,025] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8500, global step 135791: loss 1.1732
[2019-04-27 18:05:55,027] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8500, global step 135791: learning rate 0.0001
[2019-04-27 18:05:55,066] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8500, global step 135806: loss 1.8071
[2019-04-27 18:05:55,066] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8500, global step 135806: loss 1.8965
[2019-04-27 18:05:55,069] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8500, global step 135806: learning rate 0.0001
[2019-04-27 18:05:55,070] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8500, global step 135806: learning rate 0.0001
[2019-04-27 18:05:55,224] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8500, global step 135879: loss 2.1513
[2019-04-27 18:05:55,225] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8500, global step 135879: learning rate 0.0001
[2019-04-27 18:05:55,330] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8500, global step 135928: loss 1.7249
[2019-04-27 18:05:55,334] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8500, global step 135928: loss 1.5860
[2019-04-27 18:05:55,336] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8500, global step 135929: learning rate 0.0001
[2019-04-27 18:05:55,338] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8500, global step 135929: learning rate 0.0001
[2019-04-27 18:05:55,399] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8500, global step 135957: loss 1.2661
[2019-04-27 18:05:55,402] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8500, global step 135958: learning rate 0.0001
[2019-04-27 18:05:55,523] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8500, global step 136020: loss 0.8999
[2019-04-27 18:05:55,524] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8500, global step 136020: learning rate 0.0001
[2019-04-27 18:05:55,642] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8500, global step 136083: loss 1.3743
[2019-04-27 18:05:55,644] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8500, global step 136083: learning rate 0.0001
[2019-04-27 18:05:55,688] A3C_AGENT_WORKER-Thread-22 INFO:Local step 8500, global step 136105: loss 1.4026
[2019-04-27 18:05:55,690] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 8500, global step 136106: learning rate 0.0001
[2019-04-27 18:05:55,696] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8500, global step 136109: loss 1.7485
[2019-04-27 18:05:55,699] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8500, global step 136111: learning rate 0.0001
[2019-04-27 18:05:55,736] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8500, global step 136127: loss 1.8005
[2019-04-27 18:05:55,739] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8500, global step 136127: learning rate 0.0001
[2019-04-27 18:05:55,779] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8500, global step 136149: loss 1.8283
[2019-04-27 18:05:55,782] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8500, global step 136149: learning rate 0.0001
[2019-04-27 18:05:55,829] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8500, global step 136173: loss 1.8855
[2019-04-27 18:05:55,832] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8500, global step 136175: learning rate 0.0001
[2019-04-27 18:05:56,087] A3C_AGENT_WORKER-Thread-21 INFO:Local step 8500, global step 136293: loss 0.9718
[2019-04-27 18:05:56,093] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 8500, global step 136294: learning rate 0.0001
[2019-04-27 18:06:02,579] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.79102641e-08 1.45148215e-05 4.95543900e-05 9.80236479e-08
 9.99935746e-01], sum to 1.0000
[2019-04-27 18:06:02,587] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7655
[2019-04-27 18:06:02,594] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.15, 82.16666666666667, 1.0, 2.0, 0.578462471790476, 1.0, 2.0, 0.578462471790476, 1.0, 2.0, 0.9209310942979105, 6.911199999999999, 6.9112, 121.94756008, 1979301.765787827, 1979301.765787828, 383765.7724029685], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5220600.0000, 
sim time next is 5221200.0000, 
raw observation next is [27.3, 80.33333333333334, 1.0, 2.0, 0.567570732424966, 1.0, 2.0, 0.567570732424966, 1.0, 2.0, 0.9035910905088317, 6.911199999999999, 6.9112, 121.94756008, 1941993.426708684, 1941993.426708685, 377989.7823080524], 
processed observation next is [1.0, 0.43478260869565216, 0.5666666666666667, 0.8033333333333335, 1.0, 1.0, 0.4852032528868643, 1.0, 1.0, 0.4852032528868643, 1.0, 1.0, 0.8794888631360395, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.6935690809673871, 0.6935690809673875, 0.7269034275154854], 
reward next is 0.2731, 
noisyNet noise sample is [array([-5.9911406e-05], dtype=float32), 0.5946837]. 
=============================================
[2019-04-27 18:06:05,914] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.5641859e-13 2.3725386e-10 4.1658561e-07 3.7449670e-12 9.9999964e-01], sum to 1.0000
[2019-04-27 18:06:05,925] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9769
[2019-04-27 18:06:05,932] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.76666666666667, 84.66666666666667, 1.0, 2.0, 0.2375432570897025, 1.0, 2.0, 0.2375432570897025, 1.0, 2.0, 0.3781766015305945, 6.911199999999999, 6.9112, 121.94756008, 812203.8946144301, 812203.8946144306, 233251.7449829931], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5283600.0000, 
sim time next is 5284200.0000, 
raw observation next is [24.68333333333334, 84.83333333333334, 1.0, 2.0, 0.2318774769923115, 1.0, 2.0, 0.2318774769923115, 1.0, 2.0, 0.3691615687426552, 6.9112, 6.9112, 121.94756008, 793073.8902701619, 793073.8902701619, 231292.380999756], 
processed observation next is [1.0, 0.13043478260869565, 0.4697530864197534, 0.8483333333333334, 1.0, 1.0, 0.08556842499084703, 1.0, 1.0, 0.08556842499084703, 1.0, 1.0, 0.211451960928319, 0.0, 0.0, 0.8096049824067558, 0.28324067509648637, 0.28324067509648637, 0.44479304038414613], 
reward next is 0.5552, 
noisyNet noise sample is [array([0.21805972], dtype=float32), 0.5746842]. 
=============================================
[2019-04-27 18:06:11,397] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9000, global step 143608: loss 0.1314
[2019-04-27 18:06:11,400] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9000, global step 143608: learning rate 0.0001
[2019-04-27 18:06:11,668] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9000, global step 143732: loss 0.0659
[2019-04-27 18:06:11,671] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9000, global step 143732: learning rate 0.0001
[2019-04-27 18:06:11,790] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9000, global step 143795: loss 0.0616
[2019-04-27 18:06:11,794] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9000, global step 143796: learning rate 0.0001
[2019-04-27 18:06:11,849] A3C_AGENT_WORKER-Thread-9 INFO:Local step 9000, global step 143824: loss 0.0371
[2019-04-27 18:06:11,853] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 9000, global step 143826: learning rate 0.0001
[2019-04-27 18:06:11,918] A3C_AGENT_WORKER-Thread-10 INFO:Local step 9000, global step 143856: loss 0.0347
[2019-04-27 18:06:11,921] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 9000, global step 143858: learning rate 0.0001
[2019-04-27 18:06:12,011] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9000, global step 143903: loss 0.0133
[2019-04-27 18:06:12,013] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9000, global step 143903: learning rate 0.0001
[2019-04-27 18:06:12,068] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9000, global step 143928: loss 0.0120
[2019-04-27 18:06:12,075] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9000, global step 143929: learning rate 0.0001
[2019-04-27 18:06:12,256] A3C_AGENT_WORKER-Thread-19 INFO:Local step 9000, global step 144012: loss 0.0041
[2019-04-27 18:06:12,260] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 9000, global step 144012: learning rate 0.0001
[2019-04-27 18:06:12,282] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9000, global step 144025: loss 0.0075
[2019-04-27 18:06:12,284] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9000, global step 144026: learning rate 0.0001
[2019-04-27 18:06:12,336] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9000, global step 144047: loss 0.0085
[2019-04-27 18:06:12,338] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9000, global step 144048: learning rate 0.0001
[2019-04-27 18:06:12,462] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9000, global step 144110: loss 0.0195
[2019-04-27 18:06:12,464] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9000, global step 144110: learning rate 0.0001
[2019-04-27 18:06:12,527] A3C_AGENT_WORKER-Thread-22 INFO:Local step 9000, global step 144142: loss 0.0188
[2019-04-27 18:06:12,534] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 9000, global step 144143: learning rate 0.0001
[2019-04-27 18:06:12,558] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9000, global step 144158: loss 0.0264
[2019-04-27 18:06:12,564] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9000, global step 144158: learning rate 0.0001
[2019-04-27 18:06:12,594] A3C_AGENT_WORKER-Thread-20 INFO:Local step 9000, global step 144173: loss 0.0389
[2019-04-27 18:06:12,598] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 9000, global step 144174: learning rate 0.0001
[2019-04-27 18:06:12,652] A3C_AGENT_WORKER-Thread-3 INFO:Local step 9000, global step 144196: loss 0.0352
[2019-04-27 18:06:12,656] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 9000, global step 144196: learning rate 0.0001
[2019-04-27 18:06:12,863] A3C_AGENT_WORKER-Thread-21 INFO:Local step 9000, global step 144300: loss 0.0432
[2019-04-27 18:06:12,866] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 9000, global step 144300: learning rate 0.0001
[2019-04-27 18:06:14,176] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.0570426e-13 1.5774411e-09 1.7471993e-07 2.0452429e-12 9.9999988e-01], sum to 1.0000
[2019-04-27 18:06:14,184] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8959
[2019-04-27 18:06:14,189] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.8, 81.0, 1.0, 2.0, 0.2575380667794914, 1.0, 2.0, 0.2575380667794914, 1.0, 2.0, 0.4100089897422284, 6.9112, 6.9112, 121.94756008, 880609.0786725722, 880609.0786725722, 240314.2428456016], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5428800.0000, 
sim time next is 5429400.0000, 
raw observation next is [28.71666666666667, 81.5, 1.0, 2.0, 0.2578398868984877, 1.0, 2.0, 0.2578398868984877, 1.0, 2.0, 0.4104894972012656, 6.9112, 6.9112, 121.94756008, 881641.6964490429, 881641.6964490429, 240422.5615052803], 
processed observation next is [1.0, 0.8695652173913043, 0.6191358024691359, 0.815, 1.0, 1.0, 0.11647605583153296, 1.0, 1.0, 0.11647605583153296, 1.0, 1.0, 0.26311187150158194, 0.0, 0.0, 0.8096049824067558, 0.31487203444608675, 0.31487203444608675, 0.46235107981784673], 
reward next is 0.5376, 
noisyNet noise sample is [array([-1.9165972], dtype=float32), 0.8186205]. 
=============================================
[2019-04-27 18:06:15,482] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.9442978e-12 6.9250450e-10 4.8443121e-07 7.3388509e-12 9.9999952e-01], sum to 1.0000
[2019-04-27 18:06:15,487] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3918
[2019-04-27 18:06:15,490] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.36666666666667, 94.33333333333334, 1.0, 2.0, 0.3015138862879022, 1.0, 2.0, 0.3015138862879022, 1.0, 2.0, 0.4800199265920732, 6.911199999999999, 6.9112, 121.94756008, 1031078.304947575, 1031078.304947575, 256632.302446108], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5463600.0000, 
sim time next is 5464200.0000, 
raw observation next is [26.43333333333334, 94.16666666666667, 1.0, 2.0, 0.3024013971359892, 1.0, 2.0, 0.3024013971359892, 1.0, 2.0, 0.4814328727664381, 6.9112, 6.9112, 121.94756008, 1034115.347723678, 1034115.347723678, 256972.7298599915], 
processed observation next is [1.0, 0.21739130434782608, 0.5345679012345682, 0.9416666666666668, 1.0, 1.0, 0.1695254727809395, 1.0, 1.0, 0.1695254727809395, 1.0, 1.0, 0.35179109095804756, 0.0, 0.0, 0.8096049824067558, 0.36932690990131356, 0.36932690990131356, 0.49417832665382977], 
reward next is 0.5058, 
noisyNet noise sample is [array([0.08217181], dtype=float32), -0.1049728]. 
=============================================
[2019-04-27 18:06:17,165] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5114277e-07 1.2343652e-06 1.1912513e-05 5.9076420e-08 9.9998665e-01], sum to 1.0000
[2019-04-27 18:06:17,175] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3401
[2019-04-27 18:06:17,182] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.1, 75.0, 1.0, 2.0, 0.6012480716488781, 1.0, 2.0, 0.6012480716488781, 1.0, 2.0, 0.957206511347667, 6.911199999999999, 6.9112, 121.94756008, 2057356.129441814, 2057356.129441814, 396056.0281513052], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5500800.0000, 
sim time next is 5501400.0000, 
raw observation next is [28.75, 76.0, 1.0, 2.0, 0.6436356305578742, 1.0, 2.0, 0.6351824772553719, 1.0, 2.0, 0.9977734948820727, 6.911200000000001, 6.9112, 121.94756008, 2173614.494605315, 2173614.494605314, 414437.4252732024], 
processed observation next is [1.0, 0.6956521739130435, 0.6203703703703703, 0.76, 1.0, 1.0, 0.5757567030450883, 1.0, 1.0, 0.5656934253040141, 1.0, 1.0, 0.9972168686025908, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.7762908909304697, 0.7762908909304693, 0.7969950486023123], 
reward next is 0.2030, 
noisyNet noise sample is [array([-0.11722115], dtype=float32), 0.08656079]. 
=============================================
[2019-04-27 18:06:18,304] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.8990397e-10 7.4342292e-09 2.3902203e-07 3.4017153e-10 9.9999976e-01], sum to 1.0000
[2019-04-27 18:06:18,312] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4714
[2019-04-27 18:06:18,321] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.05, 78.0, 1.0, 2.0, 0.5632981284925997, 1.0, 2.0, 0.5632981284925997, 1.0, 2.0, 0.896788965899509, 6.9112, 6.9112, 121.94756008, 1927358.562966214, 1927358.562966214, 375741.461669028], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5502600.0000, 
sim time next is 5503200.0000, 
raw observation next is [27.7, 79.0, 1.0, 2.0, 0.5223984489512847, 1.0, 2.0, 0.5223984489512847, 1.0, 2.0, 0.8316753440601653, 6.911199999999999, 6.9112, 121.94756008, 1787277.850079181, 1787277.850079182, 354718.4684271293], 
processed observation next is [1.0, 0.6956521739130435, 0.5814814814814815, 0.79, 1.0, 1.0, 0.4314267249420056, 1.0, 1.0, 0.4314267249420056, 1.0, 1.0, 0.7895941800752065, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.6383135178854218, 0.6383135178854221, 0.6821509008214025], 
reward next is 0.3178, 
noisyNet noise sample is [array([-0.03112664], dtype=float32), -1.1410607]. 
=============================================
[2019-04-27 18:06:23,082] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.46772081e-10 1.88754257e-08 7.31705711e-07 1.14693206e-10
 9.99999285e-01], sum to 1.0000
[2019-04-27 18:06:23,089] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1524
[2019-04-27 18:06:23,094] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.2, 74.83333333333334, 1.0, 2.0, 0.8322669011641528, 1.0, 2.0, 0.7294981125585112, 1.0, 2.0, 0.9977734948820727, 6.911199999999999, 6.9112, 122.9355304227544, 2496788.795999139, 2496788.79599914, 466961.5438005166], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5580600.0000, 
sim time next is 5581200.0000, 
raw observation next is [29.9, 75.66666666666667, 1.0, 2.0, 0.8080470020141212, 1.0, 2.0, 0.7173881629834952, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.94756008, 2455311.735970824, 2455311.735970824, 459536.059084869], 
processed observation next is [1.0, 0.6086956521739131, 0.6629629629629629, 0.7566666666666667, 1.0, 1.0, 0.7714845262072871, 1.0, 1.0, 0.6635573368851133, 1.0, 1.0, 0.9972168686025908, 0.0, 0.0, 0.8096049824067558, 0.8768970485610086, 0.8768970485610086, 0.883723190547825], 
reward next is 0.1163, 
noisyNet noise sample is [array([0.8089255], dtype=float32), -0.4395571]. 
=============================================
[2019-04-27 18:06:24,272] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.08014956e-13 8.71412997e-10 3.06220670e-07 1.00571996e-13
 9.99999642e-01], sum to 1.0000
[2019-04-27 18:06:24,281] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9268
[2019-04-27 18:06:24,285] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.58333333333334, 97.0, 1.0, 2.0, 0.2011532688160659, 1.0, 2.0, 0.2011532688160659, 1.0, 2.0, 0.3202425550597858, 6.911200000000001, 6.9112, 121.94756008, 687724.0547203819, 687724.0547203814, 220971.7381099183], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5620200.0000, 
sim time next is 5620800.0000, 
raw observation next is [23.56666666666667, 97.0, 1.0, 2.0, 0.2006623458028395, 1.0, 2.0, 0.2006623458028395, 1.0, 2.0, 0.3194609896344831, 6.9112, 6.9112, 121.94756008, 686044.8843387, 686044.8843387, 220811.1388752305], 
processed observation next is [0.0, 0.043478260869565216, 0.4283950617283952, 0.97, 1.0, 1.0, 0.04840755452718989, 1.0, 1.0, 0.04840755452718989, 1.0, 1.0, 0.14932623704310385, 0.0, 0.0, 0.8096049824067558, 0.2450160301209643, 0.2450160301209643, 0.42463680552928945], 
reward next is 0.5754, 
noisyNet noise sample is [array([0.04571754], dtype=float32), 0.27060857]. 
=============================================
[2019-04-27 18:06:24,848] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-27 18:06:24,849] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 18:06:24,850] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 18:06:24,850] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:06:24,852] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 18:06:24,851] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 18:06:24,855] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:06:24,853] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 18:06:24,855] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:06:24,856] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:06:24,858] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:06:24,876] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run7
[2019-04-27 18:06:24,877] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run7
[2019-04-27 18:06:24,877] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run7
[2019-04-27 18:06:24,896] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run7
[2019-04-27 18:06:24,946] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run7
[2019-04-27 18:06:39,713] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0252527], dtype=float32), 0.033393595]
[2019-04-27 18:06:39,714] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [22.34612420666667, 73.22976096666667, 1.0, 2.0, 0.1941328841550211, 1.0, 2.0, 0.1941328841550211, 1.0, 2.0, 0.3162520614799042, 6.9112, 6.9112, 121.94756008, 708178.284600982, 708178.284600982, 217567.5842307126]
[2019-04-27 18:06:39,717] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:06:39,722] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [6.5358004e-11 2.5387628e-08 1.3099320e-05 3.8083692e-10 9.9998689e-01], sampled 0.16552094881357982
[2019-04-27 18:07:07,747] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0252527], dtype=float32), 0.033393595]
[2019-04-27 18:07:07,748] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [22.03333333333333, 92.33333333333334, 1.0, 2.0, 0.1612461421669242, 1.0, 2.0, 0.1612461421669242, 1.0, 2.0, 0.2587451147513837, 6.9112, 6.9112, 121.94756008, 572849.5381798918, 572849.5381798918, 208034.8223221856]
[2019-04-27 18:07:07,749] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 18:07:07,750] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [2.62046183e-11 1.32803555e-08 9.36233937e-06 1.83237786e-10
 9.99990582e-01], sampled 0.9321319930960855
[2019-04-27 18:07:17,026] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0252527], dtype=float32), 0.033393595]
[2019-04-27 18:07:17,027] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [33.16666666666666, 52.5, 1.0, 2.0, 0.2335627654401884, 1.0, 2.0, 0.2335627654401884, 1.0, 2.0, 0.3718395291890054, 6.9112, 6.9112, 121.94756008, 798586.7758872112, 798586.7758872112, 231872.4316731909]
[2019-04-27 18:07:17,028] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 18:07:17,030] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.0792801e-11 6.0989596e-09 5.5401724e-06 6.8004748e-11 9.9999440e-01], sampled 0.7518371512426869
[2019-04-27 18:07:28,482] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0252527], dtype=float32), 0.033393595]
[2019-04-27 18:07:28,485] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [21.1, 87.5, 1.0, 2.0, 0.16, 1.0, 2.0, 0.16, 1.0, 2.0, 0.2275048108629659, 6.911200000000001, 6.9112, 121.94756008, 509418.528684254, 509418.5286842536, 197146.9585685469]
[2019-04-27 18:07:28,486] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 18:07:28,487] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [6.7610227e-12 4.9971729e-09 5.9443414e-06 5.2493877e-11 9.9999404e-01], sampled 0.6193100042584199
[2019-04-27 18:08:03,465] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0252527], dtype=float32), 0.033393595]
[2019-04-27 18:08:03,466] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [26.0, 74.0, 1.0, 2.0, 0.1824337376094821, 1.0, 2.0, 0.1824337376094821, 1.0, 2.0, 0.2907321656145407, 6.911200000000001, 6.9112, 121.94756008, 630977.8137010031, 630977.8137010027, 214939.4919987171]
[2019-04-27 18:08:03,468] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 18:08:03,470] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [3.1127444e-11 1.5820204e-08 1.0618078e-05 2.1684345e-10 9.9998939e-01], sampled 0.9928075743322246
[2019-04-27 18:08:08,389] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 4609.0871 2894581360.9424 12.0000
[2019-04-27 18:08:08,454] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 4393.7495 2875724989.5005 8.0000
[2019-04-27 18:08:08,681] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 4491.0618 2940649000.2520 28.0000
[2019-04-27 18:08:08,717] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 4511.6288 3107345126.5194 0.0000
[2019-04-27 18:08:08,860] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 4278.2079 2919930870.4678 33.0000
[2019-04-27 18:08:09,874] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 150000, evaluation results [150000.0, 4511.628780081537, 3107345126.5194182, 0.0, 4609.087071503921, 2894581360.9424286, 12.0, 4393.7495302092475, 2875724989.5005326, 8.0, 4491.061819972539, 2940649000.2520275, 28.0, 4278.207880014976, 2919930870.467761, 33.0]
[2019-04-27 18:08:11,273] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1012265e-14 3.3218997e-11 2.3493913e-08 2.8926002e-13 1.0000000e+00], sum to 1.0000
[2019-04-27 18:08:11,284] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9667
[2019-04-27 18:08:11,287] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.6, 95.0, 1.0, 2.0, 0.2128692071750506, 1.0, 2.0, 0.2128692071750506, 1.0, 2.0, 0.3388947104887636, 6.9112, 6.9112, 121.94756008, 727798.7545019295, 727798.7545019295, 224844.4858943542], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5644800.0000, 
sim time next is 5645400.0000, 
raw observation next is [24.75, 94.16666666666667, 1.0, 2.0, 0.2137878025055323, 1.0, 2.0, 0.2137878025055323, 1.0, 2.0, 0.3403571441714519, 6.9112, 6.9112, 121.94756008, 730940.9244666635, 730940.9244666635, 225151.3787126241], 
processed observation next is [0.0, 0.34782608695652173, 0.4722222222222222, 0.9416666666666668, 1.0, 1.0, 0.06403309822087179, 1.0, 1.0, 0.06403309822087179, 1.0, 1.0, 0.17544643021431486, 0.0, 0.0, 0.8096049824067558, 0.2610503301666655, 0.2610503301666655, 0.4329834206012002], 
reward next is 0.5670, 
noisyNet noise sample is [array([-0.15223], dtype=float32), -1.707363]. 
=============================================
[2019-04-27 18:08:12,865] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.4121626e-15 7.5649868e-12 3.8686991e-08 5.7820201e-15 1.0000000e+00], sum to 1.0000
[2019-04-27 18:08:12,874] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5424
[2019-04-27 18:08:12,880] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.9, 65.0, 1.0, 2.0, 0.2414715896949347, 1.0, 2.0, 0.2414715896949347, 1.0, 2.0, 0.3844306349749853, 6.9112, 6.9112, 121.94756008, 825642.8162785824, 825642.8162785824, 234621.6663948785], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5677200.0000, 
sim time next is 5677800.0000, 
raw observation next is [30.73333333333333, 66.16666666666667, 1.0, 2.0, 0.2391316055129566, 1.0, 2.0, 0.2391316055129566, 1.0, 2.0, 0.3807053039493118, 6.9112, 6.9112, 121.94756008, 817637.6449431264, 817637.6449431264, 233804.6087440434], 
processed observation next is [0.0, 0.7391304347826086, 0.6938271604938271, 0.6616666666666667, 1.0, 1.0, 0.09420429227732928, 1.0, 1.0, 0.09420429227732928, 1.0, 1.0, 0.2258816299366397, 0.0, 0.0, 0.8096049824067558, 0.29201344462254514, 0.29201344462254514, 0.4496242475846988], 
reward next is 0.5504, 
noisyNet noise sample is [array([0.31857023], dtype=float32), -0.36733502]. 
=============================================
[2019-04-27 18:08:13,201] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9500, global step 151602: loss 0.0098
[2019-04-27 18:08:13,202] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9500, global step 151602: learning rate 0.0001
[2019-04-27 18:08:13,513] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9500, global step 151751: loss 0.0024
[2019-04-27 18:08:13,516] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9500, global step 151751: learning rate 0.0001
[2019-04-27 18:08:13,554] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9500, global step 151769: loss 0.0049
[2019-04-27 18:08:13,556] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9500, global step 151769: learning rate 0.0001
[2019-04-27 18:08:13,695] A3C_AGENT_WORKER-Thread-9 INFO:Local step 9500, global step 151836: loss 0.0006
[2019-04-27 18:08:13,697] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 9500, global step 151836: learning rate 0.0001
[2019-04-27 18:08:13,714] A3C_AGENT_WORKER-Thread-10 INFO:Local step 9500, global step 151841: loss 0.0016
[2019-04-27 18:08:13,722] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 9500, global step 151844: learning rate 0.0001
[2019-04-27 18:08:13,763] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9500, global step 151866: loss 0.0106
[2019-04-27 18:08:13,766] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9500, global step 151866: learning rate 0.0001
[2019-04-27 18:08:13,966] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9500, global step 151963: loss 0.0889
[2019-04-27 18:08:13,968] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9500, global step 151965: learning rate 0.0001
[2019-04-27 18:08:14,092] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9500, global step 152021: loss 0.0226
[2019-04-27 18:08:14,093] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9500, global step 152021: learning rate 0.0001
[2019-04-27 18:08:14,165] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9500, global step 152059: loss 0.0097
[2019-04-27 18:08:14,168] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9500, global step 152059: learning rate 0.0001
[2019-04-27 18:08:14,177] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9500, global step 152062: loss 0.0012
[2019-04-27 18:08:14,179] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9500, global step 152062: learning rate 0.0001
[2019-04-27 18:08:14,210] A3C_AGENT_WORKER-Thread-19 INFO:Local step 9500, global step 152076: loss 0.0025
[2019-04-27 18:08:14,212] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 9500, global step 152076: learning rate 0.0001
[2019-04-27 18:08:14,300] A3C_AGENT_WORKER-Thread-22 INFO:Local step 9500, global step 152127: loss 0.0057
[2019-04-27 18:08:14,303] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 9500, global step 152130: learning rate 0.0001
[2019-04-27 18:08:14,318] A3C_AGENT_WORKER-Thread-3 INFO:Local step 9500, global step 152139: loss 0.0007
[2019-04-27 18:08:14,320] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 9500, global step 152139: learning rate 0.0001
[2019-04-27 18:08:14,326] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9500, global step 152141: loss 0.0051
[2019-04-27 18:08:14,328] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9500, global step 152143: learning rate 0.0001
[2019-04-27 18:08:14,434] A3C_AGENT_WORKER-Thread-20 INFO:Local step 9500, global step 152189: loss 0.0007
[2019-04-27 18:08:14,438] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 9500, global step 152190: learning rate 0.0001
[2019-04-27 18:08:14,617] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.275314e-12 5.873760e-09 5.560735e-07 6.469512e-12 9.999994e-01], sum to 1.0000
[2019-04-27 18:08:14,629] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3103
[2019-04-27 18:08:14,637] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [21.36666666666667, 97.0, 1.0, 2.0, 0.1638153170099683, 1.0, 2.0, 0.1638153170099683, 1.0, 2.0, 0.2630416223350938, 6.911199999999999, 6.9112, 121.94756008, 582931.275253518, 582931.2752535184, 208787.2938872305], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5716200.0000, 
sim time next is 5716800.0000, 
raw observation next is [21.3, 97.0, 1.0, 2.0, 0.1624347420993975, 1.0, 2.0, 0.1624347420993975, 1.0, 2.0, 0.260968806975009, 6.9112, 6.9112, 121.94756008, 578772.4657289751, 578772.4657289751, 208335.7585843765], 
processed observation next is [0.0, 0.17391304347826086, 0.3444444444444445, 0.97, 1.0, 1.0, 0.0028985024992827474, 1.0, 1.0, 0.0028985024992827474, 1.0, 1.0, 0.07621100871876126, 0.0, 0.0, 0.8096049824067558, 0.20670445204606255, 0.20670445204606255, 0.40064568958533947], 
reward next is 0.5994, 
noisyNet noise sample is [array([0.73631346], dtype=float32), 1.0133463]. 
=============================================
[2019-04-27 18:08:14,728] A3C_AGENT_WORKER-Thread-21 INFO:Local step 9500, global step 152324: loss 0.1388
[2019-04-27 18:08:14,730] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 9500, global step 152324: learning rate 0.0001
[2019-04-27 18:08:17,689] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.0460595e-07 1.4005963e-05 8.6611718e-01 6.3061674e-07 1.3386796e-01], sum to 1.0000
[2019-04-27 18:08:17,698] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9087
[2019-04-27 18:08:17,706] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.75, 62.0, 1.0, 2.0, 0.2639535038170516, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4220769402066469, 6.911199999999999, 6.9112, 121.9260426156618, 618746.1933695928, 618746.1933695932, 185341.2205358859], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5752200.0000, 
sim time next is 5752800.0000, 
raw observation next is [27.9, 62.0, 1.0, 2.0, 0.1798910586374466, 1.0, 1.0, 0.1798910586374466, 1.0, 2.0, 0.2869366467196426, 6.911199999999999, 6.9112, 121.94756008, 625736.7894853883, 625736.7894853888, 214106.2140835706], 
processed observation next is [0.0, 0.6086956521739131, 0.5888888888888888, 0.62, 1.0, 1.0, 0.023679831711245936, 1.0, 0.5, 0.023679831711245936, 1.0, 1.0, 0.10867080839955322, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.2234774248162101, 0.22347742481621027, 0.41174271939148194], 
reward next is 0.5883, 
noisyNet noise sample is [array([-1.1829768], dtype=float32), 0.21032545]. 
=============================================
[2019-04-27 18:08:20,720] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.5252680e-15 1.9591231e-10 9.9995565e-01 3.2315464e-13 4.4379995e-05], sum to 1.0000
[2019-04-27 18:08:20,730] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4280
[2019-04-27 18:08:20,735] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [22.58333333333334, 84.0, 1.0, 2.0, 0.2324943699135074, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3765643334947358, 6.911199999999999, 6.9112, 121.9260426156618, 560754.3700962414, 560754.3700962418, 176582.4657524467], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5813400.0000, 
sim time next is 5814000.0000, 
raw observation next is [22.7, 83.0, 1.0, 2.0, 0.2490089249192489, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4033066935360624, 6.911199999999999, 6.9112, 121.9260426156618, 600588.0397331796, 600588.0397331801, 180593.443197838], 
processed observation next is [1.0, 0.30434782608695654, 0.39629629629629626, 0.83, 1.0, 1.0, 0.10596300585624868, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.254133366920078, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.21449572847613557, 0.21449572847613574, 0.3472950830727654], 
reward next is 0.6527, 
noisyNet noise sample is [array([-0.07904186], dtype=float32), 0.2410224]. 
=============================================
[2019-04-27 18:08:20,746] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[67.31556 ]
 [67.28423 ]
 [67.279884]
 [67.28368 ]
 [67.29702 ]], R is [[67.27201843]
 [67.25971222]
 [67.24742126]
 [67.23421478]
 [67.21898651]].
[2019-04-27 18:08:23,374] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.8543390e-15 1.3250621e-10 9.9999321e-01 1.3115468e-14 6.7515789e-06], sum to 1.0000
[2019-04-27 18:08:23,383] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5080
[2019-04-27 18:08:23,387] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.28333333333333, 67.16666666666667, 1.0, 2.0, 0.2129651645712938, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3480237835932412, 6.911199999999999, 6.9112, 121.9260426156618, 519863.0729555225, 519863.0729555229, 171380.3571859711], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5863800.0000, 
sim time next is 5864400.0000, 
raw observation next is [24.1, 68.0, 1.0, 2.0, 0.2123638704054649, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3472499800693795, 6.9112, 6.9112, 121.9260426156618, 518759.2091526674, 518759.2091526674, 171202.3054047607], 
processed observation next is [1.0, 0.9130434782608695, 0.4481481481481482, 0.68, 1.0, 1.0, 0.06233794095888678, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.18406247508672438, 0.0, 0.0, 0.8094621288201359, 0.18527114612595263, 0.18527114612595263, 0.3292352027014629], 
reward next is 0.6708, 
noisyNet noise sample is [array([0.57515484], dtype=float32), -0.80225]. 
=============================================
[2019-04-27 18:08:25,235] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3256541e-11 8.1471609e-07 9.9842548e-01 2.6166399e-11 1.5737003e-03], sum to 1.0000
[2019-04-27 18:08:25,244] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2496
[2019-04-27 18:08:25,252] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.6, 64.0, 1.0, 2.0, 0.4880857874360287, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7943740179177993, 6.9112, 6.9112, 121.9260426156618, 1185916.738595904, 1185916.738595904, 250422.8381078985], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5910000.0000, 
sim time next is 5910600.0000, 
raw observation next is [24.85, 63.0, 1.0, 2.0, 0.4927800832464719, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8012988235199, 6.9112, 6.9112, 121.9260426156618, 1195895.702998184, 1195895.702998184, 252110.9223928703], 
processed observation next is [1.0, 0.391304347826087, 0.475925925925926, 0.63, 1.0, 1.0, 0.3961667657696094, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.7516235293998749, 0.0, 0.0, 0.8094621288201359, 0.42710560821363713, 0.42710560821363713, 0.484828696909366], 
reward next is 0.5152, 
noisyNet noise sample is [array([-1.1811684], dtype=float32), 1.1914322]. 
=============================================
[2019-04-27 18:08:30,090] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10000, global step 159648: loss 0.0434
[2019-04-27 18:08:30,091] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10000, global step 159648: learning rate 0.0001
[2019-04-27 18:08:30,173] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10000, global step 159691: loss 0.0573
[2019-04-27 18:08:30,175] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10000, global step 159691: learning rate 0.0001
[2019-04-27 18:08:30,208] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10000, global step 159708: loss 0.0391
[2019-04-27 18:08:30,212] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10000, global step 159709: learning rate 0.0001
[2019-04-27 18:08:30,405] A3C_AGENT_WORKER-Thread-10 INFO:Local step 10000, global step 159803: loss 0.0631
[2019-04-27 18:08:30,408] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 10000, global step 159803: learning rate 0.0001
[2019-04-27 18:08:30,515] A3C_AGENT_WORKER-Thread-9 INFO:Local step 10000, global step 159852: loss 0.0581
[2019-04-27 18:08:30,518] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 10000, global step 159853: learning rate 0.0001
[2019-04-27 18:08:30,595] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10000, global step 159891: loss 0.0559
[2019-04-27 18:08:30,597] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10000, global step 159891: learning rate 0.0001
[2019-04-27 18:08:30,767] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10000, global step 159975: loss 0.0286
[2019-04-27 18:08:30,768] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10000, global step 159975: learning rate 0.0001
[2019-04-27 18:08:30,960] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10000, global step 160063: loss 0.0274
[2019-04-27 18:08:30,963] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10000, global step 160064: learning rate 0.0001
[2019-04-27 18:08:30,965] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10000, global step 160066: loss 0.0260
[2019-04-27 18:08:30,967] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10000, global step 160066: learning rate 0.0001
[2019-04-27 18:08:30,970] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10000, global step 160067: loss 0.0262
[2019-04-27 18:08:30,971] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10000, global step 160067: learning rate 0.0001
[2019-04-27 18:08:31,041] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10000, global step 160100: loss 0.0275
[2019-04-27 18:08:31,046] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10000, global step 160102: learning rate 0.0001
[2019-04-27 18:08:31,073] A3C_AGENT_WORKER-Thread-19 INFO:Local step 10000, global step 160115: loss 0.0254
[2019-04-27 18:08:31,075] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 10000, global step 160116: learning rate 0.0001
[2019-04-27 18:08:31,154] A3C_AGENT_WORKER-Thread-20 INFO:Local step 10000, global step 160157: loss 0.0240
[2019-04-27 18:08:31,157] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 10000, global step 160157: learning rate 0.0001
[2019-04-27 18:08:31,166] A3C_AGENT_WORKER-Thread-3 INFO:Local step 10000, global step 160162: loss 0.0235
[2019-04-27 18:08:31,167] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 10000, global step 160162: learning rate 0.0001
[2019-04-27 18:08:31,195] A3C_AGENT_WORKER-Thread-22 INFO:Local step 10000, global step 160176: loss 0.0280
[2019-04-27 18:08:31,196] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 10000, global step 160176: learning rate 0.0001
[2019-04-27 18:08:31,518] A3C_AGENT_WORKER-Thread-21 INFO:Local step 10000, global step 160333: loss 0.0220
[2019-04-27 18:08:31,521] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 10000, global step 160334: learning rate 0.0001
[2019-04-27 18:08:35,633] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.6795984e-15 3.0217124e-12 9.9997389e-01 1.1074916e-14 2.6054548e-05], sum to 1.0000
[2019-04-27 18:08:35,639] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8454
[2019-04-27 18:08:35,646] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.51666666666667, 56.83333333333334, 1.0, 2.0, 0.5800790732156745, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9238098613189792, 6.911199999999999, 6.9112, 121.9260426156618, 1329772.460897149, 1329772.46089715, 286094.2300590047], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6083400.0000, 
sim time next is 6084000.0000, 
raw observation next is [29.3, 58.0, 1.0, 2.0, 0.7270706167137716, 0.0, 2.0, 0.0, 1.0, 2.0, 0.994265758653368, 6.911199999999999, 6.9112, 121.9260426156618, 1546801.452729265, 1546801.452729265, 321623.1580406151], 
processed observation next is [1.0, 0.43478260869565216, 0.6407407407407407, 0.58, 1.0, 1.0, 0.6750840675163947, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.9928321983167099, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.5524290902604517, 0.5524290902604517, 0.618506073155029], 
reward next is 0.3815, 
noisyNet noise sample is [array([0.31315875], dtype=float32), -0.16034657]. 
=============================================
[2019-04-27 18:08:35,664] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[65.94038 ]
 [65.29841 ]
 [64.466034]
 [64.19544 ]
 [64.13923 ]], R is [[65.67848969]
 [65.4715271 ]
 [65.25037384]
 [64.9679718 ]
 [64.67446899]].
[2019-04-27 18:08:39,803] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3921858e-15 6.7330003e-10 9.9999964e-01 6.7501690e-13 3.8538010e-07], sum to 1.0000
[2019-04-27 18:08:39,810] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6992
[2019-04-27 18:08:39,814] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [22.5, 90.66666666666667, 1.0, 2.0, 0.2662010643385183, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4278794500870398, 6.911199999999999, 6.9112, 121.9260426156618, 633031.188098319, 633031.1880983195, 185471.4537667326], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6153000.0000, 
sim time next is 6153600.0000, 
raw observation next is [22.6, 90.33333333333334, 1.0, 2.0, 0.2501718157918298, 0.0, 2.0, 0.0, 1.0, 2.0, 0.402036011335112, 6.911200000000001, 6.9112, 121.9260426156618, 594634.0928394153, 594634.0928394148, 181472.0277509133], 
processed observation next is [1.0, 0.21739130434782608, 0.39259259259259266, 0.9033333333333334, 1.0, 1.0, 0.10734739975217832, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.25254501416888997, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.21236931887121976, 0.2123693188712196, 0.3489846687517563], 
reward next is 0.6510, 
noisyNet noise sample is [array([0.5029817], dtype=float32), 0.98152524]. 
=============================================
[2019-04-27 18:08:46,786] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10500, global step 167612: loss 0.1510
[2019-04-27 18:08:46,792] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10500, global step 167613: learning rate 0.0001
[2019-04-27 18:08:46,893] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10500, global step 167660: loss 0.0965
[2019-04-27 18:08:46,896] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10500, global step 167661: learning rate 0.0001
[2019-04-27 18:08:46,992] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10500, global step 167705: loss 0.0240
[2019-04-27 18:08:46,995] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10500, global step 167705: learning rate 0.0001
[2019-04-27 18:08:47,224] A3C_AGENT_WORKER-Thread-10 INFO:Local step 10500, global step 167814: loss 0.0080
[2019-04-27 18:08:47,228] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 10500, global step 167816: learning rate 0.0001
[2019-04-27 18:08:47,314] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10500, global step 167853: loss 0.0015
[2019-04-27 18:08:47,314] A3C_AGENT_WORKER-Thread-9 INFO:Local step 10500, global step 167853: loss 0.0015
[2019-04-27 18:08:47,317] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 10500, global step 167856: learning rate 0.0001
[2019-04-27 18:08:47,318] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10500, global step 167853: learning rate 0.0001
[2019-04-27 18:08:47,614] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10500, global step 167999: loss 0.0223
[2019-04-27 18:08:47,615] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10500, global step 167999: learning rate 0.0001
[2019-04-27 18:08:47,717] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10500, global step 168049: loss 0.0030
[2019-04-27 18:08:47,721] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10500, global step 168049: learning rate 0.0001
[2019-04-27 18:08:47,733] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10500, global step 168056: loss 0.0040
[2019-04-27 18:08:47,736] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10500, global step 168058: learning rate 0.0001
[2019-04-27 18:08:47,742] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10500, global step 168059: loss 0.0193
[2019-04-27 18:08:47,744] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10500, global step 168060: learning rate 0.0001
[2019-04-27 18:08:47,798] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10500, global step 168085: loss 0.0560
[2019-04-27 18:08:47,799] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10500, global step 168085: learning rate 0.0001
[2019-04-27 18:08:47,882] A3C_AGENT_WORKER-Thread-3 INFO:Local step 10500, global step 168125: loss 0.0237
[2019-04-27 18:08:47,884] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 10500, global step 168126: learning rate 0.0001
[2019-04-27 18:08:47,932] A3C_AGENT_WORKER-Thread-19 INFO:Local step 10500, global step 168148: loss 0.0046
[2019-04-27 18:08:47,935] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 10500, global step 168148: learning rate 0.0001
[2019-04-27 18:08:48,039] A3C_AGENT_WORKER-Thread-22 INFO:Local step 10500, global step 168199: loss 0.0035
[2019-04-27 18:08:48,040] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 10500, global step 168199: learning rate 0.0001
[2019-04-27 18:08:48,051] A3C_AGENT_WORKER-Thread-20 INFO:Local step 10500, global step 168204: loss 0.0113
[2019-04-27 18:08:48,053] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 10500, global step 168204: learning rate 0.0001
[2019-04-27 18:08:48,372] A3C_AGENT_WORKER-Thread-21 INFO:Local step 10500, global step 168353: loss 0.0051
[2019-04-27 18:08:48,374] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 10500, global step 168354: learning rate 0.0001
[2019-04-27 18:08:50,275] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.2659599e-18 1.1937127e-12 1.0000000e+00 1.4205309e-18 1.6094816e-11], sum to 1.0000
[2019-04-27 18:08:50,283] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6530
[2019-04-27 18:08:50,287] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.0, 86.5, 1.0, 2.0, 0.3026888132000964, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4820051610656613, 6.911200000000001, 6.9112, 121.9260426156618, 692772.9235884821, 692772.9235884816, 195922.244100561], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6334200.0000, 
sim time next is 6334800.0000, 
raw observation next is [25.06666666666666, 86.33333333333333, 1.0, 2.0, 0.304181369092255, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4843496883783334, 6.911199999999999, 6.9112, 121.9260426156618, 695524.3007758554, 695524.3007758559, 196336.6422933102], 
processed observation next is [0.0, 0.30434782608695654, 0.48395061728395034, 0.8633333333333333, 1.0, 1.0, 0.17164448701458931, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.35543711047291676, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.24840153599137693, 0.2484015359913771, 0.37757046594867344], 
reward next is 0.6224, 
noisyNet noise sample is [array([-0.65885186], dtype=float32), 0.302142]. 
=============================================
[2019-04-27 18:09:02,303] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-27 18:09:02,304] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 18:09:02,305] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:09:02,305] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 18:09:02,306] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 18:09:02,306] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:09:02,307] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:09:02,308] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 18:09:02,309] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 18:09:02,309] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:09:02,310] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:09:02,326] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run8
[2019-04-27 18:09:02,344] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run8
[2019-04-27 18:09:02,346] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run8
[2019-04-27 18:09:02,391] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run8
[2019-04-27 18:09:02,407] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run8
[2019-04-27 18:09:03,505] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.0252527], dtype=float32), 0.04064039]
[2019-04-27 18:09:03,507] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [15.21589662166667, 88.52890381833333, 1.0, 2.0, 0.16, 1.0, 2.0, 0.16, 1.0, 2.0, 0.2, 6.911200000000001, 6.9112, 121.94756008, 271584.3547417162, 271584.3547417158, 136455.8256115202]
[2019-04-27 18:09:03,507] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:09:03,511] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [4.1807275e-13 1.4496248e-08 9.8365925e-02 2.2288857e-15 9.0163398e-01], sampled 0.09697763391390812
[2019-04-27 18:09:05,158] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0252527], dtype=float32), 0.04064039]
[2019-04-27 18:09:05,161] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [18.33333333333334, 50.66666666666666, 1.0, 2.0, 0.16, 1.0, 2.0, 0.16, 1.0, 2.0, 0.2, 6.9112, 6.9112, 121.94756008, 292156.610748719, 292156.610748719, 133114.2104612186]
[2019-04-27 18:09:05,164] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 18:09:05,168] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [3.4229357e-14 2.5109279e-09 5.0525524e-02 1.0115622e-16 9.4947445e-01], sampled 0.07309916368874037
[2019-04-27 18:09:09,542] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0252527], dtype=float32), 0.04064039]
[2019-04-27 18:09:09,543] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [17.25, 79.5, 1.0, 2.0, 0.16, 1.0, 2.0, 0.16, 1.0, 2.0, 0.2, 6.9112, 6.9112, 121.94756008, 357970.5512327405, 357970.5512327405, 166894.9508952739]
[2019-04-27 18:09:09,545] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 18:09:09,548] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [7.2647197e-14 4.4633568e-09 6.1524633e-02 2.8359198e-16 9.3847537e-01], sampled 0.6844988314591814
[2019-04-27 18:09:12,471] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0252527], dtype=float32), 0.04064039]
[2019-04-27 18:09:12,472] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [35.7, 19.0, 1.0, 2.0, 0.4586439406892692, 1.0, 2.0, 0.4586439406892692, 1.0, 2.0, 0.7389498824993874, 6.9112, 6.9112, 121.94756008, 1645243.790226187, 1645243.790226187, 323447.4441665819]
[2019-04-27 18:09:12,474] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 18:09:12,477] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [2.1944172e-14 9.8769559e-10 4.9101328e-03 4.7062646e-17 9.9508989e-01], sampled 0.22998176025994976
[2019-04-27 18:09:46,432] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0252527], dtype=float32), 0.04064039]
[2019-04-27 18:09:46,433] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [19.33333333333334, 96.0, 1.0, 2.0, 0.16, 1.0, 2.0, 0.16, 1.0, 2.0, 0.215916573192332, 6.911199999999999, 6.9112, 121.94756008, 484032.8243857736, 484032.824385774, 192514.5218912849]
[2019-04-27 18:09:46,435] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 18:09:46,437] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.2685450e-14 1.2179514e-09 3.1527352e-02 3.2600776e-17 9.6847266e-01], sampled 0.5927390870940616
[2019-04-27 18:09:51,033] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0252527], dtype=float32), 0.04064039]
[2019-04-27 18:09:51,034] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [23.0, 89.83333333333334, 1.0, 2.0, 0.2023362666455424, 1.0, 1.0, 0.2023362666455424, 1.0, 2.0, 0.3234784276516977, 6.9112, 6.9112, 121.94756008, 710960.4031922712, 710960.4031922712, 221239.3971652374]
[2019-04-27 18:09:51,035] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 18:09:51,037] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.5561052e-13 6.8425625e-09 6.3945338e-02 7.5182544e-16 9.3605465e-01], sampled 0.41942392188383204
[2019-04-27 18:10:14,582] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.0252527], dtype=float32), 0.04064039]
[2019-04-27 18:10:14,584] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [31.1, 75.0, 1.0, 2.0, 0.7418137542889848, 1.0, 2.0, 0.684271539120927, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.94756008, 2341819.094912706, 2341819.094912706, 440665.1312506829]
[2019-04-27 18:10:14,586] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 18:10:14,588] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [2.6225900e-14 6.2913269e-10 7.9599232e-04 3.9804593e-17 9.9920398e-01], sampled 0.7363306710602306
[2019-04-27 18:10:39,479] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0252527], dtype=float32), 0.04064039]
[2019-04-27 18:10:39,483] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [23.75, 60.66666666666667, 1.0, 2.0, 0.16, 1.0, 2.0, 0.16, 1.0, 2.0, 0.2016228126151696, 6.9112, 6.9112, 121.94756008, 451616.0568649761, 451616.0568649761, 187608.4554175223]
[2019-04-27 18:10:39,485] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 18:10:39,489] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.12405905e-14 1.33145939e-09 6.13513030e-02 3.02134700e-17
 9.38648760e-01], sampled 0.7505706428235054
[2019-04-27 18:10:41,909] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.0252527], dtype=float32), 0.04064039]
[2019-04-27 18:10:41,910] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [19.0, 83.0, 1.0, 2.0, 0.16, 1.0, 1.0, 0.16, 1.0, 2.0, 0.2, 6.9112, 6.9112, 121.94756008, 385669.0534308596, 385669.0534308596, 174303.5638795636]
[2019-04-27 18:10:41,914] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 18:10:41,917] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.0230576e-14 1.1868271e-09 5.9153754e-02 2.6159155e-17 9.4084626e-01], sampled 0.6788223293560642
[2019-04-27 18:10:45,556] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 4445.3886 2926902708.5483 28.0000
[2019-04-27 18:10:45,715] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 4587.3489 2880960956.5895 12.0000
[2019-04-27 18:10:45,920] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 4294.7224 2905659479.8603 33.0000
[2019-04-27 18:10:46,034] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 4515.1993 3095510519.0272 0.0000
[2019-04-27 18:10:46,073] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 4411.8427 2862500871.2263 10.0000
[2019-04-27 18:10:47,088] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 175000, evaluation results [175000.0, 4515.199315133241, 3095510519.0271516, 0.0, 4587.348889989217, 2880960956.589454, 12.0, 4411.84270726349, 2862500871.226261, 10.0, 4445.388556516545, 2926902708.5482755, 28.0, 4294.722429243351, 2905659479.8603272, 33.0]
[2019-04-27 18:10:48,496] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11000, global step 175673: loss 1.0896
[2019-04-27 18:10:48,498] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11000, global step 175673: learning rate 0.0001
[2019-04-27 18:10:48,515] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11000, global step 175684: loss 0.9109
[2019-04-27 18:10:48,518] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11000, global step 175685: learning rate 0.0001
[2019-04-27 18:10:48,578] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11000, global step 175712: loss 1.3646
[2019-04-27 18:10:48,580] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11000, global step 175713: learning rate 0.0001
[2019-04-27 18:10:48,963] A3C_AGENT_WORKER-Thread-9 INFO:Local step 11000, global step 175895: loss 2.2778
[2019-04-27 18:10:48,966] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 11000, global step 175895: learning rate 0.0001
[2019-04-27 18:10:48,990] A3C_AGENT_WORKER-Thread-10 INFO:Local step 11000, global step 175906: loss 2.2676
[2019-04-27 18:10:48,991] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 11000, global step 175907: learning rate 0.0001
[2019-04-27 18:10:48,998] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11000, global step 175909: loss 2.2984
[2019-04-27 18:10:49,000] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11000, global step 175910: learning rate 0.0001
[2019-04-27 18:10:49,098] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11000, global step 175958: loss 2.3158
[2019-04-27 18:10:49,100] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11000, global step 175958: learning rate 0.0001
[2019-04-27 18:10:49,228] A3C_AGENT_WORKER-Thread-3 INFO:Local step 11000, global step 176019: loss 2.3014
[2019-04-27 18:10:49,233] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11000, global step 176021: loss 2.3333
[2019-04-27 18:10:49,234] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 11000, global step 176021: learning rate 0.0001
[2019-04-27 18:10:49,239] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11000, global step 176022: learning rate 0.0001
[2019-04-27 18:10:49,275] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11000, global step 176039: loss 2.0619
[2019-04-27 18:10:49,276] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11000, global step 176039: learning rate 0.0001
[2019-04-27 18:10:49,420] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11000, global step 176108: loss 2.3086
[2019-04-27 18:10:49,423] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11000, global step 176108: learning rate 0.0001
[2019-04-27 18:10:49,432] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11000, global step 176113: loss 2.2281
[2019-04-27 18:10:49,433] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11000, global step 176114: learning rate 0.0001
[2019-04-27 18:10:49,466] A3C_AGENT_WORKER-Thread-22 INFO:Local step 11000, global step 176127: loss 2.1050
[2019-04-27 18:10:49,467] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 11000, global step 176127: learning rate 0.0001
[2019-04-27 18:10:49,496] A3C_AGENT_WORKER-Thread-20 INFO:Local step 11000, global step 176140: loss 1.9768
[2019-04-27 18:10:49,499] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 11000, global step 176140: learning rate 0.0001
[2019-04-27 18:10:49,544] A3C_AGENT_WORKER-Thread-19 INFO:Local step 11000, global step 176164: loss 1.8969
[2019-04-27 18:10:49,545] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 11000, global step 176164: learning rate 0.0001
[2019-04-27 18:10:49,756] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.03740625e-08 4.47921320e-06 7.96497116e-05 3.43845230e-11
 9.99915838e-01], sum to 1.0000
[2019-04-27 18:10:49,766] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5837
[2019-04-27 18:10:49,771] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.0, 35.0, 1.0, 2.0, 0.2745438267336608, 1.0, 2.0, 0.2745438267336608, 1.0, 2.0, 0.4510849301483454, 6.9112, 6.9112, 121.94756008, 1011581.856133658, 1011581.856133658, 244915.5186744817], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6613200.0000, 
sim time next is 6613800.0000, 
raw observation next is [28.21666666666667, 37.83333333333334, 1.0, 2.0, 0.2809211558826147, 1.0, 2.0, 0.2809211558826147, 1.0, 2.0, 0.4611418667993433, 6.911199999999999, 6.9112, 121.94756008, 1034113.677068954, 1034113.677068954, 247330.8351749327], 
processed observation next is [1.0, 0.5652173913043478, 0.6006172839506173, 0.3783333333333334, 1.0, 1.0, 0.14395375700311272, 1.0, 1.0, 0.14395375700311272, 1.0, 1.0, 0.32642733349917913, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.36932631323891213, 0.36932631323891213, 0.4756362214902552], 
reward next is 0.5244, 
noisyNet noise sample is [array([-0.00027266], dtype=float32), 0.32009882]. 
=============================================
[2019-04-27 18:10:49,918] A3C_AGENT_WORKER-Thread-21 INFO:Local step 11000, global step 176338: loss 1.2352
[2019-04-27 18:10:49,922] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 11000, global step 176338: learning rate 0.0001
[2019-04-27 18:10:54,377] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.5609081e-08 9.9894506e-01 3.8190954e-04 1.1119618e-08 6.7290448e-04], sum to 1.0000
[2019-04-27 18:10:54,385] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9193
[2019-04-27 18:10:54,390] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 37.0, 1.0, 2.0, 0.7120968927433495, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 905593.5176126278, 905593.5176126278, 181628.7581254059], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6692400.0000, 
sim time next is 6693000.0000, 
raw observation next is [27.15, 36.5, 1.0, 2.0, 0.7999602192573273, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1016978.123732565, 1016978.123732565, 199628.7747214955], 
processed observation next is [1.0, 0.4782608695652174, 0.561111111111111, 0.365, 1.0, 1.0, 0.7618574038777706, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3632064727616304, 0.3632064727616304, 0.3839014898490298], 
reward next is 0.6161, 
noisyNet noise sample is [array([-0.9133836], dtype=float32), -0.70237607]. 
=============================================
[2019-04-27 18:10:54,403] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[21.753334]
 [21.907967]
 [21.892967]
 [22.005655]
 [22.16407 ]], R is [[22.0642128 ]
 [22.49428558]
 [22.94130516]
 [23.3862133 ]
 [23.82671928]].
[2019-04-27 18:11:01,991] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.6762945e-17 9.9999988e-01 1.4753725e-07 3.8213382e-19 1.9575624e-10], sum to 1.0000
[2019-04-27 18:11:02,001] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8777
[2019-04-27 18:11:02,005] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.6, 77.0, 1.0, 2.0, 0.416773156466196, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 513210.8926760508, 513210.8926760504, 130931.0255363606], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6831000.0000, 
sim time next is 6831600.0000, 
raw observation next is [22.53333333333333, 76.66666666666667, 1.0, 2.0, 0.4116452378887566, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 507707.1646789314, 507707.1646789314, 130216.7840741685], 
processed observation next is [0.0, 0.043478260869565216, 0.3901234567901234, 0.7666666666666667, 1.0, 1.0, 0.2995776641532817, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.18132398738533265, 0.18132398738533265, 0.25041689245032406], 
reward next is 0.7496, 
noisyNet noise sample is [array([-0.6804898], dtype=float32), 0.44901747]. 
=============================================
[2019-04-27 18:11:05,150] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11500, global step 183639: loss 0.1859
[2019-04-27 18:11:05,152] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11500, global step 183639: learning rate 0.0001
[2019-04-27 18:11:05,250] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11500, global step 183682: loss 0.1750
[2019-04-27 18:11:05,256] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11500, global step 183684: learning rate 0.0001
[2019-04-27 18:11:05,366] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11500, global step 183739: loss 0.0953
[2019-04-27 18:11:05,369] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11500, global step 183740: learning rate 0.0001
[2019-04-27 18:11:05,538] A3C_AGENT_WORKER-Thread-9 INFO:Local step 11500, global step 183819: loss 0.0048
[2019-04-27 18:11:05,540] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 11500, global step 183819: learning rate 0.0001
[2019-04-27 18:11:05,681] A3C_AGENT_WORKER-Thread-10 INFO:Local step 11500, global step 183891: loss 0.0006
[2019-04-27 18:11:05,683] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 11500, global step 183892: learning rate 0.0001
[2019-04-27 18:11:05,775] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11500, global step 183934: loss 0.0252
[2019-04-27 18:11:05,776] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11500, global step 183935: learning rate 0.0001
[2019-04-27 18:11:05,854] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11500, global step 183973: loss 0.0956
[2019-04-27 18:11:05,857] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11500, global step 183974: learning rate 0.0001
[2019-04-27 18:11:05,959] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11500, global step 184022: loss 0.0146
[2019-04-27 18:11:05,960] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11500, global step 184023: learning rate 0.0001
[2019-04-27 18:11:06,027] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11500, global step 184059: loss 0.0007
[2019-04-27 18:11:06,032] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11500, global step 184060: learning rate 0.0001
[2019-04-27 18:11:06,075] A3C_AGENT_WORKER-Thread-22 INFO:Local step 11500, global step 184078: loss 0.0529
[2019-04-27 18:11:06,078] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 11500, global step 184078: learning rate 0.0001
[2019-04-27 18:11:06,100] A3C_AGENT_WORKER-Thread-3 INFO:Local step 11500, global step 184090: loss 0.0575
[2019-04-27 18:11:06,103] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 11500, global step 184092: learning rate 0.0001
[2019-04-27 18:11:06,169] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11500, global step 184130: loss 0.0712
[2019-04-27 18:11:06,171] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11500, global step 184132: learning rate 0.0001
[2019-04-27 18:11:06,190] A3C_AGENT_WORKER-Thread-19 INFO:Local step 11500, global step 184139: loss 0.0870
[2019-04-27 18:11:06,192] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 11500, global step 184140: learning rate 0.0001
[2019-04-27 18:11:06,196] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11500, global step 184141: loss 0.0921
[2019-04-27 18:11:06,201] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11500, global step 184142: learning rate 0.0001
[2019-04-27 18:11:06,329] A3C_AGENT_WORKER-Thread-20 INFO:Local step 11500, global step 184203: loss 0.0288
[2019-04-27 18:11:06,330] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 11500, global step 184203: learning rate 0.0001
[2019-04-27 18:11:06,504] A3C_AGENT_WORKER-Thread-21 INFO:Local step 11500, global step 184285: loss 0.0230
[2019-04-27 18:11:06,507] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 11500, global step 184285: learning rate 0.0001
[2019-04-27 18:11:14,640] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.9717131e-14 1.0000000e+00 2.1308124e-09 3.3640888e-16 9.9000164e-10], sum to 1.0000
[2019-04-27 18:11:14,647] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7201
[2019-04-27 18:11:14,652] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.56666666666667, 80.16666666666667, 1.0, 2.0, 0.4811946042583637, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 580123.5632637322, 580123.5632637318, 140189.2594300052], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7067400.0000, 
sim time next is 7068000.0000, 
raw observation next is [23.53333333333333, 80.33333333333334, 1.0, 2.0, 0.4812273709869352, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 580253.4186186891, 580253.4186186891, 140197.3265905106], 
processed observation next is [1.0, 0.8260869565217391, 0.4271604938271604, 0.8033333333333335, 1.0, 1.0, 0.3824135368892086, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.20723336379238894, 0.20723336379238894, 0.26961024344328965], 
reward next is 0.7304, 
noisyNet noise sample is [array([-1.1247917], dtype=float32), -1.8117123]. 
=============================================
[2019-04-27 18:11:14,664] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[44.236885]
 [44.460552]
 [44.527782]
 [44.64938 ]
 [44.494656]], R is [[43.7102356 ]
 [44.00353622]
 [44.2939949 ]
 [44.58164978]
 [44.86632156]].
[2019-04-27 18:11:14,674] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0878166e-14 1.0000000e+00 2.9585432e-09 1.9783022e-16 1.8378643e-08], sum to 1.0000
[2019-04-27 18:11:14,684] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7205
[2019-04-27 18:11:14,695] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1356682.768401338 W.
[2019-04-27 18:11:14,699] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.0, 64.0, 1.0, 2.0, 0.5815509143683514, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9286637119226152, 6.911199999999999, 6.9112, 121.9260426156618, 1356682.768401338, 1356682.768401338, 286201.77580309], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7052400.0000, 
sim time next is 7053000.0000, 
raw observation next is [26.73333333333333, 65.16666666666667, 1.0, 2.0, 0.6319804307479815, 1.0, 1.0, 0.6319804307479815, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1468576.05007586, 1468576.050075861, 280545.7076295557], 
processed observation next is [1.0, 0.6521739130434783, 0.545679012345679, 0.6516666666666667, 1.0, 1.0, 0.5618814651761684, 1.0, 0.5, 0.5618814651761684, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.5244914464556643, 0.5244914464556646, 0.5395109762106841], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0319306], dtype=float32), -0.0062201885]. 
=============================================
[2019-04-27 18:11:14,714] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[43.09812 ]
 [42.56893 ]
 [42.17589 ]
 [42.307037]
 [42.18265 ]], R is [[42.05052567]
 [41.63002014]
 [41.21372223]
 [41.17151642]
 [41.14794922]].
[2019-04-27 18:11:14,794] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3728427e-14 1.0000000e+00 3.3525964e-09 1.2374240e-15 1.1346398e-09], sum to 1.0000
[2019-04-27 18:11:14,803] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1914
[2019-04-27 18:11:14,809] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1722326.128244303 W.
[2019-04-27 18:11:14,816] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.93333333333333, 68.66666666666666, 1.0, 2.0, 0.7418760818899074, 1.0, 1.0, 0.7418760818899074, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1722326.128244303, 1722326.128244304, 321852.0466094775], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7054800.0000, 
sim time next is 7055400.0000, 
raw observation next is [25.66666666666667, 69.83333333333334, 1.0, 2.0, 0.8358514917810926, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9762258329154044, 6.911199999999999, 6.9112, 121.9260426156618, 1692540.094066133, 1692540.094066134, 339635.7535548771], 
processed observation next is [1.0, 0.6521739130434783, 0.506172839506173, 0.6983333333333335, 1.0, 1.0, 0.8045851092632055, 0.0, 0.5, -0.1904761904761905, 1.0, 0.5, 0.9702822911442555, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.6044786050236189, 0.6044786050236193, 0.6531456799132253], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.05430843], dtype=float32), -0.20013218]. 
=============================================
[2019-04-27 18:11:17,175] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2419328e-19 1.0000000e+00 8.8318206e-09 4.8025448e-21 6.9880862e-14], sum to 1.0000
[2019-04-27 18:11:17,184] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4235
[2019-04-27 18:11:17,187] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.03333333333333, 79.66666666666667, 1.0, 2.0, 0.4080152142950841, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 503993.7797441803, 503993.7797441799, 129718.1045314433], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7098000.0000, 
sim time next is 7098600.0000, 
raw observation next is [21.91666666666667, 80.33333333333333, 1.0, 2.0, 0.4034467712515118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 498531.6993320047, 498531.6993320047, 129074.5464082837], 
processed observation next is [1.0, 0.13043478260869565, 0.36728395061728414, 0.8033333333333332, 1.0, 1.0, 0.28981758482322834, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.17804703547571596, 0.17804703547571596, 0.24822028155439174], 
reward next is 0.7518, 
noisyNet noise sample is [array([-2.278684], dtype=float32), -1.333096]. 
=============================================
[2019-04-27 18:11:19,984] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1408965e-19 1.0000000e+00 1.8533557e-09 2.4941876e-23 4.0199530e-14], sum to 1.0000
[2019-04-27 18:11:19,994] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2650
[2019-04-27 18:11:20,003] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 69.5, 1.0, 2.0, 0.3588850515407486, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 449114.4558819932, 449114.4558819932, 123065.9166364992], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7147800.0000, 
sim time next is 7148400.0000, 
raw observation next is [22.33333333333334, 70.66666666666667, 1.0, 2.0, 0.361028259809406, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 451762.1644870634, 451762.1644870634, 123352.515482957], 
processed observation next is [1.0, 0.7391304347826086, 0.38271604938271625, 0.7066666666666667, 1.0, 1.0, 0.23931935691595954, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16134363017395123, 0.16134363017395123, 0.23721637592876346], 
reward next is 0.7628, 
noisyNet noise sample is [array([-1.650991], dtype=float32), -1.2976229]. 
=============================================
[2019-04-27 18:11:21,507] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.0897305e-20 1.0000000e+00 2.9419487e-09 5.8545763e-22 1.4530793e-13], sum to 1.0000
[2019-04-27 18:11:21,516] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2191
[2019-04-27 18:11:21,521] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.7, 93.16666666666666, 1.0, 2.0, 0.3728903853533176, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 464632.555560694, 464632.5555606935, 124920.2368460677], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7185000.0000, 
sim time next is 7185600.0000, 
raw observation next is [19.7, 93.0, 1.0, 2.0, 0.3716086723071907, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 463156.6129633108, 463156.6129633108, 124748.1803286566], 
processed observation next is [1.0, 0.17391304347826086, 0.28518518518518515, 0.93, 1.0, 1.0, 0.2519150860799889, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16541307605832528, 0.16541307605832528, 0.23990034678587807], 
reward next is 0.7601, 
noisyNet noise sample is [array([1.8580222], dtype=float32), 0.3760526]. 
=============================================
[2019-04-27 18:11:21,907] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12000, global step 191625: loss 0.0700
[2019-04-27 18:11:21,910] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12000, global step 191626: learning rate 0.0001
[2019-04-27 18:11:22,004] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12000, global step 191671: loss 0.0364
[2019-04-27 18:11:22,008] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12000, global step 191672: learning rate 0.0001
[2019-04-27 18:11:22,108] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12000, global step 191716: loss 0.0112
[2019-04-27 18:11:22,111] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12000, global step 191718: learning rate 0.0001
[2019-04-27 18:11:22,311] A3C_AGENT_WORKER-Thread-9 INFO:Local step 12000, global step 191814: loss 0.0037
[2019-04-27 18:11:22,313] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 12000, global step 191814: learning rate 0.0001
[2019-04-27 18:11:22,413] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12000, global step 191860: loss 0.0021
[2019-04-27 18:11:22,416] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12000, global step 191861: learning rate 0.0001
[2019-04-27 18:11:22,487] A3C_AGENT_WORKER-Thread-10 INFO:Local step 12000, global step 191894: loss 0.0094
[2019-04-27 18:11:22,488] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 12000, global step 191894: learning rate 0.0001
[2019-04-27 18:11:22,553] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12000, global step 191929: loss 0.0037
[2019-04-27 18:11:22,558] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12000, global step 191930: learning rate 0.0001
[2019-04-27 18:11:22,635] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12000, global step 191961: loss 0.0328
[2019-04-27 18:11:22,641] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12000, global step 191961: learning rate 0.0001
[2019-04-27 18:11:22,890] A3C_AGENT_WORKER-Thread-22 INFO:Local step 12000, global step 192080: loss 0.0253
[2019-04-27 18:11:22,893] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 12000, global step 192081: learning rate 0.0001
[2019-04-27 18:11:22,926] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12000, global step 192098: loss 0.0050
[2019-04-27 18:11:22,927] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12000, global step 192098: learning rate 0.0001
[2019-04-27 18:11:22,949] A3C_AGENT_WORKER-Thread-3 INFO:Local step 12000, global step 192107: loss 0.0101
[2019-04-27 18:11:22,952] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 12000, global step 192107: learning rate 0.0001
[2019-04-27 18:11:22,966] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12000, global step 192114: loss 0.0182
[2019-04-27 18:11:22,967] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12000, global step 192115: learning rate 0.0001
[2019-04-27 18:11:23,089] A3C_AGENT_WORKER-Thread-20 INFO:Local step 12000, global step 192173: loss 0.0002
[2019-04-27 18:11:23,093] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 12000, global step 192176: learning rate 0.0001
[2019-04-27 18:11:23,110] A3C_AGENT_WORKER-Thread-19 INFO:Local step 12000, global step 192183: loss 0.0015
[2019-04-27 18:11:23,113] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 12000, global step 192184: learning rate 0.0001
[2019-04-27 18:11:23,208] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12000, global step 192228: loss 0.0064
[2019-04-27 18:11:23,212] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12000, global step 192228: learning rate 0.0001
[2019-04-27 18:11:23,432] A3C_AGENT_WORKER-Thread-21 INFO:Local step 12000, global step 192334: loss 0.0231
[2019-04-27 18:11:23,436] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 12000, global step 192336: learning rate 0.0001
[2019-04-27 18:11:27,707] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.3961516e-18 1.0000000e+00 1.1315753e-09 3.4266658e-20 7.8343202e-14], sum to 1.0000
[2019-04-27 18:11:27,719] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8675
[2019-04-27 18:11:27,726] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 73.66666666666667, 1.0, 2.0, 0.8591769826395732, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1043815.083411236, 1043815.083411236, 211411.5303169634], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7299600.0000, 
sim time next is 7300200.0000, 
raw observation next is [24.2, 72.83333333333333, 1.0, 2.0, 0.868022465709464, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1053397.361253308, 1053397.361253308, 213336.4285134923], 
processed observation next is [1.0, 0.4782608695652174, 0.45185185185185184, 0.7283333333333333, 1.0, 1.0, 0.8428838877493618, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3762133433047528, 0.3762133433047528, 0.4102623625259468], 
reward next is 0.5897, 
noisyNet noise sample is [array([-0.04929281], dtype=float32), -0.2657963]. 
=============================================
[2019-04-27 18:11:38,702] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12500, global step 199649: loss 0.1196
[2019-04-27 18:11:38,706] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12500, global step 199649: learning rate 0.0001
[2019-04-27 18:11:38,736] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12500, global step 199667: loss 0.0786
[2019-04-27 18:11:38,743] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12500, global step 199669: learning rate 0.0001
[2019-04-27 18:11:38,843] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12500, global step 199717: loss 0.0182
[2019-04-27 18:11:38,845] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12500, global step 199717: learning rate 0.0001
[2019-04-27 18:11:39,031] A3C_AGENT_WORKER-Thread-9 INFO:Local step 12500, global step 199807: loss 0.0337
[2019-04-27 18:11:39,032] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 12500, global step 199808: learning rate 0.0001
[2019-04-27 18:11:39,224] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2558906e-21 1.0000000e+00 9.5784092e-12 1.7065497e-22 8.4517993e-17], sum to 1.0000
[2019-04-27 18:11:39,236] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2140
[2019-04-27 18:11:39,244] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.61666666666667, 85.33333333333334, 1.0, 2.0, 0.5053624576827138, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 601213.4193547318, 601213.4193547318, 143682.6583547848], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7498200.0000, 
sim time next is 7498800.0000, 
raw observation next is [23.5, 86.0, 1.0, 2.0, 0.5039846648111316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 599858.3117616738, 599858.3117616738, 143475.924496233], 
processed observation next is [0.0, 0.8260869565217391, 0.42592592592592593, 0.86, 1.0, 1.0, 0.40950555334658517, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2142351113434549, 0.2142351113434549, 0.2759152394158327], 
reward next is 0.7241, 
noisyNet noise sample is [array([-1.4714756], dtype=float32), 0.16766803]. 
=============================================
[2019-04-27 18:11:39,245] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12500, global step 199905: loss 0.0066
[2019-04-27 18:11:39,246] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12500, global step 199906: learning rate 0.0001
[2019-04-27 18:11:39,303] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12500, global step 199926: loss 0.0576
[2019-04-27 18:11:39,307] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12500, global step 199927: learning rate 0.0001
[2019-04-27 18:11:39,322] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12500, global step 199935: loss 0.0432
[2019-04-27 18:11:39,325] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12500, global step 199935: learning rate 0.0001
[2019-04-27 18:11:39,449] A3C_AGENT_WORKER-Thread-10 INFO:Local step 12500, global step 199998: loss 0.0288
[2019-04-27 18:11:39,451] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 12500, global step 199999: learning rate 0.0001
[2019-04-27 18:11:39,456] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-04-27 18:11:39,458] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 18:11:39,459] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:11:39,459] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 18:11:39,460] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 18:11:39,463] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 18:11:39,464] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:11:39,462] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 18:11:39,464] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:11:39,465] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:11:39,466] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:11:39,485] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run9
[2019-04-27 18:11:39,486] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run9
[2019-04-27 18:11:39,523] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run9
[2019-04-27 18:11:39,542] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run9
[2019-04-27 18:11:39,543] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run9
[2019-04-27 18:12:19,709] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01008193], dtype=float32), 0.04121059]
[2019-04-27 18:12:19,710] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [28.5, 70.0, 1.0, 2.0, 0.6455087970578021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 735667.6184293989, 735667.6184293989, 166083.8566489474]
[2019-04-27 18:12:19,711] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 18:12:19,714] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [3.3911024e-19 1.0000000e+00 4.1690862e-10 1.0435204e-20 6.7332072e-14], sampled 0.981868456060292
[2019-04-27 18:12:25,340] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.01008193], dtype=float32), 0.04121059]
[2019-04-27 18:12:25,340] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [23.149704515, 99.95368167999999, 1.0, 2.0, 0.5818032416226904, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 673814.9617245551, 673814.9617245547, 155493.5459297475]
[2019-04-27 18:12:25,341] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:12:25,344] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [4.0031116e-18 1.0000000e+00 1.5409645e-09 1.4126892e-19 2.9136917e-13], sampled 0.6263712487180904
[2019-04-27 18:12:31,340] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.01008193], dtype=float32), 0.04121059]
[2019-04-27 18:12:31,341] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [30.46666666666667, 71.0, 1.0, 2.0, 0.999268701740487, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.9258648820842, 1854408.179466185, 1854408.179466185, 379265.0999940978]
[2019-04-27 18:12:31,341] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 18:12:31,344] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.0671966e-18 1.0000000e+00 2.2826240e-10 1.5897347e-20 1.8336300e-13], sampled 0.6978088382432717
[2019-04-27 18:12:31,345] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1854408.179466185 W.
[2019-04-27 18:12:32,139] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.01008193], dtype=float32), 0.04121059]
[2019-04-27 18:12:32,141] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [24.6, 86.5, 1.0, 2.0, 0.5700444573516193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 663136.9901746376, 663136.9901746376, 153641.3302437108]
[2019-04-27 18:12:32,144] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 18:12:32,146] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.2855036e-18 1.0000000e+00 6.9301354e-10 3.3099816e-20 1.0487199e-13], sampled 0.8662986027647493
[2019-04-27 18:12:36,492] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.01008193], dtype=float32), 0.04121059]
[2019-04-27 18:12:36,494] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [37.39429793, 42.91312293, 1.0, 2.0, 0.8744283431957238, 1.0, 1.0, 0.8744283431957238, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 121.925852147514, 1994681.736852583, 1994681.736852583, 375470.8376450147]
[2019-04-27 18:12:36,496] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:12:36,498] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.2001292e-17 1.0000000e+00 1.7249263e-09 3.5511885e-19 9.3436456e-13], sampled 0.2646863029326234
[2019-04-27 18:12:36,498] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 1994681.736852583 W.
[2019-04-27 18:12:40,362] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01008193], dtype=float32), 0.04121059]
[2019-04-27 18:12:40,363] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [23.70724491, 91.34072260833335, 1.0, 2.0, 0.5584746382928846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 653656.5401477442, 653656.5401477442, 151882.9255934503]
[2019-04-27 18:12:40,364] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 18:12:40,367] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [8.1684773e-19 1.0000000e+00 5.3299426e-10 2.0067019e-20 7.3517500e-14], sampled 0.6032414839761029
[2019-04-27 18:12:47,632] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.01008193], dtype=float32), 0.04121059]
[2019-04-27 18:12:47,633] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [25.40242027, 93.42445676, 1.0, 2.0, 0.7229990645883847, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 824028.4954497188, 824028.4954497188, 180542.1509643589]
[2019-04-27 18:12:47,634] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:12:47,635] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [7.6614652e-19 1.0000000e+00 5.2467891e-10 2.1233436e-20 8.3992458e-14], sampled 0.9861974647498379
[2019-04-27 18:12:52,043] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.01008193], dtype=float32), 0.04121059]
[2019-04-27 18:12:52,044] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [28.3, 88.0, 1.0, 2.0, 0.8989356298141978, 1.0, 2.0, 0.8989356298141978, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260424847156, 2050650.144034949, 2050650.144034949, 386333.9905016348]
[2019-04-27 18:12:52,045] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 18:12:52,050] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.1116500e-17 1.0000000e+00 1.3562389e-09 3.4763834e-19 8.2868255e-13], sampled 0.002886997317643636
[2019-04-27 18:12:52,053] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2050650.144034949 W.
[2019-04-27 18:13:23,347] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8924.1046 2120496270.4403 430.0000
[2019-04-27 18:13:23,433] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8700.2439 2195058951.0868 572.0000
[2019-04-27 18:13:23,479] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8771.0838 2170637693.1962 493.0000
[2019-04-27 18:13:23,482] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8099.2011 2445351856.0114 746.0000
[2019-04-27 18:13:23,665] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8584.5265 2248663206.3456 553.0000
[2019-04-27 18:13:24,682] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 200000, evaluation results [200000.0, 8099.201060320109, 2445351856.0114403, 746.0, 8771.08379319977, 2170637693.1962304, 493.0, 8924.104571701966, 2120496270.440313, 430.0, 8584.526542641555, 2248663206.345631, 553.0, 8700.243902040113, 2195058951.086834, 572.0]
[2019-04-27 18:13:24,766] A3C_AGENT_WORKER-Thread-22 INFO:Local step 12500, global step 200051: loss 0.0005
[2019-04-27 18:13:24,767] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 12500, global step 200051: learning rate 0.0001
[2019-04-27 18:13:24,816] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12500, global step 200071: loss 0.0107
[2019-04-27 18:13:24,820] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12500, global step 200072: learning rate 0.0001
[2019-04-27 18:13:24,828] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12500, global step 200073: loss 0.0120
[2019-04-27 18:13:24,834] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12500, global step 200076: learning rate 0.0001
[2019-04-27 18:13:24,845] A3C_AGENT_WORKER-Thread-19 INFO:Local step 12500, global step 200079: loss 0.0315
[2019-04-27 18:13:24,847] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 12500, global step 200079: learning rate 0.0001
[2019-04-27 18:13:24,969] A3C_AGENT_WORKER-Thread-3 INFO:Local step 12500, global step 200142: loss 0.0906
[2019-04-27 18:13:24,973] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 12500, global step 200142: learning rate 0.0001
[2019-04-27 18:13:25,153] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12500, global step 200231: loss 0.0076
[2019-04-27 18:13:25,156] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12500, global step 200232: learning rate 0.0001
[2019-04-27 18:13:25,182] A3C_AGENT_WORKER-Thread-20 INFO:Local step 12500, global step 200242: loss 0.0103
[2019-04-27 18:13:25,184] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 12500, global step 200242: learning rate 0.0001
[2019-04-27 18:13:25,345] A3C_AGENT_WORKER-Thread-21 INFO:Local step 12500, global step 200319: loss 0.0006
[2019-04-27 18:13:25,347] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 12500, global step 200319: learning rate 0.0001
[2019-04-27 18:13:28,643] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.9301588e-21 1.0000000e+00 4.6031537e-10 3.1706544e-23 2.6824742e-16], sum to 1.0000
[2019-04-27 18:13:28,653] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4707
[2019-04-27 18:13:28,659] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 63.33333333333333, 1.0, 2.0, 0.5520400656207464, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 645809.8496416914, 645809.8496416914, 150802.1874930103], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7573200.0000, 
sim time next is 7573800.0000, 
raw observation next is [28.0, 62.66666666666667, 1.0, 2.0, 0.5463956992590916, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 640646.0460006793, 640646.0460006793, 149933.6753898141], 
processed observation next is [0.0, 0.6521739130434783, 0.5925925925925926, 0.6266666666666667, 1.0, 1.0, 0.4599948800703471, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.22880215928595687, 0.22880215928595687, 0.2883339911342579], 
reward next is 0.7117, 
noisyNet noise sample is [array([0.33482787], dtype=float32), 0.9474019]. 
=============================================
[2019-04-27 18:13:29,929] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.7060105e-21 1.0000000e+00 6.2365987e-11 3.9696920e-23 1.4643690e-14], sum to 1.0000
[2019-04-27 18:13:29,940] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8052
[2019-04-27 18:13:29,944] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.46666666666667, 86.66666666666667, 1.0, 2.0, 0.5108887307068392, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 607464.7010280294, 607464.7010280294, 144545.8838547877], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7597200.0000, 
sim time next is 7597800.0000, 
raw observation next is [23.4, 87.0, 1.0, 2.0, 0.5089571798216583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 605412.4574962032, 605412.4574962032, 144248.5903848878], 
processed observation next is [0.0, 0.9565217391304348, 0.42222222222222217, 0.87, 1.0, 1.0, 0.41542521407340277, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2162187348200726, 0.2162187348200726, 0.27740113535555344], 
reward next is 0.7226, 
noisyNet noise sample is [array([-0.65424544], dtype=float32), 0.8780376]. 
=============================================
[2019-04-27 18:13:36,822] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.1603449e-20 1.0000000e+00 1.7699470e-11 6.0375178e-22 1.5159385e-15], sum to 1.0000
[2019-04-27 18:13:36,831] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3690
[2019-04-27 18:13:36,837] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.26666666666667, 90.33333333333334, 1.0, 2.0, 0.30752376547701, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 391551.9362631745, 391551.9362631745, 116506.9253880441], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7706400.0000, 
sim time next is 7707000.0000, 
raw observation next is [18.18333333333334, 92.16666666666667, 1.0, 2.0, 0.3103585014018716, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 394639.0130860892, 394639.0130860892, 116858.779770845], 
processed observation next is [1.0, 0.17391304347826086, 0.22901234567901263, 0.9216666666666667, 1.0, 1.0, 0.17899821595460905, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.14094250467360328, 0.14094250467360328, 0.22472842263624038], 
reward next is 0.7753, 
noisyNet noise sample is [array([0.9878574], dtype=float32), -0.81440926]. 
=============================================
[2019-04-27 18:13:36,848] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[75.30825 ]
 [75.21894 ]
 [75.16523 ]
 [75.06736 ]
 [75.225426]], R is [[75.41648102]
 [75.43826294]
 [75.45972443]
 [75.48184967]
 [75.49961853]].
[2019-04-27 18:13:37,052] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.6312916e-17 1.0000000e+00 1.2480206e-09 1.4960582e-19 4.8082106e-13], sum to 1.0000
[2019-04-27 18:13:37,059] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1032
[2019-04-27 18:13:37,061] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.21666666666667, 52.0, 1.0, 2.0, 0.9083957979895543, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.916662343411864, 6.9112, 121.9259202585451, 1100552.338293386, 1097755.133054895, 222311.0855331777], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7735800.0000, 
sim time next is 7736400.0000, 
raw observation next is [28.4, 51.0, 1.0, 2.0, 0.8835631675674779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260393263343, 1068187.210292104, 1068187.210292104, 216674.179084951], 
processed observation next is [1.0, 0.5652173913043478, 0.6074074074074074, 0.51, 1.0, 1.0, 0.8613847232946166, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.809462106982422, 0.38149543224717997, 0.38149543224717997, 0.4166811136249058], 
reward next is 0.5833, 
noisyNet noise sample is [array([-1.5419722], dtype=float32), 0.35463738]. 
=============================================
[2019-04-27 18:13:40,592] A3C_AGENT_WORKER-Thread-13 INFO:Local step 13000, global step 207628: loss 0.1829
[2019-04-27 18:13:40,593] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 13000, global step 207629: learning rate 0.0001
[2019-04-27 18:13:40,624] A3C_AGENT_WORKER-Thread-18 INFO:Local step 13000, global step 207640: loss 0.2563
[2019-04-27 18:13:40,627] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 13000, global step 207640: learning rate 0.0001
[2019-04-27 18:13:40,661] A3C_AGENT_WORKER-Thread-15 INFO:Local step 13000, global step 207658: loss 0.2263
[2019-04-27 18:13:40,666] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 13000, global step 207659: learning rate 0.0001
[2019-04-27 18:13:41,040] A3C_AGENT_WORKER-Thread-9 INFO:Local step 13000, global step 207845: loss 0.0896
[2019-04-27 18:13:41,043] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 13000, global step 207846: learning rate 0.0001
[2019-04-27 18:13:41,187] A3C_AGENT_WORKER-Thread-17 INFO:Local step 13000, global step 207915: loss 0.0710
[2019-04-27 18:13:41,189] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 13000, global step 207915: learning rate 0.0001
[2019-04-27 18:13:41,237] A3C_AGENT_WORKER-Thread-16 INFO:Local step 13000, global step 207938: loss 0.0834
[2019-04-27 18:13:41,237] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 13000, global step 207938: learning rate 0.0001
[2019-04-27 18:13:41,296] A3C_AGENT_WORKER-Thread-10 INFO:Local step 13000, global step 207973: loss 0.1273
[2019-04-27 18:13:41,298] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 13000, global step 207974: learning rate 0.0001
[2019-04-27 18:13:41,309] A3C_AGENT_WORKER-Thread-22 INFO:Local step 13000, global step 207979: loss 0.0812
[2019-04-27 18:13:41,311] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 13000, global step 207980: learning rate 0.0001
[2019-04-27 18:13:41,320] A3C_AGENT_WORKER-Thread-12 INFO:Local step 13000, global step 207986: loss 0.0437
[2019-04-27 18:13:41,322] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 13000, global step 207986: learning rate 0.0001
[2019-04-27 18:13:41,507] A3C_AGENT_WORKER-Thread-2 INFO:Local step 13000, global step 208076: loss 0.0131
[2019-04-27 18:13:41,510] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 13000, global step 208076: learning rate 0.0001
[2019-04-27 18:13:41,525] A3C_AGENT_WORKER-Thread-19 INFO:Local step 13000, global step 208083: loss 0.0102
[2019-04-27 18:13:41,529] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 13000, global step 208084: learning rate 0.0001
[2019-04-27 18:13:41,542] A3C_AGENT_WORKER-Thread-3 INFO:Local step 13000, global step 208091: loss 0.0113
[2019-04-27 18:13:41,544] A3C_AGENT_WORKER-Thread-14 INFO:Local step 13000, global step 208091: loss 0.0023
[2019-04-27 18:13:41,545] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 13000, global step 208091: learning rate 0.0001
[2019-04-27 18:13:41,545] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 13000, global step 208091: learning rate 0.0001
[2019-04-27 18:13:41,813] A3C_AGENT_WORKER-Thread-20 INFO:Local step 13000, global step 208220: loss 0.0695
[2019-04-27 18:13:41,815] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 13000, global step 208221: learning rate 0.0001
[2019-04-27 18:13:41,954] A3C_AGENT_WORKER-Thread-11 INFO:Local step 13000, global step 208290: loss 0.1851
[2019-04-27 18:13:41,956] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 13000, global step 208290: learning rate 0.0001
[2019-04-27 18:13:42,099] A3C_AGENT_WORKER-Thread-21 INFO:Local step 13000, global step 208353: loss 0.3102
[2019-04-27 18:13:42,101] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 13000, global step 208353: learning rate 0.0001
[2019-04-27 18:13:42,740] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.5495751e-12 9.9999976e-01 1.8735666e-07 6.2299086e-13 4.9240668e-08], sum to 1.0000
[2019-04-27 18:13:42,746] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3359
[2019-04-27 18:13:42,753] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1390625.990426525 W.
[2019-04-27 18:13:42,759] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.13333333333333, 35.0, 1.0, 2.0, 0.575258218878199, 0.0, 1.0, 0.0, 1.0, 1.0, 0.932783605223593, 6.911200000000001, 6.9112, 121.9260426156255, 1390625.990426525, 1390625.990426524, 282076.2667602759], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7836000.0000, 
sim time next is 7836600.0000, 
raw observation next is [31.16666666666666, 35.5, 1.0, 2.0, 0.5824249203337503, 1.0, 1.0, 0.5824249203337503, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1398579.360827322, 1398579.360827322, 265110.2448341479], 
processed observation next is [1.0, 0.6956521739130435, 0.7098765432098764, 0.355, 1.0, 1.0, 0.5028868099211312, 1.0, 0.5, 0.5028868099211312, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.49949262886690066, 0.49949262886690066, 0.509827393911823], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0602014], dtype=float32), -0.28235766]. 
=============================================
[2019-04-27 18:13:48,875] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0022611e-12 9.9841881e-01 1.5755985e-03 9.2954084e-13 5.5447949e-06], sum to 1.0000
[2019-04-27 18:13:48,886] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3178
[2019-04-27 18:13:48,893] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.01666666666667, 57.16666666666667, 1.0, 2.0, 0.4980179346387755, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 594465.5973739398, 594465.5973739398, 142601.434049864], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7930200.0000, 
sim time next is 7930800.0000, 
raw observation next is [27.8, 58.0, 1.0, 2.0, 0.496250783793686, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 592842.8620973319, 592842.8620973319, 142342.391791212], 
processed observation next is [1.0, 0.8260869565217391, 0.5851851851851853, 0.58, 1.0, 1.0, 0.4002985521353405, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21172959360618995, 0.21172959360618995, 0.2737353688292538], 
reward next is 0.7263, 
noisyNet noise sample is [array([-0.05577897], dtype=float32), 0.48204616]. 
=============================================
[2019-04-27 18:13:49,842] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:13:49,843] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:13:49,844] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res8/Eplus-env-sub_run2
[2019-04-27 18:13:49,906] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:13:49,907] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:13:49,908] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res13/Eplus-env-sub_run2
[2019-04-27 18:13:49,930] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:13:49,931] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:13:49,934] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res10/Eplus-env-sub_run2
[2019-04-27 18:13:50,224] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:13:50,224] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:13:50,226] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res4/Eplus-env-sub_run2
[2019-04-27 18:13:50,435] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:13:50,435] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:13:50,437] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res12/Eplus-env-sub_run2
[2019-04-27 18:13:50,457] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:13:50,457] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:13:50,459] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res5/Eplus-env-sub_run2
[2019-04-27 18:13:50,473] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:13:50,474] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:13:50,476] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res11/Eplus-env-sub_run2
[2019-04-27 18:13:50,491] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:13:50,492] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:13:50,493] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:13:50,495] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res7/Eplus-env-sub_run2
[2019-04-27 18:13:50,493] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:13:50,515] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res17/Eplus-env-sub_run2
[2019-04-27 18:13:50,535] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:13:50,536] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:13:50,537] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res14/Eplus-env-sub_run2
[2019-04-27 18:13:50,558] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:13:50,558] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:13:50,561] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:13:50,562] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res9/Eplus-env-sub_run2
[2019-04-27 18:13:50,562] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:13:50,583] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res2/Eplus-env-sub_run2
[2019-04-27 18:13:50,581] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:13:50,609] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:13:50,611] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res3/Eplus-env-sub_run2
[2019-04-27 18:13:50,635] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:13:50,635] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:13:50,637] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res15/Eplus-env-sub_run2
[2019-04-27 18:13:50,673] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:13:50,673] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:13:50,675] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res6/Eplus-env-sub_run2
[2019-04-27 18:13:50,725] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:13:50,726] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:13:50,727] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res16/Eplus-env-sub_run2
[2019-04-27 18:13:55,737] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3667499e-16 1.1058591e-05 9.9926108e-01 2.9162264e-15 7.2782801e-04], sum to 1.0000
[2019-04-27 18:13:55,746] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0410
[2019-04-27 18:13:55,752] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.2, 41.5, 1.0, 2.0, 0.1974168981821676, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3237926891429665, 6.911199999999999, 6.9112, 121.9260426156618, 483872.9951429921, 483872.9951429925, 167605.9635612164], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 64200.0000, 
sim time next is 64800.0000, 
raw observation next is [29.1, 42.0, 1.0, 2.0, 0.2009770620591477, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3295059262193575, 6.9112, 6.9112, 121.9260426156618, 492399.0682072404, 492399.0682072404, 168431.86380648], 
processed observation next is [1.0, 0.782608695652174, 0.6333333333333334, 0.42, 1.0, 1.0, 0.048782216737080594, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.16188240777419682, 0.0, 0.0, 0.8094621288201359, 0.1758568100740144, 0.1758568100740144, 0.3239074303970769], 
reward next is 0.6761, 
noisyNet noise sample is [array([1.2793976], dtype=float32), -1.1778084]. 
=============================================
[2019-04-27 18:13:56,983] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.5398510e-14 1.0046226e-03 9.9780136e-01 3.1998966e-11 1.1940130e-03], sum to 1.0000
[2019-04-27 18:13:56,992] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5010
[2019-04-27 18:13:56,997] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.8, 65.0, 1.0, 2.0, 0.2144230489950564, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3499021926036051, 6.911199999999999, 6.9112, 121.9260426156618, 522518.6557235957, 522518.6557235962, 171812.2587339779], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 82800.0000, 
sim time next is 83400.0000, 
raw observation next is [24.68333333333333, 65.5, 1.0, 2.0, 0.2145727814585534, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3502404338470596, 6.9112, 6.9112, 121.9260426156618, 523054.8940471184, 523054.8940471184, 171829.2276889396], 
processed observation next is [1.0, 1.0, 0.46975308641975294, 0.655, 1.0, 1.0, 0.06496759697446834, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.18780054230882448, 0.0, 0.0, 0.8094621288201359, 0.18680531930254227, 0.18680531930254227, 0.33044082247873], 
reward next is 0.6696, 
noisyNet noise sample is [array([-0.09733088], dtype=float32), -0.30902717]. 
=============================================
[2019-04-27 18:13:57,373] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.9285170e-14 5.8748876e-04 9.9929881e-01 7.5583550e-12 1.1364453e-04], sum to 1.0000
[2019-04-27 18:13:57,379] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2816
[2019-04-27 18:13:57,384] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [23.6, 71.33333333333333, 1.0, 2.0, 0.2104747321266072, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3442255514369105, 6.911199999999999, 6.9112, 121.9260426156618, 514254.5074780318, 514254.5074780322, 170755.0426030976], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 89400.0000, 
sim time next is 90000.0000, 
raw observation next is [23.5, 72.0, 1.0, 2.0, 0.2105996188933928, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3444199649281963, 6.911199999999999, 6.9112, 121.9260426156618, 514542.7970830501, 514542.7970830505, 170785.6133789265], 
processed observation next is [1.0, 0.043478260869565216, 0.42592592592592593, 0.72, 1.0, 1.0, 0.06023764153975332, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.1805249561602454, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1837652846725179, 0.18376528467251801, 0.32843387188255097], 
reward next is 0.6716, 
noisyNet noise sample is [array([0.45567667], dtype=float32), 0.3333981]. 
=============================================
[2019-04-27 18:13:57,394] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[61.201927]
 [61.16617 ]
 [61.196373]
 [61.267128]
 [61.710915]], R is [[61.31425858]
 [61.3727417 ]
 [61.43062592]
 [61.48783493]
 [61.54450226]].
[2019-04-27 18:13:58,446] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.5895206e-12 3.8433075e-03 9.9260968e-01 8.6131144e-11 3.5469846e-03], sum to 1.0000
[2019-04-27 18:13:58,453] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0397
[2019-04-27 18:13:58,457] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [21.9, 75.33333333333334, 1.0, 2.0, 0.2224235636532685, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3692063860792938, 6.9112, 6.9112, 121.9260426156618, 551364.3081173482, 551364.3081173482, 172591.9194348519], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 109200.0000, 
sim time next is 109800.0000, 
raw observation next is [22.0, 75.0, 1.0, 2.0, 0.2227147026713839, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3694515661642485, 6.911199999999999, 6.9112, 121.9260426156618, 551789.9142875596, 551789.91428756, 172699.3215688297], 
processed observation next is [1.0, 0.2608695652173913, 0.37037037037037035, 0.75, 1.0, 1.0, 0.07466036032307607, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.21181445770531063, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.19706782653127128, 0.19706782653127145, 0.33211407994005715], 
reward next is 0.6679, 
noisyNet noise sample is [array([0.83849335], dtype=float32), 1.5995916]. 
=============================================
[2019-04-27 18:13:58,963] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9075659e-09 4.7846753e-03 9.8774892e-01 3.0499695e-08 7.4663260e-03], sum to 1.0000
[2019-04-27 18:13:58,971] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2873
[2019-04-27 18:13:58,976] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [37.4, 14.0, 1.0, 2.0, 0.697080880403546, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9556753029668248, 6.911200000000001, 6.9112, 121.9260426156618, 1572564.806127291, 1572564.806127291, 303191.7842832784], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 138000.0000, 
sim time next is 138600.0000, 
raw observation next is [37.4, 13.5, 1.0, 2.0, 0.706863024920697, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9557788785774721, 6.9112, 6.9112, 121.9260426156618, 1586872.653328247, 1586872.653328247, 304481.8278145028], 
processed observation next is [1.0, 0.6086956521739131, 0.9407407407407407, 0.135, 1.0, 1.0, 0.6510274106198773, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.9447235982218402, 0.0, 0.0, 0.8094621288201359, 0.5667402333315168, 0.5667402333315168, 0.5855419765663515], 
reward next is 0.4145, 
noisyNet noise sample is [array([0.49188244], dtype=float32), -0.0051058982]. 
=============================================
[2019-04-27 18:13:59,457] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.51249182e-07 1.41037116e-02 9.08266723e-01 1.15382545e-05
 7.76179060e-02], sum to 1.0000
[2019-04-27 18:13:59,466] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0478
[2019-04-27 18:13:59,470] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.26666666666667, 42.0, 1.0, 2.0, 0.601012887151465, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9620007362285873, 6.911200000000001, 6.9112, 121.9260426156618, 1414217.770677322, 1414217.770677322, 293374.0405727322], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 127200.0000, 
sim time next is 127800.0000, 
raw observation next is [31.65, 40.0, 1.0, 2.0, 0.6216599421366005, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9705954269757547, 6.911199999999999, 6.9112, 121.9260426156618, 1447782.850202315, 1447782.850202316, 297836.7089122556], 
processed observation next is [1.0, 0.4782608695652174, 0.7277777777777777, 0.4, 1.0, 1.0, 0.5495951692102387, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.9632442837196933, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.517065303643684, 0.5170653036436843, 0.5727629017543376], 
reward next is 0.4272, 
noisyNet noise sample is [array([0.13163394], dtype=float32), -1.9468722]. 
=============================================
[2019-04-27 18:14:06,015] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1507444e-16 3.6504476e-05 9.9996006e-01 7.6424246e-14 3.5153537e-06], sum to 1.0000
[2019-04-27 18:14:06,023] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3175
[2019-04-27 18:14:06,032] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.91666666666666, 25.33333333333334, 1.0, 2.0, 0.1778278379644496, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3082512042503862, 6.911199999999999, 6.9112, 121.9260426156618, 452309.9849383421, 452309.9849383425, 160251.6842029537], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 244200.0000, 
sim time next is 244800.0000, 
raw observation next is [29.7, 26.0, 1.0, 2.0, 0.1768607349432163, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3065599922964047, 6.911199999999999, 6.9112, 121.9260426156618, 449839.979502729, 449839.9795027294, 160046.0689441109], 
processed observation next is [0.0, 0.8695652173913043, 0.6555555555555556, 0.26, 1.0, 1.0, 0.02007230350382893, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.13319999037050584, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.16065713553668895, 0.16065713553668906, 0.3077809018155979], 
reward next is 0.6922, 
noisyNet noise sample is [array([-1.442582], dtype=float32), 0.9128969]. 
=============================================
[2019-04-27 18:14:11,297] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.8386260e-16 9.9998784e-01 1.2125456e-05 4.3627470e-15 9.5648975e-11], sum to 1.0000
[2019-04-27 18:14:11,308] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1925
[2019-04-27 18:14:11,315] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.3, 46.0, 1.0, 2.0, 0.2850492714174047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 367584.3988154322, 367584.3988154322, 113735.2502815941], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 343200.0000, 
sim time next is 343800.0000, 
raw observation next is [23.15, 46.5, 1.0, 2.0, 0.2833036039235055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 365426.5926981753, 365426.5926981753, 113217.1536443969], 
processed observation next is [0.0, 1.0, 0.4129629629629629, 0.465, 1.0, 1.0, 0.14679000467083989, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.13050949739220546, 0.13050949739220546, 0.21772529546999403], 
reward next is 0.7823, 
noisyNet noise sample is [array([0.15777399], dtype=float32), -1.4070629]. 
=============================================
[2019-04-27 18:14:19,175] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-27 18:14:19,176] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 18:14:19,177] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:14:19,178] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 18:14:19,179] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 18:14:19,179] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:14:19,179] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:14:19,180] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 18:14:19,181] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 18:14:19,183] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:14:19,183] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:14:19,197] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run10
[2019-04-27 18:14:19,219] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run10
[2019-04-27 18:14:19,221] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run10
[2019-04-27 18:14:19,237] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run10
[2019-04-27 18:14:19,239] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run10
[2019-04-27 18:14:22,573] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0095262], dtype=float32), 0.040308796]
[2019-04-27 18:14:22,574] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [33.03810017, 6.0100131045, 1.0, 2.0, 0.3657542250570388, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 471837.4155345578, 471837.4155345578, 107554.6902974168]
[2019-04-27 18:14:22,577] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:14:22,580] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [5.6509918e-14 1.0000000e+00 2.7117304e-08 1.9318928e-12 2.0258069e-09], sampled 0.22169423825035273
[2019-04-27 18:14:29,838] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.0095262], dtype=float32), 0.040308796]
[2019-04-27 18:14:29,840] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [23.13333333333333, 46.33333333333333, 1.0, 2.0, 0.2656185921356027, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 342629.6308212939, 342629.6308212939, 109818.0398948398]
[2019-04-27 18:14:29,843] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 18:14:29,845] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [4.5781985e-14 1.0000000e+00 1.7100223e-08 1.4630556e-12 1.2321898e-09], sampled 0.34893779087504573
[2019-04-27 18:14:56,863] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0095262], dtype=float32), 0.040308796]
[2019-04-27 18:14:56,866] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [29.5, 64.0, 1.0, 2.0, 0.6293488007611747, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 717241.9240619403, 717241.9240619403, 163199.8021816847]
[2019-04-27 18:14:56,866] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 18:14:56,871] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [4.6121262e-14 1.0000000e+00 1.5155333e-08 1.5435626e-12 1.1290636e-09], sampled 0.24093628027332115
[2019-04-27 18:15:13,177] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0095262], dtype=float32), 0.040308796]
[2019-04-27 18:15:13,178] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [31.79368148166666, 70.76052706333334, 1.0, 2.0, 0.8349729599529908, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 951728.4839315707, 951728.4839315707, 203284.4883497731]
[2019-04-27 18:15:13,179] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:15:13,182] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.3403715e-13 1.0000000e+00 2.7417128e-08 4.4213708e-12 2.9105205e-09], sampled 0.5386425232760352
[2019-04-27 18:15:28,848] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0095262], dtype=float32), 0.040308796]
[2019-04-27 18:15:28,848] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [29.09695817333333, 79.23189099333334, 1.0, 2.0, 0.7932848232772818, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 904183.0005958242, 904183.0005958242, 194562.7081831988]
[2019-04-27 18:15:28,850] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:15:28,853] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.0272282e-13 1.0000000e+00 3.4336349e-08 4.0487978e-12 3.6742285e-09], sampled 0.04054216300833369
[2019-04-27 18:15:34,628] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0095262], dtype=float32), 0.040308796]
[2019-04-27 18:15:34,628] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [24.3, 91.0, 1.0, 2.0, 0.6040049137053279, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 697667.0943625002, 697667.0943625002, 159219.5466225503]
[2019-04-27 18:15:34,630] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 18:15:34,632] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [3.1052177e-14 1.0000000e+00 1.4318790e-08 1.2022309e-12 1.2631702e-09], sampled 0.29582144705459923
[2019-04-27 18:15:38,199] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0095262], dtype=float32), 0.040308796]
[2019-04-27 18:15:38,203] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [29.04905215666667, 37.29203274, 1.0, 2.0, 0.5538451777528179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 689414.354208937, 689414.354208937, 152465.5929767399]
[2019-04-27 18:15:38,203] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:15:38,209] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [2.7614285e-13 1.0000000e+00 4.4476423e-08 7.8361814e-12 5.1248392e-09], sampled 0.7866332750263789
[2019-04-27 18:15:38,498] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.0095262], dtype=float32), 0.040308796]
[2019-04-27 18:15:38,498] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [21.36666666666667, 91.0, 1.0, 2.0, 0.4325344872536278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 528065.785898513, 528065.785898513, 133095.0360588067]
[2019-04-27 18:15:38,499] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 18:15:38,502] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [7.3834249e-14 1.0000000e+00 1.5210937e-08 2.0379380e-12 1.1483103e-09], sampled 0.2584687746228561
[2019-04-27 18:15:44,593] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0095262], dtype=float32), 0.040308796]
[2019-04-27 18:15:44,595] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [23.33333333333333, 92.33333333333334, 1.0, 2.0, 0.5866280843811902, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 689238.9114838786, 689238.9114838786, 156748.2881491595]
[2019-04-27 18:15:44,596] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 18:15:44,598] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [7.379285e-14 1.000000e+00 1.664618e-08 2.161485e-12 1.305457e-09], sampled 0.48849230078776174
[2019-04-27 18:16:03,017] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8099.9424 2445324199.3218 746.0000
[2019-04-27 18:16:03,101] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8699.9782 2195165741.0222 572.0000
[2019-04-27 18:16:03,184] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8583.7021 2248718286.8236 553.0000
[2019-04-27 18:16:03,207] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8771.0838 2170637693.1962 493.0000
[2019-04-27 18:16:03,307] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8924.2184 2120437061.2969 430.0000
[2019-04-27 18:16:04,324] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 225000, evaluation results [225000.0, 8099.942407935641, 2445324199.321786, 746.0, 8771.08379319977, 2170637693.1962304, 493.0, 8924.218435439243, 2120437061.2969296, 430.0, 8583.702050975664, 2248718286.823596, 553.0, 8699.978174100708, 2195165741.0221944, 572.0]
[2019-04-27 18:16:07,359] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.7942620e-19 1.0000000e+00 1.6167745e-10 4.1717838e-17 1.0435684e-12], sum to 1.0000
[2019-04-27 18:16:07,365] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4967
[2019-04-27 18:16:07,371] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.3, 73.0, 1.0, 2.0, 0.2988458956170584, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 380948.0458494136, 380948.0458494136, 115431.7378860021], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 539400.0000, 
sim time next is 540000.0000, 
raw observation next is [20.5, 72.0, 1.0, 2.0, 0.3007088923217126, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 383093.8742331467, 383093.8742331467, 115660.9020448967], 
processed observation next is [1.0, 0.2608695652173913, 0.3148148148148148, 0.72, 1.0, 1.0, 0.16751058609727693, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1368192407975524, 0.1368192407975524, 0.22242481162480135], 
reward next is 0.7776, 
noisyNet noise sample is [array([0.2759609], dtype=float32), -0.026069231]. 
=============================================
[2019-04-27 18:16:07,385] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[73.41886]
 [73.51049]
 [73.58227]
 [73.67448]
 [73.66808]], R is [[73.36267853]
 [73.40706635]
 [73.45143127]
 [73.49575806]
 [73.54003143]].
[2019-04-27 18:16:12,508] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7651652e-16 9.9999988e-01 1.5899434e-09 1.2185818e-10 1.5069845e-07], sum to 1.0000
[2019-04-27 18:16:12,514] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3556
[2019-04-27 18:16:12,518] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.16666666666667, 57.0, 1.0, 2.0, 0.4416331101338671, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 555148.0899734411, 555148.0899734411, 134787.7030329776], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 631200.0000, 
sim time next is 631800.0000, 
raw observation next is [24.45, 56.0, 1.0, 2.0, 0.4578880194938251, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 574934.2751851243, 574934.2751851243, 137217.5998057962], 
processed observation next is [1.0, 0.30434782608695654, 0.4611111111111111, 0.56, 1.0, 1.0, 0.35462859463550617, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.20533366970897296, 0.20533366970897296, 0.26387999962653114], 
reward next is 0.7361, 
noisyNet noise sample is [array([0.9426261], dtype=float32), -0.93732166]. 
=============================================
[2019-04-27 18:16:17,167] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.6437189e-17 1.0000000e+00 1.2848414e-11 1.4625011e-11 4.3624077e-08], sum to 1.0000
[2019-04-27 18:16:17,175] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7080
[2019-04-27 18:16:17,180] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 54.0, 1.0, 2.0, 0.4568728632101202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 578452.5066020227, 578452.5066020227, 137124.6042342954], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 717000.0000, 
sim time next is 717600.0000, 
raw observation next is [24.1, 54.00000000000001, 1.0, 2.0, 0.4100490740951208, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 518679.3911358484, 518679.3911358484, 130222.5151801996], 
processed observation next is [1.0, 0.30434782608695654, 0.4481481481481482, 0.54, 1.0, 1.0, 0.2976774691608581, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.18524263969137444, 0.18524263969137444, 0.25042791380807616], 
reward next is 0.7496, 
noisyNet noise sample is [array([0.04396416], dtype=float32), 0.83251715]. 
=============================================
[2019-04-27 18:16:21,140] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.2584319e-25 1.0000000e+00 2.3102185e-21 5.3024872e-20 3.0465656e-20], sum to 1.0000
[2019-04-27 18:16:21,148] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9242
[2019-04-27 18:16:21,155] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.66666666666667, 58.0, 1.0, 2.0, 0.3055435975247331, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 389285.3410063101, 389285.3410063105, 116261.0815725421], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 796200.0000, 
sim time next is 796800.0000, 
raw observation next is [22.73333333333333, 58.0, 1.0, 2.0, 0.3068195908695055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 390675.5552164813, 390675.5552164813, 116419.0566527934], 
processed observation next is [0.0, 0.21739130434782608, 0.39753086419753075, 0.58, 1.0, 1.0, 0.17478522722560177, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1395269840058862, 0.1395269840058862, 0.2238828012553719], 
reward next is 0.7761, 
noisyNet noise sample is [array([1.2519884], dtype=float32), 0.527399]. 
=============================================
[2019-04-27 18:16:23,531] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.7672114e-26 1.0000000e+00 1.9166279e-20 2.2691697e-19 8.2620771e-21], sum to 1.0000
[2019-04-27 18:16:23,540] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9567
[2019-04-27 18:16:23,545] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.26666666666667, 39.66666666666667, 1.0, 2.0, 0.4005263449718034, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 495716.3703255517, 495716.3703255513, 128680.7546200353], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 847200.0000, 
sim time next is 847800.0000, 
raw observation next is [29.05, 40.0, 1.0, 2.0, 0.396734118066064, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 491737.139182343, 491737.139182343, 128163.7051225248], 
processed observation next is [0.0, 0.8260869565217391, 0.6314814814814815, 0.4, 1.0, 1.0, 0.2818263310310286, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.17562040685083677, 0.17562040685083677, 0.24646866369716308], 
reward next is 0.7535, 
noisyNet noise sample is [array([-1.3143431], dtype=float32), -0.85212994]. 
=============================================
[2019-04-27 18:16:26,595] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.0586918e-25 1.0000000e+00 6.1771638e-20 1.3574464e-18 2.1196797e-18], sum to 1.0000
[2019-04-27 18:16:26,604] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1137
[2019-04-27 18:16:26,609] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.3, 62.0, 1.0, 2.0, 0.389452763564373, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 483954.9489428662, 483954.9489428657, 127173.8624910522], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 901800.0000, 
sim time next is 902400.0000, 
raw observation next is [24.43333333333333, 61.66666666666667, 1.0, 2.0, 0.3916040999866934, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 486170.7539669792, 486170.7539669792, 127463.7596529743], 
processed observation next is [0.0, 0.43478260869565216, 0.4604938271604937, 0.6166666666666667, 1.0, 1.0, 0.27571916665082546, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.173632412131064, 0.173632412131064, 0.24512261471725827], 
reward next is 0.7549, 
noisyNet noise sample is [array([0.98980767], dtype=float32), -1.2294537]. 
=============================================
[2019-04-27 18:16:33,445] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.0819453e-13 1.0000000e+00 1.8610168e-10 2.9104413e-10 1.6646574e-10], sum to 1.0000
[2019-04-27 18:16:33,452] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0416
[2019-04-27 18:16:33,460] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.13333333333333, 44.33333333333334, 1.0, 2.0, 0.2844059778141696, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 365487.9617538574, 365487.9617538574, 113667.5071172594], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1023000.0000, 
sim time next is 1023600.0000, 
raw observation next is [24.06666666666667, 44.66666666666667, 1.0, 2.0, 0.2834315310009489, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 364243.4703237468, 364243.4703237464, 113549.8643961878], 
processed observation next is [1.0, 0.8695652173913043, 0.4469135802469137, 0.4466666666666667, 1.0, 1.0, 0.1469422988106535, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.13008695368705242, 0.13008695368705228, 0.2183651238388227], 
reward next is 0.7816, 
noisyNet noise sample is [array([0.16699381], dtype=float32), -0.91648126]. 
=============================================
[2019-04-27 18:16:35,059] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.7022828e-26 1.0000000e+00 9.1712431e-21 1.9659529e-20 2.5506221e-21], sum to 1.0000
[2019-04-27 18:16:35,062] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4480
[2019-04-27 18:16:35,068] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.3, 56.00000000000001, 1.0, 2.0, 0.2824649857660376, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 362053.7714594791, 362053.7714594787, 113436.9872218997], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1038000.0000, 
sim time next is 1038600.0000, 
raw observation next is [22.2, 57.0, 1.0, 2.0, 0.2843492279573451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 364252.5772545736, 364252.5772545736, 113664.6203547071], 
processed observation next is [1.0, 0.0, 0.37777777777777777, 0.57, 1.0, 1.0, 0.14803479518731558, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1300902061623477, 0.1300902061623477, 0.21858580837443675], 
reward next is 0.7814, 
noisyNet noise sample is [array([0.40954626], dtype=float32), -0.7468009]. 
=============================================
[2019-04-27 18:16:36,075] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.6853198e-24 1.0000000e+00 7.4521050e-20 3.9653868e-19 5.5483805e-20], sum to 1.0000
[2019-04-27 18:16:36,085] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6644
[2019-04-27 18:16:36,092] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.33333333333333, 71.33333333333334, 1.0, 2.0, 0.3182387287525402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 406278.9636301027, 406278.9636301022, 117859.2349278032], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1053600.0000, 
sim time next is 1054200.0000, 
raw observation next is [20.26666666666667, 71.66666666666666, 1.0, 2.0, 0.312073112548416, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 398497.7701074292, 398497.7701074292, 117080.923247401], 
processed observation next is [1.0, 0.17391304347826086, 0.3061728395061729, 0.7166666666666666, 1.0, 1.0, 0.18103941970049525, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1423206321812247, 0.1423206321812247, 0.22515562162961733], 
reward next is 0.7748, 
noisyNet noise sample is [array([-0.4064591], dtype=float32), -0.77442056]. 
=============================================
[2019-04-27 18:16:37,114] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.0389537e-19 1.0000000e+00 1.0538687e-14 4.2914229e-13 7.5601282e-15], sum to 1.0000
[2019-04-27 18:16:37,119] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0070
[2019-04-27 18:16:37,125] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.7, 54.33333333333333, 1.0, 2.0, 0.5790210521306727, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 734146.3103040134, 734146.310304013, 156967.404572022], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1072200.0000, 
sim time next is 1072800.0000, 
raw observation next is [23.8, 54.0, 1.0, 2.0, 0.6427076719819256, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 814538.4403317051, 814538.4403317051, 168361.3089252553], 
processed observation next is [1.0, 0.43478260869565216, 0.43703703703703706, 0.54, 1.0, 1.0, 0.5746519904546733, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.29090658583275186, 0.29090658583275186, 0.32377174793318325], 
reward next is 0.6762, 
noisyNet noise sample is [array([1.0452998], dtype=float32), 0.8704213]. 
=============================================
[2019-04-27 18:16:39,290] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1279276e-25 1.0000000e+00 4.5865575e-20 1.9194824e-19 4.4491085e-20], sum to 1.0000
[2019-04-27 18:16:39,302] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3499
[2019-04-27 18:16:39,309] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.66666666666667, 67.33333333333334, 1.0, 2.0, 0.3263826580007151, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 413571.8897500925, 413571.8897500925, 118879.7606055034], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1109400.0000, 
sim time next is 1110000.0000, 
raw observation next is [21.53333333333333, 67.66666666666667, 1.0, 2.0, 0.3238172658218677, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 410667.6601055425, 410667.6601055425, 118554.4372922216], 
processed observation next is [1.0, 0.8695652173913043, 0.35308641975308636, 0.6766666666666667, 1.0, 1.0, 0.19502055454984252, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.14666702146626517, 0.14666702146626517, 0.22798930248504154], 
reward next is 0.7720, 
noisyNet noise sample is [array([-0.82974935], dtype=float32), 0.16521147]. 
=============================================
[2019-04-27 18:16:39,318] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[79.32665 ]
 [79.27436 ]
 [79.08167 ]
 [79.212395]
 [78.9832  ]], R is [[79.36279297]
 [79.34055328]
 [79.31771851]
 [79.29402924]
 [79.26978302]].
[2019-04-27 18:16:41,870] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.4020248e-19 1.0000000e+00 7.6041790e-15 1.0234454e-14 6.1699028e-15], sum to 1.0000
[2019-04-27 18:16:41,878] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0148
[2019-04-27 18:16:41,884] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.1, 65.0, 1.0, 2.0, 0.601236510788263, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 768020.6159507565, 768020.615950756, 160894.8669915523], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1162800.0000, 
sim time next is 1163400.0000, 
raw observation next is [21.16666666666667, 65.0, 1.0, 2.0, 0.5449191596278085, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 695799.2911000772, 695799.2911000772, 151191.0978902326], 
processed observation next is [1.0, 0.4782608695652174, 0.33950617283950635, 0.65, 1.0, 1.0, 0.45823709479501007, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.24849974682145615, 0.24849974682145615, 0.2907521113273704], 
reward next is 0.7092, 
noisyNet noise sample is [array([-0.3800523], dtype=float32), -0.44368008]. 
=============================================
[2019-04-27 18:16:43,499] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.3091603e-28 1.0000000e+00 1.6284634e-22 7.0018302e-21 2.6549402e-21], sum to 1.0000
[2019-04-27 18:16:43,510] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1288
[2019-04-27 18:16:43,517] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.6, 78.5, 1.0, 2.0, 0.3545446156438301, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 446883.88297647, 446883.88297647, 122535.2918662251], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1189800.0000, 
sim time next is 1190400.0000, 
raw observation next is [20.53333333333333, 79.0, 1.0, 2.0, 0.3535872335483449, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 445677.629261822, 445677.629261822, 122407.8889201113], 
processed observation next is [1.0, 0.782608695652174, 0.3160493827160493, 0.79, 1.0, 1.0, 0.23046099231945824, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.15917058187922214, 0.15917058187922214, 0.23539978638482945], 
reward next is 0.7646, 
noisyNet noise sample is [array([-0.00996143], dtype=float32), 0.075833105]. 
=============================================
[2019-04-27 18:16:50,514] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1198611e-23 1.0000000e+00 9.0125182e-18 4.0296501e-17 1.6450239e-19], sum to 1.0000
[2019-04-27 18:16:50,525] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3682
[2019-04-27 18:16:50,529] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.56666666666667, 79.0, 1.0, 2.0, 0.3844689866584043, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 478753.5898859363, 478753.5898859363, 126503.2744975835], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1322400.0000, 
sim time next is 1323000.0000, 
raw observation next is [21.8, 78.0, 1.0, 2.0, 0.3885253910981185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 483204.0152868385, 483204.0152868385, 127053.3399416569], 
processed observation next is [1.0, 0.30434782608695654, 0.362962962962963, 0.78, 1.0, 1.0, 0.27205403702156966, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.17257286260244234, 0.17257286260244234, 0.2443333460416479], 
reward next is 0.7557, 
noisyNet noise sample is [array([1.2536432], dtype=float32), 0.80231696]. 
=============================================
[2019-04-27 18:16:50,545] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[68.29403]
 [68.35046]
 [68.47099]
 [68.50928]
 [68.57578]], R is [[68.29563141]
 [68.36940002]
 [68.44029236]
 [68.51696777]
 [68.59239197]].
[2019-04-27 18:16:55,605] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5655342e-22 1.0000000e+00 3.1698361e-13 1.7658046e-13 2.2138034e-15], sum to 1.0000
[2019-04-27 18:16:55,611] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6413
[2019-04-27 18:16:55,617] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.91666666666667, 27.66666666666666, 1.0, 2.0, 0.3576033788033732, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 450874.6708758818, 450874.6708758818, 122945.1460309329], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1421400.0000, 
sim time next is 1422000.0000, 
raw observation next is [31.1, 27.0, 1.0, 2.0, 0.359358068999974, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 453230.9300340283, 453230.9300340283, 123181.8489391798], 
processed observation next is [0.0, 0.4782608695652174, 0.7074074074074075, 0.27, 1.0, 1.0, 0.2373310345237786, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16186818929786725, 0.16186818929786725, 0.23688817103688425], 
reward next is 0.7631, 
noisyNet noise sample is [array([0.905201], dtype=float32), -0.56096864]. 
=============================================
[2019-04-27 18:16:55,641] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[75.32458]
 [75.21741]
 [75.10921]
 [75.00374]
 [74.89737]], R is [[75.47625732]
 [75.48506165]
 [75.49417114]
 [75.50350952]
 [75.51303101]].
[2019-04-27 18:16:56,686] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-27 18:16:56,690] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 18:16:56,691] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 18:16:56,692] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:16:56,694] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:16:56,696] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 18:16:56,697] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 18:16:56,700] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:16:56,702] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:16:56,702] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 18:16:56,705] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:16:56,711] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run11
[2019-04-27 18:16:56,731] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run11
[2019-04-27 18:16:56,732] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run11
[2019-04-27 18:16:56,751] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run11
[2019-04-27 18:16:56,788] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run11
[2019-04-27 18:16:57,839] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.01604273], dtype=float32), 0.05103051]
[2019-04-27 18:16:57,841] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [19.12874297666666, 78.89496878, 1.0, 2.0, 0.3054731874923106, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 389607.1377904998, 389607.1377904998, 116252.8863917655]
[2019-04-27 18:16:57,842] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:16:57,844] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [7.0228752e-23 1.0000000e+00 1.0091593e-14 5.1904282e-15 1.5989886e-16], sampled 0.4925339241520613
[2019-04-27 18:17:10,106] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01604273], dtype=float32), 0.05103051]
[2019-04-27 18:17:10,107] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [24.71877180333333, 58.35702348666668, 1.0, 2.0, 0.391632291035808, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 487307.732860855, 487307.732860855, 127490.4244617949]
[2019-04-27 18:17:10,107] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 18:17:10,110] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.1059571e-24 1.0000000e+00 1.0558721e-15 6.2608726e-16 1.5056495e-17], sampled 0.12191329759323666
[2019-04-27 18:17:26,013] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.01604273], dtype=float32), 0.05103051]
[2019-04-27 18:17:26,014] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [22.58333333333333, 75.66666666666666, 1.0, 2.0, 0.3935062812383626, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 486493.8752432722, 486493.8752432722, 127683.4921558507]
[2019-04-27 18:17:26,016] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 18:17:26,019] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [5.4038211e-24 1.0000000e+00 1.6280840e-15 1.1565562e-15 2.4983756e-17], sampled 0.011473890958371813
[2019-04-27 18:17:53,742] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01604273], dtype=float32), 0.05103051]
[2019-04-27 18:17:53,743] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [32.270519905, 78.37363863666667, 1.0, 2.0, 0.9020730589405078, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.9260426156618, 1743450.910178822, 1743450.910178822, 357427.9938124559]
[2019-04-27 18:17:53,744] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 18:17:53,747] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [6.3427515e-23 1.0000000e+00 4.9122897e-14 1.5627543e-14 1.7732652e-15], sampled 0.004184333504171245
[2019-04-27 18:17:53,749] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1743450.910178822 W.
[2019-04-27 18:18:22,014] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.01604273], dtype=float32), 0.05103051]
[2019-04-27 18:18:22,017] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [29.76794623666667, 39.34526443999999, 1.0, 2.0, 0.6294233861001173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 773303.7658164947, 773303.7658164947, 165449.855666143]
[2019-04-27 18:18:22,020] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:18:22,022] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [5.3892461e-24 1.0000000e+00 3.6897729e-15 2.1179856e-15 7.1599358e-17], sampled 0.6065226936673817
[2019-04-27 18:18:27,296] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.01604273], dtype=float32), 0.05103051]
[2019-04-27 18:18:27,298] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [30.73333333333333, 48.33333333333333, 1.0, 2.0, 0.5315744975266801, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 626190.8660331797, 626190.8660331797, 147638.1230235645]
[2019-04-27 18:18:27,300] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 18:18:27,305] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [2.7824341e-24 1.0000000e+00 1.6982110e-15 9.9844866e-16 2.6779028e-17], sampled 0.8379045679766979
[2019-04-27 18:18:40,421] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8701.7435 2195019782.1641 572.0000
[2019-04-27 18:18:40,515] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8771.9095 2170601483.0346 493.0000
[2019-04-27 18:18:40,545] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8924.2399 2120425912.2928 430.0000
[2019-04-27 18:18:40,642] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8100.7222 2445312557.4025 746.0000
[2019-04-27 18:18:40,674] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8584.5265 2248663206.3456 553.0000
[2019-04-27 18:18:41,689] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 250000, evaluation results [250000.0, 8100.722247181571, 2445312557.402538, 746.0, 8771.90946536216, 2170601483.03465, 493.0, 8924.239875831723, 2120425912.2928398, 430.0, 8584.526542641555, 2248663206.345631, 553.0, 8701.74352259033, 2195019782.1641326, 572.0]
[2019-04-27 18:18:48,000] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.7961068e-20 1.0000000e+00 8.8669488e-12 4.4130254e-14 1.5972954e-14], sum to 1.0000
[2019-04-27 18:18:48,009] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4545
[2019-04-27 18:18:48,014] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 70.0, 1.0, 2.0, 0.4305730858864099, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 530499.3643040359, 530499.3643040359, 132938.1119257331], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1548000.0000, 
sim time next is 1548600.0000, 
raw observation next is [23.58333333333334, 69.16666666666667, 1.0, 2.0, 0.4241532756847238, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 523594.0053470071, 523594.0053470066, 132028.2361733138], 
processed observation next is [0.0, 0.9565217391304348, 0.4290123456790126, 0.6916666666666668, 1.0, 1.0, 0.31446818533895693, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.18699785905250255, 0.18699785905250235, 0.2539004541794496], 
reward next is 0.7461, 
noisyNet noise sample is [array([-1.4544419], dtype=float32), -0.5569786]. 
=============================================
[2019-04-27 18:18:48,316] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.5830546e-22 1.0000000e+00 1.8459001e-14 8.0909019e-15 1.6240761e-16], sum to 1.0000
[2019-04-27 18:18:48,325] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3241
[2019-04-27 18:18:48,328] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 74.16666666666667, 1.0, 2.0, 0.4570737071116124, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 558087.0356500836, 558087.0356500836, 136737.2689291612], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1545000.0000, 
sim time next is 1545600.0000, 
raw observation next is [23.6, 73.33333333333334, 1.0, 2.0, 0.4516590420500184, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 552490.8291008892, 552490.8291008887, 135953.4825038609], 
processed observation next is [0.0, 0.9130434782608695, 0.4296296296296297, 0.7333333333333334, 1.0, 1.0, 0.3472131452976409, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.19731815325031757, 0.1973181532503174, 0.2614490048151171], 
reward next is 0.7386, 
noisyNet noise sample is [array([-1.0671432], dtype=float32), -0.28012136]. 
=============================================
[2019-04-27 18:18:54,413] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.7634762e-17 1.0000000e+00 2.7377502e-09 1.4192004e-08 1.2851852e-10], sum to 1.0000
[2019-04-27 18:18:54,420] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4554
[2019-04-27 18:18:54,425] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.95, 64.5, 1.0, 2.0, 0.3214411428918963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 408082.1401107764, 408082.1401107764, 118254.8163625456], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1647000.0000, 
sim time next is 1647600.0000, 
raw observation next is [21.86666666666667, 65.0, 1.0, 2.0, 0.3203261706314846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 406688.9905649445, 406688.990564944, 118113.0995051569], 
processed observation next is [1.0, 0.043478260869565216, 0.36543209876543226, 0.65, 1.0, 1.0, 0.1908644888470055, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.14524606805890874, 0.14524606805890858, 0.2271405759714556], 
reward next is 0.7729, 
noisyNet noise sample is [array([-1.120628], dtype=float32), -0.9588459]. 
=============================================
[2019-04-27 18:18:57,486] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.3557799e-18 1.0000000e+00 3.9471887e-10 6.9045565e-09 5.9803378e-11], sum to 1.0000
[2019-04-27 18:18:57,498] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7494
[2019-04-27 18:18:57,502] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.75, 70.0, 1.0, 2.0, 0.4007846838849375, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 493183.4229695576, 493183.4229695576, 128648.0167188991], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1704600.0000, 
sim time next is 1705200.0000, 
raw observation next is [23.7, 70.66666666666667, 1.0, 2.0, 0.4055112561046562, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 498661.612519639, 498661.612519639, 129306.9179822821], 
processed observation next is [1.0, 0.7391304347826086, 0.4333333333333333, 0.7066666666666667, 1.0, 1.0, 0.2922753048864955, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.17809343304272823, 0.17809343304272823, 0.24866714996592712], 
reward next is 0.7513, 
noisyNet noise sample is [array([0.6667859], dtype=float32), -0.71538794]. 
=============================================
[2019-04-27 18:19:01,432] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4691564e-08 3.1156464e-02 1.2056023e-02 9.5066017e-01 6.1274273e-03], sum to 1.0000
[2019-04-27 18:19:01,439] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8081
[2019-04-27 18:19:01,443] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.73333333333333, 63.33333333333333, 1.0, 2.0, 0.3272883803932438, 1.0, 2.0, 0.3272883803932438, 1.0, 1.0, 0.5245443339523641, 6.911199999999999, 6.9112, 121.94756008, 1159415.179039066, 1159415.179039066, 266548.556007792], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1777800.0000, 
sim time next is 1778400.0000, 
raw observation next is [25.8, 63.0, 1.0, 2.0, 0.5129048983696785, 1.0, 2.0, 0.5129048983696785, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1219160.334371304, 1219160.334371305, 241596.3138712186], 
processed observation next is [1.0, 0.6086956521739131, 0.5111111111111112, 0.63, 1.0, 1.0, 0.42012487901152196, 1.0, 1.0, 0.42012487901152196, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.43541440513260854, 0.43541440513260893, 0.4646082959061896], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.17953183], dtype=float32), -0.121814966]. 
=============================================
[2019-04-27 18:19:03,704] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.1797523e-07 9.7115928e-01 2.7201308e-03 2.5439005e-02 6.8118225e-04], sum to 1.0000
[2019-04-27 18:19:03,714] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4599
[2019-04-27 18:19:03,717] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.4, 92.0, 1.0, 2.0, 0.325218156789964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 412495.1360751531, 412495.1360751536, 118734.1251524777], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1825200.0000, 
sim time next is 1825800.0000, 
raw observation next is [18.43333333333333, 91.83333333333334, 1.0, 2.0, 0.3330516797476147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 422337.3035555952, 422337.3035555952, 119741.625715256], 
processed observation next is [1.0, 0.13043478260869565, 0.2382716049382715, 0.9183333333333334, 1.0, 1.0, 0.20601390446144605, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.15083475126985543, 0.15083475126985543, 0.23027235714472308], 
reward next is 0.7697, 
noisyNet noise sample is [array([0.3914717], dtype=float32), -0.08874603]. 
=============================================
[2019-04-27 18:19:04,746] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2164207e-17 9.9998093e-01 4.7959192e-09 1.9020899e-05 1.0557792e-08], sum to 1.0000
[2019-04-27 18:19:04,754] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3070
[2019-04-27 18:19:04,761] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.66666666666667, 77.0, 1.0, 2.0, 0.6762171850748693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 842888.981361981, 842888.981361981, 174426.753269049], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1848000.0000, 
sim time next is 1848600.0000, 
raw observation next is [21.7, 77.0, 1.0, 2.0, 0.6994464106291115, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 871478.3485273995, 871478.3485273995, 178884.9154112911], 
processed observation next is [1.0, 0.391304347826087, 0.3592592592592592, 0.77, 1.0, 1.0, 0.6421981078917994, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3112422673312141, 0.3112422673312141, 0.34400945271402134], 
reward next is 0.6560, 
noisyNet noise sample is [array([0.36738697], dtype=float32), -0.08160081]. 
=============================================
[2019-04-27 18:19:06,655] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.3902719e-13 9.9874711e-01 5.2954979e-06 1.2442493e-03 3.3806157e-06], sum to 1.0000
[2019-04-27 18:19:06,663] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8801
[2019-04-27 18:19:06,668] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.21666666666667, 89.83333333333333, 1.0, 2.0, 0.4262578833797172, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 522750.7584975074, 522750.758497507, 132246.1322709394], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1882200.0000, 
sim time next is 1882800.0000, 
raw observation next is [21.2, 90.0, 1.0, 2.0, 0.4260917032198803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 522515.5840173315, 522515.5840173315, 132221.1580685293], 
processed observation next is [1.0, 0.8260869565217391, 0.34074074074074073, 0.9, 1.0, 1.0, 0.3167758371665242, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.18661270857761839, 0.18661270857761839, 0.2542714578240948], 
reward next is 0.7457, 
noisyNet noise sample is [array([-0.6264736], dtype=float32), 0.15371601]. 
=============================================
[2019-04-27 18:19:15,810] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.9925018e-18 1.0000000e+00 3.8667739e-08 5.5738894e-09 1.3069848e-08], sum to 1.0000
[2019-04-27 18:19:15,819] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0007
[2019-04-27 18:19:15,827] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 63.0, 1.0, 2.0, 0.5725778069390548, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 663793.2016338893, 663793.2016338893, 153964.4684291083], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2043000.0000, 
sim time next is 2043600.0000, 
raw observation next is [28.66666666666666, 63.0, 1.0, 2.0, 0.5760564884621784, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 666975.0936751642, 666975.0936751642, 154511.9500034205], 
processed observation next is [0.0, 0.6521739130434783, 0.6172839506172837, 0.63, 1.0, 1.0, 0.4953053434073552, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.23820539059827292, 0.23820539059827292, 0.29713836539119326], 
reward next is 0.7029, 
noisyNet noise sample is [array([0.0533341], dtype=float32), -0.722059]. 
=============================================
[2019-04-27 18:19:17,501] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2071817e-14 9.9998319e-01 1.0322011e-06 8.0785821e-06 7.6773904e-06], sum to 1.0000
[2019-04-27 18:19:17,510] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5381
[2019-04-27 18:19:17,516] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 77.33333333333334, 1.0, 2.0, 0.559975905935405, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 654338.1253912471, 654338.1253912471, 152086.2231920943], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2064000.0000, 
sim time next is 2064600.0000, 
raw observation next is [25.55, 77.5, 1.0, 2.0, 0.5539844438216828, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 648893.3454298056, 648893.3454298056, 151158.3628880849], 
processed observation next is [0.0, 0.9130434782608695, 0.5018518518518519, 0.775, 1.0, 1.0, 0.46902909978771756, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2317476233677877, 0.2317476233677877, 0.29068915940016327], 
reward next is 0.7093, 
noisyNet noise sample is [array([0.534154], dtype=float32), -0.57855386]. 
=============================================
[2019-04-27 18:19:18,327] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.3525135e-16 9.9999404e-01 4.7805911e-06 4.1283789e-07 8.2666656e-07], sum to 1.0000
[2019-04-27 18:19:18,338] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8180
[2019-04-27 18:19:18,344] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 84.0, 1.0, 2.0, 0.4481729997737318, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 544980.9924951999, 544980.9924951999, 135338.636332073], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2080800.0000, 
sim time next is 2081400.0000, 
raw observation next is [22.38333333333333, 84.5, 1.0, 2.0, 0.4449400148607286, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 541490.4279942287, 541490.4279942282, 134871.7181836431], 
processed observation next is [0.0, 0.08695652173913043, 0.38456790123456774, 0.845, 1.0, 1.0, 0.3392143034056293, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.19338943856936738, 0.19338943856936722, 0.25936868881469827], 
reward next is 0.7406, 
noisyNet noise sample is [array([1.0833024], dtype=float32), -0.051622033]. 
=============================================
[2019-04-27 18:19:18,363] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2296254e-16 9.9999928e-01 3.9033756e-07 3.4716547e-07 3.7783646e-08], sum to 1.0000
[2019-04-27 18:19:18,377] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9722
[2019-04-27 18:19:18,385] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 82.33333333333334, 1.0, 2.0, 0.4594817465831111, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 556297.2624681975, 556297.2624681975, 136957.2107927169], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2077800.0000, 
sim time next is 2078400.0000, 
raw observation next is [22.9, 82.66666666666667, 1.0, 2.0, 0.4583501793244761, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 555410.2477779665, 555410.247777967, 136802.1563783914], 
processed observation next is [0.0, 0.043478260869565216, 0.4037037037037037, 0.8266666666666667, 1.0, 1.0, 0.35517878491009053, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1983608027778452, 0.19836080277784537, 0.263081069958445], 
reward next is 0.7369, 
noisyNet noise sample is [array([0.09516833], dtype=float32), -0.043264203]. 
=============================================
[2019-04-27 18:19:19,030] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.8086711e-14 9.9998951e-01 7.2689636e-06 1.2259231e-06 2.0416890e-06], sum to 1.0000
[2019-04-27 18:19:19,038] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7301
[2019-04-27 18:19:19,043] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.96666666666667, 92.33333333333334, 1.0, 2.0, 0.4264716585103703, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 522483.6514258181, 522483.6514258176, 132262.8151353437], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2089200.0000, 
sim time next is 2089800.0000, 
raw observation next is [20.85, 93.0, 1.0, 2.0, 0.4250632821242188, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 521047.2783759242, 521047.2783759242, 132066.2929049886], 
processed observation next is [0.0, 0.17391304347826086, 0.32777777777777783, 0.93, 1.0, 1.0, 0.31555152633835576, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1860883137056872, 0.1860883137056872, 0.25397364020190116], 
reward next is 0.7460, 
noisyNet noise sample is [array([0.7251897], dtype=float32), 0.6777327]. 
=============================================
[2019-04-27 18:19:24,430] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.8693617e-11 9.9300450e-01 3.7728462e-03 1.9736728e-04 3.0252996e-03], sum to 1.0000
[2019-04-27 18:19:24,440] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8169
[2019-04-27 18:19:24,450] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1604094.419262303 W.
[2019-04-27 18:19:24,453] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [24.45, 89.0, 1.0, 2.0, 1.02, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.734746560400988, 6.9112, 121.9227228355241, 1604094.419262303, 1182376.455353444, 246671.015597291], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2190600.0000, 
sim time next is 2191200.0000, 
raw observation next is [24.43333333333333, 89.33333333333334, 1.0, 2.0, 0.6354251205879831, 1.0, 1.0, 0.6354251205879831, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9255466594163, 1449025.015828885, 1449025.015828885, 280440.8433230385], 
processed observation next is [1.0, 0.34782608695652173, 0.4604938271604937, 0.8933333333333334, 1.0, 1.0, 0.5659822864142655, 1.0, 0.5, 0.5659822864142655, 0.0, 1.0, -0.25, 0.0, 0.0, 0.809458836186424, 0.5175089342246018, 0.5175089342246018, 0.5393093140827663], 
reward next is 0.4607, 
noisyNet noise sample is [array([0.26145998], dtype=float32), 0.31382698]. 
=============================================
[2019-04-27 18:19:27,290] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.2784147e-15 1.2368625e-02 7.3952426e-05 1.4023021e-05 9.8754334e-01], sum to 1.0000
[2019-04-27 18:19:27,299] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2429
[2019-04-27 18:19:27,307] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [22.7, 95.83333333333333, 1.0, 2.0, 0.1821612133313304, 1.0, 2.0, 0.1821612133313304, 1.0, 2.0, 0.2905217967838662, 6.911199999999999, 6.9112, 121.94756008, 633198.5350070584, 633198.5350070589, 214828.8727290529], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2238600.0000, 
sim time next is 2239200.0000, 
raw observation next is [22.7, 96.0, 1.0, 2.0, 0.1826494752845769, 1.0, 2.0, 0.1826494752845769, 1.0, 2.0, 0.2912781056917544, 6.911200000000001, 6.9112, 121.94756008, 634615.2171973451, 634615.2171973446, 214986.4268689624], 
processed observation next is [1.0, 0.9565217391304348, 0.39629629629629626, 0.96, 1.0, 1.0, 0.02696366105306773, 1.0, 1.0, 0.02696366105306773, 1.0, 1.0, 0.11409763211469297, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.22664829185619467, 0.2266482918561945, 0.4134354362864661], 
reward next is 0.5866, 
noisyNet noise sample is [array([0.14006935], dtype=float32), -1.2333874]. 
=============================================
[2019-04-27 18:19:27,428] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.5488381e-13 9.0854832e-05 1.3539511e-04 2.8274176e-04 9.9949098e-01], sum to 1.0000
[2019-04-27 18:19:27,439] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9114
[2019-04-27 18:19:27,443] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [22.7, 94.66666666666666, 1.0, 2.0, 0.1791340368532888, 1.0, 2.0, 0.1791340368532888, 1.0, 2.0, 0.2858598267791899, 6.911200000000001, 6.9112, 121.94756008, 624586.4183121303, 624586.4183121298, 213849.772464234], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2234400.0000, 
sim time next is 2235000.0000, 
raw observation next is [22.7, 94.83333333333333, 1.0, 2.0, 0.1792199752360678, 1.0, 2.0, 0.1792199752360678, 1.0, 2.0, 0.2859744934080223, 6.9112, 6.9112, 121.94756008, 624642.1232237419, 624642.1232237419, 213879.989482526], 
processed observation next is [1.0, 0.8695652173913043, 0.39629629629629626, 0.9483333333333333, 1.0, 1.0, 0.02288092290008072, 1.0, 1.0, 0.02288092290008072, 1.0, 1.0, 0.10746811676002783, 0.0, 0.0, 0.8096049824067558, 0.22308647257990782, 0.22308647257990782, 0.41130767208178076], 
reward next is 0.5887, 
noisyNet noise sample is [array([-1.0249473], dtype=float32), -2.3429477]. 
=============================================
[2019-04-27 18:19:27,452] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[72.360115]
 [72.106316]
 [71.86152 ]
 [71.57926 ]
 [71.471664]], R is [[72.47060394]
 [72.33464813]
 [72.19981384]
 [72.06617737]
 [71.93402863]].
[2019-04-27 18:19:30,283] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [6.3516515e-13 4.4390024e-08 5.5317798e-07 9.8395421e-06 9.9998951e-01], sum to 1.0000
[2019-04-27 18:19:30,287] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6865
[2019-04-27 18:19:30,296] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.7, 81.0, 1.0, 2.0, 0.3488104554621297, 1.0, 2.0, 0.3488104554621297, 1.0, 2.0, 0.5554464355035669, 6.9112, 6.9112, 121.94756008, 1197794.15004503, 1197794.15004503, 275412.7878417881], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2293200.0000, 
sim time next is 2293800.0000, 
raw observation next is [24.8, 80.66666666666667, 1.0, 2.0, 0.3350525259582216, 1.0, 2.0, 0.3350525259582216, 1.0, 2.0, 0.5334529202225375, 6.911200000000001, 6.9112, 121.94756008, 1147588.731432568, 1147588.731432567, 269812.4962384958], 
processed observation next is [1.0, 0.5652173913043478, 0.4740740740740741, 0.8066666666666668, 1.0, 1.0, 0.2083958642359781, 1.0, 1.0, 0.2083958642359781, 1.0, 1.0, 0.41681615027817187, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.4098531183687743, 0.40985311836877386, 0.5188701850740304], 
reward next is 0.4811, 
noisyNet noise sample is [array([-1.5350205], dtype=float32), 1.0654593]. 
=============================================
[2019-04-27 18:19:33,382] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0620532e-11 5.4004054e-06 1.7562230e-03 1.5931790e-05 9.9822241e-01], sum to 1.0000
[2019-04-27 18:19:33,389] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1768
[2019-04-27 18:19:33,394] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [21.63333333333333, 96.0, 1.0, 2.0, 0.1633911430833918, 1.0, 2.0, 0.1633911430833918, 1.0, 2.0, 0.2621349747721374, 6.911199999999999, 6.9112, 121.94756008, 580178.8062616806, 580178.806261681, 208701.2732928671], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2349600.0000, 
sim time next is 2350200.0000, 
raw observation next is [21.61666666666667, 96.0, 1.0, 2.0, 0.1626058480987521, 1.0, 2.0, 0.1626058480987521, 1.0, 2.0, 0.2609077570587599, 6.911199999999999, 6.9112, 121.94756008, 577573.9919314898, 577573.9919314903, 208454.3110679121], 
processed observation next is [1.0, 0.17391304347826086, 0.356172839506173, 0.96, 1.0, 1.0, 0.0031022001175620047, 1.0, 1.0, 0.0031022001175620047, 1.0, 1.0, 0.07613469632344987, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.20627642568981777, 0.20627642568981794, 0.4008736751306002], 
reward next is 0.5991, 
noisyNet noise sample is [array([0.33398873], dtype=float32), -1.5931911]. 
=============================================
[2019-04-27 18:19:34,058] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.1447833e-10 9.6118983e-06 8.4246189e-04 2.3780570e-04 9.9891007e-01], sum to 1.0000
[2019-04-27 18:19:34,064] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2360
[2019-04-27 18:19:34,068] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.46666666666667, 40.33333333333333, 1.0, 2.0, 0.2987606324145755, 1.0, 2.0, 0.2987606324145755, 1.0, 2.0, 0.4865121665796969, 6.911199999999999, 6.9112, 121.94756008, 1089588.595665109, 1089588.595665109, 254528.4498571471], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2367600.0000, 
sim time next is 2368200.0000, 
raw observation next is [28.58333333333334, 40.16666666666667, 1.0, 2.0, 0.3058148153300163, 1.0, 2.0, 0.3058148153300163, 1.0, 2.0, 0.4973003343256643, 6.911199999999999, 6.9112, 121.94756008, 1113248.648049701, 1113248.648049701, 257322.3895141951], 
processed observation next is [1.0, 0.391304347826087, 0.6141975308641977, 0.4016666666666667, 1.0, 1.0, 0.17358906586906703, 1.0, 1.0, 0.17358906586906703, 1.0, 1.0, 0.3716254179070803, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.39758880287489323, 0.39758880287489323, 0.4948507490657598], 
reward next is 0.5051, 
noisyNet noise sample is [array([-0.16518149], dtype=float32), 0.5893719]. 
=============================================
[2019-04-27 18:19:34,130] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-27 18:19:34,132] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 18:19:34,133] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 18:19:34,134] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:19:34,137] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:19:34,139] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 18:19:34,141] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:19:34,141] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 18:19:34,142] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 18:19:34,142] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:19:34,143] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:19:34,161] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run12
[2019-04-27 18:19:34,184] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run12
[2019-04-27 18:19:34,184] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run12
[2019-04-27 18:19:34,185] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run12
[2019-04-27 18:19:34,214] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run12
[2019-04-27 18:19:55,158] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00819722], dtype=float32), 0.051889606]
[2019-04-27 18:19:55,159] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [28.79886173, 23.77630264, 1.0, 2.0, 0.16, 1.0, 2.0, 0.16, 1.0, 2.0, 0.2, 6.911199999999999, 6.9112, 121.94756008, 416424.9904380579, 416424.9904380584, 177676.0175398682]
[2019-04-27 18:19:55,159] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:19:55,166] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [3.7365541e-10 7.6540106e-04 1.6852391e-03 7.6925830e-04 9.9678010e-01], sampled 0.9733900405237097
[2019-04-27 18:20:17,961] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00819722], dtype=float32), 0.051889606]
[2019-04-27 18:20:17,963] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [25.91069718166667, 95.5538571, 1.0, 2.0, 0.2400830077912621, 1.0, 2.0, 0.2400830077912621, 1.0, 2.0, 0.3822199673613831, 6.9112, 6.9112, 121.94756008, 820892.416794909, 820892.416794909, 234136.4429257094]
[2019-04-27 18:20:17,965] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 18:20:17,968] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [2.2731754e-10 1.5094962e-04 9.7901956e-04 4.5324833e-04 9.9841678e-01], sampled 0.4078493801715306
[2019-04-27 18:20:19,105] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00819722], dtype=float32), 0.051889606]
[2019-04-27 18:20:19,111] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [30.63333333333333, 63.5, 1.0, 2.0, 0.3135945913232463, 1.0, 2.0, 0.3135945913232463, 1.0, 2.0, 0.4992528024494364, 6.911200000000001, 6.9112, 121.94756008, 1072419.247356022, 1072419.247356022, 261303.8249314208]
[2019-04-27 18:20:19,113] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 18:20:19,115] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.5877309e-10 2.5316680e-05 5.5254879e-04 2.2464176e-04 9.9919754e-01], sampled 0.8791461234915711
[2019-04-27 18:20:43,711] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00819722], dtype=float32), 0.051889606]
[2019-04-27 18:20:43,714] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [27.0, 94.0, 1.0, 2.0, 0.2647166574887663, 1.0, 2.0, 0.2647166574887663, 1.0, 2.0, 0.4214375399417717, 6.911200000000001, 6.9112, 121.94756008, 905169.5823451462, 905169.5823451458, 242904.310374875]
[2019-04-27 18:20:43,715] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 18:20:43,718] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [2.0095270e-10 1.8327638e-04 8.6622528e-04 4.5444214e-04 9.9849606e-01], sampled 0.48815774441642723
[2019-04-27 18:20:49,685] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00819722], dtype=float32), 0.051889606]
[2019-04-27 18:20:49,686] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [36.0, 44.0, 1.0, 2.0, 0.4861300577212915, 1.0, 2.0, 0.4861300577212915, 1.0, 2.0, 0.7739348840429762, 6.911200000000001, 6.9112, 121.94756008, 1663077.593206028, 1663077.593206028, 336833.6695729973]
[2019-04-27 18:20:49,686] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 18:20:49,689] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [6.6367363e-11 4.1697672e-06 3.2090701e-04 1.0390523e-04 9.9957103e-01], sampled 0.031242650456738574
[2019-04-27 18:21:01,638] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00819722], dtype=float32), 0.051889606]
[2019-04-27 18:21:01,639] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [26.7801488, 68.07331859, 1.0, 2.0, 0.1860775871173085, 1.0, 2.0, 0.1860775871173085, 1.0, 2.0, 0.2965989377448183, 6.9112, 6.9112, 121.94756008, 644521.6613554335, 644521.6613554335, 216093.047358661]
[2019-04-27 18:21:01,640] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:21:01,642] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [2.9147607e-10 1.3789447e-04 1.1101069e-03 4.3768997e-04 9.9831438e-01], sampled 0.18568045639925546
[2019-04-27 18:21:18,069] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 4514.6904 3106310531.5697 0.0000
[2019-04-27 18:21:18,269] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 4489.1967 2939995586.1240 28.0000
[2019-04-27 18:21:18,407] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 4276.1369 2919045686.4436 33.0000
[2019-04-27 18:21:18,426] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 4393.5021 2875144832.6516 8.0000
[2019-04-27 18:21:18,475] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 4609.6954 2893761446.0263 12.0000
[2019-04-27 18:21:19,488] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 275000, evaluation results [275000.0, 4514.690420369925, 3106310531.569679, 0.0, 4609.695367676185, 2893761446.0262804, 12.0, 4393.502125757902, 2875144832.6516004, 8.0, 4489.196733649485, 2939995586.124022, 28.0, 4276.136898354132, 2919045686.443558, 33.0]
[2019-04-27 18:21:24,578] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0177596e-17 1.0000000e+00 2.0928394e-11 1.4993281e-09 1.3561968e-10], sum to 1.0000
[2019-04-27 18:21:24,588] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1214
[2019-04-27 18:21:24,592] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.55, 40.83333333333334, 1.0, 2.0, 0.690414133870927, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 858567.6507586589, 858567.6507586589, 177101.1202064626], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2448600.0000, 
sim time next is 2449200.0000, 
raw observation next is [28.8, 39.66666666666667, 1.0, 2.0, 0.8998659019421662, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.03102216157045, 6.9112, 121.9256169694904, 1180333.471369643, 1118974.029270043, 221346.7511587579], 
processed observation next is [1.0, 0.34782608695652173, 0.6222222222222222, 0.3966666666666667, 1.0, 1.0, 0.8807927404073407, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.011982216157044956, 0.0, 0.8094593029721987, 0.4215476683463011, 0.3996335818821582, 0.4256668291514575], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.4692655], dtype=float32), -0.6345854]. 
=============================================
[2019-04-27 18:21:25,331] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.0020362e-14 1.0000000e+00 6.3762533e-12 2.4928390e-10 1.0113928e-10], sum to 1.0000
[2019-04-27 18:21:25,338] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7431
[2019-04-27 18:21:25,346] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1633647.660563105 W.
[2019-04-27 18:21:25,351] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.71666666666667, 23.83333333333333, 1.0, 2.0, 0.6711198317807452, 1.0, 1.0, 0.6711198317807452, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156332, 1633647.660563105, 1633647.660563105, 297759.6865979458], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2463000.0000, 
sim time next is 2463600.0000, 
raw observation next is [33.83333333333334, 23.66666666666666, 1.0, 2.0, 0.6354297760566874, 1.0, 2.0, 0.6354297760566874, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1542688.949906012, 1542688.949906012, 284415.0978815915], 
processed observation next is [1.0, 0.5217391304347826, 0.8086419753086423, 0.2366666666666666, 1.0, 1.0, 0.5659878286389135, 1.0, 1.0, 0.5659878286389135, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.5509603392521472, 0.5509603392521472, 0.5469521113107528], 
reward next is 0.4530, 
noisyNet noise sample is [array([-0.4558664], dtype=float32), -1.6561835]. 
=============================================
[2019-04-27 18:21:34,558] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.1281193e-27 1.0000000e+00 9.3628244e-24 1.7286006e-18 9.5226151e-23], sum to 1.0000
[2019-04-27 18:21:34,567] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9866
[2019-04-27 18:21:34,573] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.83333333333333, 87.66666666666667, 1.0, 2.0, 0.5438887946281804, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 640082.8378376578, 640082.8378376578, 149621.2618700154], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2629200.0000, 
sim time next is 2629800.0000, 
raw observation next is [23.75, 87.0, 1.0, 2.0, 0.5367374160216688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 633760.4096457134, 633760.4096457134, 148537.1651921099], 
processed observation next is [0.0, 0.43478260869565216, 0.4351851851851852, 0.87, 1.0, 1.0, 0.44849692383531997, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.22634300344489763, 0.22634300344489763, 0.28564839460021135], 
reward next is 0.7144, 
noisyNet noise sample is [array([-1.4259851], dtype=float32), 0.2024442]. 
=============================================
[2019-04-27 18:21:36,112] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.4228323e-23 1.0000000e+00 9.3974762e-21 1.3743924e-16 1.8245876e-20], sum to 1.0000
[2019-04-27 18:21:36,122] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7923
[2019-04-27 18:21:36,125] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 74.0, 1.0, 2.0, 0.6077294814416737, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 699693.7046705369, 699693.7046705369, 159759.5279240899], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2649000.0000, 
sim time next is 2649600.0000, 
raw observation next is [27.0, 74.0, 1.0, 2.0, 0.6074667606701533, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 699390.9522016334, 699390.9522016334, 159713.8206546671], 
processed observation next is [0.0, 0.6956521739130435, 0.5555555555555556, 0.74, 1.0, 1.0, 0.5326985246073253, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.24978248292915478, 0.24978248292915478, 0.3071419627974367], 
reward next is 0.6929, 
noisyNet noise sample is [array([-1.0905031], dtype=float32), -0.48347783]. 
=============================================
[2019-04-27 18:21:41,582] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.9982320e-26 1.0000000e+00 2.8716809e-22 1.3878415e-18 2.0142311e-22], sum to 1.0000
[2019-04-27 18:21:41,589] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6220
[2019-04-27 18:21:41,593] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.6518209374643252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 742864.8672195461, 742864.8672195461, 167221.2449869135], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2757600.0000, 
sim time next is 2758200.0000, 
raw observation next is [25.91666666666667, 84.83333333333333, 1.0, 2.0, 0.6449019684677452, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 734975.7019669173, 734975.7019669173, 165973.6906181919], 
processed observation next is [0.0, 0.9565217391304348, 0.5154320987654323, 0.8483333333333333, 1.0, 1.0, 0.5772642481758871, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.26249132213104187, 0.26249132213104187, 0.3191801742657537], 
reward next is 0.6808, 
noisyNet noise sample is [array([0.9683175], dtype=float32), 0.11999626]. 
=============================================
[2019-04-27 18:21:44,311] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0315881e-08 9.9571037e-01 2.9620762e-05 4.2351056e-03 2.4767702e-05], sum to 1.0000
[2019-04-27 18:21:44,318] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2652
[2019-04-27 18:21:44,322] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2098567.613598559 W.
[2019-04-27 18:21:44,327] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.13333333333333, 63.00000000000001, 1.0, 2.0, 0.6132777090539977, 1.0, 2.0, 0.6132777090539977, 1.0, 1.0, 0.9763580858745563, 6.911199999999999, 6.9112, 121.94756008, 2098567.613598559, 2098567.61359856, 402657.4227489699], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2805600.0000, 
sim time next is 2806200.0000, 
raw observation next is [31.35, 63.0, 1.0, 2.0, 0.7080855735068424, 1.0, 2.0, 0.667407448729856, 1.0, 2.0, 0.9977734948820727, 6.911200000000001, 6.9112, 121.94756008, 2284030.396792152, 2284030.396792151, 431419.8708912617], 
processed observation next is [1.0, 0.4782608695652174, 0.7166666666666667, 0.63, 1.0, 1.0, 0.6524828256033838, 1.0, 1.0, 0.6040564865831619, 1.0, 1.0, 0.9972168686025908, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.8157251417114828, 0.8157251417114825, 0.8296535978678109], 
reward next is 0.1703, 
noisyNet noise sample is [array([0.32031786], dtype=float32), -0.6806865]. 
=============================================
[2019-04-27 18:21:45,257] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.8088084e-09 9.9999487e-01 6.2941727e-09 5.1003303e-06 9.3945101e-09], sum to 1.0000
[2019-04-27 18:21:45,267] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6300
[2019-04-27 18:21:45,274] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2414696.887581783 W.
[2019-04-27 18:21:45,280] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.83333333333333, 57.33333333333333, 1.0, 2.0, 0.7843455223798328, 1.0, 2.0, 0.7055374231663512, 1.0, 1.0, 0.9977734948820727, 6.911200000000001, 6.9112, 121.94756008, 2414696.887581783, 2414696.887581783, 452674.1552310205], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2819400.0000, 
sim time next is 2820000.0000, 
raw observation next is [32.86666666666667, 57.66666666666667, 1.0, 2.0, 0.7125167819342328, 1.0, 2.0, 0.6696230529435512, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.94756008, 2291622.46260302, 2291622.46260302, 432620.4801290514], 
processed observation next is [1.0, 0.6521739130434783, 0.7728395061728395, 0.5766666666666667, 1.0, 1.0, 0.6577580737312295, 1.0, 1.0, 0.6066941106470847, 1.0, 1.0, 0.9972168686025908, 0.0, 0.0, 0.8096049824067558, 0.8184365937867929, 0.8184365937867929, 0.8319624617866372], 
reward next is 0.1680, 
noisyNet noise sample is [array([0.01878992], dtype=float32), 1.5027348]. 
=============================================
[2019-04-27 18:21:45,294] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[19.419033]
 [20.067408]
 [20.30957 ]
 [19.443666]
 [19.964062]], R is [[18.87963676]
 [18.8203125 ]
 [18.63210869]
 [18.44578743]
 [18.40390778]].
[2019-04-27 18:21:47,989] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3948444e-29 1.0000000e+00 2.5110011e-29 5.7932754e-24 5.0665937e-31], sum to 1.0000
[2019-04-27 18:21:47,996] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8488
[2019-04-27 18:21:48,003] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.4, 94.0, 1.0, 2.0, 0.5736945876654089, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 683408.4484021896, 683408.4484021896, 154919.4182314637], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2869200.0000, 
sim time next is 2869800.0000, 
raw observation next is [22.33333333333334, 94.00000000000001, 1.0, 2.0, 0.569494795220718, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 679207.9421276824, 679207.9421276824, 154237.2180370298], 
processed observation next is [1.0, 0.21739130434782608, 0.38271604938271625, 0.9400000000000002, 1.0, 1.0, 0.48749380383418806, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.24257426504560084, 0.24257426504560084, 0.29661003468659575], 
reward next is 0.7034, 
noisyNet noise sample is [array([1.168375], dtype=float32), 0.19559143]. 
=============================================
[2019-04-27 18:21:54,373] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.7620084e-19 1.0000000e+00 3.6584247e-20 2.9244300e-14 2.9335330e-20], sum to 1.0000
[2019-04-27 18:21:54,378] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1104
[2019-04-27 18:21:54,386] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2103268.808982334 W.
[2019-04-27 18:21:54,397] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.13333333333333, 82.66666666666667, 1.0, 2.0, 0.6146499533044545, 1.0, 2.0, 0.6146499533044545, 1.0, 1.0, 0.9785427434121586, 6.911199999999999, 6.9112, 121.94756008, 2103268.808982334, 2103268.808982334, 403415.4039702605], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2979600.0000, 
sim time next is 2980200.0000, 
raw observation next is [28.06666666666667, 83.33333333333333, 1.0, 2.0, 0.9317654644371169, 1.0, 2.0, 0.9317654644371169, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 2125630.635504576, 2125630.635504577, 401211.2267369943], 
processed observation next is [1.0, 0.4782608695652174, 0.5950617283950619, 0.8333333333333333, 1.0, 1.0, 0.9187684100441869, 1.0, 1.0, 0.9187684100441869, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.7591537983944914, 0.7591537983944917, 0.7715600514172967], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.37377697], dtype=float32), 0.112276085]. 
=============================================
[2019-04-27 18:21:55,181] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.6185683e-19 1.0000000e+00 2.2530390e-20 1.5051473e-13 1.0514005e-19], sum to 1.0000
[2019-04-27 18:21:55,188] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8831
[2019-04-27 18:21:55,195] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.41666666666666, 88.33333333333334, 1.0, 2.0, 0.2869917491800121, 1.0, 2.0, 0.2869917491800121, 1.0, 2.0, 0.4569002113633251, 6.9112, 6.9112, 121.94756008, 981385.577836901, 981385.577836901, 251124.3191797224], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2999400.0000, 
sim time next is 3000000.0000, 
raw observation next is [26.13333333333333, 90.66666666666667, 1.0, 2.0, 0.6600236137406804, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 752217.8497954209, 752217.8497954209, 168716.3934320926], 
processed observation next is [1.0, 0.7391304347826086, 0.5234567901234567, 0.9066666666666667, 1.0, 1.0, 0.5952662068341433, 0.0, 0.5, -0.1904761904761905, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2686492320697932, 0.2686492320697932, 0.3244546027540242], 
reward next is 0.6755, 
noisyNet noise sample is [array([0.43006778], dtype=float32), 0.8845156]. 
=============================================
[2019-04-27 18:21:55,203] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[39.807552]
 [38.087482]
 [39.167667]
 [40.07831 ]
 [39.718876]], R is [[41.77257538]
 [41.87191772]
 [41.45319748]
 [41.32084656]
 [41.27134323]].
[2019-04-27 18:22:08,013] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.9093214e-24 1.0000000e+00 1.2899052e-25 4.6565537e-17 3.0097928e-23], sum to 1.0000
[2019-04-27 18:22:08,021] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5184
[2019-04-27 18:22:08,028] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.76666666666667, 71.66666666666667, 1.0, 2.0, 0.4762879776924986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 574678.7252459248, 574678.7252459243, 139449.8416613665], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3223200.0000, 
sim time next is 3223800.0000, 
raw observation next is [25.15, 68.5, 1.0, 2.0, 0.4721661160352361, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 570792.5515275891, 570792.5515275891, 138854.3467980342], 
processed observation next is [0.0, 0.30434782608695654, 0.487037037037037, 0.685, 1.0, 1.0, 0.37162632861337636, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.20385448268842468, 0.20385448268842468, 0.2670275899962196], 
reward next is 0.7330, 
noisyNet noise sample is [array([1.5622761], dtype=float32), -1.2160814]. 
=============================================
[2019-04-27 18:22:12,232] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-27 18:22:12,233] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 18:22:12,233] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 18:22:12,235] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 18:22:12,237] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 18:22:12,236] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:22:12,238] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:22:12,242] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:22:12,241] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:22:12,239] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 18:22:12,246] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:22:12,261] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run13
[2019-04-27 18:22:12,261] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run13
[2019-04-27 18:22:12,302] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run13
[2019-04-27 18:22:12,321] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run13
[2019-04-27 18:22:12,322] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run13
[2019-04-27 18:22:16,330] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03174505], dtype=float32), 0.056129824]
[2019-04-27 18:22:16,331] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [30.25315326833333, 19.700696325, 1.0, 2.0, 0.3404921692122467, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 439238.8687676339, 439238.8687676339, 119514.4153277188]
[2019-04-27 18:22:16,333] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 18:22:16,336] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [5.3671243e-26 1.0000000e+00 2.4318945e-25 9.1744812e-17 5.8910165e-25], sampled 0.5881559045851735
[2019-04-27 18:22:18,131] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03174505], dtype=float32), 0.056129824]
[2019-04-27 18:22:18,131] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [18.25, 65.5, 1.0, 2.0, 0.2585246626270864, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 333476.9626234794, 333476.9626234794, 94275.31543185216]
[2019-04-27 18:22:18,133] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 18:22:18,138] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.027955e-25 1.000000e+00 3.717062e-25 9.624535e-17 8.741501e-25], sampled 0.49295614410956967
[2019-04-27 18:22:22,666] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03174505], dtype=float32), 0.056129824]
[2019-04-27 18:22:22,667] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [29.48333333333333, 30.33333333333334, 1.0, 2.0, 0.3526728739744572, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 446831.2075585652, 446831.2075585652, 122311.7713925933]
[2019-04-27 18:22:22,667] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 18:22:22,673] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [3.9330113e-27 1.0000000e+00 2.5959262e-26 2.9657969e-17 5.7707055e-26], sampled 0.23928707803592197
[2019-04-27 18:22:51,772] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03174505], dtype=float32), 0.056129824]
[2019-04-27 18:22:51,773] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [27.65743447666667, 85.13618786666667, 1.0, 2.0, 0.7434609364143451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 847362.5313359417, 847362.5313359417, 184538.847601063]
[2019-04-27 18:22:51,774] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 18:22:51,778] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.5309575e-27 1.0000000e+00 6.4393787e-27 1.1977784e-17 1.6077574e-26], sampled 0.7342023632089445
[2019-04-27 18:22:56,591] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03174505], dtype=float32), 0.056129824]
[2019-04-27 18:22:56,593] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [28.61670565, 59.501995815, 1.0, 2.0, 0.5610058039942751, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 656123.1399074136, 656123.1399074141, 152282.4118541529]
[2019-04-27 18:22:56,595] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 18:22:56,598] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [2.66771361e-27 1.00000000e+00 1.27632194e-26 2.14270417e-17
 3.13335182e-26], sampled 0.9861998641389247
[2019-04-27 18:23:50,316] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.03174505], dtype=float32), 0.056129824]
[2019-04-27 18:23:50,318] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [29.0, 23.0, 1.0, 2.0, 0.3330072034675571, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 429580.6093245231, 429580.6093245231, 118395.0104567834]
[2019-04-27 18:23:50,319] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 18:23:50,323] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [2.6907620e-26 1.0000000e+00 2.0005134e-25 1.3416331e-16 4.2309534e-25], sampled 0.06910564250586382
[2019-04-27 18:23:55,649] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8701.6901 2195047582.3563 572.0000
[2019-04-27 18:23:55,811] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8771.9095 2170601483.0346 493.0000
[2019-04-27 18:23:56,082] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8924.2184 2120437061.2969 430.0000
[2019-04-27 18:23:56,091] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8099.9424 2445324199.3218 746.0000
[2019-04-27 18:23:56,110] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8583.7021 2248718286.8236 553.0000
[2019-04-27 18:23:57,124] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 300000, evaluation results [300000.0, 8099.942407935641, 2445324199.321786, 746.0, 8771.90946536216, 2170601483.03465, 493.0, 8924.218435439243, 2120437061.2969296, 430.0, 8583.702050975664, 2248718286.823596, 553.0, 8701.690060682284, 2195047582.3563175, 572.0]
[2019-04-27 18:23:58,346] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.8898887e-23 1.0000000e+00 5.1305440e-23 4.9694525e-15 2.0556816e-23], sum to 1.0000
[2019-04-27 18:23:58,354] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5904
[2019-04-27 18:23:58,358] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.6595120227149743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 751634.5122163635, 751634.5122163635, 168619.1155783043], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3330000.0000, 
sim time next is 3330600.0000, 
raw observation next is [27.01666666666667, 78.83333333333334, 1.0, 2.0, 0.6508786466934139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 741790.4411517085, 741790.4411517085, 167051.9533119324], 
processed observation next is [0.0, 0.5652173913043478, 0.5561728395061729, 0.7883333333333334, 1.0, 1.0, 0.5843793413016832, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2649251575541816, 0.2649251575541816, 0.32125375636910075], 
reward next is 0.6787, 
noisyNet noise sample is [array([0.28588146], dtype=float32), 0.14026263]. 
=============================================
[2019-04-27 18:23:59,456] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.9999011e-28 1.0000000e+00 3.1432286e-26 1.0153239e-18 2.1392153e-26], sum to 1.0000
[2019-04-27 18:23:59,470] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7420
[2019-04-27 18:23:59,478] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.01666666666667, 78.83333333333334, 1.0, 2.0, 0.6508786466934139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 741790.4411517085, 741790.4411517085, 167051.9533119324], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3330600.0000, 
sim time next is 3331200.0000, 
raw observation next is [27.03333333333334, 78.66666666666667, 1.0, 2.0, 0.6565299177079419, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 748234.1996471302, 748234.1996471302, 168076.2590798209], 
processed observation next is [0.0, 0.5652173913043478, 0.5567901234567904, 0.7866666666666667, 1.0, 1.0, 0.591107044890407, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.26722649987397507, 0.26722649987397507, 0.32322357515350175], 
reward next is 0.6768, 
noisyNet noise sample is [array([1.134157], dtype=float32), -0.14214712]. 
=============================================
[2019-04-27 18:24:05,680] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.8134934e-22 1.0000000e+00 1.2765413e-18 1.0555147e-13 1.6587205e-22], sum to 1.0000
[2019-04-27 18:24:05,686] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8907
[2019-04-27 18:24:05,690] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 94.0, 1.0, 2.0, 0.6596301877091532, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 751769.2488673206, 751769.2488673211, 168641.3127360309], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3456000.0000, 
sim time next is 3456600.0000, 
raw observation next is [25.0, 94.00000000000001, 1.0, 2.0, 0.6623386144748133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 754857.5164333698, 754857.5164333694, 169135.6628289404], 
processed observation next is [1.0, 0.0, 0.48148148148148145, 0.9400000000000002, 1.0, 1.0, 0.5980221600890635, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.26959197015477493, 0.26959197015477476, 0.3252608900556546], 
reward next is 0.6747, 
noisyNet noise sample is [array([-0.0721852], dtype=float32), 0.8420571]. 
=============================================
[2019-04-27 18:24:09,110] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.2080752e-09 8.3324599e-01 7.9868222e-08 1.6675389e-01 6.5778143e-09], sum to 1.0000
[2019-04-27 18:24:09,122] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7363
[2019-04-27 18:24:09,128] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1734159.359754372 W.
[2019-04-27 18:24:09,132] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.08333333333333, 80.83333333333333, 1.0, 2.0, 0.8939333576524339, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.9260426156618, 1734159.359754372, 1734159.359754372, 355659.0654903144], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3514200.0000, 
sim time next is 3514800.0000, 
raw observation next is [27.16666666666667, 77.66666666666667, 1.0, 2.0, 0.4511561516911907, 1.0, 1.0, 0.4511561516911907, 1.0, 2.0, 0.7182552866224553, 6.911199999999999, 6.9112, 121.94756008, 1543321.049521307, 1543321.049521308, 320215.8070862856], 
processed observation next is [1.0, 0.6956521739130435, 0.5617283950617286, 0.7766666666666667, 1.0, 1.0, 0.3466144662990365, 1.0, 0.5, 0.3466144662990365, 1.0, 1.0, 0.647819108278069, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.5511860891147525, 0.5511860891147529, 0.6157996290120877], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.16293778], dtype=float32), -0.017636936]. 
=============================================
[2019-04-27 18:24:14,407] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.2048192e-13 9.9532360e-01 1.7414259e-10 4.6763974e-03 2.2861411e-11], sum to 1.0000
[2019-04-27 18:24:14,414] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8194
[2019-04-27 18:24:14,419] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.25, 79.66666666666667, 1.0, 2.0, 0.6439701896105124, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 751366.7768818693, 751366.7768818693, 166640.5069797423], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3604200.0000, 
sim time next is 3604800.0000, 
raw observation next is [25.2, 80.33333333333334, 1.0, 2.0, 0.5415828952822179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 633841.0947013455, 633841.0947013455, 149095.4977443161], 
processed observation next is [1.0, 0.7391304347826086, 0.4888888888888889, 0.8033333333333335, 1.0, 1.0, 0.45426535152644987, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.22637181953619484, 0.22637181953619484, 0.2867221110467617], 
reward next is 0.7133, 
noisyNet noise sample is [array([1.1504786], dtype=float32), 0.10856784]. 
=============================================
[2019-04-27 18:24:22,410] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.3744544e-16 8.4819740e-06 1.7346411e-13 9.9999154e-01 6.1300418e-14], sum to 1.0000
[2019-04-27 18:24:22,419] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0637
[2019-04-27 18:24:22,426] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.53333333333333, 77.66666666666667, 1.0, 2.0, 0.9359419230629733, 1.0, 2.0, 0.9359419230629733, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 2135169.755794343, 2135169.755794343, 403131.2022791447], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3756000.0000, 
sim time next is 3756600.0000, 
raw observation next is [29.65, 78.0, 1.0, 2.0, 0.9617228085801921, 1.0, 2.0, 0.9617228085801921, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 2194056.102720405, 2194056.102720405, 415114.0402239379], 
processed observation next is [1.0, 0.4782608695652174, 0.6537037037037037, 0.78, 1.0, 1.0, 0.9544319149764192, 1.0, 1.0, 0.9544319149764192, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.7835914652572875, 0.7835914652572875, 0.7982962311998806], 
reward next is 0.2017, 
noisyNet noise sample is [array([0.65594566], dtype=float32), 0.91375023]. 
=============================================
[2019-04-27 18:24:22,786] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6778344e-14 9.7668642e-05 1.9230773e-11 9.9990237e-01 1.6408496e-11], sum to 1.0000
[2019-04-27 18:24:22,797] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8385
[2019-04-27 18:24:22,802] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.66666666666667, 76.33333333333334, 1.0, 2.0, 0.7077764703080994, 1.0, 2.0, 0.7077764703080994, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426142441, 1614184.219957815, 1614184.219957815, 307122.1944102613], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3759600.0000, 
sim time next is 3760200.0000, 
raw observation next is [31.0, 75.0, 1.0, 2.0, 0.6532757629803622, 1.0, 2.0, 0.6532757629803622, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156614, 1489771.253573479, 1489771.253573478, 286858.0654081365], 
processed observation next is [1.0, 0.5217391304347826, 0.7037037037037037, 0.75, 1.0, 1.0, 0.5872330511670978, 1.0, 1.0, 0.5872330511670978, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201332, 0.5320611619905282, 0.5320611619905279, 0.5516501257848779], 
reward next is 0.4483, 
noisyNet noise sample is [array([-0.19267063], dtype=float32), -1.3944705]. 
=============================================
[2019-04-27 18:24:24,870] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0805649e-12 9.9867553e-01 3.3793093e-10 1.3244831e-03 1.1560555e-08], sum to 1.0000
[2019-04-27 18:24:24,885] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1540
[2019-04-27 18:24:24,892] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.75, 74.0, 1.0, 2.0, 0.6954518332404814, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 792615.6738845436, 792615.6738845436, 175286.0002065618], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3797400.0000, 
sim time next is 3798000.0000, 
raw observation next is [28.5, 77.0, 1.0, 2.0, 0.7093725731524533, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 808489.6933387736, 808489.6933387736, 177928.1119759968], 
processed observation next is [1.0, 1.0, 0.6111111111111112, 0.77, 1.0, 1.0, 0.6540149680386348, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.288746319049562, 0.288746319049562, 0.3421694461076862], 
reward next is 0.6578, 
noisyNet noise sample is [array([-0.7559571], dtype=float32), -0.54531854]. 
=============================================
[2019-04-27 18:24:24,907] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[49.525177]
 [49.584335]
 [49.70172 ]
 [49.709625]
 [49.987053]], R is [[49.535923  ]
 [49.70347595]
 [49.8751564 ]
 [50.05334473]
 [50.23180389]].
[2019-04-27 18:24:49,501] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.00616574e-13 1.17613046e-04 1.36055718e-07 9.99882102e-01
 1.29685702e-07], sum to 1.0000
[2019-04-27 18:24:49,511] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1745
[2019-04-27 18:24:49,519] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.66666666666666, 63.33333333333333, 1.0, 2.0, 0.2321007208530077, 1.0, 2.0, 0.2321007208530077, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 558505.5430934525, 558505.5430934529, 166332.492748015], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4236000.0000, 
sim time next is 4236600.0000, 
raw observation next is [25.83333333333334, 60.66666666666666, 1.0, 2.0, 0.225925663395993, 1.0, 2.0, 0.225925663395993, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 546189.3536171963, 546189.3536171968, 165072.5804650221], 
processed observation next is [1.0, 0.0, 0.5123456790123458, 0.6066666666666666, 1.0, 1.0, 0.07848293261427737, 1.0, 1.0, 0.07848293261427737, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.19506762629185584, 0.195067626291856, 0.3174472701250425], 
reward next is 0.6826, 
noisyNet noise sample is [array([0.2237668], dtype=float32), 1.8692043]. 
=============================================
[2019-04-27 18:24:49,523] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-27 18:24:49,525] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 18:24:49,526] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:24:49,526] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 18:24:49,526] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 18:24:49,529] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 18:24:49,530] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:24:49,528] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 18:24:49,531] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:24:49,531] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:24:49,534] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:24:49,547] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run14
[2019-04-27 18:24:49,548] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run14
[2019-04-27 18:24:49,583] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run14
[2019-04-27 18:24:49,599] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run14
[2019-04-27 18:24:49,619] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run14
[2019-04-27 18:25:02,779] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01490322], dtype=float32), 0.06228501]
[2019-04-27 18:25:02,780] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [25.28217805, 51.42619974, 1.0, 2.0, 0.1816632471628688, 1.0, 2.0, 0.1816632471628688, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 453020.2421922668, 453020.2421922673, 156167.3207300943]
[2019-04-27 18:25:02,780] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 18:25:02,784] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.1338535e-13 1.7780770e-03 8.3917548e-08 9.9822122e-01 5.9667639e-07], sampled 0.7477569915872911
[2019-04-27 18:25:09,704] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01490322], dtype=float32), 0.06228501]
[2019-04-27 18:25:09,706] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [25.53333333333333, 43.66666666666667, 1.0, 2.0, 0.16, 1.0, 2.0, 0.16, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 390855.020408652, 390855.0204086524, 149777.5037716192]
[2019-04-27 18:25:09,708] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 18:25:09,711] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [5.6890538e-13 2.2046776e-02 1.6922094e-07 9.7795206e-01 1.0337400e-06], sampled 0.03747911081298627
[2019-04-27 18:25:26,217] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01490322], dtype=float32), 0.06228501]
[2019-04-27 18:25:26,218] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [34.76666666666667, 44.66666666666667, 1.0, 2.0, 0.3325570111515591, 1.0, 2.0, 0.3325570111515591, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 758022.1716669037, 758022.1716669041, 188457.7735492768]
[2019-04-27 18:25:26,219] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 18:25:26,224] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [3.5391069e-14 7.2654352e-06 5.4143261e-08 9.9999225e-01 4.6195328e-07], sampled 0.9218933753358936
[2019-04-27 18:25:29,383] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01490322], dtype=float32), 0.06228501]
[2019-04-27 18:25:29,385] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [24.66666666666666, 89.0, 1.0, 2.0, 0.5641130256062327, 1.0, 2.0, 0.5641130256062327, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1286268.181288274, 1286268.181288274, 255921.6971222264]
[2019-04-27 18:25:29,388] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 18:25:29,390] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [6.5735606e-13 2.3397264e-05 1.2691811e-07 9.9997509e-01 1.3902011e-06], sampled 0.09782375209037364
[2019-04-27 18:25:31,320] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01490322], dtype=float32), 0.06228501]
[2019-04-27 18:25:31,321] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [20.6650198, 95.51042301833333, 1.0, 2.0, 0.218346017775971, 1.0, 2.0, 0.218346017775971, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 528825.7788891055, 528825.778889106, 163461.0390203343]
[2019-04-27 18:25:31,322] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 18:25:31,325] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [4.51714241e-13 6.59072073e-03 1.11375165e-07 9.93408322e-01
 8.55339636e-07], sampled 0.9107301148146746
[2019-04-27 18:25:47,714] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.01490322], dtype=float32), 0.06228501]
[2019-04-27 18:25:47,714] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [28.0, 84.0, 1.0, 2.0, 0.6752521230546245, 1.0, 2.0, 0.6752521230546245, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1539937.922659215, 1539937.922659215, 294903.9894519997]
[2019-04-27 18:25:47,715] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 18:25:47,718] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [5.8975080e-13 1.4319998e-04 1.3171314e-07 9.9985540e-01 1.2634111e-06], sampled 0.16005267395041234
[2019-04-27 18:25:55,108] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.01490322], dtype=float32), 0.06228501]
[2019-04-27 18:25:55,109] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [24.55, 91.50000000000001, 1.0, 2.0, 0.6269923642079436, 1.0, 2.0, 0.6269923642079436, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1429776.981123169, 1429776.981123169, 277449.8714721529]
[2019-04-27 18:25:55,111] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 18:25:55,113] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [9.8188244e-13 9.3121424e-05 1.7345991e-07 9.9990511e-01 1.6517245e-06], sampled 0.22080279406225267
[2019-04-27 18:26:04,252] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.01490322], dtype=float32), 0.06228501]
[2019-04-27 18:26:04,254] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [29.23944887333334, 88.62087886666667, 1.0, 2.0, 0.4219895599625312, 1.0, 2.0, 0.4219895599625312, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 962000.4321765371, 962000.4321765371, 212315.0978196437]
[2019-04-27 18:26:04,255] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:26:04,258] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [5.5485137e-14 9.0699032e-06 5.2126307e-08 9.9999046e-01 5.0024244e-07], sampled 0.4483304417990528
[2019-04-27 18:26:06,983] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.01490322], dtype=float32), 0.06228501]
[2019-04-27 18:26:06,983] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [28.9, 78.0, 1.0, 2.0, 0.3752068121508961, 1.0, 2.0, 0.3752068121508961, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 855291.3063201379, 855291.3063201384, 199489.0665941007]
[2019-04-27 18:26:06,984] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 18:26:06,986] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [5.1370800e-14 1.4237396e-04 5.6359301e-08 9.9985719e-01 4.9446834e-07], sampled 0.1520548347130556
[2019-04-27 18:26:30,514] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.01490322], dtype=float32), 0.06228501]
[2019-04-27 18:26:30,514] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [22.35145901, 67.32112488, 1.0, 2.0, 0.1878709955712118, 1.0, 2.0, 0.1878709955712118, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 468896.1972560175, 468896.1972560179, 157472.9652133956]
[2019-04-27 18:26:30,517] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:26:30,520] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [8.9416383e-14 3.5523323e-04 6.8281082e-08 9.9964404e-01 5.3907667e-07], sampled 0.5692419847404221
[2019-04-27 18:26:32,599] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.01490322], dtype=float32), 0.06228501]
[2019-04-27 18:26:32,600] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [30.6847171, 60.99318029, 1.0, 2.0, 0.2862519389362157, 1.0, 2.0, 0.2862519389362157, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 652430.6275498333, 652430.6275498337, 177195.4886155001]
[2019-04-27 18:26:32,601] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:26:32,604] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.41076658e-13 2.19485781e-04 1.11930305e-07 9.99779522e-01
 8.02696604e-07], sampled 0.8638711651415838
[2019-04-27 18:26:33,917] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 7786.4403 2407372273.7532 23.0000
[2019-04-27 18:26:34,458] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 7167.3929 2433839840.0529 32.0000
[2019-04-27 18:26:34,500] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7508.2296 2665694318.1989 66.0000
[2019-04-27 18:26:34,584] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 7044.7070 2486025585.8622 47.0000
[2019-04-27 18:26:34,680] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 7477.6494 2462672376.7426 46.0000
[2019-04-27 18:26:35,695] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 325000, evaluation results [325000.0, 7508.229611557251, 2665694318.1988626, 66.0, 7167.392853211368, 2433839840.052894, 32.0, 7786.440302130584, 2407372273.753182, 23.0, 7044.707031549994, 2486025585.862158, 47.0, 7477.6493719287, 2462672376.742563, 46.0]
[2019-04-27 18:26:36,969] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.3065243e-12 3.1818165e-06 2.3147534e-07 9.9999607e-01 5.1699470e-07], sum to 1.0000
[2019-04-27 18:26:36,976] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4168
[2019-04-27 18:26:36,982] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.5, 62.66666666666666, 1.0, 2.0, 0.6765668971309121, 1.0, 2.0, 0.6765668971309121, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1558803.642557723, 1558803.642557724, 296186.1333275032], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4266600.0000, 
sim time next is 4267200.0000, 
raw observation next is [28.0, 59.33333333333334, 1.0, 2.0, 0.6633985326931222, 1.0, 2.0, 0.6633985326931222, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1533009.916353913, 1533009.916353913, 291540.8001656347], 
processed observation next is [1.0, 0.391304347826087, 0.5925925925925926, 0.5933333333333334, 1.0, 1.0, 0.5992839674918121, 1.0, 1.0, 0.5992839674918121, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.5475035415549689, 0.5475035415549689, 0.5606553849339129], 
reward next is 0.4393, 
noisyNet noise sample is [array([1.6505805], dtype=float32), -1.2145398]. 
=============================================
[2019-04-27 18:26:37,539] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.2390623e-22 1.1744114e-14 4.7902323e-13 1.0000000e+00 1.9926068e-11], sum to 1.0000
[2019-04-27 18:26:37,544] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3181
[2019-04-27 18:26:37,550] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.26666666666667, 37.33333333333334, 1.0, 2.0, 0.5742301585475014, 1.0, 2.0, 0.5742301585475014, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 1351501.779811281, 1351501.779811281, 261234.5004222772], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4281000.0000, 
sim time next is 4281600.0000, 
raw observation next is [32.53333333333333, 36.66666666666667, 1.0, 2.0, 0.6652748962266164, 1.0, 2.0, 0.6652748962266164, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1560547.246071614, 1560547.246071615, 293302.1923723746], 
processed observation next is [1.0, 0.5652173913043478, 0.7604938271604937, 0.3666666666666667, 1.0, 1.0, 0.6015177336031148, 1.0, 1.0, 0.6015177336031148, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.5573383021684336, 0.5573383021684339, 0.5640426776391819], 
reward next is 0.4360, 
noisyNet noise sample is [array([-0.344807], dtype=float32), 0.72676283]. 
=============================================
[2019-04-27 18:26:40,318] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.6353217e-14 5.4358975e-06 7.9714031e-09 9.9999452e-01 3.6583785e-08], sum to 1.0000
[2019-04-27 18:26:40,326] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4835
[2019-04-27 18:26:40,332] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.33333333333333, 94.0, 1.0, 2.0, 0.3896012039965766, 1.0, 2.0, 0.3896012039965766, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 896961.0733450283, 896961.0733450283, 203774.5791775177], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4333200.0000, 
sim time next is 4333800.0000, 
raw observation next is [23.16666666666667, 94.0, 1.0, 2.0, 0.3792984449668307, 1.0, 2.0, 0.3792984449668307, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 876510.6663758877, 876510.6663758882, 201142.3734340181], 
processed observation next is [1.0, 0.13043478260869565, 0.4135802469135804, 0.94, 1.0, 1.0, 0.26106957734146513, 1.0, 1.0, 0.26106957734146513, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.31303952370567417, 0.31303952370567434, 0.38681225660388097], 
reward next is 0.6132, 
noisyNet noise sample is [array([-0.7927555], dtype=float32), 0.14914457]. 
=============================================
[2019-04-27 18:26:42,875] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.6101550e-06 5.5637538e-05 6.1254931e-04 9.9555379e-01 3.7734453e-03], sum to 1.0000
[2019-04-27 18:26:42,890] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3164
[2019-04-27 18:26:42,897] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 51.0, 1.0, 2.0, 0.7873289461296211, 1.0, 2.0, 0.7070291350412452, 1.0, 1.0, 0.9977734948820727, 6.9112, 6.9112, 121.94756008, 2419809.179972263, 2419809.179972263, 453531.2159146811], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4375800.0000, 
sim time next is 4376400.0000, 
raw observation next is [33.0, 50.33333333333333, 1.0, 2.0, 1.009768501949448, 1.0, 2.0, 1.009768501949448, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 121.9258769091519, 2303808.061386177, 2303808.061386177, 438057.9975813996], 
processed observation next is [1.0, 0.6521739130434783, 0.7777777777777778, 0.5033333333333333, 1.0, 1.0, 1.0116291689874382, 1.0, 1.0, 1.0116291689874382, 0.0, 0.5, -0.25, 8.881784197001253e-17, 0.0, 0.8094610287012328, 0.822788593352206, 0.822788593352206, 0.8424192261180762], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.2174287], dtype=float32), -0.14129291]. 
=============================================
[2019-04-27 18:26:46,647] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2920928e-15 7.6589873e-05 3.7102041e-06 9.9976295e-01 1.5667710e-04], sum to 1.0000
[2019-04-27 18:26:46,653] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1772
[2019-04-27 18:26:46,657] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.5, 81.5, 1.0, 2.0, 0.32127018143793, 1.0, 2.0, 0.32127018143793, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 732282.9643369211, 732282.9643369211, 185643.4808087829], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4440600.0000, 
sim time next is 4441200.0000, 
raw observation next is [26.66666666666667, 80.66666666666667, 1.0, 2.0, 0.3229639904145829, 1.0, 2.0, 0.3229639904145829, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 736145.5793862322, 736145.5793862325, 186062.9320100784], 
processed observation next is [0.0, 0.391304347826087, 0.5432098765432101, 0.8066666666666668, 1.0, 1.0, 0.19400475049355106, 1.0, 1.0, 0.19400475049355106, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.26290913549508294, 0.26290913549508305, 0.35781333078861227], 
reward next is 0.6422, 
noisyNet noise sample is [array([-1.475806], dtype=float32), -0.31299475]. 
=============================================
[2019-04-27 18:26:47,854] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.5216808e-17 5.5453925e-06 4.7680285e-08 9.9998629e-01 8.0660047e-06], sum to 1.0000
[2019-04-27 18:26:47,863] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4114
[2019-04-27 18:26:47,871] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.3666570790107373, 1.0, 2.0, 0.3666570790107373, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 835791.3950273665, 835791.3950273665, 197227.1205366458], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4462800.0000, 
sim time next is 4463400.0000, 
raw observation next is [30.0, 70.0, 1.0, 2.0, 0.3677152344647214, 1.0, 2.0, 0.3677152344647214, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 838204.7694741348, 838204.7694741352, 197505.6656160125], 
processed observation next is [0.0, 0.6521739130434783, 0.6666666666666666, 0.7, 1.0, 1.0, 0.24728004102943021, 1.0, 1.0, 0.24728004102943021, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2993588462407624, 0.2993588462407626, 0.37981858772310095], 
reward next is 0.6202, 
noisyNet noise sample is [array([-0.96827507], dtype=float32), 0.09119026]. 
=============================================
[2019-04-27 18:26:52,619] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.3154198e-14 1.1169548e-05 8.1301188e-08 9.9998713e-01 1.5952718e-06], sum to 1.0000
[2019-04-27 18:26:52,627] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3389
[2019-04-27 18:26:52,630] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.0, 100.0, 1.0, 2.0, 0.3001026962212248, 1.0, 2.0, 0.3001026962212248, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 687505.4891411051, 687505.4891411054, 180659.5176396027], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4546800.0000, 
sim time next is 4547400.0000, 
raw observation next is [23.0, 100.0, 1.0, 2.0, 0.2991616186868833, 1.0, 2.0, 0.2991616186868833, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 685852.9834937097, 685852.9834937102, 180458.2351849432], 
processed observation next is [0.0, 0.6521739130434783, 0.4074074074074074, 1.0, 1.0, 1.0, 0.16566859367486106, 1.0, 1.0, 0.16566859367486106, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.24494749410489633, 0.2449474941048965, 0.3470350676633523], 
reward next is 0.6530, 
noisyNet noise sample is [array([-1.1517807], dtype=float32), 1.3003871]. 
=============================================
[2019-04-27 18:26:53,340] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.9740780e-16 5.5624993e-04 2.5481600e-07 9.9940979e-01 3.3764434e-05], sum to 1.0000
[2019-04-27 18:26:53,354] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0576
[2019-04-27 18:26:53,364] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.13333333333333, 96.0, 1.0, 2.0, 0.2844835710732909, 1.0, 2.0, 0.2844835710732909, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 657772.7668545742, 657772.7668545747, 177241.3644904346], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4566000.0000, 
sim time next is 4566600.0000, 
raw observation next is [23.1, 97.0, 1.0, 2.0, 0.286217641252597, 1.0, 2.0, 0.286217641252597, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 660526.4577357398, 660526.4577357402, 177590.8823515607], 
processed observation next is [0.0, 0.8695652173913043, 0.41111111111111115, 0.97, 1.0, 1.0, 0.15025909672928217, 1.0, 1.0, 0.15025909672928217, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.23590230633419276, 0.23590230633419293, 0.3415209275991552], 
reward next is 0.6585, 
noisyNet noise sample is [array([-0.9954631], dtype=float32), -0.8456719]. 
=============================================
[2019-04-27 18:26:53,568] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.3678780e-16 4.7693584e-06 1.2307142e-08 9.9999523e-01 5.2123870e-08], sum to 1.0000
[2019-04-27 18:26:53,576] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9499
[2019-04-27 18:26:53,581] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.33333333333333, 94.0, 1.0, 2.0, 0.2865147539931838, 1.0, 2.0, 0.2865147539931838, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 662329.4128457914, 662329.4128457914, 177714.3928688305], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4564200.0000, 
sim time next is 4564800.0000, 
raw observation next is [23.2, 94.0, 1.0, 2.0, 0.2830252775218292, 1.0, 2.0, 0.2830252775218292, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 656113.584995315, 656113.584995315, 176978.7491025567], 
processed observation next is [0.0, 0.8695652173913043, 0.4148148148148148, 0.94, 1.0, 1.0, 0.14645866371646335, 1.0, 1.0, 0.14645866371646335, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.23432628035546962, 0.23432628035546962, 0.3403437482741475], 
reward next is 0.6597, 
noisyNet noise sample is [array([-0.21901189], dtype=float32), 2.1269956]. 
=============================================
[2019-04-27 18:26:54,939] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2673533e-11 8.4690510e-05 1.4637807e-05 9.9966609e-01 2.3454733e-04], sum to 1.0000
[2019-04-27 18:26:54,946] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5748
[2019-04-27 18:26:54,952] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [21.4, 98.66666666666667, 1.0, 2.0, 0.3181034560490819, 1.0, 2.0, 0.3181034560490819, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 753316.4406560761, 753316.4406560761, 186139.8815752495], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4587600.0000, 
sim time next is 4588200.0000, 
raw observation next is [21.3, 99.0, 1.0, 2.0, 0.2786970485061127, 1.0, 2.0, 0.2786970485061127, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 661993.7624240194, 661993.7624240197, 176658.6086917669], 
processed observation next is [1.0, 0.08695652173913043, 0.3444444444444445, 0.99, 1.0, 1.0, 0.14130601012632465, 1.0, 1.0, 0.14130601012632465, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.23642634372286406, 0.2364263437228642, 0.33972809363801326], 
reward next is 0.6603, 
noisyNet noise sample is [array([-0.31847784], dtype=float32), -1.5032995]. 
=============================================
[2019-04-27 18:27:05,552] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.6872145e-15 7.1439729e-08 3.4855362e-11 9.9999690e-01 2.9718178e-06], sum to 1.0000
[2019-04-27 18:27:05,559] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7715
[2019-04-27 18:27:05,565] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.91666666666667, 93.16666666666666, 1.0, 2.0, 0.3286930705337008, 1.0, 2.0, 0.3286930705337008, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 751335.5322836888, 751335.5322836888, 187593.3182699091], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4780200.0000, 
sim time next is 4780800.0000, 
raw observation next is [23.9, 93.0, 1.0, 2.0, 0.3502918908235377, 1.0, 2.0, 0.3502918908235377, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 801138.2635046727, 801138.2635046731, 193097.418608312], 
processed observation next is [1.0, 0.34782608695652173, 0.4407407407407407, 0.93, 1.0, 1.0, 0.22653796526611628, 1.0, 1.0, 0.22653796526611628, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.28612080839452597, 0.28612080839452614, 0.37134118963136925], 
reward next is 0.6287, 
noisyNet noise sample is [array([-0.638309], dtype=float32), -2.2547917]. 
=============================================
[2019-04-27 18:27:12,473] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.5327766e-18 1.2349096e-15 4.6598387e-09 1.9538276e-04 9.9980468e-01], sum to 1.0000
[2019-04-27 18:27:12,481] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5866
[2019-04-27 18:27:12,485] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.16666666666667, 69.83333333333333, 1.0, 2.0, 0.323268996051101, 1.0, 2.0, 0.323268996051101, 1.0, 2.0, 0.5146547698495467, 6.9112, 6.9112, 121.94756008, 1105527.284938748, 1105527.284938748, 265103.4437204025], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4900200.0000, 
sim time next is 4900800.0000, 
raw observation next is [30.73333333333333, 73.66666666666667, 1.0, 2.0, 0.2447839449252022, 1.0, 2.0, 0.2447839449252022, 1.0, 2.0, 0.3897040123774499, 6.9112, 6.9112, 121.94756008, 836974.6473987018, 836974.6473987018, 235783.4800707096], 
processed observation next is [1.0, 0.7391304347826086, 0.6938271604938271, 0.7366666666666667, 1.0, 1.0, 0.10093326776809786, 1.0, 1.0, 0.10093326776809786, 1.0, 1.0, 0.23713001547181234, 0.0, 0.0, 0.8096049824067558, 0.29891951692810775, 0.29891951692810775, 0.4534297693667492], 
reward next is 0.5466, 
noisyNet noise sample is [array([-0.5285695], dtype=float32), -0.053959407]. 
=============================================
[2019-04-27 18:27:27,824] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-27 18:27:27,825] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 18:27:27,826] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:27:27,826] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 18:27:27,828] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:27:27,828] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 18:27:27,829] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 18:27:27,830] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 18:27:27,832] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:27:27,833] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:27:27,831] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:27:27,850] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run15
[2019-04-27 18:27:27,850] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run15
[2019-04-27 18:27:27,891] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run15
[2019-04-27 18:27:27,913] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run15
[2019-04-27 18:27:27,913] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run15
[2019-04-27 18:27:30,186] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00211614], dtype=float32), 0.06578648]
[2019-04-27 18:27:30,187] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [26.60583487166667, 71.15422756, 1.0, 2.0, 0.1878868812111244, 1.0, 2.0, 0.1878868812111244, 1.0, 2.0, 0.2992544799250413, 6.9112, 6.9112, 121.94756008, 646610.4655905134, 646610.4655905134, 216686.5856709249]
[2019-04-27 18:27:30,188] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 18:27:30,193] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [2.2589160e-15 4.9254623e-08 4.3832604e-09 1.4325899e-05 9.9998558e-01], sampled 0.7698010749999136
[2019-04-27 18:27:38,878] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00211614], dtype=float32), 0.06578648]
[2019-04-27 18:27:38,879] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [18.75492775666667, 43.33935846, 1.0, 2.0, 0.16, 1.0, 2.0, 0.16, 1.0, 2.0, 0.2158483421753832, 6.9112, 6.9112, 121.94756008, 462385.1550756904, 462385.1550756904, 162177.117485953]
[2019-04-27 18:27:38,880] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:27:38,884] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [2.0932657e-14 2.3235789e-07 1.2636905e-08 4.0544121e-05 9.9995923e-01], sampled 0.27413731994890267
[2019-04-27 18:28:00,693] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00211614], dtype=float32), 0.06578648]
[2019-04-27 18:28:00,694] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [21.15, 96.66666666666667, 1.0, 2.0, 0.1729072974889183, 1.0, 2.0, 0.1729072974889183, 1.0, 2.0, 0.278103279148129, 6.9112, 6.9112, 121.94756008, 617642.383934575, 617642.383934575, 211513.4292149376]
[2019-04-27 18:28:00,694] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 18:28:00,698] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [9.7872751e-15 5.2782343e-07 8.2405434e-09 4.0411585e-05 9.9995899e-01], sampled 0.7128780905314694
[2019-04-27 18:28:14,987] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00211614], dtype=float32), 0.06578648]
[2019-04-27 18:28:14,988] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [26.45, 82.16666666666667, 1.0, 2.0, 0.2100206348537487, 1.0, 2.0, 0.2100206348537487, 1.0, 2.0, 0.3343596905817273, 6.9112, 6.9112, 121.94756008, 718054.9392712358, 718054.9392712358, 223895.8109272044]
[2019-04-27 18:28:14,992] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 18:28:14,994] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [5.0828694e-15 2.5925951e-07 5.6455685e-09 2.7859327e-05 9.9997187e-01], sampled 0.3783886275928554
[2019-04-27 18:28:23,129] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00211614], dtype=float32), 0.06578648]
[2019-04-27 18:28:23,131] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [33.95, 48.0, 1.0, 2.0, 0.226581171684702, 1.0, 2.0, 0.226581171684702, 1.0, 2.0, 0.3607246045556363, 6.9112, 6.9112, 121.94756008, 774703.5789746419, 774703.5789746419, 229474.5738349414]
[2019-04-27 18:28:23,131] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 18:28:23,133] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [8.5062236e-16 1.9941382e-08 2.0345428e-09 9.5388850e-06 9.9999046e-01], sampled 0.4204874028537908
[2019-04-27 18:28:30,020] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00211614], dtype=float32), 0.06578648]
[2019-04-27 18:28:30,023] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [31.63004553, 63.73927256333334, 1.0, 2.0, 0.2377768592053242, 1.0, 2.0, 0.2377768592053242, 1.0, 2.0, 0.378548503706554, 6.9112, 6.9112, 121.94756008, 813003.0465618155, 813003.0465618155, 233332.9675126052]
[2019-04-27 18:28:30,025] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 18:28:30,027] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [2.2529184e-16 1.6342820e-10 4.5223492e-10 2.2905501e-06 9.9999774e-01], sampled 0.7820632362746908
[2019-04-27 18:28:45,716] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00211614], dtype=float32), 0.06578648]
[2019-04-27 18:28:45,716] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [21.5, 97.0, 1.0, 2.0, 0.1665141401163906, 1.0, 2.0, 0.1665141401163906, 1.0, 2.0, 0.2670977620590417, 6.9112, 6.9112, 121.94756008, 591000.3767679029, 591000.3767679029, 209670.518107934]
[2019-04-27 18:28:45,717] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 18:28:45,721] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [8.3803568e-15 7.2800657e-07 8.3332976e-09 3.9597304e-05 9.9995971e-01], sampled 0.5451038939337772
[2019-04-27 18:28:58,601] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00211614], dtype=float32), 0.06578648]
[2019-04-27 18:28:58,602] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [25.67893762, 56.14812226833334, 1.0, 2.0, 0.16, 1.0, 2.0, 0.16, 1.0, 2.0, 0.2261176753240929, 6.9112, 6.9112, 121.94756008, 506890.945382716, 506890.945382716, 195888.6556691152]
[2019-04-27 18:28:58,603] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:28:58,605] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.4877507e-14 5.0894374e-07 1.3598664e-08 4.2951760e-05 9.9995649e-01], sampled 0.5876954058213211
[2019-04-27 18:29:04,772] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00211614], dtype=float32), 0.06578648]
[2019-04-27 18:29:04,774] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [31.0, 28.0, 1.0, 2.0, 0.2066731953446562, 1.0, 2.0, 0.2066731953446562, 1.0, 2.0, 0.3412768648376686, 6.9112, 6.9112, 121.94756008, 765099.0985535885, 765099.0985535885, 220919.5360622806]
[2019-04-27 18:29:04,774] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 18:29:04,776] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [4.1032610e-15 2.2647132e-08 4.4584043e-09 1.4534702e-05 9.9998546e-01], sampled 0.5874180493958906
[2019-04-27 18:29:12,539] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 4489.4114 2940729484.9691 28.0000
[2019-04-27 18:29:12,705] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 4609.0871 2894650061.9814 12.0000
[2019-04-27 18:29:12,755] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 4511.0818 3107453456.8527 0.0000
[2019-04-27 18:29:12,761] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 4393.5610 2875848145.5416 8.0000
[2019-04-27 18:29:12,920] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 4276.0345 2920083890.6750 33.0000
[2019-04-27 18:29:13,936] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 350000, evaluation results [350000.0, 4511.081762653399, 3107453456.8527336, 0.0, 4609.087071503921, 2894650061.9814215, 12.0, 4393.560954835539, 2875848145.5415955, 8.0, 4489.411365254514, 2940729484.9691443, 28.0, 4276.03447544665, 2920083890.6749926, 33.0]
[2019-04-27 18:29:16,363] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.1204228e-08 1.2292745e-06 2.0617113e-05 8.7840314e-04 9.9909973e-01], sum to 1.0000
[2019-04-27 18:29:16,369] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4011
[2019-04-27 18:29:16,374] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.96666666666667, 77.0, 1.0, 2.0, 0.5889978999202705, 1.0, 2.0, 0.5889978999202705, 1.0, 2.0, 0.9377038389954834, 6.911200000000001, 6.9112, 121.94756008, 2015391.086477409, 2015391.086477409, 389413.6750426812], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5226000.0000, 
sim time next is 5226600.0000, 
raw observation next is [27.98333333333333, 78.0, 1.0, 2.0, 0.5980763998469633, 1.0, 2.0, 0.5980763998469633, 1.0, 2.0, 0.9521571065448794, 6.9112, 6.9112, 121.94756008, 2046490.840063918, 2046490.840063918, 394328.5138260337], 
processed observation next is [1.0, 0.4782608695652174, 0.5919753086419752, 0.78, 1.0, 1.0, 0.5215195236273372, 1.0, 1.0, 0.5215195236273372, 1.0, 1.0, 0.9401963831810991, 0.0, 0.0, 0.8096049824067558, 0.7308895857371135, 0.7308895857371135, 0.7583240650500648], 
reward next is 0.2417, 
noisyNet noise sample is [array([-0.00043906], dtype=float32), 1.8290077]. 
=============================================
[2019-04-27 18:29:24,158] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0452921e-15 1.3963632e-09 5.9133975e-10 3.6436149e-06 9.9999630e-01], sum to 1.0000
[2019-04-27 18:29:24,167] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9960
[2019-04-27 18:29:24,172] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.85, 87.5, 1.0, 2.0, 0.2970829327813798, 1.0, 2.0, 0.2970829327813798, 1.0, 2.0, 0.4729657043036083, 6.911199999999999, 6.9112, 121.94756008, 1015915.858668122, 1015915.858668122, 254939.2623485978], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5365800.0000, 
sim time next is 5366400.0000, 
raw observation next is [24.8, 87.66666666666667, 1.0, 2.0, 0.2860424705498783, 1.0, 2.0, 0.2860424705498783, 1.0, 2.0, 0.4553889288683055, 6.9112, 6.9112, 121.94756008, 978137.390991226, 978137.390991226, 250768.3683324126], 
processed observation next is [1.0, 0.08695652173913043, 0.4740740740740741, 0.8766666666666667, 1.0, 1.0, 0.15005056017842655, 1.0, 1.0, 0.15005056017842655, 1.0, 1.0, 0.31923616108538183, 0.0, 0.0, 0.8096049824067558, 0.3493347824968664, 0.3493347824968664, 0.4822468621777165], 
reward next is 0.5178, 
noisyNet noise sample is [array([-0.617091], dtype=float32), 1.0042447]. 
=============================================
[2019-04-27 18:29:24,518] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.12023373e-13 3.05526072e-07 2.85890085e-08 1.53639012e-05
 9.99984264e-01], sum to 1.0000
[2019-04-27 18:29:24,525] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8107
[2019-04-27 18:29:24,529] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.5, 89.33333333333334, 1.0, 2.0, 0.2488005062230073, 1.0, 2.0, 0.2488005062230073, 1.0, 2.0, 0.3960985087738247, 6.911199999999998, 6.9112, 121.94756008, 850715.8482964631, 850715.8482964641, 237200.5183967372], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5370000.0000, 
sim time next is 5370600.0000, 
raw observation next is [24.45, 89.66666666666666, 1.0, 2.0, 0.2421632950937415, 1.0, 2.0, 0.2421632950937415, 1.0, 2.0, 0.385531852497158, 6.9112, 6.9112, 121.94756008, 828009.1817145685, 828009.1817145685, 234863.7766840464], 
processed observation next is [1.0, 0.13043478260869565, 0.4611111111111111, 0.8966666666666666, 1.0, 1.0, 0.09781344654016844, 1.0, 1.0, 0.09781344654016844, 1.0, 1.0, 0.23191481562144747, 0.0, 0.0, 0.8096049824067558, 0.29571756489806017, 0.29571756489806017, 0.45166110900778156], 
reward next is 0.5483, 
noisyNet noise sample is [array([-0.6700634], dtype=float32), 1.4416785]. 
=============================================
[2019-04-27 18:29:27,157] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.5291182e-08 1.7817678e-04 3.9527664e-04 1.3311980e-02 9.8611450e-01], sum to 1.0000
[2019-04-27 18:29:27,165] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0473
[2019-04-27 18:29:27,171] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.4, 76.5, 1.0, 2.0, 0.2441849104249091, 1.0, 2.0, 0.2441849104249091, 1.0, 2.0, 0.3887503299437917, 6.9112, 6.9112, 121.94756008, 834925.2902436443, 834925.2902436443, 235572.9137166526], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5423400.0000, 
sim time next is 5424000.0000, 
raw observation next is [29.33333333333334, 77.0, 1.0, 2.0, 0.2453923264518639, 1.0, 2.0, 0.2453923264518639, 1.0, 2.0, 0.3906725755814983, 6.9112, 6.9112, 121.94756008, 839055.987276833, 839055.987276833, 235997.5371995819], 
processed observation next is [1.0, 0.782608695652174, 0.6419753086419755, 0.77, 1.0, 1.0, 0.10165753149031417, 1.0, 1.0, 0.10165753149031417, 1.0, 1.0, 0.23834071947687288, 0.0, 0.0, 0.8096049824067558, 0.29966285259886893, 0.29966285259886893, 0.4538414176915036], 
reward next is 0.5462, 
noisyNet noise sample is [array([-1.8373144], dtype=float32), 0.100637816]. 
=============================================
[2019-04-27 18:29:27,185] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[25.430885]
 [25.234715]
 [24.94556 ]
 [24.67168 ]
 [24.532635]], R is [[25.86319923]
 [26.15154266]
 [26.43680191]
 [26.71777916]
 [26.99975777]].
[2019-04-27 18:29:27,443] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0251113e-09 1.4566436e-05 2.5046845e-06 3.6681185e-03 9.9631482e-01], sum to 1.0000
[2019-04-27 18:29:27,454] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9382
[2019-04-27 18:29:27,460] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.2, 78.0, 1.0, 2.0, 0.2535369671260405, 1.0, 2.0, 0.2535369671260405, 1.0, 2.0, 0.403639108787216, 6.9112, 6.9112, 121.94756008, 866920.2398786257, 866920.2398786257, 238883.115710158], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5425200.0000, 
sim time next is 5425800.0000, 
raw observation next is [29.13333333333333, 78.5, 1.0, 2.0, 0.2545208015615846, 1.0, 2.0, 0.2545208015615846, 1.0, 2.0, 0.4052054052498529, 6.911200000000001, 6.9112, 121.94756008, 870286.179613279, 870286.1796132785, 239234.1881636682], 
processed observation next is [1.0, 0.8260869565217391, 0.6345679012345677, 0.785, 1.0, 1.0, 0.11252476376379116, 1.0, 1.0, 0.11252476376379116, 1.0, 1.0, 0.2565067565623161, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.31081649271902817, 0.310816492719028, 0.4600657464685927], 
reward next is 0.5399, 
noisyNet noise sample is [array([-0.14739276], dtype=float32), 0.71434027]. 
=============================================
[2019-04-27 18:29:30,050] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2789243e-13 2.4212738e-07 4.6874653e-08 2.8487045e-04 9.9971479e-01], sum to 1.0000
[2019-04-27 18:29:30,059] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0646
[2019-04-27 18:29:30,069] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.7, 90.66666666666667, 1.0, 2.0, 0.5092005039831464, 1.0, 2.0, 0.5092005039831464, 1.0, 2.0, 0.8106637858438293, 6.9112, 6.9112, 121.94756008, 1742079.798176423, 1742079.798176423, 348127.7159103935], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5473200.0000, 
sim time next is 5473800.0000, 
raw observation next is [27.85, 90.0, 1.0, 2.0, 0.5538892953708808, 1.0, 2.0, 0.5538892953708808, 1.0, 2.0, 0.8818097971454304, 6.911199999999999, 6.9112, 121.94756008, 1895131.52272556, 1895131.52272556, 370825.116715604], 
processed observation next is [1.0, 0.34782608695652173, 0.5870370370370371, 0.9, 1.0, 1.0, 0.46891582782247715, 1.0, 1.0, 0.46891582782247715, 1.0, 1.0, 0.852262246431788, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.6768326866877, 0.6768326866877, 0.7131252244530846], 
reward next is 0.2869, 
noisyNet noise sample is [array([0.8209714], dtype=float32), -1.1158394]. 
=============================================
[2019-04-27 18:29:35,637] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [9.0543684e-11 2.6428966e-09 5.5167339e-07 3.8652724e-04 9.9961299e-01], sum to 1.0000
[2019-04-27 18:29:35,647] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1809
[2019-04-27 18:29:35,652] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.1, 74.66666666666666, 1.0, 2.0, 0.6774656919186417, 1.0, 2.0, 0.6520975079357556, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.94756008, 2231570.589749169, 2231570.589749169, 423239.6131097631], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5578800.0000, 
sim time next is 5579400.0000, 
raw observation next is [30.3, 74.33333333333334, 1.0, 2.0, 0.6991197052266481, 1.0, 2.0, 0.6629245145897589, 1.0, 2.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.94756008, 2268669.217500289, 2268669.217500289, 429003.6069341602], 
processed observation next is [1.0, 0.5652173913043478, 0.6777777777777778, 0.7433333333333334, 1.0, 1.0, 0.6418091728888669, 1.0, 1.0, 0.5987196602259034, 1.0, 1.0, 0.9972168686025908, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.8102390062501033, 0.8102390062501033, 0.8250069364118466], 
reward next is 0.1750, 
noisyNet noise sample is [array([0.79923725], dtype=float32), -0.1563184]. 
=============================================
[2019-04-27 18:29:41,200] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.1360083e-15 9.4239688e-08 2.1227732e-10 2.1798773e-05 9.9997807e-01], sum to 1.0000
[2019-04-27 18:29:41,209] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5911
[2019-04-27 18:29:41,219] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.48333333333333, 66.66666666666667, 1.0, 2.0, 0.2365628358901545, 1.0, 2.0, 0.2365628358901545, 1.0, 2.0, 0.3766157390508248, 6.9112, 6.9112, 121.94756008, 808849.8865121132, 808849.8865121132, 232911.1890414429], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5674200.0000, 
sim time next is 5674800.0000, 
raw observation next is [30.56666666666667, 66.33333333333334, 1.0, 2.0, 0.2360438123431197, 1.0, 2.0, 0.2360438123431197, 1.0, 2.0, 0.3757894366605284, 6.911199999999999, 6.9112, 121.94756008, 807074.3196855967, 807074.3196855971, 232731.1201898781], 
processed observation next is [0.0, 0.6956521739130435, 0.6876543209876544, 0.6633333333333334, 1.0, 1.0, 0.09052834802752344, 1.0, 1.0, 0.09052834802752344, 1.0, 1.0, 0.21973679582566047, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.2882408284591417, 0.28824082845914184, 0.44755984651899633], 
reward next is 0.5524, 
noisyNet noise sample is [array([1.438633], dtype=float32), 1.3344283]. 
=============================================
[2019-04-27 18:29:45,342] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.3554067e-12 9.9999952e-01 1.6440072e-11 2.3437519e-07 2.4387089e-07], sum to 1.0000
[2019-04-27 18:29:45,352] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4891
[2019-04-27 18:29:45,355] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.95, 62.0, 1.0, 2.0, 0.5372341065001436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 632163.2599130527, 632163.2599130527, 148529.5383209225], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5754600.0000, 
sim time next is 5755200.0000, 
raw observation next is [27.96666666666667, 62.0, 1.0, 2.0, 0.5384129084865275, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 633363.1154420188, 633363.1154420188, 148714.0303334832], 
processed observation next is [0.0, 0.6086956521739131, 0.5913580246913581, 0.62, 1.0, 1.0, 0.45049155772205646, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.22620111265786388, 0.22620111265786388, 0.2859885198720831], 
reward next is 0.7140, 
noisyNet noise sample is [array([-1.0494314], dtype=float32), -0.39739203]. 
=============================================
[2019-04-27 18:29:46,982] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3209939e-22 1.0000000e+00 7.2303263e-20 2.4205783e-15 4.2858808e-15], sum to 1.0000
[2019-04-27 18:29:46,991] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8782
[2019-04-27 18:29:46,997] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.9, 79.66666666666667, 1.0, 2.0, 0.5386107447419665, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 634914.955542841, 634914.955542841, 148800.2985801305], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5777400.0000, 
sim time next is 5778000.0000, 
raw observation next is [24.8, 80.0, 1.0, 2.0, 0.5356245784624534, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 631996.4142400594, 631996.4142400594, 148337.8431348591], 
processed observation next is [0.0, 0.9130434782608695, 0.4740740740740741, 0.8, 1.0, 1.0, 0.4471721172172064, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.22571300508573552, 0.22571300508573552, 0.28526508295165215], 
reward next is 0.7147, 
noisyNet noise sample is [array([-0.87982285], dtype=float32), 1.1540662]. 
=============================================
[2019-04-27 18:29:47,014] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[69.37989]
 [69.37998]
 [69.3685 ]
 [69.35633]
 [69.34026]], R is [[69.39988708]
 [69.41973114]
 [69.43856812]
 [69.45653534]
 [69.47355652]].
[2019-04-27 18:29:57,779] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0097009e-24 1.0000000e+00 7.7431973e-21 2.2662285e-19 2.8519911e-19], sum to 1.0000
[2019-04-27 18:29:57,790] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9431
[2019-04-27 18:29:57,794] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.6, 81.5, 1.0, 2.0, 0.444042087099989, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 541860.0935732728, 541860.0935732728, 134781.5853977911], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5964600.0000, 
sim time next is 5965200.0000, 
raw observation next is [22.6, 81.0, 1.0, 2.0, 0.4407898351510403, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 538454.303919212, 538454.303919212, 134316.6413874249], 
processed observation next is [1.0, 0.043478260869565216, 0.39259259259259266, 0.81, 1.0, 1.0, 0.33427361327504795, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1923051085425757, 0.1923051085425757, 0.25830123343735556], 
reward next is 0.7417, 
noisyNet noise sample is [array([0.6528486], dtype=float32), 0.34172323]. 
=============================================
[2019-04-27 18:30:05,769] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.6017355e-16 9.9999976e-01 2.6743133e-10 2.7035929e-10 2.9633713e-07], sum to 1.0000
[2019-04-27 18:30:05,781] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1986
[2019-04-27 18:30:05,787] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.36666666666667, 57.66666666666666, 1.0, 2.0, 0.5354709277737343, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 623662.4829116212, 623662.4829116208, 147969.0192298317], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6111600.0000, 
sim time next is 6112200.0000, 
raw observation next is [29.23333333333333, 58.33333333333334, 1.0, 2.0, 0.5407419736756691, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 629779.087530199, 629779.087530199, 148824.7340329396], 
processed observation next is [1.0, 0.7391304347826086, 0.6382716049382715, 0.5833333333333335, 1.0, 1.0, 0.45326425437579654, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.22492110268935678, 0.22492110268935678, 0.2862014116018069], 
reward next is 0.7138, 
noisyNet noise sample is [array([0.13122615], dtype=float32), -0.627106]. 
=============================================
[2019-04-27 18:30:05,912] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-27 18:30:05,913] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 18:30:05,914] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 18:30:05,915] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:30:05,916] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:30:05,916] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 18:30:05,918] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 18:30:05,919] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 18:30:05,921] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:30:05,922] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:30:05,919] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:30:05,935] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run16
[2019-04-27 18:30:05,936] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run16
[2019-04-27 18:30:05,954] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run16
[2019-04-27 18:30:05,975] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run16
[2019-04-27 18:30:06,002] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run16
[2019-04-27 18:30:44,110] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.01797843], dtype=float32), 0.070237294]
[2019-04-27 18:30:44,110] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [29.22884958, 83.2694961, 1.0, 2.0, 0.858315593453312, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156558, 978352.1373119068, 978352.1373119068, 208293.580235855]
[2019-04-27 18:30:44,112] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:30:44,115] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.2067874e-22 1.0000000e+00 2.2703669e-17 4.1671712e-17 9.8646304e-15], sampled 0.18033354468517215
[2019-04-27 18:30:44,958] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.01797843], dtype=float32), 0.070237294]
[2019-04-27 18:30:44,959] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [25.9767232, 74.72898147, 1.0, 2.0, 0.5328446316744682, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 625974.357962411, 625974.357962411, 147774.0789906147]
[2019-04-27 18:30:44,960] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:30:44,962] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.0817119e-22 1.0000000e+00 1.0992690e-17 1.7419464e-17 2.8215025e-15], sampled 0.16440833446751357
[2019-04-27 18:30:52,391] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.01797843], dtype=float32), 0.070237294]
[2019-04-27 18:30:52,393] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [23.0, 94.0, 1.0, 2.0, 0.5506974275374057, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 648856.3776503887, 648856.3776503887, 150774.3192576704]
[2019-04-27 18:30:52,395] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 18:30:52,400] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [9.2041040e-24 1.0000000e+00 2.0660010e-18 3.6665178e-18 8.2984225e-16], sampled 0.7361119895218622
[2019-04-27 18:31:13,218] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01797843], dtype=float32), 0.070237294]
[2019-04-27 18:31:13,220] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [23.45489995666667, 88.48575368, 1.0, 2.0, 0.6955135227291046, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 824550.0626979842, 824550.0626979837, 176776.1515343101]
[2019-04-27 18:31:13,221] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 18:31:13,223] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [2.2693307e-22 1.0000000e+00 2.3568308e-17 3.8944530e-17 6.6648453e-15], sampled 0.3840333238822874
[2019-04-27 18:31:14,547] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01797843], dtype=float32), 0.070237294]
[2019-04-27 18:31:14,549] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [28.09048795166667, 85.93670307333332, 1.0, 2.0, 0.9591216215372095, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9977734948820727, 6.911200000000001, 6.9112, 121.9260426156618, 1808575.127519357, 1808575.127519357, 370043.610705612]
[2019-04-27 18:31:14,549] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 18:31:14,552] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [4.0364194e-17 9.9999988e-01 2.2188931e-11 5.5772754e-11 6.1982441e-08], sampled 0.6263555870363131
[2019-04-27 18:31:14,554] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1808575.127519357 W.
[2019-04-27 18:31:50,839] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8101.8890 2445418968.2785 742.0000
[2019-04-27 18:31:50,891] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8589.1994 2249198689.9054 544.0000
[2019-04-27 18:31:51,035] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8922.0226 2120555485.0184 430.0000
[2019-04-27 18:31:51,056] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8768.4793 2170856538.1319 493.0000
[2019-04-27 18:31:51,266] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8700.4032 2195716736.8676 572.0000
[2019-04-27 18:31:52,281] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 375000, evaluation results [375000.0, 8101.889039400502, 2445418968.278465, 742.0, 8768.479278690636, 2170856538.1318502, 493.0, 8922.022624557043, 2120555485.018368, 430.0, 8589.199446855526, 2249198689.9053736, 544.0, 8700.403225083586, 2195716736.8676405, 572.0]
[2019-04-27 18:31:54,926] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.5147669e-20 1.0000000e+00 4.5766596e-15 4.4725711e-14 1.0328363e-11], sum to 1.0000
[2019-04-27 18:31:54,932] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4680
[2019-04-27 18:31:54,939] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 85.66666666666667, 1.0, 2.0, 0.5592191833166501, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 660156.4161806185, 660156.4161806185, 152241.4929332614], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6162000.0000, 
sim time next is 6162600.0000, 
raw observation next is [24.1, 85.33333333333334, 1.0, 2.0, 0.5628674139279833, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 663783.0009402285, 663783.0009402281, 152824.1869196192], 
processed observation next is [1.0, 0.30434782608695654, 0.4481481481481482, 0.8533333333333334, 1.0, 1.0, 0.47960406419998014, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.237065357478653, 0.2370653574786529, 0.29389266715311385], 
reward next is 0.7061, 
noisyNet noise sample is [array([-0.8373625], dtype=float32), -0.4315755]. 
=============================================
[2019-04-27 18:31:56,622] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.6648487e-17 1.0000000e+00 8.0126056e-16 1.6908806e-15 4.9835573e-15], sum to 1.0000
[2019-04-27 18:31:56,636] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9533
[2019-04-27 18:31:56,639] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.71666666666667, 59.33333333333333, 1.0, 2.0, 0.5456168687824297, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 639119.733959976, 639119.733959976, 149779.897990376], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6202200.0000, 
sim time next is 6202800.0000, 
raw observation next is [28.6, 60.0, 1.0, 2.0, 0.5473855503614418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 641025.5394610317, 641025.5394610317, 150063.4694142607], 
processed observation next is [1.0, 0.8260869565217391, 0.6148148148148148, 0.6, 1.0, 1.0, 0.4611732742398116, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.22893769266465416, 0.22893769266465416, 0.2885835950274244], 
reward next is 0.7114, 
noisyNet noise sample is [array([0.61429894], dtype=float32), -0.7650506]. 
=============================================
[2019-04-27 18:31:57,709] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.9677032e-32 1.0000000e+00 1.8834763e-27 2.2126006e-26 9.1549359e-25], sum to 1.0000
[2019-04-27 18:31:57,715] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7529
[2019-04-27 18:31:57,721] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.2, 77.0, 1.0, 2.0, 0.4893858700537079, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 588245.7135229128, 588245.7135229128, 141399.5589991224], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6226200.0000, 
sim time next is 6226800.0000, 
raw observation next is [24.13333333333333, 77.33333333333334, 1.0, 2.0, 0.4862316547172054, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 584625.3984355974, 584625.398435597, 140915.4986687577], 
processed observation next is [0.0, 0.043478260869565216, 0.44938271604938257, 0.7733333333333334, 1.0, 1.0, 0.38837101752048264, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2087947851555705, 0.20879478515557034, 0.2709913435937648], 
reward next is 0.7290, 
noisyNet noise sample is [array([-0.6903889], dtype=float32), -0.43917182]. 
=============================================
[2019-04-27 18:32:03,160] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1741931e-27 1.0000000e+00 7.2971456e-24 1.0768367e-23 1.4310320e-20], sum to 1.0000
[2019-04-27 18:32:03,163] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6512
[2019-04-27 18:32:03,173] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.18333333333333, 88.00000000000001, 1.0, 2.0, 0.5703239981978017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 666527.6851175794, 666527.685117579, 153823.334998965], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6318600.0000, 
sim time next is 6319200.0000, 
raw observation next is [24.16666666666667, 88.0, 1.0, 2.0, 0.5687658656293605, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 664930.0563520597, 664930.0563520597, 153570.9040292649], 
processed observation next is [0.0, 0.13043478260869565, 0.45061728395061745, 0.88, 1.0, 1.0, 0.48662603051114345, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2374750201257356, 0.2374750201257356, 0.2953286615947402], 
reward next is 0.7047, 
noisyNet noise sample is [array([-1.4232756], dtype=float32), -1.0216875]. 
=============================================
[2019-04-27 18:32:06,540] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2127671e-24 1.0000000e+00 1.2369906e-21 3.1770990e-21 1.6076957e-19], sum to 1.0000
[2019-04-27 18:32:06,549] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3341
[2019-04-27 18:32:06,552] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.46666666666667, 67.33333333333334, 1.0, 2.0, 0.6825491346180683, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000003, 6.9112, 121.9260426156618, 777902.8350825787, 777902.8350825774, 172866.2782798299], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6380400.0000, 
sim time next is 6381000.0000, 
raw observation next is [29.35, 68.0, 1.0, 2.0, 0.685726127676014, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 781525.5073587134, 781525.507358713, 173458.9171792014], 
processed observation next is [0.0, 0.8695652173913043, 0.6425925925925926, 0.68, 1.0, 1.0, 0.6258644377095405, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.27911625262811196, 0.2791162526281118, 0.3335748407292335], 
reward next is 0.6664, 
noisyNet noise sample is [array([-0.53883266], dtype=float32), -1.478226]. 
=============================================
[2019-04-27 18:32:06,571] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[61.36241 ]
 [61.37628 ]
 [61.335247]
 [61.322647]
 [61.315487]], R is [[61.41248322]
 [61.46592331]
 [61.52064514]
 [61.57124329]
 [61.6211586 ]].
[2019-04-27 18:32:12,502] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [7.7610593e-21 1.0000000e+00 1.2431523e-17 1.8429509e-19 1.6115235e-16], sum to 1.0000
[2019-04-27 18:32:12,510] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3677
[2019-04-27 18:32:12,518] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.66666666666666, 83.16666666666667, 1.0, 2.0, 0.9739884014549907, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.984405909098804, 6.9112, 121.9257347976942, 1147812.503581155, 1110324.629695855, 234506.467981191], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6491400.0000, 
sim time next is 6492000.0000, 
raw observation next is [26.63333333333333, 83.33333333333334, 1.0, 2.0, 0.9235599935678587, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9259985322894, 1052772.139142042, 1052772.139142042, 222787.6358753516], 
processed observation next is [1.0, 0.13043478260869565, 0.5419753086419752, 0.8333333333333335, 1.0, 1.0, 0.9089999923426889, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094618361523866, 0.37599004969358646, 0.37599004969358646, 0.4284377612987531], 
reward next is 0.5716, 
noisyNet noise sample is [array([1.2106415], dtype=float32), 0.4141799]. 
=============================================
[2019-04-27 18:32:12,542] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[48.12298 ]
 [47.93937 ]
 [47.039513]
 [46.147472]
 [46.793808]], R is [[48.42658615]
 [48.12531662]
 [48.21999359]
 [47.73779297]
 [47.74216843]].
[2019-04-27 18:32:18,842] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7411664e-20 1.0000000e+00 3.9515547e-19 1.0203253e-17 2.0270733e-14], sum to 1.0000
[2019-04-27 18:32:18,849] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5832
[2019-04-27 18:32:18,852] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 37.0, 1.0, 2.0, 0.7923899917379342, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1004755.286921699, 1004755.286921699, 198000.3102443784], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6606000.0000, 
sim time next is 6606600.0000, 
raw observation next is [27.41666666666666, 37.5, 1.0, 2.0, 0.8523870331662353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 1079391.884099541, 1079391.88409954, 210985.263774009], 
processed observation next is [1.0, 0.4782608695652174, 0.5709876543209874, 0.375, 1.0, 1.0, 0.8242702775788515, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.3854971014641218, 0.38549710146412147, 0.40574089187309426], 
reward next is 0.5943, 
noisyNet noise sample is [array([-0.25962427], dtype=float32), -0.7101597]. 
=============================================
[2019-04-27 18:32:19,264] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.1450525e-18 1.0000000e+00 8.3821249e-17 2.6853294e-17 2.5032079e-13], sum to 1.0000
[2019-04-27 18:32:19,273] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2240
[2019-04-27 18:32:19,277] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.53333333333333, 38.0, 1.0, 2.0, 0.878060740169089, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.980231988841693, 6.9112, 121.9256952744962, 1144900.793808441, 1109550.346218481, 216714.0705291158], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6607200.0000, 
sim time next is 6607800.0000, 
raw observation next is [27.65, 38.5, 1.0, 2.0, 0.9141073167179632, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.212552016056443, 6.9112, 121.9247645585539, 1306974.890501013, 1152657.341351918, 224959.1453081437], 
processed observation next is [1.0, 0.4782608695652174, 0.5796296296296296, 0.385, 1.0, 1.0, 0.8977468056166228, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.030135201605644292, 0.0, 0.8094536438500253, 0.4667767466075047, 0.41166333619711354, 0.4326137409771994], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.3335524], dtype=float32), 0.38657555]. 
=============================================
[2019-04-27 18:32:26,280] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.1881507e-23 1.0000000e+00 9.0520267e-13 2.6608884e-16 6.6096034e-14], sum to 1.0000
[2019-04-27 18:32:26,281] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2769
[2019-04-27 18:32:26,287] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.28333333333333, 73.0, 1.0, 2.0, 0.3445827640792515, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 434687.2158777668, 434687.2158777668, 121222.0940351203], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6735000.0000, 
sim time next is 6735600.0000, 
raw observation next is [21.0, 75.0, 1.0, 2.0, 0.3439169260861508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 433849.9503119604, 433849.9503119604, 121134.6677327529], 
processed observation next is [1.0, 1.0, 0.3333333333333333, 0.75, 1.0, 1.0, 0.21894872153113193, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.15494641082570015, 0.15494641082570015, 0.2329512841014479], 
reward next is 0.7670, 
noisyNet noise sample is [array([-0.72522587], dtype=float32), -0.35975263]. 
=============================================
[2019-04-27 18:32:33,113] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3475816e-23 1.0000000e+00 6.4204853e-14 6.4896442e-17 1.9957903e-15], sum to 1.0000
[2019-04-27 18:32:33,123] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4924
[2019-04-27 18:32:33,130] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.76666666666667, 56.66666666666667, 1.0, 2.0, 0.4389646023959998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 534728.0280825008, 534728.0280825008, 134004.1450696203], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6862200.0000, 
sim time next is 6862800.0000, 
raw observation next is [27.03333333333333, 55.33333333333334, 1.0, 2.0, 0.4394320728422613, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 535239.8508782705, 535239.8508782705, 134071.2876402762], 
processed observation next is [0.0, 0.43478260869565216, 0.55679012345679, 0.5533333333333335, 1.0, 1.0, 0.33265722957412064, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1911570895993823, 0.1911570895993823, 0.25782939930822346], 
reward next is 0.7422, 
noisyNet noise sample is [array([0.6119703], dtype=float32), 0.6291385]. 
=============================================
[2019-04-27 18:32:37,730] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1281031e-21 1.0000000e+00 3.6020625e-13 6.0266493e-17 2.6229668e-11], sum to 1.0000
[2019-04-27 18:32:37,737] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5489
[2019-04-27 18:32:37,741] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 82.0, 1.0, 2.0, 0.4382819179523227, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 535399.322390278, 535399.322390278, 133947.2526159368], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6937200.0000, 
sim time next is 6937800.0000, 
raw observation next is [22.61666666666667, 81.66666666666667, 1.0, 2.0, 0.4403702714223137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 537415.8328206602, 537415.8328206602, 134239.8193498268], 
processed observation next is [0.0, 0.30434782608695654, 0.39320987654321005, 0.8166666666666668, 1.0, 1.0, 0.3337741326456116, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.19193422600737864, 0.19193422600737864, 0.2581534987496669], 
reward next is 0.7418, 
noisyNet noise sample is [array([-0.89981145], dtype=float32), -1.3813381]. 
=============================================
[2019-04-27 18:32:40,089] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1037134e-19 1.0000000e+00 2.5254419e-12 1.1927049e-15 4.8280470e-11], sum to 1.0000
[2019-04-27 18:32:40,101] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0731
[2019-04-27 18:32:40,108] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.9, 53.0, 1.0, 2.0, 0.4688073845849569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 567657.0767631304, 567657.0767631304, 138371.7943564291], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6980400.0000, 
sim time next is 6981000.0000, 
raw observation next is [27.63333333333333, 53.83333333333334, 1.0, 2.0, 0.464760583383924, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 563544.0623737776, 563544.0623737772, 137781.8043464956], 
processed observation next is [0.0, 0.8260869565217391, 0.5790123456790122, 0.5383333333333334, 1.0, 1.0, 0.36281021831419524, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.20126573656206342, 0.20126573656206326, 0.2649650083586454], 
reward next is 0.7350, 
noisyNet noise sample is [array([-0.47780418], dtype=float32), -0.7157096]. 
=============================================
[2019-04-27 18:32:40,124] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[70.54367]
 [70.53371]
 [70.52159]
 [70.51537]
 [70.50511]], R is [[70.57492065]
 [70.60307312]
 [70.62973022]
 [70.65512085]
 [70.67952728]].
[2019-04-27 18:32:43,807] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-27 18:32:43,809] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 18:32:43,810] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:32:43,811] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 18:32:43,811] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:32:43,813] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 18:32:43,813] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:32:43,815] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 18:32:43,816] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 18:32:43,818] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:32:43,820] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:32:43,839] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run17
[2019-04-27 18:32:43,860] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run17
[2019-04-27 18:32:43,860] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run17
[2019-04-27 18:32:43,861] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run17
[2019-04-27 18:32:43,920] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run17
[2019-04-27 18:32:51,600] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01662349], dtype=float32), 0.0710961]
[2019-04-27 18:32:51,600] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [18.5, 73.33333333333334, 1.0, 2.0, 0.2952560415093766, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 380869.3858838268, 380869.3858838268, 109992.6877427101]
[2019-04-27 18:32:51,601] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 18:32:51,603] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [2.1280255e-13 9.9999988e-01 4.0120522e-08 4.4017107e-10 6.5237778e-08], sampled 0.39493483722488953
[2019-04-27 18:33:08,952] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.01662349], dtype=float32), 0.0710961]
[2019-04-27 18:33:08,955] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [23.5, 73.0, 1.0, 2.0, 0.4268867787227445, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 523705.7989789845, 523705.7989789845, 132342.4494365684]
[2019-04-27 18:33:08,956] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 18:33:08,958] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [6.7861576e-12 9.9997211e-01 5.0196486e-06 7.3757953e-08 2.2712356e-05], sampled 0.5164986344684169
[2019-04-27 18:33:24,410] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.01662349], dtype=float32), 0.0710961]
[2019-04-27 18:33:24,412] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [28.15, 68.5, 1.0, 2.0, 0.6080968462792012, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 696858.7651794392, 696858.7651794387, 159665.9784981019]
[2019-04-27 18:33:24,412] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 18:33:24,417] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.0730198e-12 9.9999118e-01 1.8607362e-06 1.5073672e-08 6.8852924e-06], sampled 0.10809553564013141
[2019-04-27 18:33:41,374] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01662349], dtype=float32), 0.0710961]
[2019-04-27 18:33:41,376] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [26.39499494666667, 90.54586429666666, 1.0, 2.0, 0.4172231295726015, 1.0, 2.0, 0.4172231295726015, 1.0, 2.0, 0.6642328102882836, 6.9112, 6.9112, 121.94756008, 1427134.366769708, 1427134.366769708, 304708.9909548532]
[2019-04-27 18:33:41,379] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 18:33:41,382] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [2.9808511e-10 1.2922811e-03 9.3967132e-03 1.0578942e-04 9.8920524e-01], sampled 0.46121377039474243
[2019-04-27 18:34:28,727] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8810.8830 2191824485.9920 327.0000
[2019-04-27 18:34:28,993] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8607.8350 2267373197.9962 348.0000
[2019-04-27 18:34:29,105] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8885.7001 2148214176.4590 323.0000
[2019-04-27 18:34:29,322] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8701.3607 2226149883.7473 379.0000
[2019-04-27 18:34:29,436] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8119.4575 2473138070.8525 439.0000
[2019-04-27 18:34:30,451] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 400000, evaluation results [400000.0, 8119.457525219738, 2473138070.852467, 439.0, 8810.883001932514, 2191824485.992025, 327.0, 8885.700119407451, 2148214176.4589796, 323.0, 8607.834979754598, 2267373197.996173, 348.0, 8701.360673133428, 2226149883.7472997, 379.0]
[2019-04-27 18:34:30,497] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7847103e-08 2.4141942e-03 2.2035998e-01 9.5036993e-04 7.7627546e-01], sum to 1.0000
[2019-04-27 18:34:30,501] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9702
[2019-04-27 18:34:30,506] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.73333333333333, 65.16666666666667, 1.0, 2.0, 0.4262678032316474, 1.0, 1.0, 0.4262678032316474, 1.0, 2.0, 0.678969017043482, 6.9112, 6.9112, 121.94756008, 1468611.143050618, 1468611.143050618, 308844.1297514561], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7053000.0000, 
sim time next is 7053600.0000, 
raw observation next is [26.46666666666667, 66.33333333333334, 1.0, 2.0, 0.5016652781054859, 1.0, 2.0, 0.5016652781054859, 1.0, 2.0, 0.798697630593445, 6.9112, 6.9112, 121.94756008, 1717713.963614356, 1717713.963614356, 344418.1308280611], 
processed observation next is [1.0, 0.6521739130434783, 0.5358024691358025, 0.6633333333333334, 1.0, 1.0, 0.40674437869700697, 1.0, 1.0, 0.40674437869700697, 1.0, 1.0, 0.7483720382418063, 0.0, 0.0, 0.8096049824067558, 0.6134692727194129, 0.6134692727194129, 0.6623425592847328], 
reward next is 0.3377, 
noisyNet noise sample is [array([0.48201686], dtype=float32), 0.14934772]. 
=============================================
[2019-04-27 18:34:37,991] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.2734700e-24 1.0000000e+00 2.6046374e-17 1.9996422e-19 3.7314139e-16], sum to 1.0000
[2019-04-27 18:34:37,999] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5803
[2019-04-27 18:34:38,002] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 81.0, 1.0, 2.0, 0.6458488332650792, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000002, 6.9112, 121.9260426156618, 801339.5047764058, 801339.5047764048, 168652.4108974049], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7201800.0000, 
sim time next is 7202400.0000, 
raw observation next is [21.66666666666667, 80.0, 1.0, 2.0, 0.6576701869972394, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 815532.3827895352, 815532.3827895352, 170836.3193853628], 
processed observation next is [1.0, 0.34782608695652173, 0.3580246913580249, 0.8, 1.0, 1.0, 0.5924645083300469, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2912615652819769, 0.2912615652819769, 0.32853138343339], 
reward next is 0.6715, 
noisyNet noise sample is [array([0.15849616], dtype=float32), 0.61108506]. 
=============================================
[2019-04-27 18:34:40,857] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.6323334e-24 1.0000000e+00 2.2914580e-17 2.7836409e-19 1.8451353e-15], sum to 1.0000
[2019-04-27 18:34:40,861] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6250
[2019-04-27 18:34:40,866] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.85, 81.5, 1.0, 2.0, 0.3630350545829575, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 454108.4918008221, 454108.4918008221, 123619.2807870416], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7255800.0000, 
sim time next is 7256400.0000, 
raw observation next is [20.83333333333334, 81.66666666666667, 1.0, 2.0, 0.3625721061262817, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 453501.8307467157, 453501.8307467157, 123556.5313070724], 
processed observation next is [1.0, 1.0, 0.3271604938271607, 0.8166666666666668, 1.0, 1.0, 0.24115726919795444, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16196493955239846, 0.16196493955239846, 0.2376087140520623], 
reward next is 0.7624, 
noisyNet noise sample is [array([-1.2488637], dtype=float32), -1.232923]. 
=============================================
[2019-04-27 18:34:48,900] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.1887764e-26 1.0000000e+00 3.8125654e-23 1.2517706e-24 2.5759853e-22], sum to 1.0000
[2019-04-27 18:34:48,902] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4499
[2019-04-27 18:34:48,907] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.23333333333333, 90.33333333333334, 1.0, 2.0, 0.9145536042149477, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.029660323659672, 6.9112, 121.9254495918345, 1179383.893749366, 1118721.915221167, 224207.8587289632], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7400400.0000, 
sim time next is 7401000.0000, 
raw observation next is [21.21666666666667, 90.16666666666666, 1.0, 2.0, 0.920116615919139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.06974164789084, 6.9112, 121.9252064374165, 1207346.21372802, 1126159.27650634, 225519.6288389837], 
processed observation next is [1.0, 0.6521739130434783, 0.3413580246913581, 0.9016666666666666, 1.0, 1.0, 0.9049007332370702, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.015854164789084014, 0.0, 0.8094565774661513, 0.4311950763314357, 0.4021997416094071, 0.43369159392112255], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7189292], dtype=float32), 0.074963234]. 
=============================================
[2019-04-27 18:34:48,946] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[61.427773]
 [61.461773]
 [61.405098]
 [61.5314  ]
 [62.24602 ]], R is [[60.8204422 ]
 [60.21223831]
 [59.90354156]
 [59.30450821]
 [58.71146393]].
[2019-04-27 18:34:52,551] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.2111572e-34 1.0000000e+00 1.1405431e-29 8.9147596e-33 2.9603276e-31], sum to 1.0000
[2019-04-27 18:34:52,561] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3055
[2019-04-27 18:34:52,568] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 88.0, 1.0, 2.0, 0.417725575832624, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 511295.9417608262, 511295.9417608262, 130986.5742570018], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7464000.0000, 
sim time next is 7464600.0000, 
raw observation next is [21.75, 87.5, 1.0, 2.0, 0.4215341201290293, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 515270.3129725517, 515270.3129725517, 131515.7940527435], 
processed observation next is [0.0, 0.391304347826087, 0.3611111111111111, 0.875, 1.0, 1.0, 0.3113501430107492, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.18402511177591133, 0.18402511177591133, 0.2529149885629683], 
reward next is 0.7471, 
noisyNet noise sample is [array([-0.1991632], dtype=float32), 0.15430208]. 
=============================================
[2019-04-27 18:34:56,169] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.7283589e-30 1.0000000e+00 2.0329407e-24 1.0610182e-26 6.1786064e-26], sum to 1.0000
[2019-04-27 18:34:56,178] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5279
[2019-04-27 18:34:56,183] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.9, 96.0, 1.0, 2.0, 0.4383501295906114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 533984.7759384912, 533984.7759384912, 133913.8438600072], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7527600.0000, 
sim time next is 7528200.0000, 
raw observation next is [20.9, 96.0, 1.0, 2.0, 0.4383429611470255, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 533976.3554238365, 533976.355423836, 133912.7984093979], 
processed observation next is [0.0, 0.13043478260869565, 0.32962962962962955, 0.96, 1.0, 1.0, 0.33136066803217323, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.19070584122279874, 0.19070584122279857, 0.2575246123257652], 
reward next is 0.7425, 
noisyNet noise sample is [array([-0.02336675], dtype=float32), 0.43126187]. 
=============================================
[2019-04-27 18:35:00,053] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.7368947e-30 1.0000000e+00 3.6594064e-25 5.0446954e-27 4.4338700e-23], sum to 1.0000
[2019-04-27 18:35:00,056] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3990776e-27 1.0000000e+00 8.0921036e-23 2.8129552e-25 8.3775237e-24], sum to 1.0000
[2019-04-27 18:35:00,064] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9515
[2019-04-27 18:35:00,067] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1039
[2019-04-27 18:35:00,072] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.2, 88.0, 1.0, 2.0, 0.5048441690310732, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 601256.5090055052, 601256.5090055052, 143625.6206045605], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7599600.0000, 
sim time next is 7600200.0000, 
raw observation next is [23.08333333333333, 87.83333333333334, 1.0, 2.0, 0.5001194341422562, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 596784.4685564176, 596784.4685564172, 142924.2605586253], 
processed observation next is [0.0, 1.0, 0.41049382716049365, 0.8783333333333334, 1.0, 1.0, 0.40490408826459073, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.21313731019872056, 0.2131373101987204, 0.2748543472281256], 
reward next is 0.7251, 
noisyNet noise sample is [array([0.04710549], dtype=float32), -1.3843708]. 
=============================================
[2019-04-27 18:35:00,075] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.2, 86.5, 1.0, 2.0, 0.4539025915212001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 551506.3924314542, 551506.3924314542, 136180.6375675554], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7605000.0000, 
sim time next is 7605600.0000, 
raw observation next is [22.1, 86.33333333333333, 1.0, 2.0, 0.4494104977753856, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 547108.9795020476, 547108.9795020476, 135541.517905049], 
processed observation next is [1.0, 0.0, 0.3740740740740741, 0.8633333333333333, 1.0, 1.0, 0.3445363068754591, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.19539606410787416, 0.19539606410787416, 0.2606567652020173], 
reward next is 0.7393, 
noisyNet noise sample is [array([-0.12857082], dtype=float32), -0.7387814]. 
=============================================
[2019-04-27 18:35:02,535] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.2678180e-17 1.0000000e+00 1.8019399e-14 2.0047417e-14 1.9224691e-11], sum to 1.0000
[2019-04-27 18:35:02,542] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2609
[2019-04-27 18:35:02,548] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1317095.785281067 W.
[2019-04-27 18:35:02,554] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.33333333333334, 78.66666666666666, 1.0, 2.0, 0.3850809514788666, 1.0, 1.0, 0.3850809514788666, 1.0, 1.0, 0.6130614159653978, 6.911199999999999, 6.9112, 121.94756008, 1317095.785281067, 1317095.785281067, 290608.4544520814], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7642200.0000, 
sim time next is 7642800.0000, 
raw observation next is [25.6, 78.0, 1.0, 2.0, 0.3876336052246675, 1.0, 2.0, 0.3876336052246675, 1.0, 2.0, 0.6171253238628415, 6.911199999999999, 6.9112, 121.94756008, 1325834.204481744, 1325834.204481745, 291707.3514335278], 
processed observation next is [1.0, 0.4782608695652174, 0.5037037037037038, 0.78, 1.0, 1.0, 0.27099238717222324, 1.0, 1.0, 0.27099238717222324, 1.0, 1.0, 0.5214066548285518, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.47351221588633713, 0.47351221588633746, 0.5609756758337072], 
reward next is 0.4390, 
noisyNet noise sample is [array([0.21173981], dtype=float32), -1.4869043]. 
=============================================
[2019-04-27 18:35:12,658] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.9159422e-16 8.0545431e-11 3.3433322e-02 4.3240824e-08 9.6656668e-01], sum to 1.0000
[2019-04-27 18:35:12,665] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5604
[2019-04-27 18:35:12,669] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.1, 35.33333333333334, 1.0, 2.0, 0.3243109210048103, 1.0, 2.0, 0.3243109210048103, 1.0, 2.0, 0.5263234566003399, 6.9112, 6.9112, 121.94756008, 1177310.079687014, 1177310.079687014, 264683.9515877891], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7823400.0000, 
sim time next is 7824000.0000, 
raw observation next is [30.2, 35.66666666666667, 1.0, 2.0, 0.3906620210003111, 1.0, 2.0, 0.3906620210003111, 1.0, 2.0, 0.6311756843698978, 6.911199999999999, 6.9112, 121.94756008, 1408386.90923651, 1408386.90923651, 292513.6919497135], 
processed observation next is [1.0, 0.5652173913043478, 0.674074074074074, 0.3566666666666667, 1.0, 1.0, 0.2745976440479894, 1.0, 1.0, 0.2745976440479894, 1.0, 1.0, 0.5389696054623722, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.502995324727325, 0.502995324727325, 0.562526330672526], 
reward next is 0.4375, 
noisyNet noise sample is [array([0.7014517], dtype=float32), -1.6917657]. 
=============================================
[2019-04-27 18:35:12,682] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[64.61151]
 [64.86185]
 [64.97981]
 [64.47633]
 [64.0067 ]], R is [[63.35548401]
 [63.21292496]
 [63.11106491]
 [63.02433395]
 [62.94142914]].
[2019-04-27 18:35:16,786] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.1689052e-12 2.8473492e-05 1.0346871e-01 4.6684854e-06 8.9649808e-01], sum to 1.0000
[2019-04-27 18:35:16,797] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3265
[2019-04-27 18:35:16,802] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.36666666666667, 51.5, 1.0, 2.0, 0.3993865700498697, 1.0, 2.0, 0.3993865700498697, 1.0, 2.0, 0.6376555638142604, 6.911199999999999, 6.9112, 121.94756008, 1396512.178870622, 1396512.178870622, 296873.7316044926], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7906200.0000, 
sim time next is 7906800.0000, 
raw observation next is [28.53333333333333, 51.0, 1.0, 2.0, 0.3520070350396191, 1.0, 2.0, 0.3520070350396191, 1.0, 2.0, 0.5621362939536211, 6.911199999999999, 6.9112, 121.94756008, 1231917.692705505, 1231917.692705505, 276715.5123146534], 
processed observation next is [1.0, 0.5217391304347826, 0.6123456790123456, 0.51, 1.0, 1.0, 0.22857980361859417, 1.0, 1.0, 0.22857980361859417, 1.0, 1.0, 0.4526703674420263, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.4399706045376804, 0.4399706045376804, 0.532145215989718], 
reward next is 0.4679, 
noisyNet noise sample is [array([0.08586869], dtype=float32), -0.13921034]. 
=============================================
[2019-04-27 18:35:18,716] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.2023654e-22 1.0000000e+00 2.1954804e-13 1.0852385e-20 1.2433095e-16], sum to 1.0000
[2019-04-27 18:35:18,723] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2880
[2019-04-27 18:35:18,729] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.23333333333333, 56.33333333333333, 1.0, 2.0, 0.4998454146936073, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 596180.8265209994, 596180.826520999, 142871.1264696526], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7929600.0000, 
sim time next is 7930200.0000, 
raw observation next is [28.01666666666667, 57.16666666666667, 1.0, 2.0, 0.4980179346384502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 594465.5973739398, 594465.5973739398, 142601.4340498274], 
processed observation next is [1.0, 0.782608695652174, 0.59320987654321, 0.5716666666666668, 1.0, 1.0, 0.40240230314101216, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21230914191926423, 0.21230914191926423, 0.27423352701889886], 
reward next is 0.7258, 
noisyNet noise sample is [array([1.0662003], dtype=float32), 0.68259484]. 
=============================================
[2019-04-27 18:35:20,051] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:35:20,052] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:35:20,053] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res10/Eplus-env-sub_run3
[2019-04-27 18:35:20,129] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:35:20,129] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:35:20,131] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res12/Eplus-env-sub_run3
[2019-04-27 18:35:20,213] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:35:20,213] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:35:20,215] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res11/Eplus-env-sub_run3
[2019-04-27 18:35:20,289] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:35:20,289] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:35:20,291] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res13/Eplus-env-sub_run3
[2019-04-27 18:35:20,329] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:35:20,330] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:35:20,331] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:35:20,331] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:35:20,332] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res14/Eplus-env-sub_run3
[2019-04-27 18:35:20,349] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res16/Eplus-env-sub_run3
[2019-04-27 18:35:20,432] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:35:20,432] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:35:20,434] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res2/Eplus-env-sub_run3
[2019-04-27 18:35:20,455] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:35:20,456] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:35:20,457] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res9/Eplus-env-sub_run3
[2019-04-27 18:35:20,478] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:35:20,479] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:35:20,481] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res15/Eplus-env-sub_run3
[2019-04-27 18:35:20,515] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:35:20,516] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:35:20,516] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:35:20,517] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res8/Eplus-env-sub_run3
[2019-04-27 18:35:20,503] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:35:20,542] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:35:20,516] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:35:20,546] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res3/Eplus-env-sub_run3
[2019-04-27 18:35:20,544] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:35:20,566] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:35:20,568] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res5/Eplus-env-sub_run3
[2019-04-27 18:35:20,544] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:35:20,593] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:35:20,595] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res7/Eplus-env-sub_run3
[2019-04-27 18:35:20,542] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:35:20,620] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:35:20,622] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res4/Eplus-env-sub_run3
[2019-04-27 18:35:20,544] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res6/Eplus-env-sub_run3
[2019-04-27 18:35:20,544] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:35:20,715] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:35:20,717] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res17/Eplus-env-sub_run3
[2019-04-27 18:35:24,274] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-04-27 18:35:24,280] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 18:35:24,281] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:35:24,282] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 18:35:24,283] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 18:35:24,284] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:35:24,285] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:35:24,286] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 18:35:24,287] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 18:35:24,287] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:35:24,289] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:35:24,299] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run18
[2019-04-27 18:35:24,300] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run18
[2019-04-27 18:35:24,346] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run18
[2019-04-27 18:35:24,364] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run18
[2019-04-27 18:35:24,383] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run18
[2019-04-27 18:35:31,126] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.03714697], dtype=float32), 0.07082295]
[2019-04-27 18:35:31,207] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [26.8, 32.0, 1.0, 2.0, 0.3060195778631902, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 393517.2915219305, 393517.2915219305, 116317.9014669643]
[2019-04-27 18:35:31,208] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 18:35:31,210] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.53036023e-20 9.99999046e-01 9.11366783e-07 1.35257212e-16
 1.06253975e-08], sampled 0.15278614045757866
[2019-04-27 18:35:32,557] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03714697], dtype=float32), 0.07082295]
[2019-04-27 18:35:32,558] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [20.136661625, 71.85509363, 1.0, 2.0, 0.2663715665409062, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 341098.022761621, 341098.022761621, 111522.7832614949]
[2019-04-27 18:35:32,560] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 18:35:32,564] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [2.0850553e-21 1.0000000e+00 1.4420555e-08 4.9472581e-18 5.8479797e-11], sampled 0.6054020422817098
[2019-04-27 18:36:03,637] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03714697], dtype=float32), 0.07082295]
[2019-04-27 18:36:03,638] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [32.0187968, 66.40195871333334, 1.0, 2.0, 0.7649280830703195, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 871843.7127353814, 871843.7127353814, 188810.0712305332]
[2019-04-27 18:36:03,640] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 18:36:03,642] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.6793233e-20 9.9999905e-01 1.0001277e-06 1.2534899e-16 1.4591361e-08], sampled 0.9706216170393833
[2019-04-27 18:36:09,085] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03714697], dtype=float32), 0.07082295]
[2019-04-27 18:36:09,086] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [29.17975678333333, 40.80742875666667, 1.0, 2.0, 0.5007928192515353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 618339.7067564857, 618339.7067564857, 143671.0359098728]
[2019-04-27 18:36:09,090] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 18:36:09,095] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [3.0256141e-20 9.9999988e-01 1.6893706e-07 6.4531079e-17 1.5749607e-09], sampled 0.3674639902821446
[2019-04-27 18:36:56,179] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03714697], dtype=float32), 0.07082295]
[2019-04-27 18:36:56,181] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [22.0, 94.0, 1.0, 2.0, 0.4827562561885135, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 579523.2482430977, 579523.2482430972, 140345.799911815]
[2019-04-27 18:36:56,182] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 18:36:56,184] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.3978121e-21 1.0000000e+00 9.2414147e-09 1.8784335e-18 4.3317148e-11], sampled 0.9754081644888378
[2019-04-27 18:36:56,427] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03714697], dtype=float32), 0.07082295]
[2019-04-27 18:36:56,429] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [22.92767423666667, 68.45973882666667, 1.0, 2.0, 0.369962178406148, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 461142.8585855247, 461142.8585855243, 124525.1230477572]
[2019-04-27 18:36:56,430] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 18:36:56,434] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [9.1549850e-22 1.0000000e+00 9.3216046e-10 4.8339060e-19 1.9034115e-12], sampled 0.7745549282825066
[2019-04-27 18:37:09,431] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8823.2056 2179438782.7044 346.0000
[2019-04-27 18:37:09,605] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8626.8572 2258113860.4818 418.0000
[2019-04-27 18:37:09,676] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8713.7944 2210899774.0387 456.0000
[2019-04-27 18:37:09,744] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8135.6097 2458394949.4270 558.0000
[2019-04-27 18:37:09,763] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8921.0454 2133420356.9881 355.0000
[2019-04-27 18:37:10,779] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 425000, evaluation results [425000.0, 8135.609693024544, 2458394949.4270353, 558.0, 8823.205639816953, 2179438782.704379, 346.0, 8921.045449236604, 2133420356.9880502, 355.0, 8626.85716022441, 2258113860.481786, 418.0, 8713.794438540708, 2210899774.03865, 456.0]
[2019-04-27 18:37:11,527] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.7469935e-20 9.9925572e-01 7.4409647e-04 2.2454252e-14 1.2763527e-07], sum to 1.0000
[2019-04-27 18:37:11,533] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1502
[2019-04-27 18:37:11,540] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1325870.30650098 W.
[2019-04-27 18:37:11,544] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.91666666666667, 37.83333333333334, 1.0, 2.0, 0.5477584174481828, 1.0, 1.0, 0.5477584174481828, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1325870.30650098, 1325870.30650098, 253764.790032779], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 57000.0000, 
sim time next is 57600.0000, 
raw observation next is [29.9, 38.0, 1.0, 2.0, 0.5501913359984116, 1.0, 2.0, 0.5501913359984116, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1330237.921418663, 1330237.921418663, 254523.8266838838], 
processed observation next is [1.0, 0.6956521739130435, 0.6629629629629629, 0.38, 1.0, 1.0, 0.46451349523620433, 1.0, 1.0, 0.46451349523620433, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.4750849719352368, 0.4750849719352368, 0.4894688974690073], 
reward next is 0.5105, 
noisyNet noise sample is [array([1.067079], dtype=float32), -0.83869886]. 
=============================================
[2019-04-27 18:37:12,562] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.3306817e-24 1.0000000e+00 1.3793330e-11 1.1947368e-21 1.4278567e-15], sum to 1.0000
[2019-04-27 18:37:12,569] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5114
[2019-04-27 18:37:12,574] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 50.5, 1.0, 2.0, 0.4142831453769842, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 509659.4113895294, 509659.4113895294, 130561.5677192735], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 73800.0000, 
sim time next is 74400.0000, 
raw observation next is [27.13333333333333, 51.33333333333333, 1.0, 2.0, 0.4137160966173486, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 508903.1912247508, 508903.1912247508, 130478.881861096], 
processed observation next is [1.0, 0.8695652173913043, 0.5604938271604937, 0.5133333333333333, 1.0, 1.0, 0.3020429721635103, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1817511397231253, 0.1817511397231253, 0.25092092665595384], 
reward next is 0.7491, 
noisyNet noise sample is [array([-0.08017473], dtype=float32), -0.3020205]. 
=============================================
[2019-04-27 18:37:12,895] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.2143785e-28 1.0000000e+00 2.9889111e-16 4.8740508e-26 7.1141516e-22], sum to 1.0000
[2019-04-27 18:37:12,903] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4092
[2019-04-27 18:37:12,906] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.2, 46.0, 1.0, 2.0, 0.41059214342089, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 505708.1031354629, 505708.1031354629, 130049.187659106], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 69600.0000, 
sim time next is 70200.0000, 
raw observation next is [28.1, 46.5, 1.0, 2.0, 0.4121869344389645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 507578.6785040268, 507578.6785040268, 130274.4503673422], 
processed observation next is [1.0, 0.8260869565217391, 0.5962962962962963, 0.465, 1.0, 1.0, 0.3002225409987672, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.18127809946572385, 0.18127809946572385, 0.25052778916796575], 
reward next is 0.7495, 
noisyNet noise sample is [array([-0.2595284], dtype=float32), -0.14491978]. 
=============================================
[2019-04-27 18:37:20,254] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.5727330e-27 1.0000000e+00 3.7284117e-16 8.3098164e-26 1.3072468e-19], sum to 1.0000
[2019-04-27 18:37:20,262] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1456
[2019-04-27 18:37:20,267] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.5, 26.0, 1.0, 2.0, 0.3553471485417526, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 451361.3470914409, 451361.3470914409, 122676.961331655], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 214800.0000, 
sim time next is 215400.0000, 
raw observation next is [30.7, 25.0, 1.0, 2.0, 0.35680598770907, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 453685.1419650791, 453685.1419650791, 122874.8815316364], 
processed observation next is [0.0, 0.4782608695652174, 0.6925925925925925, 0.25, 1.0, 1.0, 0.23429284251079766, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16203040784467113, 0.16203040784467113, 0.23629784909930077], 
reward next is 0.7637, 
noisyNet noise sample is [array([0.568125], dtype=float32), -0.21012925]. 
=============================================
[2019-04-27 18:37:20,749] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.8736402e-30 1.0000000e+00 2.1114916e-15 7.5169106e-29 7.8955915e-20], sum to 1.0000
[2019-04-27 18:37:20,758] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2564
[2019-04-27 18:37:20,762] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.5, 16.33333333333334, 1.0, 2.0, 0.3695238298456479, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 474660.6093586666, 474660.6093586666, 124595.422849475], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 220800.0000, 
sim time next is 221400.0000, 
raw observation next is [32.7, 15.5, 1.0, 2.0, 0.3706103712346164, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 476516.1196079421, 476516.1196079421, 124740.9945402077], 
processed observation next is [0.0, 0.5652173913043478, 0.7666666666666667, 0.155, 1.0, 1.0, 0.25072663242216237, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1701843284314079, 0.1701843284314079, 0.23988652796193788], 
reward next is 0.7601, 
noisyNet noise sample is [array([1.060826], dtype=float32), 0.6961221]. 
=============================================
[2019-04-27 18:37:22,744] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0448407e-24 1.0000000e+00 2.3457047e-13 1.0117300e-22 1.5600537e-17], sum to 1.0000
[2019-04-27 18:37:22,751] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3415
[2019-04-27 18:37:22,757] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.96666666666667, 47.66666666666666, 1.0, 2.0, 0.274022381838598, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 353472.4371227418, 353472.4371227418, 102092.5025243843], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 262200.0000, 
sim time next is 262800.0000, 
raw observation next is [21.8, 48.0, 1.0, 2.0, 0.2735384884498241, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 352848.1000845965, 352848.1000845965, 101244.9891323704], 
processed observation next is [0.0, 0.043478260869565216, 0.362962962962963, 0.48, 1.0, 1.0, 0.13516486720217155, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1260171786016416, 0.1260171786016416, 0.1947019021776354], 
reward next is 0.8053, 
noisyNet noise sample is [array([0.7144044], dtype=float32), 0.18416819]. 
=============================================
[2019-04-27 18:37:25,373] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.5381346e-28 1.0000000e+00 1.1670693e-14 8.0400297e-26 1.5463064e-19], sum to 1.0000
[2019-04-27 18:37:25,382] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5178
[2019-04-27 18:37:25,387] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.8, 31.0, 1.0, 2.0, 0.3311048314185583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 424091.9421314269, 424091.9421314269, 119506.6492231625], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 313200.0000, 
sim time next is 313800.0000, 
raw observation next is [27.83333333333334, 31.0, 1.0, 2.0, 0.3322229722996384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 425478.4019685152, 425478.4019685152, 119651.2238747008], 
processed observation next is [0.0, 0.6521739130434783, 0.58641975308642, 0.31, 1.0, 1.0, 0.20502734797575997, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.15195657213161257, 0.15195657213161257, 0.2300985074513477], 
reward next is 0.7699, 
noisyNet noise sample is [array([0.5164325], dtype=float32), 1.9452912]. 
=============================================
[2019-04-27 18:37:25,703] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.5778488e-29 1.0000000e+00 5.8621507e-15 3.1587687e-27 3.1866028e-21], sum to 1.0000
[2019-04-27 18:37:25,710] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3082
[2019-04-27 18:37:25,713] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.86666666666667, 31.0, 1.0, 2.0, 0.3317700349346681, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 424802.6205242433, 424802.6205242433, 119592.7471696195], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 314400.0000, 
sim time next is 315000.0000, 
raw observation next is [27.9, 31.0, 1.0, 2.0, 0.3320646657634081, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 425081.2305176589, 425081.2305176589, 119630.8873721238], 
processed observation next is [0.0, 0.6521739130434783, 0.5888888888888888, 0.31, 1.0, 1.0, 0.20483888781358106, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.15181472518487818, 0.15181472518487818, 0.23005939879254578], 
reward next is 0.7699, 
noisyNet noise sample is [array([-0.4941155], dtype=float32), 1.4606518]. 
=============================================
[2019-04-27 18:37:25,724] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[83.03136 ]
 [83.002945]
 [82.97953 ]
 [82.93531 ]
 [82.88374 ]], R is [[83.00627899]
 [82.94622803]
 [82.88667297]
 [82.82798767]
 [82.76991272]].
[2019-04-27 18:37:30,167] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.0142382e-15 1.0000000e+00 4.3006829e-09 7.1889021e-14 7.8270246e-12], sum to 1.0000
[2019-04-27 18:37:30,173] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5201
[2019-04-27 18:37:30,180] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.11666666666667, 25.66666666666667, 1.0, 2.0, 0.6597899151634946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 840014.1595006975, 840014.1595006975, 171571.2809262991], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 393000.0000, 
sim time next is 393600.0000, 
raw observation next is [30.23333333333333, 25.33333333333334, 1.0, 2.0, 0.8346452348527232, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1062498.224518893, 1062498.224518893, 207124.8592980781], 
processed observation next is [1.0, 0.5652173913043478, 0.6753086419753086, 0.2533333333333334, 1.0, 1.0, 0.8031490891103849, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3794636516138904, 0.3794636516138904, 0.3983170371116887], 
reward next is 0.6017, 
noisyNet noise sample is [array([2.0452173], dtype=float32), -0.25161684]. 
=============================================
[2019-04-27 18:37:36,205] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1945531e-14 9.9985802e-01 1.3908221e-04 1.9142905e-10 2.8345669e-06], sum to 1.0000
[2019-04-27 18:37:36,215] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5409
[2019-04-27 18:37:36,222] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 53.0, 1.0, 2.0, 0.3136569475441648, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 399175.0892156783, 399175.0892156783, 117275.0900786458], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 514800.0000, 
sim time next is 515400.0000, 
raw observation next is [23.43333333333333, 54.0, 1.0, 2.0, 0.3130455575148792, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 398469.8502616474, 398469.8502616469, 117198.5845507922], 
processed observation next is [1.0, 1.0, 0.42345679012345666, 0.54, 1.0, 1.0, 0.18219709227961814, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.14231066080773122, 0.14231066080773103, 0.2253818933669081], 
reward next is 0.7746, 
noisyNet noise sample is [array([-1.3314028], dtype=float32), 0.38900372]. 
=============================================
[2019-04-27 18:37:37,805] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.9013223e-14 9.9938560e-01 6.1396643e-04 4.3488846e-10 5.1059146e-07], sum to 1.0000
[2019-04-27 18:37:37,816] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4152
[2019-04-27 18:37:37,823] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.2, 73.0, 1.0, 2.0, 0.332281431349273, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 423802.9791165702, 423802.9791165697, 119656.5252924639], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 529200.0000, 
sim time next is 529800.0000, 
raw observation next is [20.11666666666667, 73.5, 1.0, 2.0, 0.3703968605895649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 472470.4378310322, 472470.4378310322, 124715.391196053], 
processed observation next is [1.0, 0.13043478260869565, 0.30061728395061743, 0.735, 1.0, 1.0, 0.25047245308281535, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16873944208251151, 0.16873944208251151, 0.23983729076164037], 
reward next is 0.7602, 
noisyNet noise sample is [array([-0.0249597], dtype=float32), -0.059201222]. 
=============================================
[2019-04-27 18:37:39,984] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.0957451e-17 9.8936754e-01 1.0626072e-02 3.0645676e-08 6.3581974e-06], sum to 1.0000
[2019-04-27 18:37:39,994] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9566
[2019-04-27 18:37:40,000] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1448959.865653774 W.
[2019-04-27 18:37:40,003] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.6, 35.0, 1.0, 2.0, 0.6049188238896414, 0.0, 1.0, 0.0, 1.0, 1.0, 0.95738443669313, 6.911199999999999, 6.9112, 121.9260426156618, 1448959.865653774, 1448959.865653774, 289577.5884426187], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 568800.0000, 
sim time next is 569400.0000, 
raw observation next is [30.7, 34.66666666666667, 1.0, 2.0, 0.5476887296438412, 1.0, 1.0, 0.5476887296438412, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1325509.599167394, 1325509.599167394, 253735.2616958346], 
processed observation next is [1.0, 0.6086956521739131, 0.6925925925925925, 0.34666666666666673, 1.0, 1.0, 0.46153420195695377, 1.0, 0.5, 0.46153420195695377, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.4733962854169264, 0.4733962854169264, 0.48795242633814345], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.22599676], dtype=float32), -0.307722]. 
=============================================
[2019-04-27 18:37:41,436] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.5967938e-24 1.0000000e+00 3.8099157e-10 2.6423724e-20 9.3395377e-15], sum to 1.0000
[2019-04-27 18:37:41,444] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4047
[2019-04-27 18:37:41,452] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.56666666666666, 34.5, 1.0, 2.0, 0.3885006365029999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 481574.8600070205, 481574.8600070205, 127015.5183068801], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 586200.0000, 
sim time next is 586800.0000, 
raw observation next is [30.4, 35.0, 1.0, 2.0, 0.387890736145533, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 480929.8525535394, 480929.852553539, 126933.3297109508], 
processed observation next is [1.0, 0.8260869565217391, 0.6814814814814815, 0.35, 1.0, 1.0, 0.2712984954113488, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.17176066162626408, 0.1717606616262639, 0.24410255713644385], 
reward next is 0.7559, 
noisyNet noise sample is [array([0.06151587], dtype=float32), -1.0291122]. 
=============================================
[2019-04-27 18:37:42,949] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.4590476e-17 4.3686165e-04 8.4818310e-01 6.0493563e-12 1.5138000e-01], sum to 1.0000
[2019-04-27 18:37:42,958] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1838
[2019-04-27 18:37:42,968] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [20.48333333333333, 70.0, 1.0, 2.0, 0.16, 1.0, 1.0, 0.16, 1.0, 2.0, 0.2, 6.911200000000001, 6.9112, 121.94756008, 387806.9343559187, 387806.9343559183, 174321.6422966536], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 622200.0000, 
sim time next is 622800.0000, 
raw observation next is [20.3, 71.0, 1.0, 2.0, 0.16, 0.0, 1.0, 0.0, 1.0, 2.0, 0.2616767533022977, 6.911199999999999, 6.9112, 121.9260426156618, 384012.0817682488, 384012.0817682493, 153839.3073937164], 
processed observation next is [1.0, 0.21739130434782608, 0.3074074074074074, 0.71, 1.0, 1.0, 0.0, 0.0, 0.5, -0.1904761904761905, 1.0, 1.0, 0.07709594162787214, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.13714717206008886, 0.13714717206008903, 0.29584482191099304], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.7555792], dtype=float32), -0.2868623]. 
=============================================
[2019-04-27 18:37:43,879] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.5989217e-14 9.9995220e-01 4.7754678e-05 3.3964932e-12 3.5866254e-08], sum to 1.0000
[2019-04-27 18:37:43,888] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4454
[2019-04-27 18:37:43,893] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.8, 62.0, 1.0, 2.0, 0.3872971682873566, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 489518.838767883, 489518.838767883, 127008.5428543904], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 628200.0000, 
sim time next is 628800.0000, 
raw observation next is [23.06666666666667, 61.0, 1.0, 2.0, 0.3787067847472869, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 478169.4941766353, 478169.4941766353, 125815.1249254338], 
processed observation next is [1.0, 0.2608695652173913, 0.40987654320987665, 0.61, 1.0, 1.0, 0.2603652199372463, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.17077481934879832, 0.17077481934879832, 0.24195216331814193], 
reward next is 0.7580, 
noisyNet noise sample is [array([1.3767012], dtype=float32), 1.3727273]. 
=============================================
[2019-04-27 18:37:44,828] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.2224514e-13 8.5702131e-04 9.9879742e-01 5.8780460e-09 3.4548552e-04], sum to 1.0000
[2019-04-27 18:37:44,836] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8010
[2019-04-27 18:37:44,840] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [34.2, 19.0, 1.0, 2.0, 0.1971831004486612, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3324850769159543, 6.9112, 6.9112, 121.9260426156618, 494351.8938465742, 494351.8938465742, 165923.7238961229], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 669600.0000, 
sim time next is 670200.0000, 
raw observation next is [33.98333333333333, 19.5, 1.0, 2.0, 0.1971499119163261, 0.0, 2.0, 0.0, 1.0, 2.0, 0.33251512380432, 6.911199999999999, 6.9112, 121.9260426156618, 494349.4806851472, 494349.4806851476, 165902.0010431643], 
processed observation next is [1.0, 0.782608695652174, 0.8141975308641973, 0.195, 1.0, 1.0, 0.044226085614673935, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.1656439047554, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.17655338595898115, 0.17655338595898126, 0.31904230969839287], 
reward next is 0.6810, 
noisyNet noise sample is [array([0.46612304], dtype=float32), 0.3724001]. 
=============================================
[2019-04-27 18:37:46,831] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.5405709e-16 5.3183001e-05 9.9991548e-01 1.0293375e-10 3.1304564e-05], sum to 1.0000
[2019-04-27 18:37:46,844] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0851
[2019-04-27 18:37:46,852] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.1, 38.5, 1.0, 2.0, 0.16, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2714809282591634, 6.9112, 6.9112, 121.9260426156618, 397705.7050359992, 397705.7050359992, 155217.456302366], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 696600.0000, 
sim time next is 697200.0000, 
raw observation next is [25.96666666666667, 38.66666666666667, 1.0, 2.0, 0.16, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2705242976893635, 6.9112, 6.9112, 121.9260426156618, 395881.6184483705, 395881.6184483705, 154885.3470288338], 
processed observation next is [1.0, 0.043478260869565216, 0.517283950617284, 0.3866666666666667, 1.0, 1.0, 0.0, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.08815537211170434, 0.0, 0.0, 0.8094621288201359, 0.14138629230298946, 0.14138629230298946, 0.2978564365939112], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.4349232], dtype=float32), 2.2955694]. 
=============================================
[2019-04-27 18:37:58,078] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.2796977e-25 1.0000000e+00 4.2123489e-08 1.5710339e-20 2.5143031e-14], sum to 1.0000
[2019-04-27 18:37:58,086] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0852
[2019-04-27 18:37:58,092] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.46666666666667, 66.0, 1.0, 2.0, 0.3160810843611737, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 402316.3432069489, 402316.3432069489, 117581.4665578347], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 886200.0000, 
sim time next is 886800.0000, 
raw observation next is [21.53333333333333, 66.0, 1.0, 2.0, 0.3179213676900728, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 404396.7162592207, 404396.7162592203, 117812.9587186895], 
processed observation next is [0.0, 0.2608695652173913, 0.35308641975308636, 0.66, 1.0, 1.0, 0.1880016282024676, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1444273986640074, 0.14442739866400725, 0.22656338215132596], 
reward next is 0.7734, 
noisyNet noise sample is [array([-0.71493775], dtype=float32), -1.3698847]. 
=============================================
[2019-04-27 18:38:02,788] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-27 18:38:02,790] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 18:38:02,792] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 18:38:02,792] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:38:02,793] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:38:02,796] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 18:38:02,797] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:38:02,797] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 18:38:02,799] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 18:38:02,799] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:38:02,800] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:38:02,816] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run19
[2019-04-27 18:38:02,817] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run19
[2019-04-27 18:38:02,854] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run19
[2019-04-27 18:38:02,855] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run19
[2019-04-27 18:38:02,873] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run19
[2019-04-27 18:38:07,767] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04005997], dtype=float32), 0.06897372]
[2019-04-27 18:38:07,768] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [25.05580972333333, 48.26527227333334, 1.0, 2.0, 0.2958127829653251, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 375735.8146435458, 375735.8146435458, 115051.7206300854]
[2019-04-27 18:38:07,771] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:38:07,774] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [3.8638988e-23 1.0000000e+00 3.9298604e-09 1.2330151e-20 1.9464440e-15], sampled 0.8476694673907129
[2019-04-27 18:38:16,436] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04005997], dtype=float32), 0.06897372]
[2019-04-27 18:38:16,440] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [24.0, 29.0, 1.0, 2.0, 0.2725278876675706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 351544.1874447761, 351544.1874447761, 92318.53534629727]
[2019-04-27 18:38:16,443] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 18:38:16,448] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.2143470e-22 1.0000000e+00 3.3056027e-09 2.7564091e-20 2.3986465e-15], sampled 0.7504878931852851
[2019-04-27 18:38:21,514] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04005997], dtype=float32), 0.06897372]
[2019-04-27 18:38:21,516] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [26.21666666666667, 55.83333333333334, 1.0, 2.0, 0.5189314675579331, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8429642254554319, 6.911199999999999, 6.9112, 121.9260426156618, 1257635.845168334, 1257635.845168335, 261323.0905495055]
[2019-04-27 18:38:21,517] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 18:38:21,520] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.4650202e-17 2.9169729e-05 9.9972910e-01 3.7638156e-12 2.4173988e-04], sampled 0.6208742372433258
[2019-04-27 18:38:37,942] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04005997], dtype=float32), 0.06897372]
[2019-04-27 18:38:37,945] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [26.60467625166667, 53.79618875000001, 1.0, 2.0, 0.6768545807053331, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 831311.5489760203, 831311.5489760198, 174243.0491798304]
[2019-04-27 18:38:37,948] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:38:37,951] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [8.1397270e-21 9.9999988e-01 1.7084956e-07 2.3803121e-18 4.2814561e-13], sampled 0.6733338803267398
[2019-04-27 18:39:49,275] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8962.2749 2128635082.6564 343.0000
[2019-04-27 18:39:49,337] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8192.6365 2452963365.5052 606.0000
[2019-04-27 18:39:49,352] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8669.6464 2257294946.2410 404.0000
[2019-04-27 18:39:49,433] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8859.7468 2180347966.3068 327.0000
[2019-04-27 18:39:49,533] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8762.0109 2205998356.6094 451.0000
[2019-04-27 18:39:50,551] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 450000, evaluation results [450000.0, 8192.636524492167, 2452963365.505188, 606.0, 8859.746751638715, 2180347966.3067694, 327.0, 8962.274905154538, 2128635082.656403, 343.0, 8669.646414120341, 2257294946.240981, 404.0, 8762.010866982037, 2205998356.6094213, 451.0]
[2019-04-27 18:39:52,593] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.3476052e-13 8.3635088e-05 9.9986291e-01 3.2236834e-08 5.3441327e-05], sum to 1.0000
[2019-04-27 18:39:52,598] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0756
[2019-04-27 18:39:52,607] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.68333333333333, 51.5, 1.0, 2.0, 0.165248654284129, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2798411417031032, 6.911199999999999, 6.9112, 121.9260426156618, 415363.803548023, 415363.8035480235, 158754.0409092094], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1014600.0000, 
sim time next is 1015200.0000, 
raw observation next is [24.2, 53.0, 1.0, 2.0, 0.1647881674141115, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2800035432113469, 6.911199999999999, 6.9112, 121.9260426156618, 415015.0963998527, 415015.0963998532, 158480.7651773003], 
processed observation next is [1.0, 0.782608695652174, 0.45185185185185184, 0.53, 1.0, 1.0, 0.005700199302513697, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.10000442901418362, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.14821967728566168, 0.14821967728566185, 0.304770702264039], 
reward next is 0.6952, 
noisyNet noise sample is [array([-0.9615723], dtype=float32), 0.43480894]. 
=============================================
[2019-04-27 18:39:53,690] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3436412e-16 9.9998736e-01 1.2577632e-05 4.5014566e-15 2.9168620e-10], sum to 1.0000
[2019-04-27 18:39:53,701] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8725
[2019-04-27 18:39:53,706] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.1, 58.0, 1.0, 2.0, 0.285991218541871, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 366142.5894785766, 366142.5894785766, 113863.3991833415], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1039200.0000, 
sim time next is 1039800.0000, 
raw observation next is [22.0, 59.0, 1.0, 2.0, 0.2877707785766819, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 368210.2537563682, 368210.2537563682, 114079.2133687257], 
processed observation next is [1.0, 0.0, 0.37037037037037035, 0.59, 1.0, 1.0, 0.1521080697341451, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1315036620558458, 0.1315036620558458, 0.21938310263216482], 
reward next is 0.7806, 
noisyNet noise sample is [array([-1.4021997], dtype=float32), -0.81555915]. 
=============================================
[2019-04-27 18:39:55,822] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.9620204e-17 9.7878176e-01 2.1218263e-02 2.1687264e-13 9.7665509e-10], sum to 1.0000
[2019-04-27 18:39:55,829] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1796
[2019-04-27 18:39:55,835] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.9, 48.33333333333334, 1.0, 2.0, 0.646213563559519, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 818695.0292837087, 818695.0292837087, 169006.8246384014], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1078800.0000, 
sim time next is 1079400.0000, 
raw observation next is [25.05, 47.66666666666667, 1.0, 2.0, 0.6445680837379273, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 816570.7232212108, 816570.7232212108, 168701.8516671158], 
processed observation next is [1.0, 0.4782608695652174, 0.48333333333333334, 0.47666666666666674, 1.0, 1.0, 0.5768667663546754, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.29163240115043243, 0.29163240115043243, 0.32442663782137654], 
reward next is 0.6756, 
noisyNet noise sample is [array([0.719026], dtype=float32), 0.39892733]. 
=============================================
[2019-04-27 18:39:56,406] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.03056181e-14 9.86074209e-01 1.39222555e-02 4.85458695e-09
 3.55400402e-06], sum to 1.0000
[2019-04-27 18:39:56,414] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2908
[2019-04-27 18:39:56,417] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.63333333333333, 42.66666666666667, 1.0, 2.0, 0.498090047558701, 1.0, 1.0, 0.498090047558701, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9257742987508, 1236359.402487854, 1236359.402487854, 238547.1863413925], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1092000.0000, 
sim time next is 1092600.0000, 
raw observation next is [26.6, 43.0, 1.0, 2.0, 0.9081281650482814, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.13983289477225, 6.9112, 121.9250361099154, 1256243.855881588, 1139164.345287299, 223466.3489495991], 
processed observation next is [1.0, 0.6521739130434783, 0.5407407407407407, 0.43, 1.0, 1.0, 0.8906287679146208, 0.0, 0.5, -0.1904761904761905, 0.0, 1.0, -0.25, 0.022863289477225025, 0.0, 0.809455446668672, 0.44865851995771, 0.40684440903117824, 0.42974297874922907], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.188212], dtype=float32), -1.3075454]. 
=============================================
[2019-04-27 18:40:06,624] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.7313636e-32 1.0000000e+00 1.1676687e-17 3.9165254e-28 2.7238094e-26], sum to 1.0000
[2019-04-27 18:40:06,634] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4737
[2019-04-27 18:40:06,638] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.13333333333333, 61.16666666666667, 1.0, 2.0, 0.4095009403373021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 504494.9710044519, 504494.9710044515, 129896.9869984991], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1277400.0000, 
sim time next is 1278000.0000, 
raw observation next is [25.0, 62.0, 1.0, 2.0, 0.4124133674617768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 507987.2172869719, 507987.2172869719, 130310.0729089934], 
processed observation next is [1.0, 0.8260869565217391, 0.48148148148148145, 0.62, 1.0, 1.0, 0.30049210412116284, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.18142400617391855, 0.18142400617391855, 0.25059629405575656], 
reward next is 0.7494, 
noisyNet noise sample is [array([1.6665773], dtype=float32), -0.4042011]. 
=============================================
[2019-04-27 18:40:06,648] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[77.51004]
 [78.03228]
 [78.14173]
 [78.6687 ]
 [78.53553]], R is [[77.49474335]
 [77.46999359]
 [77.44602966]
 [77.4220047 ]
 [77.39810181]].
[2019-04-27 18:40:10,556] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0421246e-22 9.9999762e-01 2.3489645e-06 2.3253166e-19 2.8156511e-12], sum to 1.0000
[2019-04-27 18:40:10,563] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3646
[2019-04-27 18:40:10,567] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.25, 39.0, 1.0, 2.0, 0.3472997001341711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 435570.1032520838, 435570.1032520838, 121543.5999774357], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1363800.0000, 
sim time next is 1364400.0000, 
raw observation next is [28.0, 40.0, 1.0, 2.0, 0.3498771863485792, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 438782.6230304992, 438782.6230304997, 121883.5461982769], 
processed observation next is [1.0, 0.8260869565217391, 0.5925925925925926, 0.4, 1.0, 1.0, 0.22604426946259432, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.15670807965374972, 0.1567080796537499, 0.23439143499668635], 
reward next is 0.7656, 
noisyNet noise sample is [array([-0.34430832], dtype=float32), -0.30003947]. 
=============================================
[2019-04-27 18:40:15,408] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.3532816e-25 9.9946111e-01 5.3891691e-04 4.6099300e-22 3.0973926e-15], sum to 1.0000
[2019-04-27 18:40:15,417] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7142
[2019-04-27 18:40:15,419] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.4, 30.0, 1.0, 2.0, 0.3626013132326431, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 456410.553389555, 456410.5533895545, 123605.6228680356], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1456200.0000, 
sim time next is 1456800.0000, 
raw observation next is [30.23333333333333, 30.66666666666666, 1.0, 2.0, 0.361299742503088, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 454609.2521585497, 454609.2521585497, 123428.3473902907], 
processed observation next is [0.0, 0.8695652173913043, 0.6753086419753086, 0.3066666666666666, 1.0, 1.0, 0.2396425505989143, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16236044719948203, 0.16236044719948203, 0.23736220651978981], 
reward next is 0.7626, 
noisyNet noise sample is [array([-0.19628361], dtype=float32), 1.3782817]. 
=============================================
[2019-04-27 18:40:18,323] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.1019146e-22 9.9999332e-01 6.6439084e-06 2.6014609e-18 2.8408417e-12], sum to 1.0000
[2019-04-27 18:40:18,327] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6171
[2019-04-27 18:40:18,335] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.95, 32.0, 1.0, 2.0, 0.4406419751899729, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 536600.2022084442, 536600.2022084438, 134246.3286060114], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1507800.0000, 
sim time next is 1508400.0000, 
raw observation next is [33.3, 31.0, 1.0, 2.0, 0.4413510452892903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 537319.9326424913, 537319.9326424913, 134346.7047152479], 
processed observation next is [0.0, 0.4782608695652174, 0.7888888888888888, 0.31, 1.0, 1.0, 0.3349417205824885, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1918999759437469, 0.1918999759437469, 0.25835904752932287], 
reward next is 0.7416, 
noisyNet noise sample is [array([-0.3115489], dtype=float32), -0.010245017]. 
=============================================
[2019-04-27 18:40:20,291] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.1846584e-22 9.9885726e-01 1.1426861e-03 6.8500388e-19 2.0975921e-12], sum to 1.0000
[2019-04-27 18:40:20,300] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4291
[2019-04-27 18:40:20,305] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.7, 60.0, 1.0, 2.0, 0.5200757473328845, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 618357.1735305368, 618357.1735305368, 146011.6221403368], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1536000.0000, 
sim time next is 1536600.0000, 
raw observation next is [27.2, 62.0, 1.0, 2.0, 0.5165174966652265, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 615338.3590598828, 615338.3590598828, 145487.3588015352], 
processed observation next is [0.0, 0.782608695652174, 0.5629629629629629, 0.62, 1.0, 1.0, 0.4244255912681268, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21976369966424386, 0.21976369966424386, 0.27978338231064465], 
reward next is 0.7202, 
noisyNet noise sample is [array([-1.2766705], dtype=float32), -0.6764959]. 
=============================================
[2019-04-27 18:40:41,467] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-27 18:40:41,469] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 18:40:41,470] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 18:40:41,470] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:40:41,471] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:40:41,471] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 18:40:41,472] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 18:40:41,474] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:40:41,471] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 18:40:41,475] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:40:41,477] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:40:41,495] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run20
[2019-04-27 18:40:41,515] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run20
[2019-04-27 18:40:41,536] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run20
[2019-04-27 18:40:41,563] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run20
[2019-04-27 18:40:41,565] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run20
[2019-04-27 18:40:48,163] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04091024], dtype=float32), 0.0646582]
[2019-04-27 18:40:48,166] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [26.18661917333333, 30.34731063666666, 1.0, 2.0, 0.3073128910275142, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 396426.2961966131, 396426.2961966131, 109769.3156804763]
[2019-04-27 18:40:48,167] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:40:48,171] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.5941182e-36 1.0000000e+00 5.7198813e-26 6.6195244e-35 1.3695551e-33], sampled 0.785608087757328
[2019-04-27 18:41:14,681] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04091024], dtype=float32), 0.0646582]
[2019-04-27 18:41:14,682] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [33.21666666666667, 39.83333333333334, 1.0, 2.0, 0.5177974695325731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 608173.4973202195, 608173.4973202195, 145349.2930616512]
[2019-04-27 18:41:14,682] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 18:41:14,684] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 5.6496686e-32 0.0000000e+00 0.0000000e+00], sampled 0.6956459850020994
[2019-04-27 18:41:18,291] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04091024], dtype=float32), 0.0646582]
[2019-04-27 18:41:18,292] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [32.75, 46.16666666666666, 1.0, 2.0, 0.5934646447694033, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 681954.2366951605, 681954.23669516, 157232.9384563773]
[2019-04-27 18:41:18,296] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 18:41:18,301] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.03911855e-36 1.00000000e+00 6.17695653e-26 5.02550582e-35
 1.46650708e-33], sampled 0.48824598514750206
[2019-04-27 18:41:53,208] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04091024], dtype=float32), 0.0646582]
[2019-04-27 18:41:53,209] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [24.9, 99.0, 1.0, 2.0, 0.7040897128019638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000002, 6.9112, 121.9260426156618, 802465.5331758194, 802465.5331758185, 176919.6373791492]
[2019-04-27 18:41:53,210] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 18:41:53,213] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [3.4470378e-36 1.0000000e+00 4.5851092e-26 3.5017902e-35 1.0723712e-33], sampled 0.5848465720780932
[2019-04-27 18:42:27,706] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8100.6463 2445352063.9460 746.0000
[2019-04-27 18:42:27,742] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8924.1431 2120476229.0038 430.0000
[2019-04-27 18:42:27,881] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8584.5265 2248663206.3456 553.0000
[2019-04-27 18:42:27,931] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8771.0838 2170637693.1962 493.0000
[2019-04-27 18:42:28,015] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8701.6686 2195058721.8889 572.0000
[2019-04-27 18:42:29,032] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 475000, evaluation results [475000.0, 8100.6462730594485, 2445352063.9460416, 746.0, 8771.08379319977, 2170637693.1962304, 493.0, 8924.143112926093, 2120476229.0037677, 430.0, 8584.526542641555, 2248663206.345631, 553.0, 8701.66863850428, 2195058721.8888793, 572.0]
[2019-04-27 18:42:29,423] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1638209e-34 1.0000000e+00 7.1952509e-26 1.4648089e-31 4.3454550e-32], sum to 1.0000
[2019-04-27 18:42:29,431] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3977
[2019-04-27 18:42:29,437] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.6, 92.0, 1.0, 2.0, 0.3603668629750033, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 450479.5538233489, 450479.5538233494, 123255.7637516863], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1912800.0000, 
sim time next is 1913400.0000, 
raw observation next is [19.6, 92.0, 1.0, 2.0, 0.3589281380241818, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 448686.7308319121, 448686.7308319126, 123063.191087905], 
processed observation next is [1.0, 0.13043478260869565, 0.28148148148148155, 0.92, 1.0, 1.0, 0.23681921193354974, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.16024526101139716, 0.16024526101139736, 0.23665998286135576], 
reward next is 0.7633, 
noisyNet noise sample is [array([-0.01765643], dtype=float32), -2.0649629]. 
=============================================
[2019-04-27 18:42:29,613] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.0584031e-38 1.0000000e+00 4.2662152e-27 3.8785835e-35 3.0671354e-35], sum to 1.0000
[2019-04-27 18:42:29,624] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2699
[2019-04-27 18:42:29,628] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.2, 91.0, 1.0, 2.0, 0.3892230504289118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 483415.7507173042, 483415.7507173042, 127136.4881577976], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1926000.0000, 
sim time next is 1926600.0000, 
raw observation next is [20.25, 90.83333333333334, 1.0, 2.0, 0.4344558438657137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 539390.7833547582, 539390.7833547577, 133602.8023625425], 
processed observation next is [1.0, 0.30434782608695654, 0.3055555555555556, 0.9083333333333334, 1.0, 1.0, 0.3267331474591829, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1926395654838422, 0.19263956548384203, 0.2569284660818125], 
reward next is 0.7431, 
noisyNet noise sample is [array([-0.7418377], dtype=float32), 0.5481462]. 
=============================================
[2019-04-27 18:42:35,476] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 3.4271163e-24 9.3046550e-37 4.9675304e-33], sum to 1.0000
[2019-04-27 18:42:35,485] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0583
[2019-04-27 18:42:35,490] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.2, 63.00000000000001, 1.0, 2.0, 0.5492427150184526, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 641478.322903193, 641478.322903193, 150295.2376084944], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2038800.0000, 
sim time next is 2039400.0000, 
raw observation next is [28.25, 63.0, 1.0, 2.0, 0.551760029563327, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 643823.4212184617, 643823.4212184612, 150684.3592150132], 
processed observation next is [0.0, 0.6086956521739131, 0.6018518518518519, 0.63, 1.0, 1.0, 0.4663809875753893, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2299369361494506, 0.22993693614945043, 0.2897776138750254], 
reward next is 0.7102, 
noisyNet noise sample is [array([1.0647706], dtype=float32), 0.10220698]. 
=============================================
[2019-04-27 18:42:36,046] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.8914519e-35 1.0000000e+00 1.5806428e-24 1.1455990e-33 4.1661221e-32], sum to 1.0000
[2019-04-27 18:42:36,055] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6614
[2019-04-27 18:42:36,061] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.86666666666667, 64.33333333333334, 1.0, 2.0, 0.5995196497451268, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 687830.1311047054, 687830.1311047054, 158221.4364205168], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2049600.0000, 
sim time next is 2050200.0000, 
raw observation next is [28.75, 65.0, 1.0, 2.0, 0.6005102148633794, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 688854.058490721, 688854.058490721, 158386.7279439515], 
processed observation next is [0.0, 0.7391304347826086, 0.6203703703703703, 0.65, 1.0, 1.0, 0.524416922456404, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.24601930660382895, 0.24601930660382895, 0.30458986143067596], 
reward next is 0.6954, 
noisyNet noise sample is [array([1.0313294], dtype=float32), 1.1031137]. 
=============================================
[2019-04-27 18:42:36,832] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.7391272e-33 1.0000000e+00 8.0570774e-24 1.6665403e-30 7.4041797e-30], sum to 1.0000
[2019-04-27 18:42:36,840] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9161
[2019-04-27 18:42:36,844] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.36666666666667, 72.33333333333334, 1.0, 2.0, 0.6058757837380789, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 696267.4917137771, 696267.4917137771, 159375.4272312747], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2056800.0000, 
sim time next is 2057400.0000, 
raw observation next is [27.25, 73.0, 1.0, 2.0, 0.6053846959445153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 695783.5429316668, 695783.5429316664, 159294.1120441635], 
processed observation next is [0.0, 0.8260869565217391, 0.5648148148148148, 0.73, 1.0, 1.0, 0.5302198761244229, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2484941224755953, 0.24849412247559513, 0.3063348308541606], 
reward next is 0.6937, 
noisyNet noise sample is [array([0.18174662], dtype=float32), 0.33539554]. 
=============================================
[2019-04-27 18:42:52,395] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.8541947e-23 1.0000000e+00 5.9480565e-10 9.4969091e-20 1.0097436e-16], sum to 1.0000
[2019-04-27 18:42:52,404] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4926
[2019-04-27 18:42:52,410] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.81666666666667, 54.5, 1.0, 2.0, 0.5208341443517752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 645774.6519207073, 645774.6519207073, 146950.1042005771], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2358600.0000, 
sim time next is 2359200.0000, 
raw observation next is [26.13333333333334, 52.00000000000001, 1.0, 2.0, 0.4326295783340272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 537490.3124129922, 537490.3124129922, 133342.1547014512], 
processed observation next is [1.0, 0.30434782608695654, 0.523456790123457, 0.52, 1.0, 1.0, 0.3245590218262229, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.19196082586178295, 0.19196082586178295, 0.25642722057971384], 
reward next is 0.7436, 
noisyNet noise sample is [array([0.19724381], dtype=float32), -1.4277915]. 
=============================================
[2019-04-27 18:42:55,448] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.2209150e-23 4.9925319e-07 9.9999940e-01 1.8083922e-13 1.0073642e-07], sum to 1.0000
[2019-04-27 18:42:55,457] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4657
[2019-04-27 18:42:55,463] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [23.55, 63.83333333333334, 1.0, 2.0, 0.1897182139676095, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3156528220475792, 6.911199999999999, 6.9112, 121.9260426156618, 471155.352028239, 471155.3520282395, 165029.421437445], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2416200.0000, 
sim time next is 2416800.0000, 
raw observation next is [23.4, 63.66666666666667, 1.0, 2.0, 0.1863581315832293, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3109220439173101, 6.911199999999999, 6.9112, 121.9260426156618, 463800.1068928704, 463800.1068928709, 164129.9582947062], 
processed observation next is [1.0, 1.0, 0.42222222222222217, 0.6366666666666667, 1.0, 1.0, 0.031378728075272984, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.1386525548966376, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1656428953188823, 0.16564289531888246, 0.3156345351821273], 
reward next is 0.6844, 
noisyNet noise sample is [array([-1.2374071], dtype=float32), -0.5651192]. 
=============================================
[2019-04-27 18:42:58,711] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2796185e-13 1.1029795e-02 9.8896432e-01 6.3537664e-09 5.8565893e-06], sum to 1.0000
[2019-04-27 18:42:58,722] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5549
[2019-04-27 18:42:58,729] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [33.6, 24.0, 1.0, 2.0, 0.5308100463547386, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8713597640683275, 6.9112, 6.9112, 121.9260426156618, 1302916.448978287, 1302916.448978287, 264596.2574288018], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2462400.0000, 
sim time next is 2463000.0000, 
raw observation next is [33.71666666666667, 23.83333333333333, 1.0, 2.0, 0.7507775423363451, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9559356565536594, 6.911199999999999, 6.9112, 121.9260426156618, 1633645.150455668, 1633645.150455669, 314558.4757315074], 
processed observation next is [1.0, 0.5217391304347826, 0.804320987654321, 0.23833333333333329, 1.0, 1.0, 0.7033065980194584, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.9449195706920743, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.58344469659131, 0.5834446965913104, 0.6049201456375143], 
reward next is 0.3951, 
noisyNet noise sample is [array([0.0887094], dtype=float32), -0.8510034]. 
=============================================
[2019-04-27 18:42:58,754] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[48.138435]
 [47.836178]
 [47.491734]
 [47.55508 ]
 [47.283665]], R is [[47.64567566]
 [47.66038132]
 [47.68175125]
 [47.70991516]
 [47.23281479]].
[2019-04-27 18:42:59,360] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.6467228e-19 3.4139343e-06 9.9999654e-01 2.7557942e-13 1.9373882e-08], sum to 1.0000
[2019-04-27 18:42:59,368] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5884
[2019-04-27 18:42:59,373] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [33.25, 25.5, 1.0, 2.0, 0.1964250934996342, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3249749374820262, 6.9112, 6.9112, 121.9260426156618, 485521.4256784333, 485521.4256784333, 166850.7054240376], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2482200.0000, 
sim time next is 2482800.0000, 
raw observation next is [33.03333333333333, 26.0, 1.0, 2.0, 0.195362594239233, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3234397422548039, 6.911199999999999, 6.9112, 121.9260426156618, 483186.3398092178, 483186.3398092182, 166572.292810624], 
processed observation next is [1.0, 0.7391304347826086, 0.7790123456790122, 0.26, 1.0, 1.0, 0.04209832647527737, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.15429967781850482, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1725665499318635, 0.17256654993186363, 0.32033133232812305], 
reward next is 0.6797, 
noisyNet noise sample is [array([-0.06766494], dtype=float32), 0.9094046]. 
=============================================
[2019-04-27 18:43:02,935] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.2168895e-17 4.6954959e-08 9.9999988e-01 1.7173313e-11 9.2693277e-08], sum to 1.0000
[2019-04-27 18:43:02,943] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1026
[2019-04-27 18:43:02,947] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.7, 30.0, 1.0, 2.0, 0.5842056431985343, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9475482779285039, 6.9112, 6.9112, 121.9260426156618, 1412846.367108712, 1412846.367108712, 285435.6779703422], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2549400.0000, 
sim time next is 2550000.0000, 
raw observation next is [32.8, 30.0, 1.0, 2.0, 0.5565185128534009, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9024072281797003, 6.911199999999999, 6.9112, 121.9260426156618, 1345307.617212204, 1345307.617212204, 275086.1409714388], 
processed observation next is [1.0, 0.5217391304347826, 0.7703703703703703, 0.3, 1.0, 1.0, 0.4720458486350011, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8780090352246253, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.4804670061472157, 0.4804670061472157, 0.5290118095604592], 
reward next is 0.4710, 
noisyNet noise sample is [array([0.30924731], dtype=float32), 1.3121996]. 
=============================================
[2019-04-27 18:43:02,964] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[44.73689 ]
 [44.473873]
 [44.15117 ]
 [44.090492]
 [44.02836 ]], R is [[45.01522446]
 [45.01615524]
 [45.00130844]
 [44.96193314]
 [44.92813492]].
[2019-04-27 18:43:03,793] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.3116231e-18 8.0128976e-12 1.0000000e+00 4.1680542e-13 5.5556866e-09], sum to 1.0000
[2019-04-27 18:43:03,800] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7937
[2019-04-27 18:43:03,808] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [33.7, 30.5, 1.0, 2.0, 0.8501669458382989, 0.0, 2.0, 0.0, 1.0, 2.0, 0.965723282915678, 6.911199999999999, 6.9112, 121.9260426156618, 1725880.958097128, 1725880.958097128, 339975.1692708175], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2557800.0000, 
sim time next is 2558400.0000, 
raw observation next is [33.66666666666666, 30.66666666666666, 1.0, 2.0, 0.8797116546738398, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9659432730428578, 6.911200000000001, 6.9112, 121.9260426156618, 1760632.239664696, 1760632.239664696, 346350.7168979899], 
processed observation next is [1.0, 0.6086956521739131, 0.8024691358024688, 0.3066666666666666, 1.0, 1.0, 0.8567995888974284, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.9574290913035721, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.6287972284516771, 0.6287972284516771, 0.6660590709576729], 
reward next is 0.3339, 
noisyNet noise sample is [array([0.17148273], dtype=float32), -0.7502018]. 
=============================================
[2019-04-27 18:43:08,951] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8276103e-20 7.6493983e-05 9.9992347e-01 3.9745370e-17 1.0111153e-09], sum to 1.0000
[2019-04-27 18:43:08,958] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0434
[2019-04-27 18:43:08,963] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.4, 75.0, 1.0, 2.0, 0.2895052315990964, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4613472875177051, 6.911199999999999, 6.9112, 121.9260426156618, 667359.8175147277, 667359.8175147282, 192302.4632430315], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2644200.0000, 
sim time next is 2644800.0000, 
raw observation next is [26.6, 74.66666666666666, 1.0, 2.0, 0.2938285944925867, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4680702060701323, 6.911199999999999, 6.9112, 121.9260426156618, 675290.7745588664, 675290.7745588669, 193494.3918744561], 
processed observation next is [0.0, 0.6086956521739131, 0.5407407407407407, 0.7466666666666666, 1.0, 1.0, 0.1593197553483175, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.3350877575876653, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.24117527662816657, 0.24117527662816673, 0.37210459975856947], 
reward next is 0.6279, 
noisyNet noise sample is [array([-2.7863243], dtype=float32), -0.110537685]. 
=============================================
[2019-04-27 18:43:09,206] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.2659905e-20 1.5226602e-06 9.9999845e-01 1.9589405e-15 4.3822588e-11], sum to 1.0000
[2019-04-27 18:43:09,213] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2555
[2019-04-27 18:43:09,217] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.0, 78.0, 1.0, 2.0, 0.2965435465892488, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4724212841214983, 6.9112, 6.9112, 121.9260426156618, 681882.8798644984, 681882.8798644984, 194209.8645761829], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2660400.0000, 
sim time next is 2661000.0000, 
raw observation next is [25.85, 78.66666666666667, 1.0, 2.0, 0.2950382080438585, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4700809506625953, 6.9112, 6.9112, 121.9260426156618, 679150.2033617734, 679150.2033617734, 193792.1367204431], 
processed observation next is [0.0, 0.8260869565217391, 0.5129629629629631, 0.7866666666666667, 1.0, 1.0, 0.16075977148078394, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.3376011883282441, 0.0, 0.0, 0.8094621288201359, 0.2425536440577762, 0.2425536440577762, 0.3726771860008521], 
reward next is 0.6273, 
noisyNet noise sample is [array([-1.7385969], dtype=float32), 1.145993]. 
=============================================
[2019-04-27 18:43:09,238] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[62.59773 ]
 [62.54074 ]
 [62.47883 ]
 [62.416447]
 [62.356438]], R is [[62.63669205]
 [62.63684464]
 [62.6360321 ]
 [62.63414383]
 [62.6310997 ]].
[2019-04-27 18:43:14,539] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.5506278e-22 1.7310905e-08 1.0000000e+00 5.4479061e-17 1.0563916e-08], sum to 1.0000
[2019-04-27 18:43:14,548] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2990
[2019-04-27 18:43:14,554] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.83333333333334, 85.66666666666667, 1.0, 2.0, 0.3261567779992564, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5192522127595024, 6.9112, 6.9112, 121.9260426156618, 743426.5651325795, 743426.5651325795, 202406.7668336333], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2758800.0000, 
sim time next is 2759400.0000, 
raw observation next is [25.75, 86.5, 1.0, 2.0, 0.3277908428769483, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5218536972625655, 6.911199999999999, 6.9112, 121.9260426156618, 747152.9917088102, 747152.9917088107, 202863.2185349737], 
processed observation next is [0.0, 0.9565217391304348, 0.5092592592592593, 0.865, 1.0, 1.0, 0.19975100342493848, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.4023171215782068, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2668403541817179, 0.2668403541817181, 0.3901215741057187], 
reward next is 0.6099, 
noisyNet noise sample is [array([0.01925655], dtype=float32), 1.5002632]. 
=============================================
[2019-04-27 18:43:16,420] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.4516685e-17 1.6554867e-11 9.9999988e-01 2.4022818e-11 7.6694320e-08], sum to 1.0000
[2019-04-27 18:43:16,427] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8657
[2019-04-27 18:43:16,434] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 2 has been changed to 4 for the demand 2050238.205322227 W.
[2019-04-27 18:43:16,440] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.83333333333334, 74.83333333333334, 1.0, 2.0, 0.5991702924869973, 1.0, 2.0, 0.5991702924869973, 1.0, 2.0, 0.9538986192534097, 6.9112, 6.9112, 121.94756008, 2050238.205322227, 2050238.205322227, 394923.7123952823], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2800200.0000, 
sim time next is 2800800.0000, 
raw observation next is [29.0, 74.0, 1.0, 2.0, 0.6237570057798623, 1.0, 2.0, 0.6237570057798623, 1.0, 2.0, 0.9930414675490006, 6.911199999999999, 6.9112, 121.94756008, 2134469.445180174, 2134469.445180175, 408471.4898498821], 
processed observation next is [1.0, 0.43478260869565216, 0.6296296296296297, 0.74, 1.0, 1.0, 0.552091673547455, 1.0, 1.0, 0.552091673547455, 1.0, 1.0, 0.9913018344362506, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.7623105161357764, 0.7623105161357767, 0.7855220958651579], 
reward next is 0.2145, 
noisyNet noise sample is [array([-0.37315682], dtype=float32), -0.39759076]. 
=============================================
[2019-04-27 18:43:17,019] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.12915211e-16 3.25855121e-10 1.00000000e+00 1.18562165e-11
 2.24823959e-09], sum to 1.0000
[2019-04-27 18:43:17,026] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9411
[2019-04-27 18:43:17,029] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.91666666666666, 63.0, 1.0, 2.0, 0.7893008255925896, 1.0, 2.0, 0.7893008255925896, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1800299.377530965, 1800299.377530965, 339353.6706246321], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2805000.0000, 
sim time next is 2805600.0000, 
raw observation next is [31.13333333333333, 63.00000000000001, 1.0, 2.0, 1.02, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9977734948820727, 8.272929027630973, 6.9112, 121.9208591056055, 2576107.389593105, 1878810.063338307, 379234.5369325178], 
processed observation next is [1.0, 0.4782608695652174, 0.7086419753086418, 0.6300000000000001, 1.0, 1.0, 1.0238095238095237, 0.0, 0.5, -0.1904761904761905, 1.0, 0.5, 0.9972168686025908, 0.13617290276309726, 0.0, 0.8094277157038327, 0.9200383534261088, 0.6710035940493954, 0.729297186408688], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.87492], dtype=float32), 0.28996244]. 
=============================================
[2019-04-27 18:43:18,146] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4879594e-09 3.0911895e-06 9.9925345e-01 3.2820024e-09 7.4347382e-04], sum to 1.0000
[2019-04-27 18:43:18,155] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0978
[2019-04-27 18:43:18,163] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 2 has been changed to 4 for the demand 2236211.02150437 W.
[2019-04-27 18:43:18,169] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.9, 58.0, 1.0, 2.0, 0.9801775047571876, 1.0, 2.0, 0.9801775047571876, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156221, 2236211.02150437, 2236211.02150437, 423832.5985999134], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2820600.0000, 
sim time next is 2821200.0000, 
raw observation next is [32.93333333333334, 58.33333333333333, 1.0, 2.0, 0.7931938467526947, 1.0, 2.0, 0.7099615853527821, 1.0, 1.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.94756008, 2429859.157431412, 2429859.157431413, 455221.6601421746], 
processed observation next is [1.0, 0.6521739130434783, 0.7753086419753088, 0.5833333333333333, 1.0, 1.0, 0.7538021985151127, 1.0, 1.0, 0.6547161730390263, 1.0, 0.5, 0.9972168686025908, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.86780684193979, 0.8678068419397903, 0.8754262695041819], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9873087], dtype=float32), -1.0249208]. 
=============================================
[2019-04-27 18:43:19,848] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-27 18:43:19,849] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 18:43:19,850] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:43:19,851] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 18:43:19,852] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:43:19,852] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 18:43:19,853] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:43:19,853] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 18:43:19,853] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 18:43:19,854] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:43:19,854] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:43:19,860] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run21
[2019-04-27 18:43:19,878] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run21
[2019-04-27 18:43:19,879] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run21
[2019-04-27 18:43:19,918] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run21
[2019-04-27 18:43:19,941] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run21
[2019-04-27 18:43:22,993] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.03387063], dtype=float32), 0.061309516]
[2019-04-27 18:43:22,995] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [29.686445395, 35.91431137666667, 1.0, 2.0, 0.239402241172738, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3976651300306894, 6.911199999999999, 6.9112, 121.9260426156618, 593807.3703418116, 593807.370341812, 176572.8676790578]
[2019-04-27 18:43:22,996] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:43:22,999] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [2.6305463e-19 1.1066773e-04 9.9988937e-01 8.4305983e-20 6.7479805e-10], sampled 0.536402004345491
[2019-04-27 18:43:25,267] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03387063], dtype=float32), 0.061309516]
[2019-04-27 18:43:25,267] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [24.0, 39.0, 1.0, 2.0, 0.16, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2378912537621612, 6.911199999999999, 6.9112, 121.9260426156618, 339709.4881280771, 339709.4881280776, 142378.0691668134]
[2019-04-27 18:43:25,268] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 18:43:25,270] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [3.9547787e-19 5.8058568e-04 9.9941945e-01 1.0822409e-19 4.4518011e-10], sampled 0.8490178254249788
[2019-04-27 18:43:31,270] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03387063], dtype=float32), 0.061309516]
[2019-04-27 18:43:31,272] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [27.0, 42.33333333333333, 1.0, 2.0, 0.1663098538188332, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2806547350511757, 6.911199999999999, 6.9112, 121.9260426156618, 417142.1227420566, 417142.1227420571, 159163.9315775821]
[2019-04-27 18:43:31,275] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 18:43:31,278] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [4.0451479e-19 2.8404282e-04 9.9971598e-01 1.1473939e-19 5.7295491e-10], sampled 0.34442343450281454
[2019-04-27 18:43:39,107] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03387063], dtype=float32), 0.061309516]
[2019-04-27 18:43:39,109] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [32.23333333333333, 27.33333333333333, 1.0, 2.0, 0.1856255907366552, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3084440015070455, 6.9112, 6.9112, 121.9260426156618, 460509.9246048856, 460509.9246048856, 164201.4790522827]
[2019-04-27 18:43:39,111] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 18:43:39,113] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [5.38870960e-22 1.41014971e-06 9.99998569e-01 4.80561442e-22
 1.01277736e-10], sampled 0.34297007890788545
[2019-04-27 18:43:55,792] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03387063], dtype=float32), 0.061309516]
[2019-04-27 18:43:55,794] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [33.0, 44.0, 1.0, 2.0, 0.4169464823030602, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6638513088171311, 6.911199999999999, 6.9112, 121.9260426156618, 952228.7980541195, 952228.7980541199, 229361.796229007]
[2019-04-27 18:43:55,797] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 18:43:55,800] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.3550485e-20 8.8505061e-07 9.9999917e-01 1.0801905e-20 5.9307642e-10], sampled 0.16234829396470418
[2019-04-27 18:44:00,374] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.03387063], dtype=float32), 0.061309516]
[2019-04-27 18:44:00,376] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [22.1770473, 83.59768340333333, 1.0, 2.0, 0.3130974977360705, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5099612150899406, 6.9112, 6.9112, 121.9260426156618, 761286.0314081839, 761286.0314081839, 196758.3291967317]
[2019-04-27 18:44:00,377] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:44:00,380] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [4.5138600e-19 5.5083074e-05 9.9994493e-01 1.2093912e-19 9.2611396e-10], sampled 0.19416190861387905
[2019-04-27 18:44:10,226] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03387063], dtype=float32), 0.061309516]
[2019-04-27 18:44:10,228] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [32.33333333333334, 47.83333333333334, 1.0, 2.0, 0.3268604180908221, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5203724307012328, 6.911199999999999, 6.9112, 121.9260426156618, 745031.1890028514, 745031.1890028518, 202601.8202023032]
[2019-04-27 18:44:10,229] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 18:44:10,231] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [8.2197965e-20 1.9392384e-05 9.9998057e-01 2.8428104e-20 6.9542988e-10], sampled 0.7325245340509385
[2019-04-27 18:44:12,330] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03387063], dtype=float32), 0.061309516]
[2019-04-27 18:44:12,332] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [30.0, 75.0, 1.0, 2.0, 1.02, 1.0, 2.0, 1.02, 1.0, 2.0, 0.9977734948820727, 22.4275910262724, 6.9112, 136.1732475593925, 11923453.43379701, 3049200.536711515, 515105.0934316146]
[2019-04-27 18:44:12,333] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 18:44:12,336] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [4.6045047e-13 3.9809451e-14 9.9072719e-01 1.5905591e-12 9.2727952e-03], sampled 0.18559258429623138
[2019-04-27 18:44:12,338] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Action function: raw action [0, 0, 1, 0, 0] has been changed to [0, 0, 0, 0, 1] for the demand 11923453.43379701 W.
[2019-04-27 18:44:18,143] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03387063], dtype=float32), 0.061309516]
[2019-04-27 18:44:18,146] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [27.35, 86.16666666666667, 1.0, 2.0, 0.3693942077371239, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5880876090469662, 6.911199999999999, 6.9112, 121.9260426156618, 842034.0817732053, 842034.0817732058, 214847.5750639917]
[2019-04-27 18:44:18,148] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 18:44:18,154] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [2.8056809e-21 1.1448094e-06 9.9999881e-01 1.8238258e-21 2.6744942e-10], sampled 0.2650010625454987
[2019-04-27 18:44:23,068] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.03387063], dtype=float32), 0.061309516]
[2019-04-27 18:44:23,070] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [31.04404842666667, 61.67268263666667, 1.0, 2.0, 0.3583545759059295, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5705121556359964, 6.911199999999999, 6.9112, 121.9260426156618, 816855.8315392636, 816855.8315392641, 211599.4515096836]
[2019-04-27 18:44:23,070] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:44:23,076] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [4.5849608e-21 6.3676305e-07 9.9999940e-01 2.9504198e-21 3.6245124e-10], sampled 0.7578338038070677
[2019-04-27 18:44:41,381] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.03387063], dtype=float32), 0.061309516]
[2019-04-27 18:44:41,381] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [34.0, 33.66666666666667, 1.0, 2.0, 0.2464508907801279, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3955428421218817, 6.9112, 6.9112, 121.9260426156618, 583976.2569017604, 583976.2569017604, 180658.3918549003]
[2019-04-27 18:44:41,382] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 18:44:41,384] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.9990999e-21 3.0476938e-06 9.9999690e-01 1.3952986e-21 1.5296292e-10], sampled 0.6761935560535928
[2019-04-27 18:44:57,952] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03387063], dtype=float32), 0.061309516]
[2019-04-27 18:44:57,953] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [30.0, 75.0, 1.0, 2.0, 0.463901728925717, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7385466606742924, 6.911199999999999, 6.9112, 121.9260426156618, 1057612.63019729, 1057612.630197291, 244646.9991965323]
[2019-04-27 18:44:57,955] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 18:44:57,958] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [5.9943659e-20 2.3842813e-06 9.9999762e-01 2.9442883e-20 9.6426078e-10], sampled 0.2825137443764777
[2019-04-27 18:45:07,729] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 7492.5206 2565975221.9196 47.0000
[2019-04-27 18:45:07,775] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 7160.4765 2623728715.5702 97.0000
[2019-04-27 18:45:07,877] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 6809.7479 2600712272.2885 61.0000
[2019-04-27 18:45:07,900] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7155.3077 2831309947.1919 210.0000
[2019-04-27 18:45:07,937] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 6579.9288 2661696570.6175 110.0000
[2019-04-27 18:45:08,951] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 500000, evaluation results [500000.0, 7155.307713616394, 2831309947.19193, 210.0, 6809.747916352162, 2600712272.2885246, 61.0, 7492.520639979948, 2565975221.919632, 47.0, 6579.928819146758, 2661696570.6174746, 110.0, 7160.476543854458, 2623728715.5702496, 97.0]
[2019-04-27 18:45:13,512] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.2883331e-18 9.1867136e-08 9.9988401e-01 4.7048988e-17 1.1589513e-04], sum to 1.0000
[2019-04-27 18:45:13,522] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8736
[2019-04-27 18:45:13,527] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.06666666666667, 92.0, 1.0, 2.0, 0.3241832543759683, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5161102988780504, 6.9112, 6.9112, 121.9260426156618, 738926.0391655882, 738926.0391655882, 201857.317532479], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2938800.0000, 
sim time next is 2939400.0000, 
raw observation next is [25.05, 92.5, 1.0, 2.0, 0.3254844118837786, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5181817840062259, 6.911199999999999, 6.9112, 121.9260426156618, 741893.2634871993, 741893.2634871998, 202219.7176168363], 
processed observation next is [1.0, 0.0, 0.48333333333333334, 0.925, 1.0, 1.0, 0.19700525224259355, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.3977272300077824, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2649618798168569, 0.2649618798168571, 0.38888407234006983], 
reward next is 0.6111, 
noisyNet noise sample is [array([-1.4323614], dtype=float32), 0.1303282]. 
=============================================
[2019-04-27 18:45:16,286] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.5401501e-20 1.9568829e-14 3.2752547e-03 1.9314348e-17 9.9672478e-01], sum to 1.0000
[2019-04-27 18:45:16,292] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5784
[2019-04-27 18:45:16,298] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.58333333333333, 86.5, 1.0, 2.0, 0.5333471323378567, 1.0, 2.0, 0.5333471323378567, 1.0, 2.0, 0.849106004585312, 6.9112, 6.9112, 121.94756008, 1824774.753142796, 1824774.753142796, 360257.5713352043], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2998200.0000, 
sim time next is 2998800.0000, 
raw observation next is [26.7, 86.0, 1.0, 2.0, 0.5429523707237307, 1.0, 2.0, 0.5429523707237307, 1.0, 2.0, 0.8643978569163985, 6.911199999999999, 6.9112, 121.94756008, 1857671.93997595, 1857671.939975951, 365170.4150571473], 
processed observation next is [1.0, 0.7391304347826086, 0.5444444444444444, 0.86, 1.0, 1.0, 0.45589567943301273, 1.0, 1.0, 0.45589567943301273, 1.0, 1.0, 0.830497321145498, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.663454264277125, 0.6634542642771254, 0.7022507981868217], 
reward next is 0.2977, 
noisyNet noise sample is [array([0.14342304], dtype=float32), 0.9643641]. 
=============================================
[2019-04-27 18:45:16,563] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.6568950e-19 1.1612822e-11 3.9063040e-02 1.4224267e-17 9.6093696e-01], sum to 1.0000
[2019-04-27 18:45:16,569] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2710
[2019-04-27 18:45:16,575] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.56666666666666, 95.33333333333333, 1.0, 2.0, 0.2207758881094659, 1.0, 2.0, 0.2207758881094659, 1.0, 2.0, 0.3514824040389737, 6.9112, 6.9112, 121.94756008, 754844.9649878243, 754844.9649878243, 227501.4792728364], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3001200.0000, 
sim time next is 3001800.0000, 
raw observation next is [25.28333333333333, 97.66666666666667, 1.0, 2.0, 0.2233935157275035, 1.0, 2.0, 0.2233935157275035, 1.0, 2.0, 0.3556497524570693, 6.9112, 6.9112, 121.94756008, 763799.2367428335, 763799.2367428335, 228388.8228708809], 
processed observation next is [1.0, 0.7391304347826086, 0.49197530864197525, 0.9766666666666667, 1.0, 1.0, 0.07546847110417082, 1.0, 1.0, 0.07546847110417082, 1.0, 1.0, 0.1945621905713366, 0.0, 0.0, 0.8096049824067558, 0.2727854416938691, 0.2727854416938691, 0.439209274751694], 
reward next is 0.5608, 
noisyNet noise sample is [array([0.21443535], dtype=float32), 0.7743701]. 
=============================================
[2019-04-27 18:45:18,932] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.0106097e-20 5.2963580e-13 2.5264285e-06 2.8335409e-17 9.9999750e-01], sum to 1.0000
[2019-04-27 18:45:18,944] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9922
[2019-04-27 18:45:18,948] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.9, 93.5, 1.0, 2.0, 0.2696802571487819, 1.0, 2.0, 0.2696802571487819, 1.0, 2.0, 0.4293397522536719, 6.911199999999999, 6.9112, 121.94756008, 922152.2766156874, 922152.2766156879, 244712.0199501727], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3033000.0000, 
sim time next is 3033600.0000, 
raw observation next is [24.93333333333333, 93.66666666666667, 1.0, 2.0, 0.2630896702114153, 1.0, 2.0, 0.2630896702114153, 1.0, 2.0, 0.4188473232089524, 6.911199999999999, 6.9112, 121.94756008, 899603.0131238762, 899603.0131238766, 242314.764956884], 
processed observation next is [1.0, 0.08695652173913043, 0.47901234567901224, 0.9366666666666668, 1.0, 1.0, 0.12272579787073254, 1.0, 1.0, 0.12272579787073254, 1.0, 1.0, 0.27355915401119046, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.32128679040138436, 0.3212867904013845, 0.4659899326093923], 
reward next is 0.5340, 
noisyNet noise sample is [array([-0.93463504], dtype=float32), 0.17805946]. 
=============================================
[2019-04-27 18:45:20,918] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.9349330e-15 4.0052704e-11 1.3579425e-04 2.9409414e-14 9.9986422e-01], sum to 1.0000
[2019-04-27 18:45:20,923] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6684
[2019-04-27 18:45:20,926] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.73333333333333, 84.66666666666667, 1.0, 2.0, 0.4576519750977415, 1.0, 2.0, 0.4576519750977415, 1.0, 2.0, 0.7285968490398833, 6.9112, 6.9112, 121.94756008, 1565564.761851674, 1565564.761851674, 323256.8825092742], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3082800.0000, 
sim time next is 3083400.0000, 
raw observation next is [27.1, 85.0, 1.0, 2.0, 0.4779439103948356, 1.0, 2.0, 0.4779439103948356, 1.0, 2.0, 0.7609022708950526, 6.9112, 6.9112, 121.94756008, 1635046.711295915, 1635046.711295915, 332895.6060570678], 
processed observation next is [1.0, 0.6956521739130435, 0.5592592592592593, 0.85, 1.0, 1.0, 0.37850465523194715, 1.0, 1.0, 0.37850465523194715, 1.0, 1.0, 0.7011278386188158, 0.0, 0.0, 0.8096049824067558, 0.5839452540342553, 0.5839452540342553, 0.6401838578020534], 
reward next is 0.3598, 
noisyNet noise sample is [array([-1.2177705], dtype=float32), 1.3619168]. 
=============================================
[2019-04-27 18:45:25,362] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.0944062e-19 3.6555470e-09 1.4255658e-05 1.1974552e-17 9.9998569e-01], sum to 1.0000
[2019-04-27 18:45:25,370] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1832
[2019-04-27 18:45:25,377] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.33333333333334, 63.0, 1.0, 2.0, 0.2136643214637332, 1.0, 2.0, 0.2136643214637332, 1.0, 2.0, 0.340160558331411, 6.9112, 6.9112, 121.94756008, 730518.5414040826, 730518.5414040826, 225110.097556575], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3181800.0000, 
sim time next is 3182400.0000, 
raw observation next is [29.8, 68.0, 1.0, 2.0, 0.2233281230178674, 1.0, 2.0, 0.2233281230178674, 1.0, 2.0, 0.3555456451336369, 6.911199999999999, 6.9112, 121.94756008, 763575.5428060354, 763575.5428060358, 228366.608872614], 
processed observation next is [1.0, 0.8695652173913043, 0.6592592592592593, 0.68, 1.0, 1.0, 0.07539062264031832, 1.0, 1.0, 0.07539062264031832, 1.0, 1.0, 0.1944320564170461, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.2727055510021555, 0.27270555100215566, 0.43916655552425765], 
reward next is 0.5608, 
noisyNet noise sample is [array([-0.24227168], dtype=float32), -0.5332537]. 
=============================================
[2019-04-27 18:45:25,708] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2103066e-19 7.1133927e-10 2.3331371e-07 1.0621846e-18 9.9999976e-01], sum to 1.0000
[2019-04-27 18:45:25,717] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8831
[2019-04-27 18:45:25,722] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.96666666666667, 54.66666666666666, 1.0, 2.0, 0.1999652925892443, 1.0, 2.0, 0.1999652925892443, 1.0, 2.0, 0.3183512582170013, 6.9112, 6.9112, 121.94756008, 683660.6652704276, 683660.6652704276, 220583.3385719677], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3185400.0000, 
sim time next is 3186000.0000, 
raw observation next is [30.0, 52.0, 1.0, 2.0, 0.1915317452947454, 1.0, 2.0, 0.1915317452947454, 1.0, 2.0, 0.3051427218166623, 6.9112, 6.9112, 121.94756008, 660902.896057382, 660902.896057382, 217853.4147102735], 
processed observation next is [1.0, 0.9130434782608695, 0.6666666666666666, 0.52, 1.0, 1.0, 0.03753779201755404, 1.0, 1.0, 0.03753779201755404, 1.0, 1.0, 0.13142840227082786, 0.0, 0.0, 0.8096049824067558, 0.23603674859192214, 0.23603674859192214, 0.41894887444283363], 
reward next is 0.5811, 
noisyNet noise sample is [array([0.06402603], dtype=float32), -0.44313055]. 
=============================================
[2019-04-27 18:45:25,739] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[53.218773]
 [53.34133 ]
 [53.40677 ]
 [53.46724 ]
 [53.57493 ]], R is [[53.20199203]
 [53.24577332]
 [53.28431702]
 [53.31781387]
 [53.34746552]].
[2019-04-27 18:45:26,596] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.8852085e-18 4.2120953e-07 2.1487084e-04 7.2197694e-17 9.9978477e-01], sum to 1.0000
[2019-04-27 18:45:26,605] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4396
[2019-04-27 18:45:26,611] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.0, 66.5, 1.0, 2.0, 0.1696024098818979, 1.0, 2.0, 0.1696024098818979, 1.0, 2.0, 0.2715940396694601, 6.9112, 6.9112, 121.94756008, 599142.2790115284, 599142.2790115284, 210709.7587125063], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3199800.0000, 
sim time next is 3200400.0000, 
raw observation next is [26.0, 65.0, 1.0, 2.0, 0.165488714870928, 1.0, 2.0, 0.165488714870928, 1.0, 2.0, 0.2654402741228437, 6.9112, 6.9112, 121.94756008, 587286.5246628606, 587286.5246628606, 209357.0614707002], 
processed observation next is [0.0, 0.043478260869565216, 0.5185185185185185, 0.65, 1.0, 1.0, 0.006534184370152384, 1.0, 1.0, 0.006534184370152384, 1.0, 1.0, 0.08180034265355463, 0.0, 0.0, 0.8096049824067558, 0.20974518737959305, 0.20974518737959305, 0.40260973359750035], 
reward next is 0.5974, 
noisyNet noise sample is [array([-0.08452477], dtype=float32), 1.4434377]. 
=============================================
[2019-04-27 18:45:38,265] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.8812016e-09 1.2977407e-05 4.3165116e-04 5.7318115e-09 9.9955529e-01], sum to 1.0000
[2019-04-27 18:45:38,275] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4346
[2019-04-27 18:45:38,279] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.5, 67.0, 1.0, 2.0, 0.6257803382762382, 1.0, 2.0, 0.6257803382762382, 1.0, 2.0, 0.9962626787785702, 6.9112, 6.9112, 121.94756008, 2141401.50240641, 2141401.50240641, 409600.8637732617], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3409200.0000, 
sim time next is 3409800.0000, 
raw observation next is [29.75, 65.66666666666667, 1.0, 2.0, 0.618277557121504, 1.0, 2.0, 0.618277557121504, 1.0, 2.0, 0.9843180068317099, 6.9112, 6.9112, 121.94756008, 2115696.799015277, 2115696.799015277, 405424.0464694251], 
processed observation next is [1.0, 0.4782608695652174, 0.6574074074074074, 0.6566666666666667, 1.0, 1.0, 0.5455685203827428, 1.0, 1.0, 0.5455685203827428, 1.0, 1.0, 0.9803975085396374, 0.0, 0.0, 0.8096049824067558, 0.7556059996483132, 0.7556059996483132, 0.7796616278258175], 
reward next is 0.2203, 
noisyNet noise sample is [array([0.01679474], dtype=float32), 0.68579483]. 
=============================================
[2019-04-27 18:45:38,801] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.6341620e-12 4.6291401e-08 3.0640655e-05 6.3383146e-11 9.9996936e-01], sum to 1.0000
[2019-04-27 18:45:38,809] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9865
[2019-04-27 18:45:38,821] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.4, 59.33333333333333, 1.0, 2.0, 0.6229174685930342, 1.0, 2.0, 0.6229174685930342, 1.0, 2.0, 0.991704897005753, 6.911199999999999, 6.9112, 121.94756008, 2131593.156743045, 2131593.156743046, 408003.5269217132], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3422400.0000, 
sim time next is 3423000.0000, 
raw observation next is [31.7, 59.16666666666667, 1.0, 2.0, 0.654929255398719, 1.0, 2.0, 0.6408292896757942, 1.0, 2.0, 0.9977734948820727, 6.911200000000001, 6.9112, 121.94756008, 2192961.79165761, 2192961.791657609, 417348.3825825464], 
processed observation next is [1.0, 0.6086956521739131, 0.7296296296296296, 0.5916666666666667, 1.0, 1.0, 0.5892014945222845, 1.0, 1.0, 0.5724158210426121, 1.0, 1.0, 0.9972168686025908, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.7832006398777178, 0.7832006398777175, 0.8025930434279738], 
reward next is 0.1974, 
noisyNet noise sample is [array([0.00059449], dtype=float32), 1.0224121]. 
=============================================
[2019-04-27 18:45:38,832] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[26.758287]
 [26.862652]
 [26.984207]
 [27.13667 ]
 [27.064323]], R is [[26.52608871]
 [26.47620583]
 [26.43766403]
 [26.41660118]
 [26.4081192 ]].
[2019-04-27 18:45:49,146] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8028833e-20 1.6269982e-14 9.8032920e-08 2.2487125e-19 9.9999988e-01], sum to 1.0000
[2019-04-27 18:45:49,154] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5276
[2019-04-27 18:45:49,162] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.95, 82.16666666666667, 1.0, 2.0, 0.1897618489221687, 1.0, 2.0, 0.1897618489221687, 1.0, 2.0, 0.3022257310976036, 6.9112, 6.9112, 121.94756008, 652704.9075419415, 652704.9075419415, 217287.8159148943], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3611400.0000, 
sim time next is 3612000.0000, 
raw observation next is [24.9, 81.33333333333334, 1.0, 2.0, 0.1870952769087447, 1.0, 2.0, 0.1870952769087447, 1.0, 2.0, 0.2980827039222886, 6.9112, 6.9112, 121.94756008, 645739.9060434449, 645739.9060434449, 216429.3963663828], 
processed observation next is [1.0, 0.8260869565217391, 0.47777777777777775, 0.8133333333333335, 1.0, 1.0, 0.032256282034219874, 1.0, 1.0, 0.032256282034219874, 1.0, 1.0, 0.12260337990286072, 0.0, 0.0, 0.8096049824067558, 0.23062139501551604, 0.23062139501551604, 0.4162103776276592], 
reward next is 0.5838, 
noisyNet noise sample is [array([-0.5371196], dtype=float32), 0.4821546]. 
=============================================
[2019-04-27 18:45:49,188] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[52.83791]
 [53.08942]
 [53.23941]
 [53.37551]
 [53.51408]], R is [[52.7478447 ]
 [52.80250549]
 [52.85548019]
 [52.90819168]
 [52.96074295]].
[2019-04-27 18:45:59,686] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-27 18:45:59,687] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 18:45:59,688] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 18:45:59,688] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:45:59,689] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 18:45:59,690] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 18:45:59,690] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:45:59,691] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:45:59,694] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:45:59,692] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 18:45:59,696] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:45:59,716] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run22
[2019-04-27 18:45:59,717] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run22
[2019-04-27 18:45:59,717] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run22
[2019-04-27 18:45:59,758] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run22
[2019-04-27 18:45:59,797] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run22
[2019-04-27 18:46:01,313] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0195115], dtype=float32), 0.060456313]
[2019-04-27 18:46:01,314] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [27.45, 40.0, 1.0, 2.0, 0.3121893545957574, 1.0, 2.0, 0.3121893545957574, 1.0, 2.0, 0.5126922035537748, 6.9112, 6.9112, 121.94756008, 1149824.676723173, 1149824.676723173, 259221.3561409434]
[2019-04-27 18:46:01,316] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 18:46:01,318] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [2.6800665e-19 5.8511281e-14 5.8460035e-08 4.9691221e-18 1.0000000e+00], sampled 0.058760130822938295
[2019-04-27 18:46:18,148] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0195115], dtype=float32), 0.060456313]
[2019-04-27 18:46:18,150] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [18.53333333333333, 89.66666666666666, 1.0, 2.0, 0.16, 1.0, 2.0, 0.16, 1.0, 2.0, 0.2, 6.9112, 6.9112, 121.94756008, 422166.4528911387, 422166.4528911387, 181036.3664050965]
[2019-04-27 18:46:18,151] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 18:46:18,153] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [6.6569535e-18 3.3359393e-09 7.0624401e-07 4.9733311e-17 9.9999928e-01], sampled 0.5137879334384245
[2019-04-27 18:46:31,644] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0195115], dtype=float32), 0.060456313]
[2019-04-27 18:46:31,645] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [23.00548826333333, 83.73744506, 1.0, 2.0, 0.1663556279711928, 1.0, 2.0, 0.1663556279711928, 1.0, 2.0, 0.266622452802894, 6.9112, 6.9112, 121.94756008, 589122.5152622246, 589122.5152622246, 209663.5098531653]
[2019-04-27 18:46:31,647] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:46:31,649] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [3.3946352e-19 7.2772578e-12 1.0414374e-07 3.9669469e-18 9.9999988e-01], sampled 0.3656199973323053
[2019-04-27 18:47:47,105] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 4609.0871 2894681934.3552 12.0000
[2019-04-27 18:47:47,573] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 4275.9573 2920152679.8633 33.0000
[2019-04-27 18:47:47,657] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 4489.4114 2940779206.0371 28.0000
[2019-04-27 18:47:47,681] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 4392.1813 2875918929.5409 8.0000
[2019-04-27 18:47:47,743] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 4510.8774 3107552039.9913 0.0000
[2019-04-27 18:47:48,759] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 525000, evaluation results [525000.0, 4510.87741702582, 3107552039.9912515, 0.0, 4609.087071503921, 2894681934.355178, 12.0, 4392.181277017246, 2875918929.540856, 8.0, 4489.411365254514, 2940779206.037067, 28.0, 4275.9572757857495, 2920152679.8633223, 33.0]
[2019-04-27 18:47:49,735] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0878891e-17 1.6118391e-10 3.5929122e-06 1.0069315e-17 9.9999642e-01], sum to 1.0000
[2019-04-27 18:47:49,749] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3127
[2019-04-27 18:47:49,753] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.7, 72.0, 1.0, 2.0, 0.1972206472171422, 1.0, 2.0, 0.1972206472171422, 1.0, 2.0, 0.3140026693627214, 6.9112, 6.9112, 121.94756008, 675226.8089619512, 675226.8089619512, 219692.8307066741], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3808800.0000, 
sim time next is 3809400.0000, 
raw observation next is [26.58333333333333, 72.33333333333333, 1.0, 2.0, 0.1949232931289673, 1.0, 2.0, 0.1949232931289673, 1.0, 2.0, 0.3103864869754305, 6.9112, 6.9112, 121.94756008, 668827.987141539, 668827.987141539, 218951.1612796206], 
processed observation next is [0.0, 0.08695652173913043, 0.5401234567901233, 0.7233333333333333, 1.0, 1.0, 0.04157534896305631, 1.0, 1.0, 0.04157534896305631, 1.0, 1.0, 0.13798310871928807, 0.0, 0.0, 0.8096049824067558, 0.23886713826483535, 0.23886713826483535, 0.4210599255377319], 
reward next is 0.5789, 
noisyNet noise sample is [array([-0.17496102], dtype=float32), 0.56626767]. 
=============================================
[2019-04-27 18:47:49,958] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.4492850e-17 3.1167875e-08 1.3894677e-06 3.2678290e-16 9.9999857e-01], sum to 1.0000
[2019-04-27 18:47:49,966] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5257
[2019-04-27 18:47:49,974] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.0, 76.5, 1.0, 2.0, 0.1927758683780793, 1.0, 2.0, 0.1927758683780793, 1.0, 2.0, 0.3069679260760209, 6.9112, 6.9112, 121.94756008, 661484.3142572959, 661484.3142572959, 218256.9867644882], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3814200.0000, 
sim time next is 3814800.0000, 
raw observation next is [26.0, 77.33333333333333, 1.0, 2.0, 0.1948174868678361, 1.0, 2.0, 0.1948174868678361, 1.0, 2.0, 0.3101686327635186, 6.9112, 6.9112, 121.94756008, 666661.3600304724, 666661.3600304724, 218911.9521887668], 
processed observation next is [0.0, 0.13043478260869565, 0.5185185185185185, 0.7733333333333333, 1.0, 1.0, 0.0414493891283763, 1.0, 1.0, 0.0414493891283763, 1.0, 1.0, 0.1377107909543982, 0.0, 0.0, 0.8096049824067558, 0.23809334286802586, 0.23809334286802586, 0.42098452343993614], 
reward next is 0.5790, 
noisyNet noise sample is [array([-1.0165879], dtype=float32), 0.29990345]. 
=============================================
[2019-04-27 18:47:52,620] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.5112722e-20 4.4588201e-11 4.3400289e-07 2.7538132e-19 9.9999952e-01], sum to 1.0000
[2019-04-27 18:47:52,632] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2130
[2019-04-27 18:47:52,637] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.5, 51.5, 1.0, 2.0, 0.2357658368901254, 1.0, 2.0, 0.2357658368901254, 1.0, 2.0, 0.3753468906863331, 6.9112, 6.9112, 121.94756008, 806123.3741686954, 806123.3741686954, 232634.7419328147], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3857400.0000, 
sim time next is 3858000.0000, 
raw observation next is [33.66666666666666, 51.0, 1.0, 2.0, 0.2371641200587849, 1.0, 2.0, 0.2371641200587849, 1.0, 2.0, 0.3775730030297427, 6.9112, 6.9112, 121.94756008, 810906.8700496632, 810906.8700496632, 233119.9854605184], 
processed observation next is [0.0, 0.6521739130434783, 0.8024691358024688, 0.51, 1.0, 1.0, 0.09186204768902964, 1.0, 1.0, 0.09186204768902964, 1.0, 1.0, 0.22196625378717838, 0.0, 0.0, 0.8096049824067558, 0.2896095964463083, 0.2896095964463083, 0.44830766434715075], 
reward next is 0.5517, 
noisyNet noise sample is [array([1.2759048], dtype=float32), -0.9401743]. 
=============================================
[2019-04-27 18:47:52,645] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[52.806393]
 [52.835384]
 [52.841244]
 [52.861053]
 [52.862682]], R is [[52.80360794]
 [52.82819748]
 [52.85510254]
 [52.88064575]
 [52.90537262]].
[2019-04-27 18:47:53,676] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.8320324e-18 9.2277901e-11 3.0602298e-08 1.8278576e-17 1.0000000e+00], sum to 1.0000
[2019-04-27 18:47:53,686] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5897
[2019-04-27 18:47:53,693] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.7, 89.0, 1.0, 2.0, 0.2505157321742116, 1.0, 2.0, 0.2505157321742116, 1.0, 2.0, 0.3988292043491513, 6.9112, 6.9112, 121.94756008, 856583.943937052, 856583.943937052, 237808.3941852691], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3884400.0000, 
sim time next is 3885000.0000, 
raw observation next is [26.58333333333333, 89.83333333333334, 1.0, 2.0, 0.24359381936969, 1.0, 2.0, 0.24359381936969, 1.0, 2.0, 0.3878092937333911, 6.911200000000001, 6.9112, 121.94756008, 832903.1137817276, 832903.1137817272, 235365.3360837373], 
processed observation next is [0.0, 1.0, 0.5401234567901233, 0.8983333333333334, 1.0, 1.0, 0.09951645163058333, 1.0, 1.0, 0.09951645163058333, 1.0, 1.0, 0.23476161716673888, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.29746539777918846, 0.2974653977791883, 0.4526256463148794], 
reward next is 0.5474, 
noisyNet noise sample is [array([-0.7479963], dtype=float32), 1.1551908]. 
=============================================
[2019-04-27 18:47:53,704] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[44.058613]
 [44.092632]
 [44.133976]
 [44.185013]
 [44.24963 ]], R is [[44.14303207]
 [44.24427795]
 [44.34133911]
 [44.4342804 ]
 [44.52331161]].
[2019-04-27 18:47:57,110] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.7361471e-22 4.4130866e-13 2.0430901e-09 1.5844008e-20 1.0000000e+00], sum to 1.0000
[2019-04-27 18:47:57,118] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4210
[2019-04-27 18:47:57,122] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.73333333333333, 65.16666666666667, 1.0, 2.0, 0.2422080855734243, 1.0, 2.0, 0.2422080855734243, 1.0, 2.0, 0.385603160399537, 6.911200000000001, 6.9112, 121.94756008, 828162.4128695498, 828162.4128695494, 234879.4634362319], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3949800.0000, 
sim time next is 3950400.0000, 
raw observation next is [30.56666666666667, 66.33333333333334, 1.0, 2.0, 0.2412982755699401, 1.0, 2.0, 0.2412982755699401, 1.0, 2.0, 0.384154713243547, 6.9112, 6.9112, 121.94756008, 825049.8994616028, 825049.8994616028, 234561.0449809902], 
processed observation next is [0.0, 0.7391304347826086, 0.6876543209876544, 0.6633333333333334, 1.0, 1.0, 0.09678366139278584, 1.0, 1.0, 0.09678366139278584, 1.0, 1.0, 0.23019339155443372, 0.0, 0.0, 0.8096049824067558, 0.29466067837914384, 0.29466067837914384, 0.4510789326557504], 
reward next is 0.5489, 
noisyNet noise sample is [array([0.17500645], dtype=float32), -0.08037952]. 
=============================================
[2019-04-27 18:48:05,214] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.4503133e-26 6.3956172e-24 7.9864396e-12 5.5757793e-25 1.0000000e+00], sum to 1.0000
[2019-04-27 18:48:05,225] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8552
[2019-04-27 18:48:05,229] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.13333333333333, 67.33333333333334, 1.0, 2.0, 0.6197753297178898, 1.0, 2.0, 0.6197753297178898, 1.0, 2.0, 0.9867025095842041, 6.911199999999999, 6.9112, 121.94756008, 2120828.143791061, 2120828.143791062, 406255.4428409364], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4120800.0000, 
sim time next is 4121400.0000, 
raw observation next is [29.16666666666666, 66.66666666666666, 1.0, 2.0, 0.6137155921572857, 1.0, 2.0, 0.6137155921572857, 1.0, 2.0, 0.9770552100358478, 6.9112, 6.9112, 121.94756008, 2100067.762230188, 2100067.762230188, 402899.1845077186], 
processed observation next is [1.0, 0.6956521739130435, 0.6358024691358023, 0.6666666666666665, 1.0, 1.0, 0.5401376097110543, 1.0, 1.0, 0.5401376097110543, 1.0, 1.0, 0.9713190125448098, 0.0, 0.0, 0.8096049824067558, 0.7500242007964958, 0.7500242007964958, 0.774806124053305], 
reward next is 0.2252, 
noisyNet noise sample is [array([0.43738288], dtype=float32), -0.8566422]. 
=============================================
[2019-04-27 18:48:16,001] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0397638e-23 1.0000000e+00 4.7634228e-15 1.9350297e-21 2.2878769e-17], sum to 1.0000
[2019-04-27 18:48:16,008] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4803
[2019-04-27 18:48:16,012] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 74.0, 1.0, 2.0, 0.6137890269806593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 706835.9890686683, 706835.9890686683, 160824.697162276], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4312200.0000, 
sim time next is 4312800.0000, 
raw observation next is [27.0, 74.0, 1.0, 2.0, 0.6130068705476998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 705931.64816092, 705931.64816092, 160687.686198488], 
processed observation next is [1.0, 0.9565217391304348, 0.5555555555555556, 0.74, 1.0, 1.0, 0.5392938935091663, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2521184457717571, 0.2521184457717571, 0.30901478115093844], 
reward next is 0.6910, 
noisyNet noise sample is [array([-0.16476516], dtype=float32), -1.6518594]. 
=============================================
[2019-04-27 18:48:29,650] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7372397e-34 1.0000000e+00 1.7276030e-28 1.8924534e-31 1.5976228e-31], sum to 1.0000
[2019-04-27 18:48:29,658] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1206
[2019-04-27 18:48:29,662] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.73333333333333, 94.0, 1.0, 2.0, 0.5880721670337664, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 682083.2105455102, 682083.2105455102, 156607.2836268769], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4562400.0000, 
sim time next is 4563000.0000, 
raw observation next is [23.6, 94.0, 1.0, 2.0, 0.5809474561304158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 675712.2408315028, 675712.2408315028, 155478.8704360882], 
processed observation next is [0.0, 0.8260869565217391, 0.4296296296296297, 0.94, 1.0, 1.0, 0.5011279239647807, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2413258002969653, 0.2413258002969653, 0.2989978277617081], 
reward next is 0.7010, 
noisyNet noise sample is [array([-1.3941702], dtype=float32), 1.1536725]. 
=============================================
[2019-04-27 18:48:29,672] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[68.54159]
 [68.48824]
 [68.46819]
 [68.47668]
 [68.48007]], R is [[68.61425018]
 [68.62693787]
 [68.63749695]
 [68.6463089 ]
 [68.65488434]].
[2019-04-27 18:48:33,103] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.4952600e-19 1.0000000e+00 2.2367240e-14 3.4120264e-15 5.6321368e-15], sum to 1.0000
[2019-04-27 18:48:33,112] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3808
[2019-04-27 18:48:33,122] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1390165.94931411 W.
[2019-04-27 18:48:33,128] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.33333333333334, 72.0, 1.0, 2.0, 0.6096377033745741, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9705630782862052, 6.911199999999999, 6.9112, 121.9260426156618, 1390165.94931411, 1390165.949314111, 297627.1065624211], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4620000.0000, 
sim time next is 4620600.0000, 
raw observation next is [27.5, 72.5, 1.0, 2.0, 0.5971036863194401, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9506085149299944, 6.9112, 6.9112, 121.9260426156618, 1361559.038329423, 1361559.038329423, 292734.3190648487], 
processed observation next is [1.0, 0.4782608695652174, 0.5740740740740741, 0.725, 1.0, 1.0, 0.5203615313326668, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.938260643662493, 0.0, 0.0, 0.8094621288201359, 0.48627108511765105, 0.48627108511765105, 0.5629506135862475], 
reward next is 0.4370, 
noisyNet noise sample is [array([-0.7484021], dtype=float32), -0.9295392]. 
=============================================
[2019-04-27 18:48:39,211] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-27 18:48:39,216] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 18:48:39,216] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:48:39,216] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 18:48:39,217] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 18:48:39,217] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 18:48:39,218] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:48:39,218] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:48:39,219] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 18:48:39,220] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:48:39,222] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:48:39,234] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run23
[2019-04-27 18:48:39,234] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run23
[2019-04-27 18:48:39,252] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run23
[2019-04-27 18:48:39,270] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run23
[2019-04-27 18:48:39,308] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run23
[2019-04-27 18:49:37,757] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0346008], dtype=float32), 0.061227642]
[2019-04-27 18:49:37,759] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [25.83687073333333, 85.84278978666666, 1.0, 2.0, 0.6278755079307392, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 715562.0915354703, 715562.0915354703, 162938.8259413578]
[2019-04-27 18:49:37,760] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 18:49:37,764] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [2.3320586e-24 1.0000000e+00 7.9433013e-18 9.7248347e-19 2.8399049e-21], sampled 0.472113760858338
[2019-04-27 18:49:42,074] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0346008], dtype=float32), 0.061227642]
[2019-04-27 18:49:42,076] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [22.87941568, 93.58486915, 1.0, 2.0, 0.5117861638243816, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 605071.8369364287, 605071.8369364287, 144554.3191484329]
[2019-04-27 18:49:42,077] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 18:49:42,080] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [5.3334945e-24 1.0000000e+00 1.9424960e-17 2.1811835e-18 5.7117851e-21], sampled 0.9101403100083192
[2019-04-27 18:49:47,264] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0346008], dtype=float32), 0.061227642]
[2019-04-27 18:49:47,265] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [23.17379121333333, 95.31396501333333, 1.0, 2.0, 0.5829484073878389, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 678489.4458153008, 678489.4458153008, 155838.2953052125]
[2019-04-27 18:49:47,267] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:49:47,272] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [8.3998595e-24 1.0000000e+00 2.3165593e-17 2.8239965e-18 7.4143042e-21], sampled 0.819387125671903
[2019-04-27 18:50:05,283] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0346008], dtype=float32), 0.061227642]
[2019-04-27 18:50:05,285] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [28.35, 61.5, 1.0, 2.0, 0.5469287657839469, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 639820.8303352107, 639820.8303352103, 149959.6585460653]
[2019-04-27 18:50:05,285] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 18:50:05,287] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [2.23116488e-24 1.00000000e+00 1.17440514e-17 2.15856958e-18
 7.72319636e-21], sampled 0.3654750163520867
[2019-04-27 18:50:17,358] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.0346008], dtype=float32), 0.061227642]
[2019-04-27 18:50:17,359] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [29.13333333333333, 76.0, 1.0, 2.0, 0.9097689819543316, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.9259930075915, 1752235.972851708, 1752235.972851708, 359088.1054256462]
[2019-04-27 18:50:17,362] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 18:50:17,364] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.6953630e-15 1.0000000e+00 4.3206433e-10 8.8707464e-10 7.1135403e-11], sampled 0.3362721526623319
[2019-04-27 18:50:17,365] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1752235.972851708 W.
[2019-04-27 18:50:25,217] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.0346008], dtype=float32), 0.061227642]
[2019-04-27 18:50:25,219] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [19.0, 83.33333333333334, 1.0, 2.0, 0.2996556792943322, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 381543.6578147153, 381543.6578147148, 115529.5558396505]
[2019-04-27 18:50:25,220] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 18:50:25,221] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [2.2301607e-24 1.0000000e+00 5.7077642e-18 7.1086284e-19 1.7420492e-21], sampled 0.4262626586234385
[2019-04-27 18:50:27,325] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8921.6064 2121124801.8085 428.0000
[2019-04-27 18:50:27,338] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8586.4774 2249009958.7371 544.0000
[2019-04-27 18:50:27,528] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8101.6416 2445697253.4616 730.0000
[2019-04-27 18:50:27,564] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8769.6839 2170869825.1832 491.0000
[2019-04-27 18:50:27,754] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8698.2750 2196087398.8262 572.0000
[2019-04-27 18:50:28,770] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 550000, evaluation results [550000.0, 8101.641554265157, 2445697253.461555, 730.0, 8769.683864091503, 2170869825.1831613, 491.0, 8921.606386011026, 2121124801.8085325, 428.0, 8586.47738570905, 2249009958.737127, 544.0, 8698.27500627268, 2196087398.8262196, 572.0]
[2019-04-27 18:50:31,399] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2118963e-22 1.0000000e+00 2.1670461e-15 5.1782063e-15 2.6207220e-18], sum to 1.0000
[2019-04-27 18:50:31,410] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1134
[2019-04-27 18:50:31,415] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.95, 93.5, 1.0, 2.0, 0.7403494323221568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 856215.0477920956, 856215.0477920956, 184546.8872942495], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4779000.0000, 
sim time next is 4779600.0000, 
raw observation next is [23.93333333333333, 93.33333333333334, 1.0, 2.0, 0.6723871681430491, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 778241.8569937038, 778241.8569937033, 171567.5850664831], 
processed observation next is [1.0, 0.30434782608695654, 0.4419753086419752, 0.9333333333333335, 1.0, 1.0, 0.6099847239798204, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2779435203548942, 0.27794352035489406, 0.3299376635893906], 
reward next is 0.6701, 
noisyNet noise sample is [array([0.21053863], dtype=float32), -1.0416639]. 
=============================================
[2019-04-27 18:50:41,330] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5857615e-20 1.0000000e+00 6.3457657e-19 2.6278597e-18 2.6843462e-17], sum to 1.0000
[2019-04-27 18:50:41,337] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7196
[2019-04-27 18:50:41,344] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1405047.07477343 W.
[2019-04-27 18:50:41,349] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.6161576292249649, 1.0, 1.0, 0.6161576292249649, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9256911864678, 1405047.07477343, 1405047.074773429, 273643.6232932839], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4971600.0000, 
sim time next is 4972200.0000, 
raw observation next is [25.7, 90.0, 1.0, 2.0, 0.5885719853047331, 1.0, 2.0, 0.5885719853047331, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260425085003, 1342087.391675296, 1342087.391675297, 264133.8835364316], 
processed observation next is [1.0, 0.5652173913043478, 0.5074074074074074, 0.9, 1.0, 1.0, 0.5102047444103965, 1.0, 1.0, 0.5102047444103965, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.809462128108695, 0.47931692559832, 0.47931692559832034, 0.5079497760315992], 
reward next is 0.4921, 
noisyNet noise sample is [array([0.9539139], dtype=float32), -1.8980144]. 
=============================================
[2019-04-27 18:50:53,778] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.2800555e-19 3.7546756e-15 1.0679401e-10 8.1769325e-10 1.0000000e+00], sum to 1.0000
[2019-04-27 18:50:53,788] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4110
[2019-04-27 18:50:53,793] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.16666666666667, 88.16666666666667, 1.0, 2.0, 0.4787564659832322, 1.0, 2.0, 0.4787564659832322, 1.0, 2.0, 0.762195885854868, 6.9112, 6.9112, 121.94756008, 1637829.012547149, 1637829.012547149, 333284.870538417], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5217000.0000, 
sim time next is 5217600.0000, 
raw observation next is [26.33333333333334, 87.33333333333334, 1.0, 2.0, 0.4736207301985724, 1.0, 2.0, 0.4736207301985724, 1.0, 2.0, 0.7540196272264519, 6.9112, 6.9112, 121.94756008, 1620243.697839288, 1620243.697839288, 330830.5660041861], 
processed observation next is [1.0, 0.391304347826087, 0.5308641975308644, 0.8733333333333334, 1.0, 1.0, 0.3733580121411576, 1.0, 1.0, 0.3733580121411576, 1.0, 1.0, 0.6925245340330649, 0.0, 0.0, 0.8096049824067558, 0.5786584635140315, 0.5786584635140315, 0.6362126269311271], 
reward next is 0.3638, 
noisyNet noise sample is [array([-0.2161866], dtype=float32), -0.069335915]. 
=============================================
[2019-04-27 18:50:55,988] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.5785236e-25 1.0000000e+00 2.4442692e-21 4.0057158e-22 6.6460515e-17], sum to 1.0000
[2019-04-27 18:50:55,997] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7639
[2019-04-27 18:50:56,006] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.43333333333333, 85.33333333333333, 1.0, 2.0, 0.7417282725855527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 845386.631788012, 845386.631788012, 184196.9815548106], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5254800.0000, 
sim time next is 5255400.0000, 
raw observation next is [27.31666666666667, 85.66666666666667, 1.0, 2.0, 0.729546243650052, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 831494.6027075688, 831494.6027075683, 181815.5539467083], 
processed observation next is [1.0, 0.8260869565217391, 0.5672839506172841, 0.8566666666666667, 1.0, 1.0, 0.6780312424405381, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.296962358109846, 0.29696235810984584, 0.34964529605136213], 
reward next is 0.6504, 
noisyNet noise sample is [array([1.0708357], dtype=float32), 0.0005002663]. 
=============================================
[2019-04-27 18:51:01,165] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2252200e-27 1.0000000e+00 5.8617944e-22 9.3101888e-23 1.3734906e-24], sum to 1.0000
[2019-04-27 18:51:01,176] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9210
[2019-04-27 18:51:01,184] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.5, 84.0, 1.0, 2.0, 0.603065139793368, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 693931.8918907859, 693931.8918907859, 158931.3693371745], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5360400.0000, 
sim time next is 5361000.0000, 
raw observation next is [25.41666666666666, 84.5, 1.0, 2.0, 0.6024872924930041, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 693371.7295733386, 693371.7295733386, 158836.3568353525], 
processed observation next is [1.0, 0.043478260869565216, 0.49691358024691334, 0.845, 1.0, 1.0, 0.5267705863011954, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.24763276056190667, 0.24763276056190667, 0.3054545323756779], 
reward next is 0.6945, 
noisyNet noise sample is [array([-1.5396494], dtype=float32), -0.56055903]. 
=============================================
[2019-04-27 18:51:01,199] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[57.046352]
 [57.13202 ]
 [57.201477]
 [57.25157 ]
 [57.276096]], R is [[56.97695923]
 [57.10155106]
 [57.22484589]
 [57.3467865 ]
 [57.46731567]].
[2019-04-27 18:51:11,922] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.2883199e-26 1.0000000e+00 5.6644637e-20 9.4410342e-19 4.9830695e-20], sum to 1.0000
[2019-04-27 18:51:11,930] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6329
[2019-04-27 18:51:11,936] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.83333333333334, 90.66666666666667, 1.0, 2.0, 0.6871937473827457, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 783199.0154510047, 783199.0154510047, 173733.5046886436], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5528400.0000, 
sim time next is 5529000.0000, 
raw observation next is [25.81666666666667, 90.83333333333334, 1.0, 2.0, 0.6874179723781133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 783454.6966841902, 783454.6966841897, 173775.4831784839], 
processed observation next is [1.0, 1.0, 0.5117283950617285, 0.9083333333333334, 1.0, 1.0, 0.627878538545373, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2798052488157822, 0.27980524881578206, 0.3341836214970844], 
reward next is 0.6658, 
noisyNet noise sample is [array([1.0337797], dtype=float32), -1.3089042]. 
=============================================
[2019-04-27 18:51:11,951] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[66.973625]
 [66.94814 ]
 [66.923134]
 [66.89385 ]
 [66.87401 ]], R is [[67.00028992]
 [66.99617767]
 [66.99230957]
 [66.98826599]
 [66.98371887]].
[2019-04-27 18:51:14,612] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1473570e-08 9.9967635e-01 1.4611031e-06 1.9299515e-04 1.2929691e-04], sum to 1.0000
[2019-04-27 18:51:14,624] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2712
[2019-04-27 18:51:14,633] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2216700.808647653 W.
[2019-04-27 18:51:14,638] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.9, 75.0, 1.0, 2.0, 0.6687861307804244, 1.0, 2.0, 0.6477577273666468, 1.0, 2.0, 0.9977734948820727, 6.911200000000001, 6.9112, 121.94756008, 2216700.808647653, 2216700.808647653, 420957.695644584], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5578200.0000, 
sim time next is 5578800.0000, 
raw observation next is [30.1, 74.66666666666666, 1.0, 2.0, 0.6774656922475767, 1.0, 2.0, 0.6520975081002232, 1.0, 2.0, 0.9977734948820727, 6.911200000000001, 6.9112, 121.94756008, 2231570.590312704, 2231570.590312704, 423239.6131965511], 
processed observation next is [1.0, 0.5652173913043478, 0.6703703703703704, 0.7466666666666666, 1.0, 1.0, 0.6160305860090198, 1.0, 1.0, 0.5858303667859799, 1.0, 1.0, 0.9972168686025908, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.7969894965402514, 0.7969894965402514, 0.8139223330702906], 
reward next is 0.1861, 
noisyNet noise sample is [array([0.19559516], dtype=float32), 1.6334362]. 
=============================================
[2019-04-27 18:51:18,598] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.0374265e-33 1.0000000e+00 2.4162551e-27 1.6880436e-24 2.5180885e-30], sum to 1.0000
[2019-04-27 18:51:18,616] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3051
[2019-04-27 18:51:18,620] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.1, 83.33333333333334, 1.0, 2.0, 0.6899704543210551, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 786365.2690658228, 786365.2690658228, 174254.6367727944], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5654400.0000, 
sim time next is 5655000.0000, 
raw observation next is [27.25, 82.66666666666667, 1.0, 2.0, 0.6920683116177165, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999998, 6.9112, 121.9260426156618, 788757.4452950526, 788757.4452950535, 174648.6955599671], 
processed observation next is [0.0, 0.43478260869565216, 0.5648148148148148, 0.8266666666666667, 1.0, 1.0, 0.6334146566877578, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -1.7763568394002506e-16, 0.0, 0.8094621288201359, 0.28169908760537593, 0.28169908760537626, 0.33586287607685983], 
reward next is 0.6641, 
noisyNet noise sample is [array([0.15322182], dtype=float32), -0.09483765]. 
=============================================
[2019-04-27 18:51:18,636] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[66.84773 ]
 [66.842415]
 [66.841225]
 [66.84979 ]
 [66.87011 ]], R is [[66.85536957]
 [66.85170746]
 [66.84880829]
 [66.84663391]
 [66.84532166]].
[2019-04-27 18:51:19,083] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-27 18:51:19,087] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 18:51:19,088] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:51:19,089] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 18:51:19,090] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 18:51:19,092] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 18:51:19,093] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:51:19,094] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:51:19,091] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 18:51:19,094] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:51:19,096] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:51:19,114] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run24
[2019-04-27 18:51:19,135] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run24
[2019-04-27 18:51:19,136] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run24
[2019-04-27 18:51:19,176] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run24
[2019-04-27 18:51:19,196] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run24
[2019-04-27 18:51:43,442] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.03704655], dtype=float32), 0.051672067]
[2019-04-27 18:51:43,445] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [20.36666666666667, 68.66666666666667, 1.0, 2.0, 0.2852464852466309, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 365305.9289177124, 365305.9289177124, 113773.0054663874]
[2019-04-27 18:51:43,446] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 18:51:43,449] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.44219675e-33 1.00000000e+00 4.93842217e-30 1.54462062e-26
 1.78913576e-32], sampled 0.5703009243572991
[2019-04-27 18:51:57,770] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03704655], dtype=float32), 0.051672067]
[2019-04-27 18:51:57,771] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [30.1, 60.66666666666667, 1.0, 2.0, 0.6300800676406193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 718075.7080037781, 718075.7080037781, 163329.076114537]
[2019-04-27 18:51:57,773] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 18:51:57,775] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [2.5546475e-33 1.0000000e+00 2.9331629e-28 7.0847026e-25 9.3919493e-30], sampled 0.1142829866327626
[2019-04-27 18:52:19,241] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03704655], dtype=float32), 0.051672067]
[2019-04-27 18:52:19,243] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [29.586248505, 52.11808901000001, 1.0, 2.0, 0.969702785926485, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.135168336446071, 6.9112, 121.9251515082692, 1252989.476261276, 1138298.508173459, 235245.3374090293]
[2019-04-27 18:52:19,243] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 18:52:19,246] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [2.0791339e-25 1.0000000e+00 1.0578933e-20 1.1325337e-17 2.8252530e-21], sampled 0.07503996595223505
[2019-04-27 18:52:19,962] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03704655], dtype=float32), 0.051672067]
[2019-04-27 18:52:19,963] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [25.132614205, 75.91153009499999, 1.0, 2.0, 0.5175611293600602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 614342.603869879, 614342.603869879, 145569.5790148146]
[2019-04-27 18:52:19,965] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 18:52:19,969] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [4.8175983e-33 1.0000000e+00 9.6728399e-29 2.3324310e-25 1.6386640e-30], sampled 0.12983899578284575
[2019-04-27 18:53:03,529] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.03704655], dtype=float32), 0.051672067]
[2019-04-27 18:53:03,530] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [17.06666666666667, 94.33333333333334, 1.0, 2.0, 0.2686294363556893, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 344548.0959569131, 344548.0959569127, 111787.10015539]
[2019-04-27 18:53:03,530] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 18:53:03,533] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [2.2577735e-33 1.0000000e+00 2.4826074e-30 8.9603367e-27 6.9674959e-33], sampled 0.3791566401999229
[2019-04-27 18:53:07,353] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8100.1096 2445526021.4990 742.0000
[2019-04-27 18:53:07,473] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8585.5492 2248930358.0590 549.0000
[2019-04-27 18:53:07,777] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8920.3704 2120862585.5759 431.0000
[2019-04-27 18:53:07,801] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8699.6958 2195765980.5083 572.0000
[2019-04-27 18:53:07,917] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8771.7906 2170663287.6069 493.0000
[2019-04-27 18:53:08,935] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 575000, evaluation results [575000.0, 8100.1095568464825, 2445526021.49902, 742.0, 8771.790610415588, 2170663287.606867, 493.0, 8920.370379725478, 2120862585.5759194, 431.0, 8585.549245936736, 2248930358.0589566, 549.0, 8699.695801804299, 2195765980.5083284, 572.0]
[2019-04-27 18:53:09,907] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.8135571e-32 1.0000000e+00 5.1672828e-27 9.4352860e-24 5.8499592e-28], sum to 1.0000
[2019-04-27 18:53:09,921] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6285
[2019-04-27 18:53:09,927] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.56666666666667, 80.66666666666667, 1.0, 2.0, 0.7559159295561809, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 861566.1377543638, 861566.1377543638, 187004.8005175037], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5685600.0000, 
sim time next is 5686200.0000, 
raw observation next is [28.4, 82.0, 1.0, 2.0, 0.759925763644112, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 866138.9867475993, 866138.9867475993, 187804.2652090203], 
processed observation next is [0.0, 0.8260869565217391, 0.6074074074074074, 0.82, 1.0, 1.0, 0.7141973376715619, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3093353524098569, 0.3093353524098569, 0.36116204847888517], 
reward next is 0.6388, 
noisyNet noise sample is [array([-0.1257925], dtype=float32), 0.82841766]. 
=============================================
[2019-04-27 18:53:10,896] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.1085238e-27 1.0000000e+00 5.5907576e-23 5.3111288e-21 2.0562992e-25], sum to 1.0000
[2019-04-27 18:53:10,904] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3232
[2019-04-27 18:53:10,908] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 94.0, 1.0, 2.0, 0.5505829811972482, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 647286.0818777919, 647286.0818777919, 150696.1030139421], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5702400.0000, 
sim time next is 5703000.0000, 
raw observation next is [23.0, 94.33333333333334, 1.0, 2.0, 0.549012748052397, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 646126.5283136271, 646126.5283136271, 150465.2529770861], 
processed observation next is [0.0, 0.0, 0.4074074074074074, 0.9433333333333335, 1.0, 1.0, 0.46311041434809164, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.23075947439772396, 0.23075947439772396, 0.28935625572516555], 
reward next is 0.7106, 
noisyNet noise sample is [array([0.22519025], dtype=float32), -0.68787766]. 
=============================================
[2019-04-27 18:53:10,919] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[59.867126]
 [60.127083]
 [60.18432 ]
 [60.248676]
 [60.310253]], R is [[59.79677582]
 [59.90900803]
 [60.01926804]
 [60.12773895]
 [60.23462296]].
[2019-04-27 18:53:11,414] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.0942044e-32 1.0000000e+00 3.8196453e-28 1.2502952e-25 2.1921307e-30], sum to 1.0000
[2019-04-27 18:53:11,430] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4202
[2019-04-27 18:53:11,433] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.03333333333333, 95.5, 1.0, 2.0, 0.4576452370001726, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 556706.6554178656, 556706.6554178651, 136762.3841161644], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5721000.0000, 
sim time next is 5721600.0000, 
raw observation next is [21.06666666666667, 95.0, 1.0, 2.0, 0.4575245076001414, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 556799.0029587015, 556799.0029587015, 136751.395700289], 
processed observation next is [0.0, 0.21739130434782608, 0.3358024691358026, 0.95, 1.0, 1.0, 0.3541958423811207, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1988567867709648, 0.1988567867709648, 0.2629834532697865], 
reward next is 0.7370, 
noisyNet noise sample is [array([0.00032656], dtype=float32), 0.6744705]. 
=============================================
[2019-04-27 18:53:18,239] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.9428904e-22 1.0000000e+00 2.6608844e-19 1.4165921e-17 3.3917459e-21], sum to 1.0000
[2019-04-27 18:53:18,247] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3211
[2019-04-27 18:53:18,251] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.7, 51.0, 1.0, 2.0, 0.3873136886792533, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 479378.4566022526, 479378.4566022526, 126834.5414026778], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5855400.0000, 
sim time next is 5856000.0000, 
raw observation next is [26.53333333333333, 52.33333333333333, 1.0, 2.0, 0.3916736507105691, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 484144.8105067955, 484144.810506796, 127425.8460657811], 
processed observation next is [1.0, 0.782608695652174, 0.5382716049382715, 0.5233333333333333, 1.0, 1.0, 0.27580196513162986, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1729088608952841, 0.1729088608952843, 0.24504970397265596], 
reward next is 0.7550, 
noisyNet noise sample is [array([-0.64937407], dtype=float32), 0.46484268]. 
=============================================
[2019-04-27 18:53:18,264] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[47.788124]
 [47.754845]
 [47.15007 ]
 [45.936413]
 [45.710262]], R is [[48.76284409]
 [49.03130341]
 [49.29799271]
 [49.5634079 ]
 [49.82787323]].
[2019-04-27 18:53:19,524] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.1368496e-30 1.0000000e+00 8.8726878e-26 1.1652585e-22 1.3134540e-28], sum to 1.0000
[2019-04-27 18:53:19,536] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7032
[2019-04-27 18:53:19,540] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.1, 68.0, 1.0, 2.0, 0.422017560661073, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 518759.2091526674, 518759.209152667, 131663.7134801377], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5864400.0000, 
sim time next is 5865000.0000, 
raw observation next is [23.91666666666667, 69.0, 1.0, 2.0, 0.420687845278886, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 517346.2829370839, 517346.2829370839, 131477.4213049434], 
processed observation next is [1.0, 0.9130434782608695, 0.4413580246913582, 0.69, 1.0, 1.0, 0.3103426729510548, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.18476652962038712, 0.18476652962038712, 0.2528411948171988], 
reward next is 0.7472, 
noisyNet noise sample is [array([-1.2748215], dtype=float32), -0.97785264]. 
=============================================
[2019-04-27 18:53:19,562] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[68.97288 ]
 [69.04084 ]
 [69.00982 ]
 [68.702354]
 [68.7656  ]], R is [[69.14191437]
 [69.19729614]
 [69.2518158 ]
 [69.30519104]
 [69.35703278]].
[2019-04-27 18:53:21,183] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.9240131e-30 1.0000000e+00 2.1775331e-26 8.2151154e-23 1.3807523e-29], sum to 1.0000
[2019-04-27 18:53:21,190] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3826
[2019-04-27 18:53:21,195] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.91666666666667, 73.66666666666667, 1.0, 2.0, 0.4039883387855939, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 505409.3781613049, 505409.3781613044, 129279.7204786983], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5903400.0000, 
sim time next is 5904000.0000, 
raw observation next is [22.1, 73.0, 1.0, 2.0, 0.3837821640829172, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 479625.7139215429, 479625.7139215425, 126441.2073877151], 
processed observation next is [1.0, 0.34782608695652173, 0.3740740740740741, 0.73, 1.0, 1.0, 0.2664073381939491, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.17129489782912247, 0.17129489782912233, 0.24315616805329826], 
reward next is 0.7568, 
noisyNet noise sample is [array([0.579358], dtype=float32), -0.31296495]. 
=============================================
[2019-04-27 18:53:21,209] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[66.181625]
 [66.23354 ]
 [66.30082 ]
 [66.24409 ]
 [66.30856 ]], R is [[66.35683441]
 [66.44465637]
 [66.53474426]
 [66.62957001]
 [66.72442627]].
[2019-04-27 18:53:22,319] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.2376509e-13 9.9999046e-01 8.7226397e-08 9.2332893e-06 2.3781520e-07], sum to 1.0000
[2019-04-27 18:53:22,327] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0160
[2019-04-27 18:53:22,334] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1393354.790147364 W.
[2019-04-27 18:53:22,340] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.4, 44.0, 1.0, 2.0, 0.5794227879068821, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9364506494037131, 6.911199999999999, 6.9112, 121.9256756621079, 1393354.790147364, 1393354.790147364, 283987.8782627614], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5928000.0000, 
sim time next is 5928600.0000, 
raw observation next is [29.35, 44.5, 1.0, 2.0, 0.5651706136489074, 0.0, 2.0, 0.0, 1.0, 2.0, 0.911888770687714, 6.911200000000001, 6.9112, 121.9260425037664, 1355028.977505339, 1355028.977505338, 278805.9110873231], 
processed observation next is [1.0, 0.6086956521739131, 0.6425925925925926, 0.445, 1.0, 1.0, 0.4823459686296517, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8898609633596424, 8.881784197001253e-17, 0.0, 0.8094621280772668, 0.4839389205376211, 0.48393892053762066, 0.5361652136294675], 
reward next is 0.4638, 
noisyNet noise sample is [array([0.09651349], dtype=float32), -1.1954416]. 
=============================================
[2019-04-27 18:53:31,924] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.5977969e-21 9.9999881e-01 5.2606988e-11 1.2073783e-06 2.2232126e-14], sum to 1.0000
[2019-04-27 18:53:31,934] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5347
[2019-04-27 18:53:31,939] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.36666666666667, 57.66666666666666, 1.0, 2.0, 0.5354983371146802, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 623662.4829132595, 623662.4829132591, 147972.0500331694], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6111600.0000, 
sim time next is 6112200.0000, 
raw observation next is [29.23333333333333, 58.33333333333334, 1.0, 2.0, 0.5407443192756752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 629779.0875301999, 629779.0875301999, 148824.9953075876], 
processed observation next is [1.0, 0.7391304347826086, 0.6382716049382715, 0.5833333333333335, 1.0, 1.0, 0.4532670467567562, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2249211026893571, 0.2249211026893571, 0.2862019140530531], 
reward next is 0.7138, 
noisyNet noise sample is [array([0.40519443], dtype=float32), 1.4273903]. 
=============================================
[2019-04-27 18:53:32,379] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.7297741e-26 1.0000000e+00 1.9402269e-20 3.5663151e-17 3.5323089e-22], sum to 1.0000
[2019-04-27 18:53:32,386] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3858
[2019-04-27 18:53:32,393] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.06666666666667, 72.0, 1.0, 2.0, 0.5383302013417816, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 634940.2816062727, 634940.2816062727, 148769.017551175], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6129600.0000, 
sim time next is 6130200.0000, 
raw observation next is [25.98333333333333, 72.5, 1.0, 2.0, 0.5373936573368905, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 633864.635811534, 633864.6358115335, 148617.3536984053], 
processed observation next is [1.0, 0.9565217391304348, 0.5179012345679012, 0.725, 1.0, 1.0, 0.4492781634962982, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.22638022707554786, 0.2263802270755477, 0.285802603266164], 
reward next is 0.7142, 
noisyNet noise sample is [array([0.01542084], dtype=float32), -1.9951676]. 
=============================================
[2019-04-27 18:53:38,484] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.5145610e-32 1.0000000e+00 2.8699513e-29 1.2790143e-23 3.5974788e-34], sum to 1.0000
[2019-04-27 18:53:38,490] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5752
[2019-04-27 18:53:38,496] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 87.0, 1.0, 2.0, 0.4715221460329455, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 569737.3137958134, 569737.3137958129, 138747.1275013859], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6237000.0000, 
sim time next is 6237600.0000, 
raw observation next is [22.4, 87.66666666666666, 1.0, 2.0, 0.4706855398522619, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 568851.1073646268, 568851.1073646268, 138623.509583969], 
processed observation next is [0.0, 0.17391304347826086, 0.38518518518518513, 0.8766666666666666, 1.0, 1.0, 0.3698637379193594, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.20316110977308102, 0.20316110977308102, 0.2665836722768634], 
reward next is 0.7334, 
noisyNet noise sample is [array([0.10902506], dtype=float32), -1.0154747]. 
=============================================
[2019-04-27 18:53:38,849] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.2314837e-35 1.0000000e+00 6.4488760e-32 3.2762922e-26 1.1458638e-36], sum to 1.0000
[2019-04-27 18:53:38,859] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9556
[2019-04-27 18:53:38,863] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.01666666666667, 86.66666666666667, 1.0, 2.0, 0.4899347279313727, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 587479.1023305188, 587479.1023305188, 141435.4678207628], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6243000.0000, 
sim time next is 6243600.0000, 
raw observation next is [23.13333333333333, 86.33333333333334, 1.0, 2.0, 0.4937418765900969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 591291.9303601391, 591291.9303601386, 142002.4580575171], 
processed observation next is [0.0, 0.2608695652173913, 0.41234567901234553, 0.8633333333333334, 1.0, 1.0, 0.3973117578453535, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2111756894143354, 0.21117568941433523, 0.2730816501106098], 
reward next is 0.7269, 
noisyNet noise sample is [array([-0.43480456], dtype=float32), -0.5329147]. 
=============================================
[2019-04-27 18:53:42,985] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.218215e-31 1.000000e+00 6.755925e-29 8.063266e-22 7.863999e-35], sum to 1.0000
[2019-04-27 18:53:42,994] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2763
[2019-04-27 18:53:43,000] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.28333333333333, 88.00000000000001, 1.0, 2.0, 0.575092045499096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 670749.7716729076, 670749.7716729072, 154569.0074889206], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6315000.0000, 
sim time next is 6315600.0000, 
raw observation next is [24.26666666666667, 88.0, 1.0, 2.0, 0.5737591394497978, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 669417.6664768452, 669417.6664768452, 154353.5819347409], 
processed observation next is [0.0, 0.08695652173913043, 0.4543209876543211, 0.88, 1.0, 1.0, 0.49257040410690217, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2390777380274447, 0.2390777380274447, 0.2968338114129633], 
reward next is 0.7032, 
noisyNet noise sample is [array([0.57475585], dtype=float32), -0.57405144]. 
=============================================
[2019-04-27 18:53:43,861] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.4460997e-31 1.0000000e+00 4.2284276e-29 9.1265615e-23 8.2257945e-34], sum to 1.0000
[2019-04-27 18:53:43,874] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3887
[2019-04-27 18:53:43,879] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.6, 80.0, 1.0, 2.0, 0.6411001880141312, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 730640.8602988226, 730640.8602988226, 165291.4859118457], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6339600.0000, 
sim time next is 6340200.0000, 
raw observation next is [26.85, 79.0, 1.0, 2.0, 0.6417951382813794, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 731433.2501939355, 731433.2501939351, 165416.4292208121], 
processed observation next is [0.0, 0.391304347826087, 0.55, 0.79, 1.0, 1.0, 0.5735656408111659, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2612261607835484, 0.26122616078354827, 0.31810851773233095], 
reward next is 0.6819, 
noisyNet noise sample is [array([0.09449198], dtype=float32), -0.29963428]. 
=============================================
[2019-04-27 18:53:44,832] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.5707805e-31 1.0000000e+00 3.3777759e-28 5.4746448e-21 3.8232237e-32], sum to 1.0000
[2019-04-27 18:53:44,839] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9908
[2019-04-27 18:53:44,844] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 59.0, 1.0, 2.0, 0.6838886156706565, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 779430.2209322713, 779430.2209322713, 173115.5035440881], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6354000.0000, 
sim time next is 6354600.0000, 
raw observation next is [31.06666666666667, 58.83333333333334, 1.0, 2.0, 0.6959674657849765, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 793203.6512101679, 793203.6512101679, 175380.8629196532], 
processed observation next is [0.0, 0.5652173913043478, 0.7061728395061729, 0.5883333333333334, 1.0, 1.0, 0.6380565068868768, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.28328701828934566, 0.28328701828934566, 0.3372708902301023], 
reward next is 0.6627, 
noisyNet noise sample is [array([1.0326357], dtype=float32), -0.9482817]. 
=============================================
[2019-04-27 18:53:45,950] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.9324696e-33 1.0000000e+00 2.0627755e-30 1.1157373e-24 7.2917895e-35], sum to 1.0000
[2019-04-27 18:53:45,958] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7631
[2019-04-27 18:53:45,962] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 55.0, 1.0, 2.0, 0.6803802411630849, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 775429.6918295887, 775429.6918295887, 172462.8589647702], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6368400.0000, 
sim time next is 6369000.0000, 
raw observation next is [31.86666666666667, 55.5, 1.0, 2.0, 0.6788031139278735, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 773631.3320697559, 773631.3320697562, 172169.8216379871], 
processed observation next is [0.0, 0.7391304347826086, 0.7358024691358026, 0.555, 1.0, 1.0, 0.6176227546760399, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2762969043106271, 0.2762969043106272, 0.3310958108422829], 
reward next is 0.6689, 
noisyNet noise sample is [array([1.4247537], dtype=float32), 1.3578331]. 
=============================================
[2019-04-27 18:53:45,980] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[76.02677 ]
 [75.967186]
 [75.91923 ]
 [75.875755]
 [75.85816 ]], R is [[75.96186829]
 [75.87059021]
 [75.7795639 ]
 [75.68899536]
 [75.60021973]].
[2019-04-27 18:53:50,213] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.5768049e-15 7.4013114e-01 3.1292721e-08 2.5986871e-01 1.3925164e-07], sum to 1.0000
[2019-04-27 18:53:50,223] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7413
[2019-04-27 18:53:50,230] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.86666666666667, 59.0, 1.0, 2.0, 0.3292480529991534, 1.0, 2.0, 0.3292480529991534, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 750476.1226142009, 750476.1226142014, 187627.94853513], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6459600.0000, 
sim time next is 6460200.0000, 
raw observation next is [30.8, 59.5, 1.0, 2.0, 0.326723458410289, 1.0, 2.0, 0.326723458410289, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 744718.8575133078, 744718.8575133078, 186997.7075431179], 
processed observation next is [1.0, 0.782608695652174, 0.6962962962962963, 0.595, 1.0, 1.0, 0.19848030763129643, 1.0, 1.0, 0.19848030763129643, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.26597102054046706, 0.26597102054046706, 0.3596109760444575], 
reward next is 0.6404, 
noisyNet noise sample is [array([0.25340366], dtype=float32), -0.23460852]. 
=============================================
[2019-04-27 18:53:50,871] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.3162075e-14 4.8827986e-09 1.1076245e-08 9.9999928e-01 7.0576931e-07], sum to 1.0000
[2019-04-27 18:53:50,876] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6585
[2019-04-27 18:53:50,880] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.25, 54.0, 1.0, 2.0, 0.9861060679044087, 1.0, 2.0, 0.9861060679044087, 0.0, 2.0, 0.0, 6.911199999999998, 6.9112, 121.9260426156618, 2249753.69331683, 2249753.693316831, 426657.4291135809], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6445800.0000, 
sim time next is 6446400.0000, 
raw observation next is [32.2, 54.0, 1.0, 2.0, 0.940144416619933, 1.0, 2.0, 0.940144416619933, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 2144768.443286479, 2144768.443286479, 405067.2539260082], 
processed observation next is [1.0, 0.6086956521739131, 0.7481481481481482, 0.54, 1.0, 1.0, 0.9287433531189678, 1.0, 1.0, 0.9287433531189678, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.7659887297451711, 0.7659887297451711, 0.7789754883192465], 
reward next is 0.2210, 
noisyNet noise sample is [array([-0.89239], dtype=float32), -0.26303273]. 
=============================================
[2019-04-27 18:53:51,765] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8438623e-22 1.0000000e+00 1.4143784e-17 1.3692564e-08 6.3858059e-20], sum to 1.0000
[2019-04-27 18:53:51,774] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5249
[2019-04-27 18:53:51,779] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.96666666666667, 80.5, 1.0, 2.0, 0.6674911915013906, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 760732.7461988628, 760732.7461988628, 170079.9011788352], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6486600.0000, 
sim time next is 6487200.0000, 
raw observation next is [26.9, 81.0, 1.0, 2.0, 0.6681746117919681, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 761512.0200732715, 761512.0200732715, 170205.4966929669], 
processed observation next is [1.0, 0.08695652173913043, 0.5518518518518518, 0.81, 1.0, 1.0, 0.6049697759428192, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.27196857859759693, 0.27196857859759693, 0.32731826287109017], 
reward next is 0.6727, 
noisyNet noise sample is [array([-0.24912745], dtype=float32), 1.2228805]. 
=============================================
[2019-04-27 18:53:53,359] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.8187733e-18 9.9999976e-01 2.0254111e-14 1.9821597e-07 7.9019853e-18], sum to 1.0000
[2019-04-27 18:53:53,368] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2775
[2019-04-27 18:53:53,372] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.3, 86.83333333333333, 1.0, 2.0, 0.7983526763896818, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 909962.7494446449, 909962.7494446449, 195597.3045824893], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6501000.0000, 
sim time next is 6501600.0000, 
raw observation next is [26.3, 87.0, 1.0, 2.0, 0.7996414693708759, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 911432.5897924753, 911432.5897924753, 195863.4686344994], 
processed observation next is [1.0, 0.2608695652173913, 0.5296296296296297, 0.87, 1.0, 1.0, 0.7614779397272332, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3255116392115983, 0.3255116392115983, 0.37666051660480654], 
reward next is 0.6233, 
noisyNet noise sample is [array([-2.8341472], dtype=float32), 1.0413651]. 
=============================================
[2019-04-27 18:53:55,749] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [6.2308242e-13 1.1432849e-01 8.8924040e-08 8.8567024e-01 1.1648765e-06], sum to 1.0000
[2019-04-27 18:53:55,756] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0039
[2019-04-27 18:53:55,764] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1652220.012548405 W.
[2019-04-27 18:53:55,769] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.35, 80.16666666666667, 1.0, 2.0, 0.7244387374318321, 1.0, 2.0, 0.7244387374318321, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1652220.012548405, 1652220.012548405, 313520.5259713898], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6533400.0000, 
sim time next is 6534000.0000, 
raw observation next is [27.4, 80.0, 1.0, 2.0, 0.7162143018122242, 1.0, 2.0, 0.7162143018122242, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1633445.478609396, 1633445.478609396, 310349.1454548673], 
processed observation next is [1.0, 0.6521739130434783, 0.5703703703703703, 0.8, 1.0, 1.0, 0.6621598831097908, 1.0, 1.0, 0.6621598831097908, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.5833733852176414, 0.5833733852176414, 0.5968252797208986], 
reward next is 0.4032, 
noisyNet noise sample is [array([1.2817205], dtype=float32), -0.7138122]. 
=============================================
[2019-04-27 18:53:55,794] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[42.761784]
 [42.461655]
 [42.185375]
 [43.068584]
 [43.21265 ]], R is [[42.9030571 ]
 [42.87110138]
 [42.44239044]
 [42.31935501]
 [42.23143768]].
[2019-04-27 18:53:59,030] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-27 18:53:59,031] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 18:53:59,031] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:53:59,032] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 18:53:59,033] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 18:53:59,034] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 18:53:59,036] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:53:59,036] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:53:59,037] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:53:59,034] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 18:53:59,039] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:53:59,055] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run25
[2019-04-27 18:53:59,056] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run25
[2019-04-27 18:53:59,056] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run25
[2019-04-27 18:53:59,114] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run25
[2019-04-27 18:53:59,114] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run25
[2019-04-27 18:54:19,601] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04093109], dtype=float32), 0.04694853]
[2019-04-27 18:54:19,602] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [21.26666666666667, 67.0, 1.0, 2.0, 0.3135996714947867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 399236.7643035961, 399236.7643035961, 117268.5953055231]
[2019-04-27 18:54:19,603] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 18:54:19,608] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [9.5867340e-29 1.0000000e+00 1.7712000e-26 1.3672441e-16 6.1125774e-30], sampled 0.8377072131330424
[2019-04-27 18:55:05,797] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04093109], dtype=float32), 0.04694853]
[2019-04-27 18:55:05,798] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [26.28333333333333, 82.33333333333333, 1.0, 2.0, 0.6322592500927642, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 720560.397682577, 720560.397682577, 163715.203986199]
[2019-04-27 18:55:05,800] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 18:55:05,803] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [6.3778222e-27 1.0000000e+00 6.3780023e-24 5.4838299e-14 1.0771021e-26], sampled 0.4708402145374644
[2019-04-27 18:55:32,947] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04093109], dtype=float32), 0.04694853]
[2019-04-27 18:55:32,948] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [18.26432021, 80.74305104499999, 1.0, 2.0, 0.3736028518624713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 479539.5493343823, 479539.5493343823, 125152.3630686519]
[2019-04-27 18:55:32,949] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:55:32,951] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [2.2363070e-28 1.0000000e+00 2.2830799e-26 1.3662742e-16 8.1307218e-30], sampled 0.3822944682262164
[2019-04-27 18:55:47,568] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8932.8180 2117490437.0644 423.0000
[2019-04-27 18:55:47,940] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8620.1151 2236793110.8988 499.0000
[2019-04-27 18:55:48,077] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8776.5237 2164807134.3935 472.0000
[2019-04-27 18:55:48,127] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8723.1407 2190521564.0156 539.0000
[2019-04-27 18:55:48,151] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8127.1919 2434800904.7596 671.0000
[2019-04-27 18:55:49,166] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 600000, evaluation results [600000.0, 8127.191946510161, 2434800904.7596083, 671.0, 8776.52373983069, 2164807134.393493, 472.0, 8932.818014537268, 2117490437.0644171, 423.0, 8620.115126455725, 2236793110.898752, 499.0, 8723.140730434943, 2190521564.0155993, 539.0]
[2019-04-27 18:55:50,520] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0215099e-19 2.0789802e-03 1.5977355e-12 9.9792105e-01 7.2844747e-13], sum to 1.0000
[2019-04-27 18:55:50,527] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4737
[2019-04-27 18:55:50,531] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.6, 37.0, 1.0, 2.0, 0.1692073275103445, 1.0, 2.0, 0.1692073275103445, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 422511.2206005823, 422511.2206005827, 153615.8497452442], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6630000.0000, 
sim time next is 6630600.0000, 
raw observation next is [28.3, 39.5, 1.0, 2.0, 0.1704338734443115, 1.0, 2.0, 0.1704338734443115, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 424090.0044466028, 424090.0044466032, 153828.1376727292], 
processed observation next is [1.0, 0.7391304347826086, 0.6037037037037037, 0.395, 1.0, 1.0, 0.012421277909894628, 1.0, 1.0, 0.012421277909894628, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1514607158737867, 0.15146071587378684, 0.2958233416783254], 
reward next is 0.7042, 
noisyNet noise sample is [array([-0.32481748], dtype=float32), 0.53644377]. 
=============================================
[2019-04-27 18:55:51,999] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.7847978e-27 1.0000000e+00 1.4288316e-23 2.8406844e-14 3.7270988e-26], sum to 1.0000
[2019-04-27 18:55:52,004] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5552
[2019-04-27 18:55:52,008] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 45.0, 1.0, 2.0, 0.3097216539408216, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 399534.3522952537, 399534.3522952537, 111863.2006415843], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6667200.0000, 
sim time next is 6667800.0000, 
raw observation next is [22.98333333333333, 44.83333333333334, 1.0, 2.0, 0.3105771934860427, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 400638.268411719, 400638.268411719, 111486.7428412996], 
processed observation next is [1.0, 0.17391304347826086, 0.40679012345679005, 0.4483333333333334, 1.0, 1.0, 0.17925856367386034, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.14308509586132823, 0.14308509586132823, 0.2143975823871146], 
reward next is 0.7856, 
noisyNet noise sample is [array([-0.5887942], dtype=float32), -0.15268716]. 
=============================================
[2019-04-27 18:56:11,026] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0924484e-19 9.9999118e-01 2.2103664e-15 8.7786602e-06 2.8472728e-17], sum to 1.0000
[2019-04-27 18:56:11,040] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4826
[2019-04-27 18:56:11,045] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.46666666666667, 82.66666666666667, 1.0, 2.0, 0.7170202897158144, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 874956.7459395184, 874956.7459395184, 181837.674925018], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7028400.0000, 
sim time next is 7029000.0000, 
raw observation next is [22.65, 82.0, 1.0, 2.0, 0.8216458693522122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1000805.540499863, 1000805.540499863, 203311.4632532453], 
processed observation next is [1.0, 0.34782608695652173, 0.3944444444444444, 0.82, 1.0, 1.0, 0.7876736539907288, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3574305501785225, 0.3574305501785225, 0.3909835831793179], 
reward next is 0.6090, 
noisyNet noise sample is [array([0.6251043], dtype=float32), -1.8169845]. 
=============================================
[2019-04-27 18:56:11,059] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[61.965523]
 [62.924606]
 [62.936283]
 [62.91935 ]
 [62.783485]], R is [[61.33654022]
 [61.37348557]
 [61.48913193]
 [61.60809326]
 [61.72761154]].
[2019-04-27 18:56:11,892] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.6021728e-16 9.2768232e-07 6.6324163e-10 9.9999893e-01 1.1687942e-07], sum to 1.0000
[2019-04-27 18:56:11,898] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3995
[2019-04-27 18:56:11,903] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.0, 60.0, 1.0, 2.0, 0.7021085229792062, 1.0, 2.0, 0.7021085229792062, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1616628.690567678, 1616628.690567678, 305744.9955068613], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7050000.0000, 
sim time next is 7050600.0000, 
raw observation next is [27.75, 61.0, 1.0, 2.0, 0.7341898854025084, 1.0, 2.0, 0.7341898854025084, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1691131.411669885, 1691131.411669885, 318165.3376745756], 
processed observation next is [1.0, 0.6086956521739131, 0.5833333333333334, 0.61, 1.0, 1.0, 0.6835593873839385, 1.0, 1.0, 0.6835593873839385, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.6039755041678161, 0.6039755041678161, 0.6118564186049531], 
reward next is 0.3881, 
noisyNet noise sample is [array([0.88537014], dtype=float32), 0.96570766]. 
=============================================
[2019-04-27 18:56:12,053] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.3855949e-18 7.2536212e-05 6.8015943e-13 9.9992740e-01 6.9790916e-11], sum to 1.0000
[2019-04-27 18:56:12,060] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8924
[2019-04-27 18:56:12,065] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.8, 78.5, 1.0, 2.0, 0.2333788817462619, 1.0, 2.0, 0.2333788817462619, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 557155.646062962, 557155.6460629625, 166441.4552347805], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7061400.0000, 
sim time next is 7062000.0000, 
raw observation next is [23.76666666666667, 78.66666666666667, 1.0, 2.0, 0.2354184073180839, 1.0, 2.0, 0.2354184073180839, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 562202.7925462706, 562202.7925462706, 166898.8790335579], 
processed observation next is [1.0, 0.7391304347826086, 0.43580246913580256, 0.7866666666666667, 1.0, 1.0, 0.08978381823581415, 1.0, 1.0, 0.08978381823581415, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2007867116236681, 0.2007867116236681, 0.3209593827568421], 
reward next is 0.6790, 
noisyNet noise sample is [array([-1.1120383], dtype=float32), 0.09580723]. 
=============================================
[2019-04-27 18:56:12,082] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[61.323063]
 [61.1484  ]
 [60.479797]
 [59.144993]
 [59.601414]], R is [[61.25514603]
 [61.32251358]
 [61.38798141]
 [61.42459488]
 [61.33699417]].
[2019-04-27 18:56:13,290] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8361516e-20 9.9997401e-01 2.5082911e-16 2.6023063e-05 2.5082911e-16], sum to 1.0000
[2019-04-27 18:56:13,295] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2745
[2019-04-27 18:56:13,300] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 82.0, 1.0, 2.0, 0.487651634700043, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 585115.0779553243, 585115.0779553243, 141093.7297450222], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7081200.0000, 
sim time next is 7081800.0000, 
raw observation next is [23.58333333333334, 81.50000000000001, 1.0, 2.0, 0.4850024738277253, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 582516.091046717, 582516.0910467166, 140703.194294135], 
processed observation next is [1.0, 1.0, 0.4290123456790126, 0.8150000000000002, 1.0, 1.0, 0.3869077069377682, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.20804146108811322, 0.20804146108811306, 0.2705830659502596], 
reward next is 0.7294, 
noisyNet noise sample is [array([1.7692044], dtype=float32), 0.5375487]. 
=============================================
[2019-04-27 18:56:30,913] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.6677583e-21 1.9381307e-06 6.8732239e-15 9.9999809e-01 9.5549209e-13], sum to 1.0000
[2019-04-27 18:56:30,928] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3111
[2019-04-27 18:56:30,930] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [21.3, 91.0, 1.0, 2.0, 0.4041961607245662, 1.0, 2.0, 0.4041961607245662, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 972304.9031433752, 972304.9031433756, 209486.9281724183], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7398000.0000, 
sim time next is 7398600.0000, 
raw observation next is [21.28333333333333, 90.83333333333334, 1.0, 2.0, 0.4985509916808002, 1.0, 2.0, 0.4985509916808002, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1196960.806042753, 1196960.806042754, 237503.7612482876], 
processed observation next is [1.0, 0.6521739130434783, 0.3438271604938271, 0.9083333333333334, 1.0, 1.0, 0.40303689485809546, 1.0, 1.0, 0.40303689485809546, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.42748600215812604, 0.4274860021581265, 0.45673800240055307], 
reward next is 0.5433, 
noisyNet noise sample is [array([0.8748233], dtype=float32), 0.5924038]. 
=============================================
[2019-04-27 18:56:31,295] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.4880913e-20 2.5723196e-07 6.6911246e-14 9.9999976e-01 4.6582058e-13], sum to 1.0000
[2019-04-27 18:56:31,301] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8044
[2019-04-27 18:56:31,307] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [21.2, 90.0, 1.0, 2.0, 0.4171372224897415, 1.0, 2.0, 0.4171372224897415, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1005001.716117542, 1005001.716117543, 213208.5462154431], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7401600.0000, 
sim time next is 7402200.0000, 
raw observation next is [21.2, 89.83333333333333, 1.0, 2.0, 0.4447911318258134, 1.0, 2.0, 0.4447911318258134, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1072218.731997747, 1072218.731997748, 221268.2918116121], 
processed observation next is [1.0, 0.6956521739130435, 0.34074074074074073, 0.8983333333333333, 1.0, 1.0, 0.3390370616973969, 1.0, 1.0, 0.3390370616973969, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.3829352614277668, 0.3829352614277671, 0.42551594579156177], 
reward next is 0.5745, 
noisyNet noise sample is [array([-0.00493528], dtype=float32), 1.6462532]. 
=============================================
[2019-04-27 18:56:31,870] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.2801877e-20 2.4482421e-08 4.2442949e-15 1.0000000e+00 2.9939610e-12], sum to 1.0000
[2019-04-27 18:56:31,885] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7541
[2019-04-27 18:56:31,895] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [21.2, 89.16666666666667, 1.0, 2.0, 0.4732609204581074, 1.0, 2.0, 0.4732609204581074, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1140585.186906827, 1140585.186906827, 229819.4295095704], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7404600.0000, 
sim time next is 7405200.0000, 
raw observation next is [21.2, 89.0, 1.0, 2.0, 0.4392390091468912, 1.0, 2.0, 0.4392390091468912, 0.0, 2.0, 0.0, 6.911199999999998, 6.9112, 121.9260426156618, 1059712.85389367, 1059712.853893671, 219661.8342761091], 
processed observation next is [1.0, 0.7391304347826086, 0.34074074074074073, 0.89, 1.0, 1.0, 0.3324273918415371, 1.0, 1.0, 0.3324273918415371, 0.0, 1.0, -0.25, -1.7763568394002506e-16, 0.0, 0.8094621288201359, 0.37846887639059645, 0.3784688763905968, 0.42242660437713286], 
reward next is 0.5776, 
noisyNet noise sample is [array([-0.58090156], dtype=float32), -0.7302593]. 
=============================================
[2019-04-27 18:56:34,617] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.7624855e-26 1.0000000e+00 2.6278705e-25 5.1111192e-11 7.5117585e-27], sum to 1.0000
[2019-04-27 18:56:34,627] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2283
[2019-04-27 18:56:34,635] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.75, 87.5, 1.0, 2.0, 0.4215341201290293, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 515270.3129725517, 515270.3129725517, 131515.7940527435], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7464600.0000, 
sim time next is 7465200.0000, 
raw observation next is [21.9, 87.0, 1.0, 2.0, 0.4253191647055198, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 519203.1123151537, 519203.1123151532, 132043.4499645171], 
processed observation next is [0.0, 0.391304347826087, 0.36666666666666664, 0.87, 1.0, 1.0, 0.3158561484589521, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.18542968296969775, 0.18542968296969756, 0.2539297114702252], 
reward next is 0.7461, 
noisyNet noise sample is [array([-0.11256912], dtype=float32), 0.75244766]. 
=============================================
[2019-04-27 18:56:36,427] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.3590609e-25 9.9999237e-01 4.1386176e-20 7.6613478e-06 9.0943403e-24], sum to 1.0000
[2019-04-27 18:56:36,436] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9840
[2019-04-27 18:56:36,443] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.06666666666667, 77.16666666666667, 1.0, 2.0, 0.5192862034927209, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 614716.0932593086, 614716.0932593083, 145780.5551076616], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7488600.0000, 
sim time next is 7489200.0000, 
raw observation next is [25.03333333333334, 77.33333333333334, 1.0, 2.0, 0.519011532248776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 614476.3394502717, 614476.3394502717, 145739.9336452468], 
processed observation next is [0.0, 0.6956521739130435, 0.48271604938271623, 0.7733333333333334, 1.0, 1.0, 0.4273946812485429, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21945583551795417, 0.21945583551795417, 0.28026910316393616], 
reward next is 0.7197, 
noisyNet noise sample is [array([0.7211757], dtype=float32), -0.03182202]. 
=============================================
[2019-04-27 18:56:36,697] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.1485179e-25 1.0000000e+00 2.2886928e-21 5.4679550e-10 5.9205096e-24], sum to 1.0000
[2019-04-27 18:56:36,705] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0005
[2019-04-27 18:56:36,709] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.73333333333333, 84.66666666666666, 1.0, 2.0, 0.5067867576239063, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 602630.3224555094, 602630.3224555091, 143897.2812901695], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7497600.0000, 
sim time next is 7498200.0000, 
raw observation next is [23.61666666666667, 85.33333333333334, 1.0, 2.0, 0.5053624576827138, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 601213.4193547318, 601213.4193547318, 143682.6583547848], 
processed observation next is [0.0, 0.782608695652174, 0.4302469135802471, 0.8533333333333334, 1.0, 1.0, 0.41114578295561166, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21471907834097564, 0.21471907834097564, 0.2763128045284323], 
reward next is 0.7237, 
noisyNet noise sample is [array([-0.50516164], dtype=float32), -0.7888089]. 
=============================================
[2019-04-27 18:56:38,864] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-27 18:56:38,867] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 18:56:38,868] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:56:38,868] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 18:56:38,869] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 18:56:38,871] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:56:38,871] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 18:56:38,872] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 18:56:38,873] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:56:38,872] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:56:38,874] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:56:38,892] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run26
[2019-04-27 18:56:38,916] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run26
[2019-04-27 18:56:38,917] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run26
[2019-04-27 18:56:38,918] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run26
[2019-04-27 18:56:38,982] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run26
[2019-04-27 18:56:49,633] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03912496], dtype=float32), 0.038464595]
[2019-04-27 18:56:49,634] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [28.95421536, 43.71246279, 1.0, 2.0, 0.357566784535761, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 440849.7778029066, 440849.7778029066, 122750.069449156]
[2019-04-27 18:56:49,637] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 18:56:49,642] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [3.3013711e-25 9.9999940e-01 5.1904687e-21 5.4197915e-07 3.0229445e-22], sampled 0.3313202549691615
[2019-04-27 18:57:05,871] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.03912496], dtype=float32), 0.038464595]
[2019-04-27 18:57:05,873] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [21.59789619, 66.96015995, 1.0, 2.0, 0.3241270124984061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 411032.5670220758, 411032.5670220762, 118593.6040822675]
[2019-04-27 18:57:05,874] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:57:05,876] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [3.4754793e-28 1.0000000e+00 1.1397408e-25 1.4550270e-12 2.2747842e-28], sampled 0.856692604277785
[2019-04-27 18:57:20,186] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03912496], dtype=float32), 0.038464595]
[2019-04-27 18:57:20,189] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [27.24572544166667, 74.36873548666668, 1.0, 2.0, 0.6418101074376773, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 733281.065813933, 733281.065813933, 165510.4276914986]
[2019-04-27 18:57:20,190] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 18:57:20,194] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [5.3046496e-22 9.9465525e-01 2.4327038e-17 5.3447895e-03 3.8731657e-17], sampled 0.19873710652256005
[2019-04-27 18:57:20,761] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.03912496], dtype=float32), 0.038464595]
[2019-04-27 18:57:20,763] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [26.16666666666667, 83.16666666666667, 1.0, 2.0, 0.6453284416701007, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 735461.9740291937, 735461.9740291933, 166050.3937523316]
[2019-04-27 18:57:20,764] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 18:57:20,766] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [7.5602303e-26 1.0000000e+00 1.0776015e-22 2.8015785e-09 2.5184996e-24], sampled 0.5737689466994116
[2019-04-27 18:57:24,544] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.03912496], dtype=float32), 0.038464595]
[2019-04-27 18:57:24,545] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [31.61950533, 36.87176385666667, 1.0, 2.0, 0.2189259284665001, 1.0, 2.0, 0.2189259284665001, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 527924.5456628853, 527924.5456628858, 163501.3978517088]
[2019-04-27 18:57:24,547] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:57:24,550] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [5.7211434e-22 8.1999695e-01 7.2246426e-16 1.8000303e-01 3.7305642e-16], sampled 0.10978019719921572
[2019-04-27 18:57:24,650] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03912496], dtype=float32), 0.038464595]
[2019-04-27 18:57:24,651] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [34.45878024666667, 42.02212226, 1.0, 2.0, 0.8050913109140184, 1.0, 2.0, 0.8050913109140184, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1836352.580826669, 1836352.580826669, 345862.0914525016]
[2019-04-27 18:57:24,653] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 18:57:24,657] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [4.7311912e-21 1.1983236e-07 1.0703747e-14 9.9999988e-01 2.8721827e-12], sampled 0.9774712759643384
[2019-04-27 18:57:44,592] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.03912496], dtype=float32), 0.038464595]
[2019-04-27 18:57:44,593] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [25.0, 89.0, 1.0, 2.0, 0.605260661179348, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 693435.8338569099, 693435.8338569099, 159165.6204921548]
[2019-04-27 18:57:44,595] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 18:57:44,597] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [8.1407650e-25 9.9999976e-01 3.7889505e-21 2.0010999e-07 3.3547995e-22], sampled 0.8504123397023973
[2019-04-27 18:57:47,460] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03912496], dtype=float32), 0.038464595]
[2019-04-27 18:57:47,463] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [26.08364660666667, 87.60959072666667, 1.0, 2.0, 0.4273938390511731, 1.0, 2.0, 0.4273938390511731, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 974328.2819705277, 974328.2819705273, 213842.2776731894]
[2019-04-27 18:57:47,463] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 18:57:47,465] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.2809059e-19 3.7797072e-04 1.5392415e-14 9.9962199e-01 1.0944971e-12], sampled 0.5004289016250558
[2019-04-27 18:58:19,103] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03912496], dtype=float32), 0.038464595]
[2019-04-27 18:58:19,106] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [24.35, 95.0, 1.0, 2.0, 0.6171559635429542, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 703988.6516594116, 703988.6516594116, 161085.3371352626]
[2019-04-27 18:58:19,106] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 18:58:19,112] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [4.5944437e-25 1.0000000e+00 1.1319483e-21 4.3306805e-08 6.5713590e-23], sampled 0.8176395359853753
[2019-04-27 18:58:26,807] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.03912496], dtype=float32), 0.038464595]
[2019-04-27 18:58:26,809] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [24.563863425, 64.00325791, 1.0, 2.0, 0.4170511336004132, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 513901.2682521841, 513901.2682521841, 130979.7143533497]
[2019-04-27 18:58:26,810] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:58:26,812] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [5.0826949e-25 9.9999976e-01 3.9275949e-21 2.6627328e-07 2.8817432e-22], sampled 0.790724423777871
[2019-04-27 18:58:27,905] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 9018.6263 2118053594.4830 270.0000
[2019-04-27 18:58:27,987] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8394.4822 2419065786.5773 340.0000
[2019-04-27 18:58:28,078] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8866.0070 2188550820.3330 318.0000
[2019-04-27 18:58:28,118] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8935.4186 2161136053.6180 254.0000
[2019-04-27 18:58:28,142] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8798.0478 2224843818.8111 268.0000
[2019-04-27 18:58:29,158] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 625000, evaluation results [625000.0, 8394.482160059239, 2419065786.5772977, 340.0, 8935.418551959978, 2161136053.6179614, 254.0, 9018.626345198483, 2118053594.4830267, 270.0, 8798.04781228314, 2224843818.811109, 268.0, 8866.00696319068, 2188550820.333044, 318.0]
[2019-04-27 18:58:31,451] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [9.5234815e-24 9.9999988e-01 2.5684250e-18 6.5021993e-08 6.2612352e-22], sum to 1.0000
[2019-04-27 18:58:31,461] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9316
[2019-04-27 18:58:31,467] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.96666666666667, 71.0, 1.0, 2.0, 0.5156973576272058, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 611271.2314245905, 611271.2314245905, 145238.4404377062], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7582800.0000, 
sim time next is 7583400.0000, 
raw observation next is [25.8, 72.0, 1.0, 2.0, 0.5156695807011609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 611275.6934671998, 611275.6934671998, 145235.4598552493], 
processed observation next is [0.0, 0.782608695652174, 0.5111111111111112, 0.72, 1.0, 1.0, 0.42341616750138195, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21831274766685707, 0.21831274766685707, 0.2792989612600948], 
reward next is 0.7207, 
noisyNet noise sample is [array([-0.31921223], dtype=float32), 0.13915947]. 
=============================================
[2019-04-27 18:58:41,293] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2067818e-21 1.7013481e-09 2.4573010e-14 1.0000000e+00 6.2386863e-13], sum to 1.0000
[2019-04-27 18:58:41,301] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3437
[2019-04-27 18:58:41,303] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [21.06666666666667, 72.0, 1.0, 2.0, 0.1955113235654307, 1.0, 2.0, 0.1955113235654307, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 491238.8531485095, 491238.8531485095, 159128.4660464184], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7784400.0000, 
sim time next is 7785000.0000, 
raw observation next is [21.1, 71.5, 1.0, 2.0, 0.182173877412506, 1.0, 2.0, 0.182173877412506, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 458227.8575588124, 458227.8575588129, 156361.2756949289], 
processed observation next is [1.0, 0.08695652173913043, 0.3370370370370371, 0.715, 1.0, 1.0, 0.026397473110126204, 1.0, 1.0, 0.026397473110126204, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.16365280627100443, 0.1636528062710046, 0.3006947609517863], 
reward next is 0.6993, 
noisyNet noise sample is [array([0.69884807], dtype=float32), 0.7603105]. 
=============================================
[2019-04-27 18:58:41,311] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[69.78094]
 [69.70595]
 [69.37725]
 [69.31754]
 [69.27625]], R is [[69.69482422]
 [69.69186401]
 [69.67633057]
 [69.6856842 ]
 [69.69463348]].
[2019-04-27 18:58:45,246] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.2258640e-24 1.0000000e+00 1.8726240e-18 1.4931816e-08 5.1120485e-21], sum to 1.0000
[2019-04-27 18:58:45,253] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4638
[2019-04-27 18:58:45,257] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.9, 68.66666666666666, 1.0, 2.0, 0.4211052345123095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 518350.3057684888, 518350.3057684888, 131550.2396525695], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7858200.0000, 
sim time next is 7858800.0000, 
raw observation next is [23.8, 69.0, 1.0, 2.0, 0.4171686459342376, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 513838.1761288228, 513838.1761288228, 130991.462353847], 
processed observation next is [1.0, 1.0, 0.43703703703703706, 0.69, 1.0, 1.0, 0.30615314992171144, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.18351363433172244, 0.18351363433172244, 0.2519066583727827], 
reward next is 0.7481, 
noisyNet noise sample is [array([1.496367], dtype=float32), -0.19358607]. 
=============================================
[2019-04-27 18:58:45,385] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2852424e-21 9.9999666e-01 1.0994211e-16 3.3893093e-06 1.0767290e-18], sum to 1.0000
[2019-04-27 18:58:45,393] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7406
[2019-04-27 18:58:45,397] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.55, 71.0, 1.0, 2.0, 0.4169201163570789, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 513243.0188625465, 513243.0188625469, 130948.4967771164], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7860600.0000, 
sim time next is 7861200.0000, 
raw observation next is [23.46666666666667, 71.66666666666667, 1.0, 2.0, 0.4168390452591194, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 513009.4173097277, 513009.4173097272, 130933.4420308272], 
processed observation next is [1.0, 1.0, 0.42469135802469143, 0.7166666666666667, 1.0, 1.0, 0.3057607681656183, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.18321764903918847, 0.1832176490391883, 0.2517950808285138], 
reward next is 0.7482, 
noisyNet noise sample is [array([-0.26775205], dtype=float32), 0.965835]. 
=============================================
[2019-04-27 18:58:48,806] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5215888e-21 9.9999976e-01 5.3472400e-18 1.9272564e-07 2.6387454e-20], sum to 1.0000
[2019-04-27 18:58:48,816] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9882
[2019-04-27 18:58:48,822] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.63333333333333, 58.5, 1.0, 2.0, 0.4941643699436969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 590956.093652282, 590956.0936522816, 142038.1173153838], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7931400.0000, 
sim time next is 7932000.0000, 
raw observation next is [27.46666666666667, 59.0, 1.0, 2.0, 0.4918132387172195, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 588793.9704587748, 588793.9704587748, 141694.6366119806], 
processed observation next is [1.0, 0.8260869565217391, 0.5728395061728396, 0.59, 1.0, 1.0, 0.39501576037764224, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21028356087813388, 0.21028356087813388, 0.27248968579227034], 
reward next is 0.7275, 
noisyNet noise sample is [array([-1.4680994], dtype=float32), -0.6776731]. 
=============================================
[2019-04-27 18:58:48,835] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[53.334915]
 [53.034958]
 [52.458874]
 [51.698215]
 [50.924595]], R is [[54.06111908]
 [54.24735641]
 [54.43114853]
 [54.61259842]
 [54.79143143]].
[2019-04-27 18:58:50,248] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:58:50,248] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:58:50,250] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res11/Eplus-env-sub_run4
[2019-04-27 18:58:50,368] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:58:50,369] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:58:50,370] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res14/Eplus-env-sub_run4
[2019-04-27 18:58:50,627] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:58:50,627] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:58:50,629] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res2/Eplus-env-sub_run4
[2019-04-27 18:58:50,649] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:58:50,649] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:58:50,650] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res3/Eplus-env-sub_run4
[2019-04-27 18:58:50,749] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:58:50,749] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:58:50,750] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res7/Eplus-env-sub_run4
[2019-04-27 18:58:50,787] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:58:50,787] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:58:50,789] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res9/Eplus-env-sub_run4
[2019-04-27 18:58:50,812] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:58:50,813] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:58:50,813] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:58:50,814] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:58:50,814] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:58:50,814] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:58:50,817] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res12/Eplus-env-sub_run4
[2019-04-27 18:58:50,816] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res13/Eplus-env-sub_run4
[2019-04-27 18:58:50,866] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res4/Eplus-env-sub_run4
[2019-04-27 18:58:50,905] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:58:50,905] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:58:50,907] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res16/Eplus-env-sub_run4
[2019-04-27 18:58:50,945] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:58:50,946] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:58:50,947] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res15/Eplus-env-sub_run4
[2019-04-27 18:58:50,976] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:58:50,977] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:58:50,982] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res10/Eplus-env-sub_run4
[2019-04-27 18:58:50,977] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:58:51,007] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:58:51,009] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res6/Eplus-env-sub_run4
[2019-04-27 18:58:51,042] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:58:51,043] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:58:51,044] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res8/Eplus-env-sub_run4
[2019-04-27 18:58:51,097] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:58:51,098] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:58:51,099] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res17/Eplus-env-sub_run4
[2019-04-27 18:58:51,145] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 18:58:51,145] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:58:51,147] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res5/Eplus-env-sub_run4
[2019-04-27 18:58:53,155] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.5440243e-32 1.0000000e+00 5.3239014e-31 1.5998480e-17 6.5938684e-35], sum to 1.0000
[2019-04-27 18:58:53,166] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6307
[2019-04-27 18:58:53,173] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.08333333333334, 75.83333333333334, 1.0, 2.0, 0.3261487005281855, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 420730.7062652097, 420730.7062652097, 113262.8290068345], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 15000.0000, 
sim time next is 15600.0000, 
raw observation next is [18.06666666666667, 75.66666666666667, 1.0, 2.0, 0.290960663736095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 375327.1513664683, 375327.1513664683, 108192.1938665509], 
processed observation next is [1.0, 0.17391304347826086, 0.22469135802469148, 0.7566666666666667, 1.0, 1.0, 0.1559055520667798, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1340454112023101, 0.1340454112023101, 0.20806191128182863], 
reward next is 0.7919, 
noisyNet noise sample is [array([1.8891927], dtype=float32), -1.7858174]. 
=============================================
[2019-04-27 18:58:55,212] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.26203426e-17 3.36782279e-04 7.22243619e-12 9.99663234e-01
 1.59270145e-12], sum to 1.0000
[2019-04-27 18:58:55,219] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4156
[2019-04-27 18:58:55,225] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.4, 40.5, 1.0, 2.0, 0.1920359153796334, 1.0, 2.0, 0.1920359153796334, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 469884.135078115, 469884.135078115, 158070.6552977993], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 63000.0000, 
sim time next is 63600.0000, 
raw observation next is [29.3, 41.0, 1.0, 2.0, 0.1949893508218577, 1.0, 2.0, 0.1949893508218577, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 477036.4859898906, 477036.4859898911, 158681.8117650141], 
processed observation next is [1.0, 0.7391304347826086, 0.6407407407407407, 0.41, 1.0, 1.0, 0.04165398907364012, 1.0, 1.0, 0.04165398907364012, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.17037017356781806, 0.17037017356781825, 0.30515733031733483], 
reward next is 0.6948, 
noisyNet noise sample is [array([-0.7869489], dtype=float32), 0.09255317]. 
=============================================
[2019-04-27 18:59:01,253] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.0158715e-29 1.0000000e+00 1.0958403e-24 1.1243850e-13 3.2775765e-28], sum to 1.0000
[2019-04-27 18:59:01,259] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1842
[2019-04-27 18:59:01,263] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.15, 12.33333333333333, 1.0, 2.0, 0.3371212754719128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 434889.3039559643, 434889.3039559643, 101611.0013962774], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 173400.0000, 
sim time next is 174000.0000, 
raw observation next is [28.9, 12.66666666666667, 1.0, 2.0, 0.3346288182085058, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 431673.1079750034, 431673.1079750034, 100979.9417050135], 
processed observation next is [0.0, 0.0, 0.6259259259259259, 0.1266666666666667, 1.0, 1.0, 0.20789145024822123, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1541689671339298, 0.1541689671339298, 0.19419219558656442], 
reward next is 0.8058, 
noisyNet noise sample is [array([0.94985205], dtype=float32), 0.48620915]. 
=============================================
[2019-04-27 18:59:01,275] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[71.976944]
 [72.14091 ]
 [72.2331  ]
 [72.23354 ]
 [72.26144 ]], R is [[71.99429321]
 [72.07894897]
 [72.16131592]
 [72.24124146]
 [72.31893158]].
[2019-04-27 18:59:17,329] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.0841207e-25 1.0000000e+00 2.7253741e-22 5.5427759e-12 3.5929511e-20], sum to 1.0000
[2019-04-27 18:59:17,336] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4649
[2019-04-27 18:59:17,341] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.78333333333333, 43.83333333333334, 1.0, 2.0, 0.9111517229973439, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.16708943530168, 6.9112, 121.9248627792052, 1275259.01728244, 1144222.023282057, 224192.4847491418], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 467400.0000, 
sim time next is 468000.0000, 
raw observation next is [27.1, 43.0, 1.0, 2.0, 0.9379008978472226, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.340227029207621, 6.9112, 121.9240586701024, 1396044.838956129, 1176348.228899416, 230439.7342788761], 
processed observation next is [1.0, 0.43478260869565216, 0.5592592592592593, 0.43, 1.0, 1.0, 0.9260724974371698, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.04290270292076208, 0.0, 0.8094489574847795, 0.4985874424843318, 0.42012436746407716, 0.4431533351516848], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5972112], dtype=float32), -0.8843373]. 
=============================================
[2019-04-27 18:59:17,358] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[62.674248]
 [63.008724]
 [63.368458]
 [63.49841 ]
 [63.633873]], R is [[61.60049438]
 [60.98448944]
 [60.39958954]
 [60.39339828]
 [60.39202499]].
[2019-04-27 18:59:17,369] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.3764445e-31 1.0000000e+00 4.9698481e-31 9.7304051e-18 1.1254030e-30], sum to 1.0000
[2019-04-27 18:59:17,379] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5888
[2019-04-27 18:59:17,382] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.9, 78.0, 1.0, 2.0, 0.2680781335689409, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 345802.984720736, 345802.9847207355, 106273.217876444], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 450000.0000, 
sim time next is 450600.0000, 
raw observation next is [18.2, 76.5, 1.0, 2.0, 0.2614084674037024, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 337197.6670878751, 337197.6670878751, 106765.6699729198], 
processed observation next is [1.0, 0.21739130434782608, 0.2296296296296296, 0.765, 1.0, 1.0, 0.12072436595678855, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.12042773824566967, 0.12042773824566967, 0.20531859610176884], 
reward next is 0.7947, 
noisyNet noise sample is [array([1.4575567], dtype=float32), 1.0331044]. 
=============================================
[2019-04-27 18:59:18,367] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.4666221e-20 1.7307919e-13 1.2479368e-13 9.9999785e-01 2.1896412e-06], sum to 1.0000
[2019-04-27 18:59:18,377] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2922
[2019-04-27 18:59:18,383] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.16666666666666, 34.33333333333334, 1.0, 2.0, 0.5897091742751592, 1.0, 2.0, 0.5897091742751592, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1436295.826133696, 1436295.826133697, 268305.4268068275], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 475800.0000, 
sim time next is 476400.0000, 
raw observation next is [30.33333333333334, 33.66666666666667, 1.0, 2.0, 0.4818982830035052, 1.0, 2.0, 0.4818982830035052, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 1175244.0403048, 1175244.0403048, 232915.9731539016], 
processed observation next is [1.0, 0.5217391304347826, 0.6790123456790126, 0.3366666666666667, 1.0, 1.0, 0.38321224167083956, 1.0, 1.0, 0.38321224167083956, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.4197300143945714, 0.4197300143945714, 0.4479153329882723], 
reward next is 0.5521, 
noisyNet noise sample is [array([0.03503115], dtype=float32), 0.22110797]. 
=============================================
[2019-04-27 18:59:21,155] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.4589597e-20 9.9999988e-01 2.7676429e-18 1.2731392e-07 6.2150081e-19], sum to 1.0000
[2019-04-27 18:59:21,162] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1457
[2019-04-27 18:59:21,169] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.33333333333333, 72.33333333333333, 1.0, 2.0, 0.3562807950630994, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 454147.4778331164, 454147.4778331164, 122810.4973430468], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 528000.0000, 
sim time next is 528600.0000, 
raw observation next is [20.26666666666667, 72.66666666666667, 1.0, 2.0, 0.3378440377307689, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 430786.8498215469, 430786.8498215469, 120378.2433952723], 
processed observation next is [1.0, 0.08695652173913043, 0.3061728395061729, 0.7266666666666667, 1.0, 1.0, 0.21171909253662963, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.15385244636483816, 0.15385244636483816, 0.23149662191398518], 
reward next is 0.7685, 
noisyNet noise sample is [array([-0.06993378], dtype=float32), 0.5973163]. 
=============================================
[2019-04-27 18:59:21,267] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-27 18:59:21,268] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 18:59:21,269] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:59:21,270] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 18:59:21,271] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 18:59:21,272] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 18:59:21,273] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:59:21,272] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 18:59:21,275] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:59:21,273] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:59:21,278] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 18:59:21,297] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run27
[2019-04-27 18:59:21,298] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run27
[2019-04-27 18:59:21,337] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run27
[2019-04-27 18:59:21,338] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run27
[2019-04-27 18:59:21,377] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run27
[2019-04-27 18:59:40,063] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.03545006], dtype=float32), 0.033800807]
[2019-04-27 18:59:40,064] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [24.87843521333333, 46.06339132833333, 1.0, 2.0, 0.3176529712730806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 405208.5366312011, 405208.5366312006, 117784.4596357502]
[2019-04-27 18:59:40,065] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 18:59:40,067] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [3.2653954e-21 1.0000000e+00 2.0292887e-19 1.7702191e-09 1.6976893e-19], sampled 0.7814940225668702
[2019-04-27 18:59:43,381] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.03545006], dtype=float32), 0.033800807]
[2019-04-27 18:59:43,385] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [34.86666666666667, 27.33333333333333, 1.0, 2.0, 0.4437566593496942, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 538606.4283272533, 538606.4283272538, 134652.6004434749]
[2019-04-27 18:59:43,386] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 18:59:43,388] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [9.1394219e-22 1.0000000e+00 2.6007626e-19 9.8447552e-09 3.4141027e-19], sampled 0.45496999006337147
[2019-04-27 19:00:02,443] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.03545006], dtype=float32), 0.033800807]
[2019-04-27 19:00:02,444] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [30.53378782333333, 81.9679453, 1.0, 2.0, 0.8994201636842787, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1025236.578187894, 1025236.578187894, 217353.9189348771]
[2019-04-27 19:00:02,446] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 19:00:02,450] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [2.7035184e-18 9.9988854e-01 4.0988115e-15 1.1143557e-04 4.9632936e-14], sampled 0.5265904161271577
[2019-04-27 19:00:18,624] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.03545006], dtype=float32), 0.033800807]
[2019-04-27 19:00:18,625] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [29.85, 68.5, 1.0, 2.0, 0.7064220544271634, 1.0, 2.0, 0.7064220544271634, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1611092.498081442, 1611092.498081442, 306604.1949228732]
[2019-04-27 19:00:18,627] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 19:00:18,631] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [9.6579615e-18 2.9016185e-09 4.5608591e-12 1.0000000e+00 3.2394741e-08], sampled 0.5434755063401172
[2019-04-27 19:00:22,083] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03545006], dtype=float32), 0.033800807]
[2019-04-27 19:00:22,085] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [25.61622176, 78.33743309333333, 1.0, 2.0, 0.5610641430057216, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 654414.5977486051, 654414.5977486051, 152215.3280230641]
[2019-04-27 19:00:22,087] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:00:22,090] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [8.1892483e-20 9.9999952e-01 2.8166483e-17 5.2687471e-07 9.3906817e-17], sampled 0.9382115313012599
[2019-04-27 19:00:34,187] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03545006], dtype=float32), 0.033800807]
[2019-04-27 19:00:34,189] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [23.532150915, 93.71575425, 1.0, 2.0, 0.5925087811850189, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 689002.8890123364, 689002.8890123364, 157447.5489183974]
[2019-04-27 19:00:34,190] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:00:34,193] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [2.5218746e-19 9.9999380e-01 2.2708564e-16 6.1741603e-06 1.5412237e-15], sampled 0.14326005738852154
[2019-04-27 19:01:10,320] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8927.9245 2160243183.5004 258.0000
[2019-04-27 19:01:10,340] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8364.0559 2418834296.3395 366.0000
[2019-04-27 19:01:10,372] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8794.7834 2223526254.7608 264.0000
[2019-04-27 19:01:10,375] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 9014.9472 2117085291.8175 282.0000
[2019-04-27 19:01:10,444] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8854.1356 2188030541.9883 332.0000
[2019-04-27 19:01:11,456] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 650000, evaluation results [650000.0, 8364.055885207828, 2418834296.3395376, 366.0, 8927.92447809115, 2160243183.5004025, 258.0, 9014.94715015891, 2117085291.8175333, 282.0, 8794.783418149149, 2223526254.760847, 264.0, 8854.13561392429, 2188030541.988311, 332.0]
[2019-04-27 19:01:16,771] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.2851834e-24 9.9998283e-01 1.6975955e-19 1.7138729e-05 5.7160957e-19], sum to 1.0000
[2019-04-27 19:01:16,778] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9594
[2019-04-27 19:01:16,783] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.41666666666667, 55.83333333333334, 1.0, 2.0, 0.6325141197736499, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 802401.9735617954, 802401.9735617954, 166495.79191252], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 612600.0000, 
sim time next is 613200.0000, 
raw observation next is [23.23333333333333, 56.66666666666667, 1.0, 2.0, 0.5275697017741664, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 669238.9006465873, 669238.9006465868, 148290.7962834565], 
processed observation next is [1.0, 0.08695652173913043, 0.4160493827160493, 0.5666666666666668, 1.0, 1.0, 0.43758297830257903, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.23901389308806686, 0.2390138930880667, 0.28517460823741636], 
reward next is 0.7148, 
noisyNet noise sample is [array([-1.6312276], dtype=float32), 0.45779797]. 
=============================================
[2019-04-27 19:01:17,711] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.2179417e-21 4.7650781e-01 4.1961989e-17 5.2349222e-01 7.5262197e-16], sum to 1.0000
[2019-04-27 19:01:17,719] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0128
[2019-04-27 19:01:17,724] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.55, 20.5, 1.0, 2.0, 0.1958045933320841, 1.0, 2.0, 0.1958045933320841, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 490020.8749292188, 490020.8749292193, 159151.9701447166], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 671400.0000, 
sim time next is 672000.0000, 
raw observation next is [33.33333333333333, 21.0, 1.0, 2.0, 0.3880249044254508, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 488329.8944411124, 488329.8944411129, 127082.8820511371], 
processed observation next is [1.0, 0.782608695652174, 0.7901234567901233, 0.21, 1.0, 1.0, 0.2714582195541081, 0.0, 0.5, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.17440353372896872, 0.17440353372896888, 0.24439015779064827], 
reward next is 0.7556, 
noisyNet noise sample is [array([-0.61823696], dtype=float32), -2.0924532]. 
=============================================
[2019-04-27 19:01:17,731] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[64.92642 ]
 [64.650055]
 [64.342926]
 [64.11367 ]
 [63.85981 ]], R is [[65.34975433]
 [65.39019775]
 [65.42993164]
 [65.46886444]
 [65.50740051]].
[2019-04-27 19:01:25,718] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.9284060e-28 1.0000000e+00 1.6943109e-28 1.0241210e-14 1.4919096e-26], sum to 1.0000
[2019-04-27 19:01:25,724] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7116
[2019-04-27 19:01:25,731] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.5, 54.0, 1.0, 2.0, 0.375405871865336, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 468079.2147791556, 468079.2147791556, 125269.6319547523], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 810000.0000, 
sim time next is 810600.0000, 
raw observation next is [25.8, 53.16666666666666, 1.0, 2.0, 0.3799260576840032, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 472983.279061183, 472983.2790611825, 125874.8014675363], 
processed observation next is [0.0, 0.391304347826087, 0.5111111111111112, 0.5316666666666666, 1.0, 1.0, 0.26181673533809907, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1689225996647082, 0.16892259966470805, 0.24206692589910825], 
reward next is 0.7579, 
noisyNet noise sample is [array([-0.9665797], dtype=float32), 0.15135008]. 
=============================================
[2019-04-27 19:01:26,493] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5628011e-30 1.0000000e+00 5.8679035e-29 2.3928594e-13 1.5064520e-27], sum to 1.0000
[2019-04-27 19:01:26,501] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0055
[2019-04-27 19:01:26,506] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 41.5, 1.0, 2.0, 0.4339501032297319, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 529749.8518705075, 529749.8518705071, 133300.9301574224], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 819000.0000, 
sim time next is 819600.0000, 
raw observation next is [30.33333333333333, 40.33333333333334, 1.0, 2.0, 0.4344444025845885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 530137.4471742013, 530137.4471742013, 133367.1174129664], 
processed observation next is [0.0, 0.4782608695652174, 0.6790123456790121, 0.40333333333333343, 1.0, 1.0, 0.32671952688641487, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.18933480256221474, 0.18933480256221474, 0.2564752257941662], 
reward next is 0.7435, 
noisyNet noise sample is [array([0.24508171], dtype=float32), 1.3336376]. 
=============================================
[2019-04-27 19:01:30,461] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.2704350e-33 1.0000000e+00 8.5049743e-31 4.0507339e-17 2.4047312e-29], sum to 1.0000
[2019-04-27 19:01:30,470] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3206
[2019-04-27 19:01:30,476] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 66.0, 1.0, 2.0, 0.3291722579744827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 417600.3411074098, 417600.3411074094, 119242.4737166924], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 889200.0000, 
sim time next is 889800.0000, 
raw observation next is [21.86666666666667, 66.0, 1.0, 2.0, 0.3303232949409688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 418778.3653628051, 418778.3653628051, 119388.4028939759], 
processed observation next is [0.0, 0.30434782608695654, 0.36543209876543226, 0.66, 1.0, 1.0, 0.20276582731067713, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.14956370191528753, 0.14956370191528753, 0.2295930824884152], 
reward next is 0.7704, 
noisyNet noise sample is [array([-1.1059253], dtype=float32), -0.10522448]. 
=============================================
[2019-04-27 19:01:38,478] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.9648770e-26 1.0000000e+00 4.3354454e-21 1.7667322e-08 5.7840792e-20], sum to 1.0000
[2019-04-27 19:01:38,484] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3927
[2019-04-27 19:01:38,489] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.8, 68.66666666666667, 1.0, 2.0, 0.3322285972302313, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 423799.8773509867, 423799.8773509867, 119649.923485034], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1048800.0000, 
sim time next is 1049400.0000, 
raw observation next is [20.75, 69.0, 1.0, 2.0, 0.3298761562610794, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 420819.3717497817, 420819.3717497812, 119346.2853250103], 
processed observation next is [1.0, 0.13043478260869565, 0.32407407407407407, 0.69, 1.0, 1.0, 0.20223351935842787, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.15029263276777918, 0.150292632767779, 0.22951208716348137], 
reward next is 0.7705, 
noisyNet noise sample is [array([0.44605023], dtype=float32), 1.6965268]. 
=============================================
[2019-04-27 19:01:38,654] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.7846705e-25 1.0000000e+00 1.4600610e-24 2.1805622e-08 9.3111121e-23], sum to 1.0000
[2019-04-27 19:01:38,662] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6271
[2019-04-27 19:01:38,667] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.46666666666667, 70.66666666666667, 1.0, 2.0, 0.3356264452395879, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 428296.3629871624, 428296.362987162, 120090.8060287826], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1052400.0000, 
sim time next is 1053000.0000, 
raw observation next is [20.4, 71.0, 1.0, 2.0, 0.3341257590100143, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 426468.4853504596, 426468.4853504596, 119896.2784113687], 
processed observation next is [1.0, 0.17391304347826086, 0.31111111111111106, 0.71, 1.0, 1.0, 0.20729257025001702, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.15231017333944985, 0.15231017333944985, 0.23056976617570904], 
reward next is 0.7694, 
noisyNet noise sample is [array([1.8990182], dtype=float32), -0.93732035]. 
=============================================
[2019-04-27 19:01:38,683] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[68.19232 ]
 [68.24079 ]
 [68.46542 ]
 [68.54604 ]
 [68.612236]], R is [[68.22228241]
 [68.30912018]
 [68.39051819]
 [68.47952271]
 [68.5679245 ]].
[2019-04-27 19:01:45,250] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.8452657e-24 9.9999988e-01 2.6659601e-22 1.6244736e-07 9.2283847e-19], sum to 1.0000
[2019-04-27 19:01:45,258] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3101
[2019-04-27 19:01:45,264] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.15, 65.5, 1.0, 2.0, 0.7439259653175846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 939913.8118372267, 939913.8118372267, 187929.711948599], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1174200.0000, 
sim time next is 1174800.0000, 
raw observation next is [22.1, 66.0, 1.0, 2.0, 0.755204129830079, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 953768.560151885, 953768.560151885, 190221.1613629556], 
processed observation next is [1.0, 0.6086956521739131, 0.3740740740740741, 0.66, 1.0, 1.0, 0.7085763450358084, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3406316286256732, 0.3406316286256732, 0.36580992569799153], 
reward next is 0.6342, 
noisyNet noise sample is [array([1.5564967], dtype=float32), 1.6130524]. 
=============================================
[2019-04-27 19:01:48,839] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.0309984e-23 2.6766390e-06 1.1990245e-15 9.9999738e-01 3.0474318e-11], sum to 1.0000
[2019-04-27 19:01:48,849] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3569
[2019-04-27 19:01:48,855] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.33333333333334, 54.66666666666667, 1.0, 2.0, 0.4011561315765349, 1.0, 2.0, 0.4011561315765349, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 971181.5098612106, 971181.5098612106, 208841.1404613815], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1267800.0000, 
sim time next is 1268400.0000, 
raw observation next is [26.36666666666667, 54.33333333333334, 1.0, 2.0, 0.4587700555808371, 1.0, 2.0, 0.4587700555808371, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1110980.752398064, 1110980.752398064, 225603.4219719578], 
processed observation next is [1.0, 0.6956521739130435, 0.5320987654320989, 0.5433333333333334, 1.0, 1.0, 0.3556786375962347, 1.0, 1.0, 0.3556786375962347, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3967788401421657, 0.3967788401421657, 0.43385273456145734], 
reward next is 0.5661, 
noisyNet noise sample is [array([0.58319056], dtype=float32), -0.09353674]. 
=============================================
[2019-04-27 19:01:54,504] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.7089655e-24 1.3958172e-09 6.7878478e-16 1.0000000e+00 1.1751879e-12], sum to 1.0000
[2019-04-27 19:01:54,514] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8706
[2019-04-27 19:01:54,521] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.5, 36.0, 1.0, 2.0, 0.3921571935361829, 1.0, 2.0, 0.3921571935361829, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 963899.3315927137, 963899.3315927142, 206760.4127329594], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1342800.0000, 
sim time next is 1343400.0000, 
raw observation next is [29.7, 35.5, 1.0, 2.0, 0.4993880897386582, 1.0, 2.0, 0.4993880897386582, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1223819.775617285, 1223819.775617285, 238558.8937376457], 
processed observation next is [1.0, 0.5652173913043478, 0.6555555555555556, 0.355, 1.0, 1.0, 0.4040334401650693, 1.0, 1.0, 0.4040334401650693, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.43707849129188747, 0.43707849129188747, 0.45876710334162635], 
reward next is 0.5412, 
noisyNet noise sample is [array([0.26321232], dtype=float32), -2.1156843]. 
=============================================
[2019-04-27 19:02:01,315] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-27 19:02:01,316] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 19:02:01,317] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 19:02:01,318] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:02:01,319] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 19:02:01,321] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:02:01,321] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 19:02:01,322] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:02:01,317] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 19:02:01,325] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:02:01,328] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:02:01,343] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run28
[2019-04-27 19:02:01,363] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run28
[2019-04-27 19:02:01,384] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run28
[2019-04-27 19:02:01,387] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run28
[2019-04-27 19:02:01,414] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run28
[2019-04-27 19:03:05,141] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04084168], dtype=float32), 0.024219006]
[2019-04-27 19:03:05,144] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [26.032673275, 69.25053753166667, 1.0, 2.0, 0.5206700169450521, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 622712.8105603862, 622712.8105603862, 146244.0301655864]
[2019-04-27 19:03:05,145] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:03:05,151] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.0803163e-24 9.9998415e-01 1.7883259e-21 1.5800391e-05 8.3651774e-20], sampled 0.46617195960891955
[2019-04-27 19:03:10,477] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04084168], dtype=float32), 0.024219006]
[2019-04-27 19:03:10,479] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [25.55, 96.0, 1.0, 2.0, 0.709096297140216, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 808174.6486404609, 808174.6486404605, 177874.4124614904]
[2019-04-27 19:03:10,480] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:03:10,483] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.0390418e-23 9.9984336e-01 3.0335647e-20 1.5657602e-04 2.0432556e-18], sampled 0.34733208815321326
[2019-04-27 19:03:21,794] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04084168], dtype=float32), 0.024219006]
[2019-04-27 19:03:21,796] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [20.47133096, 80.70785365, 1.0, 2.0, 0.3618585527338922, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 454802.2036862663, 454802.2036862663, 123495.8302385513]
[2019-04-27 19:03:21,797] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 19:03:21,800] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.3011618e-27 1.0000000e+00 2.3686826e-25 5.2662297e-10 9.6380156e-25], sampled 0.4594955627692928
[2019-04-27 19:03:38,526] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04084168], dtype=float32), 0.024219006]
[2019-04-27 19:03:38,527] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [22.91666666666667, 94.0, 1.0, 2.0, 0.5238704924706874, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 618547.0235739442, 618547.0235739442, 146453.1786078713]
[2019-04-27 19:03:38,527] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:03:38,530] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [9.7978104e-28 1.0000000e+00 1.5277737e-25 4.4985463e-10 5.7359664e-25], sampled 0.47162164817286356
[2019-04-27 19:03:49,149] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04084168], dtype=float32), 0.024219006]
[2019-04-27 19:03:49,150] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [27.48634957333334, 58.2006685, 1.0, 2.0, 0.4666098955183654, 1.0, 2.0, 0.4666098955183654, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1096832.937185427, 1096832.937185427, 226745.2170762732]
[2019-04-27 19:03:49,152] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:03:49,157] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [7.4165555e-24 1.2658088e-09 4.0712288e-17 1.0000000e+00 5.4848471e-13], sampled 0.3461534011410252
[2019-04-27 19:03:51,142] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8959.2261 2161429252.3239 208.0000
[2019-04-27 19:03:51,200] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8825.5569 2224028837.9595 224.0000
[2019-04-27 19:03:51,305] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8412.8521 2417944952.4905 311.0000
[2019-04-27 19:03:51,366] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 9036.0067 2118228831.8972 237.0000
[2019-04-27 19:03:51,378] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8876.1297 2187911281.9003 291.0000
[2019-04-27 19:03:52,392] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 675000, evaluation results [675000.0, 8412.852130748315, 2417944952.490507, 311.0, 8959.22606390838, 2161429252.3238907, 208.0, 9036.006725137217, 2118228831.8972309, 237.0, 8825.556937384928, 2224028837.9594684, 224.0, 8876.129732276588, 2187911281.9002666, 291.0]
[2019-04-27 19:03:52,614] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.5378317e-25 1.0000000e+00 4.0098171e-20 2.2265365e-08 2.2333983e-22], sum to 1.0000
[2019-04-27 19:03:52,624] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4159
[2019-04-27 19:03:52,638] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.58333333333334, 44.0, 1.0, 2.0, 0.3480162457341759, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 438376.5815098548, 438376.5815098552, 121666.0541988707], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1468200.0000, 
sim time next is 1468800.0000, 
raw observation next is [26.3, 45.0, 1.0, 2.0, 0.3473736113380555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 437814.8854369894, 437814.8854369894, 121584.5092259655], 
processed observation next is [0.0, 0.0, 0.5296296296296297, 0.45, 1.0, 1.0, 0.22306382302149463, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.15636245908463908, 0.15636245908463908, 0.2338163638960875], 
reward next is 0.7662, 
noisyNet noise sample is [array([1.275064], dtype=float32), -0.7035235]. 
=============================================
[2019-04-27 19:03:59,070] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.6676842e-18 3.2314583e-07 1.6256067e-12 9.9999964e-01 4.1250722e-11], sum to 1.0000
[2019-04-27 19:03:59,079] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1526
[2019-04-27 19:03:59,082] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.88333333333333, 53.5, 1.0, 2.0, 0.4645519477610787, 1.0, 2.0, 0.4645519477610787, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1131778.205573138, 1131778.205573139, 227562.0966344427], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1595400.0000, 
sim time next is 1596000.0000, 
raw observation next is [25.96666666666667, 53.00000000000001, 1.0, 2.0, 0.4622466618732559, 1.0, 2.0, 0.4622466618732559, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1127118.995794554, 1127118.995794555, 226892.1721506667], 
processed observation next is [1.0, 0.4782608695652174, 0.517283950617284, 0.53, 1.0, 1.0, 0.35981745461101894, 1.0, 1.0, 0.35981745461101894, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.402542498498055, 0.40254249849805535, 0.43633110028974365], 
reward next is 0.5637, 
noisyNet noise sample is [array([2.450728], dtype=float32), -1.6545725]. 
=============================================
[2019-04-27 19:03:59,093] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[51.152355]
 [50.988934]
 [51.117126]
 [51.235233]
 [51.42532 ]], R is [[51.10531235]
 [51.15664291]
 [51.18185806]
 [51.20990753]
 [51.24147034]].
[2019-04-27 19:04:02,034] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0325958e-23 9.9999833e-01 1.4638260e-20 1.7148127e-06 8.6904141e-18], sum to 1.0000
[2019-04-27 19:04:02,041] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6945
[2019-04-27 19:04:02,046] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.83333333333333, 70.66666666666667, 1.0, 2.0, 0.355521423999187, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 452064.5918548274, 452064.591854827, 122703.4372911982], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1651200.0000, 
sim time next is 1651800.0000, 
raw observation next is [20.61666666666667, 71.83333333333333, 1.0, 2.0, 0.3467797542484743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 441187.6842340823, 441187.6842340823, 121545.1328999073], 
processed observation next is [1.0, 0.08695652173913043, 0.319135802469136, 0.7183333333333333, 1.0, 1.0, 0.22235685029580274, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.15756703008360082, 0.15756703008360082, 0.23374064019212942], 
reward next is 0.7663, 
noisyNet noise sample is [array([0.49194586], dtype=float32), 1.0478393]. 
=============================================
[2019-04-27 19:04:03,223] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.9439319e-28 1.0000000e+00 1.3576487e-25 8.2366447e-11 8.1286959e-25], sum to 1.0000
[2019-04-27 19:04:03,231] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3460
[2019-04-27 19:04:03,235] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.86666666666667, 84.66666666666667, 1.0, 2.0, 0.5253966471744916, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 668631.0008025742, 668631.0008025742, 147950.0568056036], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1672800.0000, 
sim time next is 1673400.0000, 
raw observation next is [18.93333333333333, 84.33333333333333, 1.0, 2.0, 0.5217613258003114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 663845.8524393649, 663845.8524393649, 147354.9830178261], 
processed observation next is [1.0, 0.34782608695652173, 0.25679012345679, 0.8433333333333333, 1.0, 1.0, 0.4306682450003707, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2370878044426303, 0.2370878044426303, 0.28337496734197326], 
reward next is 0.7166, 
noisyNet noise sample is [array([-0.46022153], dtype=float32), -2.5032992]. 
=============================================
[2019-04-27 19:04:04,278] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8258858e-23 4.3671491e-07 6.8790239e-17 9.9999952e-01 8.4170545e-15], sum to 1.0000
[2019-04-27 19:04:04,285] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6975
[2019-04-27 19:04:04,291] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.43333333333333, 69.0, 1.0, 2.0, 0.4940041446826872, 1.0, 2.0, 0.4940041446826872, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1200019.161367651, 1200019.161367651, 236545.609032289], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1696200.0000, 
sim time next is 1696800.0000, 
raw observation next is [23.46666666666667, 69.0, 1.0, 2.0, 0.4942659006387403, 1.0, 2.0, 0.4942659006387403, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1199747.351382291, 1199747.351382291, 236599.0952429961], 
processed observation next is [1.0, 0.6521739130434783, 0.42469135802469143, 0.69, 1.0, 1.0, 0.39793559599850037, 1.0, 1.0, 0.39793559599850037, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.4284811969222468, 0.4284811969222468, 0.4549982600826848], 
reward next is 0.5450, 
noisyNet noise sample is [array([0.9364477], dtype=float32), -0.68034726]. 
=============================================
[2019-04-27 19:04:04,905] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.2795578e-26 8.4346118e-07 3.0975900e-18 9.9999917e-01 8.6525548e-16], sum to 1.0000
[2019-04-27 19:04:04,915] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2274
[2019-04-27 19:04:04,920] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.6, 72.0, 1.0, 2.0, 0.2095230819143027, 1.0, 2.0, 0.2095230819143027, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 510090.9518778315, 510090.9518778319, 161662.7114723309], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1706400.0000, 
sim time next is 1707000.0000, 
raw observation next is [23.55, 72.5, 1.0, 2.0, 0.2127094659739609, 1.0, 2.0, 0.2127094659739609, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 517542.7395184678, 517542.7395184682, 162333.2320512979], 
processed observation next is [1.0, 0.782608695652174, 0.4277777777777778, 0.725, 1.0, 1.0, 0.06274936425471535, 1.0, 1.0, 0.06274936425471535, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.18483669268516706, 0.18483669268516723, 0.3121792924063421], 
reward next is 0.6878, 
noisyNet noise sample is [array([0.39714095], dtype=float32), -0.32033047]. 
=============================================
[2019-04-27 19:04:04,936] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[79.8967 ]
 [79.98961]
 [80.04779]
 [80.24111]
 [80.3055 ]], R is [[79.61850739]
 [79.51143646]
 [79.40663147]
 [79.30367279]
 [79.20269012]].
[2019-04-27 19:04:05,065] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1088912e-27 9.9999988e-01 3.2767232e-22 1.7499241e-07 3.7075547e-20], sum to 1.0000
[2019-04-27 19:04:05,073] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3195
[2019-04-27 19:04:05,078] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.05, 78.33333333333334, 1.0, 2.0, 0.4385931182796685, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 535488.0416661269, 535488.0416661269, 133984.7548870018], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1713000.0000, 
sim time next is 1713600.0000, 
raw observation next is [23.0, 79.0, 1.0, 2.0, 0.4425668127030529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 539973.6819800945, 539973.6819800945, 134560.7884687568], 
processed observation next is [1.0, 0.8695652173913043, 0.4074074074074074, 0.79, 1.0, 1.0, 0.3363890627417297, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.19284774356431947, 0.19284774356431947, 0.25877074705530156], 
reward next is 0.7412, 
noisyNet noise sample is [array([0.00262972], dtype=float32), -0.8570119]. 
=============================================
[2019-04-27 19:04:11,125] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.1861677e-27 1.0000000e+00 2.7714370e-25 1.3470699e-11 2.1866919e-24], sum to 1.0000
[2019-04-27 19:04:11,135] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8437
[2019-04-27 19:04:11,140] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.96666666666667, 83.66666666666667, 1.0, 2.0, 0.3661199651335592, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 461351.0107296385, 461351.0107296385, 124087.1220542682], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1837200.0000, 
sim time next is 1837800.0000, 
raw observation next is [20.1, 83.0, 1.0, 2.0, 0.3629908797728466, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 457155.5864622861, 457155.5864622861, 123661.5328101723], 
processed observation next is [1.0, 0.2608695652173913, 0.30000000000000004, 0.83, 1.0, 1.0, 0.24165580925338884, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16326985230795932, 0.16326985230795932, 0.23781064001956212], 
reward next is 0.7622, 
noisyNet noise sample is [array([-0.4510364], dtype=float32), -2.0259094]. 
=============================================
[2019-04-27 19:04:11,593] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.4263268e-23 4.0570784e-01 1.3783270e-16 5.9429210e-01 7.7179928e-14], sum to 1.0000
[2019-04-27 19:04:11,599] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5105
[2019-04-27 19:04:11,603] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [21.9, 82.0, 1.0, 2.0, 0.3027530553485946, 1.0, 2.0, 0.3027530553485946, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 737041.9472002728, 737041.9472002728, 183074.2011307833], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1861200.0000, 
sim time next is 1861800.0000, 
raw observation next is [21.88333333333333, 82.33333333333334, 1.0, 2.0, 0.2849070759509176, 1.0, 2.0, 0.2849070759509176, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 693586.0208831144, 693586.0208831149, 178737.1096464344], 
processed observation next is [1.0, 0.5652173913043478, 0.36604938271604925, 0.8233333333333335, 1.0, 1.0, 0.14869889994156854, 1.0, 1.0, 0.14869889994156854, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.24770929317254087, 0.24770929317254103, 0.3437252108585277], 
reward next is 0.6563, 
noisyNet noise sample is [array([-0.23456514], dtype=float32), -0.26403973]. 
=============================================
[2019-04-27 19:04:12,023] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.5752326e-26 9.9998796e-01 6.9928306e-21 1.2083577e-05 5.7324089e-21], sum to 1.0000
[2019-04-27 19:04:12,031] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0041
[2019-04-27 19:04:12,034] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 78.0, 1.0, 2.0, 0.6334731168769738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 784945.8212353423, 784945.8212353423, 166356.9375919092], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1854000.0000, 
sim time next is 1854600.0000, 
raw observation next is [21.98333333333333, 78.33333333333333, 1.0, 2.0, 0.6824530677599957, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 845256.7037931122, 845256.7037931122, 175493.9522836035], 
processed observation next is [1.0, 0.4782608695652174, 0.36975308641975296, 0.7833333333333333, 1.0, 1.0, 0.6219679378095186, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3018773942118258, 0.3018773942118258, 0.33748836977616054], 
reward next is 0.6625, 
noisyNet noise sample is [array([1.1165282], dtype=float32), 0.20219412]. 
=============================================
[2019-04-27 19:04:16,741] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.5202811e-23 1.0000000e+00 1.0253723e-21 7.8157316e-12 8.9100499e-22], sum to 1.0000
[2019-04-27 19:04:16,747] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2338
[2019-04-27 19:04:16,751] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.55, 88.0, 1.0, 2.0, 0.6966893196615462, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 852663.1299940506, 852663.1299940506, 177954.4474474662], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1935000.0000, 
sim time next is 1935600.0000, 
raw observation next is [21.66666666666666, 87.66666666666667, 1.0, 2.0, 0.7614299948919875, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 930823.2108392285, 930823.2108392285, 190793.7588034443], 
processed observation next is [1.0, 0.391304347826087, 0.35802469135802445, 0.8766666666666667, 1.0, 1.0, 0.715988089157128, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.33243686101401015, 0.33243686101401015, 0.3669110746220083], 
reward next is 0.6331, 
noisyNet noise sample is [array([-0.9065654], dtype=float32), 0.111904785]. 
=============================================
[2019-04-27 19:04:17,658] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.9391244e-26 3.8846751e-17 4.8133206e-17 1.0000000e+00 5.1575884e-14], sum to 1.0000
[2019-04-27 19:04:17,666] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1703
[2019-04-27 19:04:17,672] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.4, 59.66666666666666, 1.0, 2.0, 0.6135721687978049, 1.0, 2.0, 0.6135721687978049, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1409300.582680891, 1409300.582680891, 273243.8880692305], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1959000.0000, 
sim time next is 1959600.0000, 
raw observation next is [28.5, 59.33333333333334, 1.0, 2.0, 0.6410387618450194, 1.0, 2.0, 0.6410387618450194, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1471206.257560717, 1471206.257560718, 282915.6874313305], 
processed observation next is [1.0, 0.6956521739130435, 0.6111111111111112, 0.5933333333333334, 1.0, 1.0, 0.5726651926726422, 1.0, 1.0, 0.5726651926726422, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.5254308062716846, 0.525430806271685, 0.5440686296756356], 
reward next is 0.4559, 
noisyNet noise sample is [array([1.9237571], dtype=float32), -0.722476]. 
=============================================
[2019-04-27 19:04:24,125] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.724300e-29 1.000000e+00 6.818849e-27 2.830560e-10 9.879747e-27], sum to 1.0000
[2019-04-27 19:04:24,135] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8960
[2019-04-27 19:04:24,139] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.38333333333333, 65.0, 1.0, 2.0, 0.582057457202338, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 672539.5021959675, 672539.5021959675, 155463.9663015378], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2117400.0000, 
sim time next is 2118000.0000, 
raw observation next is [28.56666666666667, 64.0, 1.0, 2.0, 0.5818937943790656, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 672370.1828270347, 672370.1828270347, 155437.1088764895], 
processed observation next is [0.0, 0.5217391304347826, 0.6135802469135804, 0.64, 1.0, 1.0, 0.5022545171179352, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2401322081525124, 0.2401322081525124, 0.2989175170701721], 
reward next is 0.7011, 
noisyNet noise sample is [array([-1.3604575], dtype=float32), 1.2608567]. 
=============================================
[2019-04-27 19:04:24,159] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[74.31957 ]
 [74.32424 ]
 [74.32282 ]
 [74.312355]
 [74.3159  ]], R is [[74.29263306]
 [74.25074005]
 [74.20930481]
 [74.16854095]
 [74.12848663]].
[2019-04-27 19:04:38,011] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3624533e-32 1.0000000e+00 2.5907207e-35 1.5579911e-23 1.8713644e-36], sum to 1.0000
[2019-04-27 19:04:38,018] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0868
[2019-04-27 19:04:38,023] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.95, 95.5, 1.0, 2.0, 0.5400411026448042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 646724.6690205596, 646724.6690205596, 149422.8293007149], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2341800.0000, 
sim time next is 2342400.0000, 
raw observation next is [21.93333333333333, 95.66666666666667, 1.0, 2.0, 0.5299297803025524, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 634622.77099183, 634622.77099183, 147770.1852187993], 
processed observation next is [1.0, 0.08695652173913043, 0.36790123456790114, 0.9566666666666667, 1.0, 1.0, 0.4403925955982767, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2266509896399393, 0.2266509896399393, 0.2841734331130756], 
reward next is 0.7158, 
noisyNet noise sample is [array([0.78713506], dtype=float32), 0.9712493]. 
=============================================
[2019-04-27 19:04:41,254] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-27 19:04:41,256] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 19:04:41,257] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 19:04:41,258] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:04:41,258] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:04:41,259] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 19:04:41,260] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 19:04:41,261] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 19:04:41,263] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:04:41,264] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:04:41,265] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:04:41,280] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run29
[2019-04-27 19:04:41,281] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run29
[2019-04-27 19:04:41,321] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run29
[2019-04-27 19:04:41,321] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run29
[2019-04-27 19:04:41,338] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run29
[2019-04-27 19:04:55,461] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03979395], dtype=float32), 0.025636846]
[2019-04-27 19:04:55,466] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [20.65, 60.16666666666666, 1.0, 2.0, 0.2899905560081428, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 374075.447557412, 374075.4475574115, 112672.0874268806]
[2019-04-27 19:04:55,469] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:04:55,475] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [8.1464749e-27 1.0000000e+00 4.4351319e-26 1.2107718e-14 6.0538076e-27], sampled 0.0698359389986215
[2019-04-27 19:05:30,626] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.03979395], dtype=float32), 0.025636846]
[2019-04-27 19:05:30,627] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [35.6, 30.5, 1.0, 2.0, 0.8162072581425944, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9815348395276668, 6.911200000000001, 6.9112, 121.9260426156618, 1662872.106770815, 1662872.106770814, 336692.4639352842]
[2019-04-27 19:05:30,629] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 19:05:30,631] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [2.3964022e-18 9.9142529e-02 6.0835522e-13 9.0085745e-01 1.2493698e-11], sampled 0.16764444926219102
[2019-04-27 19:05:36,271] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03979395], dtype=float32), 0.025636846]
[2019-04-27 19:05:36,272] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [28.43333333333333, 72.66666666666667, 1.0, 2.0, 0.6659886468732444, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 759019.463924042, 759019.463924042, 169805.1030175858]
[2019-04-27 19:05:36,273] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:05:36,276] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [6.8452572e-25 1.0000000e+00 2.1767059e-23 1.2025733e-11 2.7921432e-23], sampled 0.6909814270769435
[2019-04-27 19:05:40,229] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03979395], dtype=float32), 0.025636846]
[2019-04-27 19:05:40,232] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [29.74178445333333, 67.30261204333333, 1.0, 2.0, 0.7034480548456149, 1.0, 2.0, 0.7034480548456149, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1604303.793922765, 1604303.793922765, 305473.0237841198]
[2019-04-27 19:05:40,235] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:05:40,237] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [7.2448693e-22 9.8746228e-12 6.9437063e-15 1.0000000e+00 5.0839246e-11], sampled 0.5177616752431485
[2019-04-27 19:05:44,266] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.03979395], dtype=float32), 0.025636846]
[2019-04-27 19:05:44,267] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [28.36895609166667, 71.40708974833333, 1.0, 2.0, 0.6344130208263972, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 723016.1209984361, 723016.1209984357, 164099.5170522579]
[2019-04-27 19:05:44,268] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 19:05:44,271] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.1827390e-27 1.0000000e+00 5.6339749e-26 9.3502368e-14 2.0279582e-26], sampled 0.006205937184375232
[2019-04-27 19:05:49,321] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.03979395], dtype=float32), 0.025636846]
[2019-04-27 19:05:49,323] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [27.46666666666667, 60.00000000000001, 1.0, 2.0, 0.4917605776250676, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 587588.8276128862, 587588.8276128862, 141645.5728358745]
[2019-04-27 19:05:49,324] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 19:05:49,328] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.1057486e-27 1.0000000e+00 2.5658786e-26 2.4001580e-14 6.1436075e-27], sampled 0.6842839220137442
[2019-04-27 19:05:55,787] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03979395], dtype=float32), 0.025636846]
[2019-04-27 19:05:55,790] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [31.08333333333334, 60.83333333333334, 1.0, 2.0, 0.5734542787306256, 1.0, 2.0, 0.5734542787306256, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1307585.916665299, 1307585.916665299, 259034.6599739984]
[2019-04-27 19:05:55,791] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:05:55,792] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.5337915e-21 1.3328599e-11 2.2580009e-15 1.0000000e+00 1.1108909e-10], sampled 0.4170830560407688
[2019-04-27 19:06:00,054] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.03979395], dtype=float32), 0.025636846]
[2019-04-27 19:06:00,055] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [36.0, 42.66666666666667, 1.0, 2.0, 0.9485957631045595, 1.0, 2.0, 0.9485957631045595, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 2164072.017616828, 2164072.017616828, 408982.897034462]
[2019-04-27 19:06:00,057] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 19:06:00,060] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.4630349e-21 1.7168247e-10 1.3707182e-14 1.0000000e+00 4.6386971e-11], sampled 0.24120060681329591
[2019-04-27 19:06:02,947] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.03979395], dtype=float32), 0.025636846]
[2019-04-27 19:06:02,948] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [21.00291332, 91.06545754666666, 1.0, 2.0, 0.4500409964104074, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 550525.412432568, 550525.412432568, 135711.1448546198]
[2019-04-27 19:06:02,948] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 19:06:02,957] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [2.7262210e-26 1.0000000e+00 2.4805026e-25 1.3630514e-13 9.9594811e-26], sampled 0.14091676066138759
[2019-04-27 19:06:07,603] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.03979395], dtype=float32), 0.025636846]
[2019-04-27 19:06:07,604] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [24.0, 89.0, 1.0, 2.0, 0.5644985264432312, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 659485.8542346435, 659485.8542346435, 152835.0528100682]
[2019-04-27 19:06:07,607] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 19:06:07,609] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [6.8287183e-26 1.0000000e+00 4.1582799e-24 3.5146385e-12 3.5785267e-24], sampled 0.25544941510101904
[2019-04-27 19:06:10,296] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03979395], dtype=float32), 0.025636846]
[2019-04-27 19:06:10,297] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [23.52348544, 84.53732278333334, 1.0, 2.0, 0.5104787937425102, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 608515.0979692393, 608515.0979692393, 144538.1556453462]
[2019-04-27 19:06:10,298] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:06:10,302] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [3.6970786e-26 1.0000000e+00 4.5317969e-25 2.9804902e-13 2.2870298e-25], sampled 0.8363905845773899
[2019-04-27 19:06:10,971] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.03979395], dtype=float32), 0.025636846]
[2019-04-27 19:06:10,973] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [23.93333333333333, 83.66666666666667, 1.0, 2.0, 0.5068277721941529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 601977.210542245, 601977.210542245, 143877.0470666403]
[2019-04-27 19:06:10,974] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 19:06:10,977] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [4.4661674e-26 1.0000000e+00 6.8901345e-25 2.5039337e-13 2.9977239e-25], sampled 0.6729673175852924
[2019-04-27 19:06:30,967] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8340.8156 2417806650.5948 413.0000
[2019-04-27 19:06:30,969] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8911.4361 2156805198.0499 305.0000
[2019-04-27 19:06:31,329] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 9009.0853 2113979416.2921 308.0000
[2019-04-27 19:06:31,342] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8844.4426 2183948781.2505 360.0000
[2019-04-27 19:06:31,382] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8786.2975 2222466284.5419 286.0000
[2019-04-27 19:06:32,396] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 700000, evaluation results [700000.0, 8340.815609091267, 2417806650.594796, 413.0, 8911.4360785956, 2156805198.04992, 305.0, 9009.085344263778, 2113979416.2921085, 308.0, 8786.29754890976, 2222466284.541891, 286.0, 8844.442569556426, 2183948781.2505436, 360.0]
[2019-04-27 19:06:32,507] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2231713e-23 1.0000000e+00 1.0804964e-20 1.6208272e-08 4.1882649e-20], sum to 1.0000
[2019-04-27 19:06:32,524] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5726
[2019-04-27 19:06:32,531] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.8, 63.0, 1.0, 2.0, 0.3455872244384061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 435837.2263587447, 435837.2263587447, 121352.5610097532], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2419200.0000, 
sim time next is 2419800.0000, 
raw observation next is [22.66666666666667, 63.0, 1.0, 2.0, 0.3411975040969739, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 430964.1874114221, 430964.1874114226, 120784.6509170909], 
processed observation next is [1.0, 0.0, 0.39506172839506193, 0.63, 1.0, 1.0, 0.21571131440115943, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.15391578121836502, 0.15391578121836522, 0.23227817484055943], 
reward next is 0.7677, 
noisyNet noise sample is [array([0.81763524], dtype=float32), 0.63242716]. 
=============================================
[2019-04-27 19:06:35,683] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0557199e-21 1.3110388e-09 1.3271520e-15 1.0000000e+00 1.9395327e-11], sum to 1.0000
[2019-04-27 19:06:35,691] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0980
[2019-04-27 19:06:35,695] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.66666666666667, 23.0, 1.0, 2.0, 0.6053364643887391, 1.0, 2.0, 0.6053364643887391, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1463609.976537478, 1463609.976537479, 273432.3164719258], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2472600.0000, 
sim time next is 2473200.0000, 
raw observation next is [34.6, 23.0, 1.0, 2.0, 0.5988174752792277, 1.0, 2.0, 0.5988174752792277, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1448872.066892143, 1448872.066892144, 271175.8029420663], 
processed observation next is [1.0, 0.6521739130434783, 0.8370370370370371, 0.23, 1.0, 1.0, 0.5224017562847948, 1.0, 1.0, 0.5224017562847948, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.5174543096043368, 0.5174543096043371, 0.5214919287347428], 
reward next is 0.4785, 
noisyNet noise sample is [array([2.1644416], dtype=float32), -0.35524237]. 
=============================================
[2019-04-27 19:06:37,317] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.8602308e-32 1.0000000e+00 2.5285143e-29 7.3931482e-14 6.6012091e-30], sum to 1.0000
[2019-04-27 19:06:37,330] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7546
[2019-04-27 19:06:37,334] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.26666666666667, 36.16666666666667, 1.0, 2.0, 0.3612225278649637, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 451923.5057644172, 451923.5057644167, 123377.2045172227], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2494200.0000, 
sim time next is 2494800.0000, 
raw observation next is [29.1, 37.0, 1.0, 2.0, 0.3653591442065292, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 456820.7471540273, 456820.7471540269, 123929.0353113813], 
processed observation next is [1.0, 0.9130434782608695, 0.6333333333333334, 0.37, 1.0, 1.0, 0.24447517167443955, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.16315026684072403, 0.1631502668407239, 0.2383250679065025], 
reward next is 0.7617, 
noisyNet noise sample is [array([0.44377962], dtype=float32), -0.14495645]. 
=============================================
[2019-04-27 19:06:37,904] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.8896096e-30 1.0000000e+00 2.2767572e-26 1.8786481e-11 1.4164553e-27], sum to 1.0000
[2019-04-27 19:06:37,912] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8431
[2019-04-27 19:06:37,917] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 49.0, 1.0, 2.0, 0.3790672305382004, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 472574.7179946811, 472574.7179946811, 125769.7694342747], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2505600.0000, 
sim time next is 2506200.0000, 
raw observation next is [26.41666666666667, 49.33333333333334, 1.0, 2.0, 0.3778878201990825, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 471146.1621983036, 471146.1621983036, 125608.7340991748], 
processed observation next is [1.0, 0.0, 0.5339506172839508, 0.4933333333333334, 1.0, 1.0, 0.2593902621417649, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16826648649939416, 0.16826648649939416, 0.24155525788302848], 
reward next is 0.7584, 
noisyNet noise sample is [array([-0.38563058], dtype=float32), 0.9225938]. 
=============================================
[2019-04-27 19:06:50,413] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [9.7367401e-36 1.0000000e+00 1.4163123e-38 3.6426656e-25 7.0016929e-37], sum to 1.0000
[2019-04-27 19:06:50,421] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9888
[2019-04-27 19:06:50,428] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.76666666666667, 63.66666666666667, 1.0, 2.0, 0.6434821545078269, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 733356.8079532294, 733356.8079532294, 165720.1948135308], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2747400.0000, 
sim time next is 2748000.0000, 
raw observation next is [29.53333333333334, 65.33333333333334, 1.0, 2.0, 0.6550489246891875, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 746545.5191854073, 746545.5191854073, 167808.2698594477], 
processed observation next is [0.0, 0.8260869565217391, 0.6493827160493829, 0.6533333333333334, 1.0, 1.0, 0.5893439579633185, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.26662339970907406, 0.26662339970907406, 0.3227082112681686], 
reward next is 0.6773, 
noisyNet noise sample is [array([-1.0546149], dtype=float32), 3.0415907]. 
=============================================
[2019-04-27 19:06:50,439] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[70.80925 ]
 [70.799614]
 [70.80241 ]
 [70.80722 ]
 [70.80636 ]], R is [[70.7465744 ]
 [70.72041321]
 [70.69471741]
 [70.67307281]
 [70.65578461]].
[2019-04-27 19:06:54,788] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4560037e-35 1.0000000e+00 7.7958310e-37 7.0218052e-30 1.2753975e-33], sum to 1.0000
[2019-04-27 19:06:54,791] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6847
[2019-04-27 19:06:54,797] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 68.0, 1.0, 2.0, 0.6744560500301537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 768674.5028649953, 768674.5028649953, 171364.4886218789], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2838000.0000, 
sim time next is 2838600.0000, 
raw observation next is [29.25, 68.5, 1.0, 2.0, 0.6791291555061129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000002, 6.9112, 121.9260426156618, 774003.1089914998, 774003.1089914988, 172230.2674901998], 
processed observation next is [1.0, 0.8695652173913043, 0.6388888888888888, 0.685, 1.0, 1.0, 0.6180108994120391, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 1.7763568394002506e-16, 0.0, 0.8094621288201359, 0.27642968178267846, 0.2764296817826781, 0.3312120528657688], 
reward next is 0.6688, 
noisyNet noise sample is [array([1.0553648], dtype=float32), -0.7156791]. 
=============================================
[2019-04-27 19:06:56,509] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4697056e-32 0.0000000e+00], sum to 1.0000
[2019-04-27 19:06:56,521] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9374
[2019-04-27 19:06:56,526] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.93333333333333, 92.66666666666667, 1.0, 2.0, 0.9439516248944975, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.021469717213401, 6.9112, 121.9255918595473, 1173669.528919145, 1117201.786670344, 229598.1353987375], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2882400.0000, 
sim time next is 2883000.0000, 
raw observation next is [22.96666666666667, 93.33333333333334, 1.0, 2.0, 0.9697949644360297, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.1722836587538, 6.9112, 121.9248427009426, 1278882.664100883, 1145185.811194439, 235596.1426864366], 
processed observation next is [1.0, 0.34782608695652173, 0.4061728395061729, 0.9333333333333335, 1.0, 1.0, 0.9640416243286068, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.026108365875380013, 0.0, 0.8094541626342239, 0.4567438086074582, 0.40899493256944247, 0.45306950516622424], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.09378087], dtype=float32), -1.3737475]. 
=============================================
[2019-04-27 19:06:56,541] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[74.79775 ]
 [75.659325]
 [76.374214]
 [77.384926]
 [77.44015 ]], R is [[73.33033752]
 [72.60415649]
 [72.46578979]
 [72.36382294]
 [72.34134674]].
[2019-04-27 19:06:56,558] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1260755e-33 0.0000000e+00], sum to 1.0000
[2019-04-27 19:06:56,565] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3394
[2019-04-27 19:06:56,572] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.26666666666667, 94.0, 1.0, 2.0, 0.5420214920616788, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 647243.188854497, 647243.188854497, 149681.0182447575], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2870400.0000, 
sim time next is 2871000.0000, 
raw observation next is [22.2, 94.0, 1.0, 2.0, 0.5305051270056265, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 634281.1623141796, 634281.1623141796, 147826.3930934278], 
processed observation next is [1.0, 0.21739130434782608, 0.37777777777777777, 0.94, 1.0, 1.0, 0.4410775321495553, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2265289865407784, 0.2265289865407784, 0.2842815251796688], 
reward next is 0.7157, 
noisyNet noise sample is [array([-1.1431652], dtype=float32), 2.3001263]. 
=============================================
[2019-04-27 19:06:56,587] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[77.360214]
 [77.31095 ]
 [77.06116 ]
 [77.48024 ]
 [77.17134 ]], R is [[77.45449829]
 [77.3921051 ]
 [77.32157898]
 [77.2504425 ]
 [77.17784119]].
[2019-04-27 19:07:14,190] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 19:07:14,199] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7383
[2019-04-27 19:07:14,204] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.75, 77.16666666666667, 1.0, 2.0, 0.4726120044331394, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 572209.2135343803, 572209.2135343803, 138950.8654830824], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3215400.0000, 
sim time next is 3216000.0000, 
raw observation next is [23.6, 78.33333333333334, 1.0, 2.0, 0.4709016191861217, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 569970.8173731556, 569970.8173731556, 138684.1707143591], 
processed observation next is [0.0, 0.21739130434782608, 0.4296296296296297, 0.7833333333333334, 1.0, 1.0, 0.37012097522157344, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.20356100620469844, 0.20356100620469844, 0.2667003282968444], 
reward next is 0.7333, 
noisyNet noise sample is [array([-0.2807132], dtype=float32), 0.5998075]. 
=============================================
[2019-04-27 19:07:14,217] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[76.737854]
 [76.81148 ]
 [76.91569 ]
 [77.000465]
 [77.0743  ]], R is [[76.65707397]
 [76.62329102]
 [76.58931732]
 [76.55431366]
 [76.51841736]].
[2019-04-27 19:07:14,992] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.5134850e-35 2.2537836e-38], sum to 1.0000
[2019-04-27 19:07:15,004] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0618
[2019-04-27 19:07:15,008] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 83.0, 1.0, 2.0, 0.4702360725328094, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 568628.3304964154, 568628.3304964158, 138565.3699262459], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3218400.0000, 
sim time next is 3219000.0000, 
raw observation next is [23.16666666666667, 82.16666666666667, 1.0, 2.0, 0.4713707457558776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 569620.3915711812, 569620.3915711812, 138726.2418059499], 
processed observation next is [0.0, 0.2608695652173913, 0.4135802469135804, 0.8216666666666668, 1.0, 1.0, 0.3706794592331877, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.20343585413256474, 0.20343585413256474, 0.26678123424221134], 
reward next is 0.7332, 
noisyNet noise sample is [array([1.1475688], dtype=float32), -2.5054746]. 
=============================================
[2019-04-27 19:07:15,022] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[67.80304 ]
 [67.82869 ]
 [67.854126]
 [67.89715 ]
 [67.93721 ]], R is [[67.80581665]
 [67.86128998]
 [67.91613007]
 [67.97031403]
 [68.02390289]].
[2019-04-27 19:07:15,287] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.2499872e-35 1.6645783e-36], sum to 1.0000
[2019-04-27 19:07:15,296] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2322
[2019-04-27 19:07:15,302] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.53333333333333, 65.33333333333333, 1.0, 2.0, 0.4668378311334212, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 565582.0547052455, 565582.0547052455, 138081.9660902531], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3224400.0000, 
sim time next is 3225000.0000, 
raw observation next is [25.91666666666667, 62.16666666666667, 1.0, 2.0, 0.4594600158497326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 558009.9874009512, 558009.9874009507, 137008.1472927195], 
processed observation next is [0.0, 0.30434782608695654, 0.5154320987654323, 0.6216666666666667, 1.0, 1.0, 0.3565000188687293, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.19928928121462544, 0.19928928121462527, 0.26347720633215294], 
reward next is 0.7365, 
noisyNet noise sample is [array([0.96872634], dtype=float32), -1.2181815]. 
=============================================
[2019-04-27 19:07:15,323] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[70.41985 ]
 [70.48609 ]
 [70.537766]
 [70.59297 ]
 [70.64628 ]], R is [[70.39965057]
 [70.43011475]
 [70.45878601]
 [70.48602295]
 [70.51211548]].
[2019-04-27 19:07:15,413] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 6.780603e-38 0.000000e+00], sum to 1.0000
[2019-04-27 19:07:15,422] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6874
[2019-04-27 19:07:15,428] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.53333333333333, 65.33333333333333, 1.0, 2.0, 0.4668378311334212, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 565582.0547052455, 565582.0547052455, 138081.9660902531], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3224400.0000, 
sim time next is 3225000.0000, 
raw observation next is [25.91666666666667, 62.16666666666667, 1.0, 2.0, 0.4594600158497326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 558009.9874009512, 558009.9874009507, 137008.1472927195], 
processed observation next is [0.0, 0.30434782608695654, 0.5154320987654323, 0.6216666666666667, 1.0, 1.0, 0.3565000188687293, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.19928928121462544, 0.19928928121462527, 0.26347720633215294], 
reward next is 0.7365, 
noisyNet noise sample is [array([-0.29384378], dtype=float32), 0.77548265]. 
=============================================
[2019-04-27 19:07:15,456] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[73.0479  ]
 [73.11714 ]
 [73.17239 ]
 [73.229004]
 [73.28569 ]], R is [[72.99909973]
 [73.00357056]
 [73.00650787]
 [73.00827026]
 [73.00914001]].
[2019-04-27 19:07:21,486] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-27 19:07:21,487] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 19:07:21,488] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:07:21,488] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 19:07:21,489] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:07:21,490] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 19:07:21,490] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:07:21,492] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 19:07:21,494] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:07:21,494] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 19:07:21,496] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:07:21,512] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run30
[2019-04-27 19:07:21,533] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run30
[2019-04-27 19:07:21,554] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run30
[2019-04-27 19:07:21,574] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run30
[2019-04-27 19:07:21,594] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run30
[2019-04-27 19:07:45,409] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04764618], dtype=float32), 0.023754846]
[2019-04-27 19:07:45,411] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [35.08333333333333, 22.83333333333334, 1.0, 2.0, 0.6703242730292888, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 826544.6455772003, 826544.6455772003, 173096.4106007661]
[2019-04-27 19:07:45,412] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:07:45,415] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [2.3527106e-37 1.0000000e+00 0.0000000e+00 1.9320832e-32 1.4323278e-33], sampled 0.713715241372479
[2019-04-27 19:08:01,909] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04764618], dtype=float32), 0.023754846]
[2019-04-27 19:08:01,910] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [26.83333333333334, 75.33333333333333, 1.0, 2.0, 0.8568665475735611, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 985129.5676868767, 985129.5676868767, 208417.650920199]
[2019-04-27 19:08:01,912] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 19:08:01,914] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [3.2389583e-36 1.0000000e+00 0.0000000e+00 1.6707470e-31 4.8712640e-33], sampled 0.47877440657057213
[2019-04-27 19:08:31,940] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04764618], dtype=float32), 0.023754846]
[2019-04-27 19:08:31,941] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [28.55, 67.5, 1.0, 2.0, 0.8419369419510119, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156539, 959671.2202556402, 959671.2202556402, 204750.4101344028]
[2019-04-27 19:08:31,942] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:08:31,947] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.1760761e-34 1.0000000e+00 7.4225067e-37 1.3857441e-29 1.2578362e-30], sampled 0.12426861400617717
[2019-04-27 19:09:06,685] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04764618], dtype=float32), 0.023754846]
[2019-04-27 19:09:06,687] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [24.0, 48.0, 1.0, 2.0, 0.2883774664185744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 368972.2334224242, 368972.2334224242, 114152.9267265302]
[2019-04-27 19:09:06,688] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:09:06,691] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.0508455e-35 3.1582467e-37], sampled 0.6021260092823417
[2019-04-27 19:09:11,708] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8694.7062 2196034964.8489 571.0000
[2019-04-27 19:09:11,726] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8094.5562 2445795817.4850 746.0000
[2019-04-27 19:09:11,928] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8581.3673 2249020318.0583 553.0000
[2019-04-27 19:09:11,945] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8919.6263 2121039341.1523 431.0000
[2019-04-27 19:09:12,075] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8766.4988 2171109916.7537 491.0000
[2019-04-27 19:09:13,092] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 725000, evaluation results [725000.0, 8094.5561833614265, 2445795817.485029, 746.0, 8766.498813668597, 2171109916.75366, 491.0, 8919.626301812208, 2121039341.1522667, 431.0, 8581.367336944755, 2249020318.0582643, 553.0, 8694.706197903712, 2196034964.848895, 571.0]
[2019-04-27 19:09:17,376] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0424138e-11 8.4255560e-04 9.4099910e-08 2.3572592e-04 9.9892163e-01], sum to 1.0000
[2019-04-27 19:09:17,382] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7972
[2019-04-27 19:09:17,388] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.6, 64.0, 1.0, 2.0, 0.6071461656432688, 1.0, 2.0, 0.6071461656432688, 1.0, 2.0, 0.9665964690742481, 6.911199999999999, 6.9112, 121.94756008, 2077561.754978239, 2077561.75497824, 399282.9411725183], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3427200.0000, 
sim time next is 3427800.0000, 
raw observation next is [30.38333333333334, 64.83333333333334, 1.0, 2.0, 0.5923040784776865, 1.0, 2.0, 0.5923040784776865, 1.0, 2.0, 0.9429673829336082, 6.9112, 6.9112, 121.94756008, 2026716.768151674, 2026716.768151674, 391198.4049063274], 
processed observation next is [1.0, 0.6956521739130435, 0.6808641975308645, 0.6483333333333334, 1.0, 1.0, 0.5146477124734363, 1.0, 1.0, 0.5146477124734363, 1.0, 1.0, 0.9287092286670101, 0.0, 0.0, 0.8096049824067558, 0.7238274171970264, 0.7238274171970264, 0.7523046248198605], 
reward next is 0.2477, 
noisyNet noise sample is [array([0.55825466], dtype=float32), 1.526311]. 
=============================================
[2019-04-27 19:09:17,426] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.8426653e-12 9.4636629e-04 1.6255225e-08 5.9999403e-04 9.9845362e-01], sum to 1.0000
[2019-04-27 19:09:17,434] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5259
[2019-04-27 19:09:17,445] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.6, 64.0, 1.0, 2.0, 0.6071461656432688, 1.0, 2.0, 0.6071461656432688, 1.0, 2.0, 0.9665964690742481, 6.911199999999999, 6.9112, 121.94756008, 2077561.754978239, 2077561.75497824, 399282.9411725183], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3427200.0000, 
sim time next is 3427800.0000, 
raw observation next is [30.38333333333334, 64.83333333333334, 1.0, 2.0, 0.5923040784776865, 1.0, 2.0, 0.5923040784776865, 1.0, 2.0, 0.9429673829336082, 6.9112, 6.9112, 121.94756008, 2026716.768151674, 2026716.768151674, 391198.4049063274], 
processed observation next is [1.0, 0.6956521739130435, 0.6808641975308645, 0.6483333333333334, 1.0, 1.0, 0.5146477124734363, 1.0, 1.0, 0.5146477124734363, 1.0, 1.0, 0.9287092286670101, 0.0, 0.0, 0.8096049824067558, 0.7238274171970264, 0.7238274171970264, 0.7523046248198605], 
reward next is 0.2477, 
noisyNet noise sample is [array([0.65713376], dtype=float32), -0.16376758]. 
=============================================
[2019-04-27 19:09:17,806] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.8053753e-20 1.0000000e+00 7.1731293e-20 2.6624406e-17 2.1936342e-17], sum to 1.0000
[2019-04-27 19:09:17,810] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6546
[2019-04-27 19:09:17,819] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2199238.05030168 W.
[2019-04-27 19:09:17,824] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.53333333333333, 60.66666666666667, 1.0, 2.0, 0.6585928520969028, 1.0, 2.0, 0.6426610880248862, 1.0, 1.0, 0.9977734948820727, 6.9112, 6.9112, 121.94756008, 2199238.05030168, 2199238.05030168, 418298.6062491789], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3424800.0000, 
sim time next is 3425400.0000, 
raw observation next is [31.3, 61.5, 1.0, 2.0, 0.9585863675965767, 1.0, 2.0, 0.9585863675965767, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 2186891.922831581, 2186891.922831581, 413642.8225035382], 
processed observation next is [1.0, 0.6521739130434783, 0.7148148148148148, 0.615, 1.0, 1.0, 0.9506980566625913, 1.0, 1.0, 0.9506980566625913, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288201359, 0.7810328295827075, 0.7810328295827075, 0.7954669663529581], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0746518], dtype=float32), -0.597513]. 
=============================================
[2019-04-27 19:09:19,396] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2564010e-36 1.0000000e+00 0.0000000e+00 1.6329518e-33 2.7418870e-33], sum to 1.0000
[2019-04-27 19:09:19,402] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3736
[2019-04-27 19:09:19,408] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 94.0, 1.0, 2.0, 0.6596301877091532, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 751769.2488673206, 751769.2488673211, 168641.3127360309], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3456000.0000, 
sim time next is 3456600.0000, 
raw observation next is [25.0, 94.00000000000001, 1.0, 2.0, 0.6623386144748133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 754857.5164333698, 754857.5164333694, 169135.6628289404], 
processed observation next is [1.0, 0.0, 0.48148148148148145, 0.9400000000000002, 1.0, 1.0, 0.5980221600890635, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.26959197015477493, 0.26959197015477476, 0.3252608900556546], 
reward next is 0.6747, 
noisyNet noise sample is [array([0.71092623], dtype=float32), -0.5029084]. 
=============================================
[2019-04-27 19:09:38,504] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7963955e-31 1.0000000e+00 2.8666783e-30 7.0833783e-25 1.2272566e-21], sum to 1.0000
[2019-04-27 19:09:38,511] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2703
[2019-04-27 19:09:38,515] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.5, 61.0, 1.0, 2.0, 0.6465257311494234, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 736827.145683931, 736827.145683931, 166268.162857049], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3835800.0000, 
sim time next is 3836400.0000, 
raw observation next is [30.66666666666666, 61.66666666666667, 1.0, 2.0, 0.6693843567870451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 762891.4406287103, 762891.4406287099, 170429.6781177777], 
processed observation next is [0.0, 0.391304347826087, 0.6913580246913578, 0.6166666666666667, 1.0, 1.0, 0.606409948556006, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.27246122879596796, 0.2724612287959678, 0.3277493809957263], 
reward next is 0.6723, 
noisyNet noise sample is [array([-0.0833856], dtype=float32), -1.5747718]. 
=============================================
[2019-04-27 19:09:43,934] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.2854537e-31 1.0000000e+00 8.9712924e-32 2.8386414e-26 1.4895770e-20], sum to 1.0000
[2019-04-27 19:09:43,945] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9297
[2019-04-27 19:09:43,949] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.8, 68.5, 1.0, 2.0, 0.7529397704525722, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 858172.1181697114, 858172.1181697114, 186414.1353679178], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3940200.0000, 
sim time next is 3940800.0000, 
raw observation next is [30.86666666666667, 69.0, 1.0, 2.0, 0.7825174332047576, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 891903.2293104073, 891903.2293104068, 192360.5748182968], 
processed observation next is [0.0, 0.6086956521739131, 0.6987654320987656, 0.69, 1.0, 1.0, 0.7410921823866162, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.31853686761085975, 0.3185368676108596, 0.36992418234287844], 
reward next is 0.6301, 
noisyNet noise sample is [array([-1.3175312], dtype=float32), 0.08889829]. 
=============================================
[2019-04-27 19:09:46,668] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.4507119e-28 1.0000000e+00 9.5333758e-30 1.7053545e-27 9.4038493e-23], sum to 1.0000
[2019-04-27 19:09:46,671] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2746
[2019-04-27 19:09:46,675] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 92.0, 1.0, 2.0, 0.8826519228536063, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1008366.369759054, 1008366.369759054, 213726.8399036905], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3992400.0000, 
sim time next is 3993000.0000, 
raw observation next is [24.68333333333333, 92.16666666666667, 1.0, 2.0, 1.004259822323145, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.18494469156619, 6.9112, 121.9250583469639, 1287714.780913994, 1147534.163450201, 241901.0347518999], 
processed observation next is [1.0, 0.21739130434782608, 0.46975308641975294, 0.9216666666666667, 1.0, 1.0, 1.005071217051363, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.027374469156618987, 0.0, 0.8094555942995492, 0.4598981360407121, 0.40983362980364324, 0.46519429759980746], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7396948], dtype=float32), -1.0998807]. 
=============================================
[2019-04-27 19:09:46,693] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[53.233387]
 [53.060814]
 [53.130604]
 [53.072342]
 [53.47312 ]], R is [[52.2845993 ]
 [52.35073853]
 [52.40259933]
 [52.44521713]
 [51.92076492]].
[2019-04-27 19:09:52,500] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6308015e-21 1.0000000e+00 1.2744381e-20 2.5109781e-17 1.0118289e-10], sum to 1.0000
[2019-04-27 19:09:52,510] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6993
[2019-04-27 19:09:52,516] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1927894.195865472 W.
[2019-04-27 19:09:52,519] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.83333333333333, 65.0, 1.0, 2.0, 0.8451815983946351, 1.0, 2.0, 0.8451815983946351, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156565, 1927894.195865472, 1927894.195865472, 362776.9636291012], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4110600.0000, 
sim time next is 4111200.0000, 
raw observation next is [28.8, 64.0, 1.0, 2.0, 0.5298799049002901, 1.0, 2.0, 0.5298799049002901, 1.0, 1.0, 0.8435860655849918, 6.911200000000001, 6.9112, 121.94756008, 1812900.069135775, 1812900.069135775, 358496.4292452987], 
processed observation next is [1.0, 0.6086956521739131, 0.6222222222222222, 0.64, 1.0, 1.0, 0.440333220119393, 1.0, 1.0, 0.440333220119393, 1.0, 0.5, 0.8044825819812398, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.647464310405634, 0.647464310405634, 0.689416210087113], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7865971], dtype=float32), -1.3442197]. 
=============================================
[2019-04-27 19:10:01,902] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.8783601e-33 1.0000000e+00 5.7727517e-34 9.7649454e-34 2.4348148e-28], sum to 1.0000
[2019-04-27 19:10:01,912] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2752
[2019-04-27 19:10:01,917] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.83333333333334, 52.5, 1.0, 2.0, 0.5674086978544605, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 656927.8714054809, 656927.8714054804, 153057.0454328195], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4299000.0000, 
sim time next is 4299600.0000, 
raw observation next is [30.66666666666667, 53.0, 1.0, 2.0, 0.5731784498035801, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 664224.2060385807, 664224.2060385807, 154053.3338039763], 
processed observation next is [1.0, 0.782608695652174, 0.6913580246913582, 0.53, 1.0, 1.0, 0.4918791069090239, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.23722293072806455, 0.23722293072806455, 0.29625641116149287], 
reward next is 0.7037, 
noisyNet noise sample is [array([-1.440648], dtype=float32), 0.854847]. 
=============================================
[2019-04-27 19:10:02,156] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-04-27 19:10:02,157] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 19:10:02,158] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:10:02,159] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 19:10:02,162] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:10:02,163] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 19:10:02,163] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 19:10:02,164] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:10:02,165] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 19:10:02,166] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:10:02,169] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:10:02,187] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run31
[2019-04-27 19:10:02,188] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run31
[2019-04-27 19:10:02,230] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run31
[2019-04-27 19:10:02,254] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run31
[2019-04-27 19:10:02,273] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run31
[2019-04-27 19:10:11,103] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05679755], dtype=float32), 0.023949632]
[2019-04-27 19:10:11,104] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [31.16666666666667, 21.33333333333333, 1.0, 2.0, 0.6972104834728666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 890508.0680128016, 890508.068012802, 178734.0200814135]
[2019-04-27 19:10:11,105] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 19:10:11,108] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.9049379e-26 1.0000000e+00 3.9917937e-27 2.9077460e-24 1.0848387e-17], sampled 0.9032522475281821
[2019-04-27 19:10:11,912] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05679755], dtype=float32), 0.023949632]
[2019-04-27 19:10:11,915] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [20.66666666666667, 69.0, 1.0, 2.0, 0.3058617502215358, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 390520.3274004331, 390520.3274004331, 116303.3695461347]
[2019-04-27 19:10:11,916] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:10:11,921] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [3.1126996e-30 1.0000000e+00 3.1134802e-32 2.4475809e-29 6.5679727e-24], sampled 0.950012147786668
[2019-04-27 19:10:12,786] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05679755], dtype=float32), 0.023949632]
[2019-04-27 19:10:12,787] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [30.4, 27.0, 1.0, 2.0, 0.3566346876671472, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 452293.3754125643, 452293.3754125643, 122843.5354592916]
[2019-04-27 19:10:12,788] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:10:12,791] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [3.7417194e-31 1.0000000e+00 1.6285171e-32 1.4143805e-29 7.6563659e-23], sampled 0.4903648665704252
[2019-04-27 19:10:16,284] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05679755], dtype=float32), 0.023949632]
[2019-04-27 19:10:16,286] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [22.83333333333334, 39.0, 1.0, 2.0, 0.262221321173397, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 338246.4197906826, 338246.4197906826, 95059.86199602012]
[2019-04-27 19:10:16,288] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 19:10:16,292] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.9475062e-30 1.0000000e+00 8.5433725e-32 7.1568339e-29 2.2748605e-22], sampled 0.27199915020799426
[2019-04-27 19:10:18,923] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05679755], dtype=float32), 0.023949632]
[2019-04-27 19:10:18,925] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [20.25, 71.5, 1.0, 2.0, 0.3043769112609764, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 388762.3817956125, 388762.3817956125, 116118.7895502459]
[2019-04-27 19:10:18,925] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:10:18,928] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [5.9372279e-30 1.0000000e+00 2.3954266e-31 1.7149631e-28 4.3235681e-22], sampled 0.24096680362795475
[2019-04-27 19:10:35,466] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05679755], dtype=float32), 0.023949632]
[2019-04-27 19:10:35,468] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [22.01838014, 92.1821396, 1.0, 2.0, 0.6073924571635767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 729078.288421049, 729078.2884210485, 160953.1109602432]
[2019-04-27 19:10:35,469] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 19:10:35,471] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [7.8751646e-28 1.0000000e+00 3.0318973e-29 3.1405918e-26 2.4074130e-20], sampled 0.35752815714749053
[2019-04-27 19:10:48,882] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05679755], dtype=float32), 0.023949632]
[2019-04-27 19:10:48,882] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [23.94064442, 93.34568569999999, 1.0, 2.0, 0.5784818256932838, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 670406.5825574426, 670406.5825574426, 154950.5432602354]
[2019-04-27 19:10:48,883] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 19:10:48,886] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [3.174227e-29 1.000000e+00 6.269261e-31 7.222358e-28 3.553853e-22], sampled 0.6186232252328558
[2019-04-27 19:11:06,046] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05679755], dtype=float32), 0.023949632]
[2019-04-27 19:11:06,049] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [26.7323091, 78.28717652, 1.0, 2.0, 0.6015969773388622, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 688905.8094188871, 688905.8094188871, 158516.6629411044]
[2019-04-27 19:11:06,050] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:11:06,054] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [5.9869711e-29 1.0000000e+00 1.2494959e-30 1.6154228e-27 7.3598294e-22], sampled 0.6752997996693162
[2019-04-27 19:11:17,294] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05679755], dtype=float32), 0.023949632]
[2019-04-27 19:11:17,295] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [26.16666666666667, 78.16666666666667, 1.0, 2.0, 0.5927505737306383, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 684725.764892267, 684725.764892267, 157280.730410182]
[2019-04-27 19:11:17,295] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:11:17,299] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [4.3730185e-28 1.0000000e+00 1.1483014e-28 1.1536025e-25 1.8582639e-18], sampled 0.4114915692788218
[2019-04-27 19:11:18,742] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05679755], dtype=float32), 0.023949632]
[2019-04-27 19:11:18,743] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [25.76666666666667, 82.0, 1.0, 2.0, 0.6047105702976565, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 696186.2646207544, 696186.2646207544, 159233.6803432572]
[2019-04-27 19:11:18,745] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:11:18,750] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.0747251e-27 1.0000000e+00 2.1871713e-28 2.1285932e-25 2.0444412e-18], sampled 0.09427570428474796
[2019-04-27 19:11:28,131] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05679755], dtype=float32), 0.023949632]
[2019-04-27 19:11:28,134] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [27.33333333333334, 72.66666666666666, 1.0, 2.0, 0.7957061153837954, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 913555.1767602565, 913555.1767602565, 195397.1129739666]
[2019-04-27 19:11:28,135] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:11:28,138] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.01123075e-25 1.00000000e+00 3.37243645e-26 3.28913283e-23
 2.29034930e-16], sampled 0.7640339943112173
[2019-04-27 19:11:34,546] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05679755], dtype=float32), 0.023949632]
[2019-04-27 19:11:34,548] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [27.93333333333333, 74.0, 1.0, 2.0, 0.6629795501798713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 755588.3413926923, 755588.3413926918, 169252.7824381839]
[2019-04-27 19:11:34,551] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:11:34,554] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.6577927e-27 1.0000000e+00 7.1278479e-28 7.2452728e-25 1.8188224e-17], sampled 0.29718809627646203
[2019-04-27 19:11:35,029] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05679755], dtype=float32), 0.023949632]
[2019-04-27 19:11:35,030] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [26.83333333333334, 86.33333333333334, 1.0, 2.0, 1.02, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 7.023309466502132, 6.9112, 121.9255504164751, 1935546.74249697, 1878136.907174256, 383731.981898638]
[2019-04-27 19:11:35,031] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:11:35,036] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.5391176e-14 7.4811107e-01 3.6019500e-13 3.0452325e-09 2.5188899e-01], sampled 0.41334089722068856
[2019-04-27 19:11:35,038] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1935546.74249697 W.
[2019-04-27 19:11:44,817] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05679755], dtype=float32), 0.023949632]
[2019-04-27 19:11:44,818] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [20.48682982, 81.98837115, 1.0, 2.0, 0.4903133499280544, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 614012.8543470848, 614012.8543470848, 142196.77720788]
[2019-04-27 19:11:44,821] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:11:44,824] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [2.9075849e-28 1.0000000e+00 7.7762265e-30 5.8479322e-27 2.6183199e-21], sampled 0.8017834963679816
[2019-04-27 19:11:52,697] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8644.6412 2257619766.7634 413.0000
[2019-04-27 19:11:53,035] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8937.7282 2129746898.4369 364.0000
[2019-04-27 19:11:53,101] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8174.9857 2458074198.7019 517.0000
[2019-04-27 19:11:53,131] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8827.0797 2178073411.7517 382.0000
[2019-04-27 19:11:53,268] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8734.9730 2209106711.9907 454.0000
[2019-04-27 19:11:54,282] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 750000, evaluation results [750000.0, 8174.985669207887, 2458074198.701932, 517.0, 8827.07973158089, 2178073411.751651, 382.0, 8937.728158006166, 2129746898.4368894, 364.0, 8644.641243292457, 2257619766.7633696, 413.0, 8734.972955427322, 2209106711.9907446, 454.0]
[2019-04-27 19:11:57,801] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.1220946e-18 3.0362219e-09 1.2401985e-14 1.8177053e-10 1.0000000e+00], sum to 1.0000
[2019-04-27 19:11:57,811] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5308
[2019-04-27 19:11:57,817] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.0, 52.33333333333334, 1.0, 2.0, 0.7342789546398795, 1.0, 2.0, 0.6805041392963744, 1.0, 2.0, 0.9977734948820727, 6.911200000000001, 6.9112, 121.94756008, 2328908.905408437, 2328908.905408437, 438578.4269521697], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4374600.0000, 
sim time next is 4375200.0000, 
raw observation next is [33.0, 51.66666666666667, 1.0, 2.0, 0.7690312823598028, 1.0, 2.0, 0.6978803031563361, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.94756008, 2388455.393066736, 2388455.393066736, 448305.0590861962], 
processed observation next is [1.0, 0.6521739130434783, 0.7777777777777778, 0.5166666666666667, 1.0, 1.0, 0.7250372409045271, 1.0, 1.0, 0.6403336942337334, 1.0, 1.0, 0.9972168686025908, 0.0, 0.0, 0.8096049824067558, 0.85301978323812, 0.85301978323812, 0.8621251136273004], 
reward next is 0.1379, 
noisyNet noise sample is [array([0.2732847], dtype=float32), -0.3499093]. 
=============================================
[2019-04-27 19:12:02,279] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.8470038e-35 1.0000000e+00 8.6661454e-37 2.4878139e-36 2.3463873e-26], sum to 1.0000
[2019-04-27 19:12:02,285] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3219
[2019-04-27 19:12:02,290] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.26666666666667, 92.0, 1.0, 2.0, 0.4909361864062245, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 589161.1846985806, 589161.1846985806, 141608.1517673643], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4425600.0000, 
sim time next is 4426200.0000, 
raw observation next is [22.2, 92.5, 1.0, 2.0, 0.4905025441756574, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 588683.007064521, 588683.0070645206, 141542.0786474962], 
processed observation next is [0.0, 0.21739130434782608, 0.37777777777777777, 0.925, 1.0, 1.0, 0.3934554097329255, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2102439310944718, 0.21024393109447165, 0.27219630509133885], 
reward next is 0.7278, 
noisyNet noise sample is [array([0.58228254], dtype=float32), -1.489871]. 
=============================================
[2019-04-27 19:12:05,658] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.3871235e-34 1.0000000e+00 2.7801891e-36 7.4694452e-33 5.9589381e-24], sum to 1.0000
[2019-04-27 19:12:05,667] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4301
[2019-04-27 19:12:05,674] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.93333333333333, 91.33333333333334, 1.0, 2.0, 0.5883734437232274, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 684607.1271207186, 684607.1271207181, 156757.154511097], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4498800.0000, 
sim time next is 4499400.0000, 
raw observation next is [23.91666666666667, 90.66666666666666, 1.0, 2.0, 0.5821452260075626, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 678814.8038914395, 678814.8038914395, 155758.6119874034], 
processed observation next is [0.0, 0.043478260869565216, 0.4413580246913582, 0.9066666666666666, 1.0, 1.0, 0.5025538404851936, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.24243385853265695, 0.24243385853265695, 0.29953579228346805], 
reward next is 0.7005, 
noisyNet noise sample is [array([0.09005496], dtype=float32), -1.5933882]. 
=============================================
[2019-04-27 19:12:09,373] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.6837447e-35 1.0000000e+00 1.3044409e-36 2.6031175e-32 5.4797109e-25], sum to 1.0000
[2019-04-27 19:12:09,386] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3385
[2019-04-27 19:12:09,396] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 94.0, 1.0, 2.0, 0.5809474561304158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 675712.2408315028, 675712.2408315028, 155478.8704360882], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4563000.0000, 
sim time next is 4563600.0000, 
raw observation next is [23.46666666666667, 94.0, 1.0, 2.0, 0.5734449492247621, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 668836.6896821251, 668836.6896821251, 154290.9621920363], 
processed observation next is [0.0, 0.8260869565217391, 0.42469135802469143, 0.94, 1.0, 1.0, 0.49219636812471673, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2388702463150447, 0.2388702463150447, 0.29671338883083903], 
reward next is 0.7033, 
noisyNet noise sample is [array([1.4517714], dtype=float32), -0.086461134]. 
=============================================
[2019-04-27 19:12:10,584] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.3262676e-30 1.0000000e+00 4.8145645e-34 5.4764133e-31 6.2426680e-26], sum to 1.0000
[2019-04-27 19:12:10,591] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2644
[2019-04-27 19:12:10,596] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.06666666666667, 99.83333333333334, 1.0, 2.0, 0.5260045931331206, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 634262.6524537991, 634262.6524537991, 147284.278668725], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4594200.0000, 
sim time next is 4594800.0000, 
raw observation next is [21.13333333333333, 99.66666666666667, 1.0, 2.0, 0.4815036466629004, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 580120.9200979457, 580120.9200979457, 140224.4578386892], 
processed observation next is [1.0, 0.17391304347826086, 0.33827160493827146, 0.9966666666666667, 1.0, 1.0, 0.3827424365034529, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.20718604289212347, 0.20718604289212347, 0.26966241892055615], 
reward next is 0.7303, 
noisyNet noise sample is [array([1.0989536], dtype=float32), 0.27703372]. 
=============================================
[2019-04-27 19:12:11,724] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3266301e-16 1.0000000e+00 2.4489875e-17 9.3068177e-15 1.5739939e-09], sum to 1.0000
[2019-04-27 19:12:11,731] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5630
[2019-04-27 19:12:11,736] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1627250.463904633 W.
[2019-04-27 19:12:11,739] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.0, 85.66666666666667, 1.0, 2.0, 0.7135004548647966, 1.0, 2.0, 0.7135004548647966, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1627250.463904633, 1627250.463904634, 309307.3252347133], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4612800.0000, 
sim time next is 4613400.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.7008846548386355, 1.0, 2.0, 0.7008846548386355, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1598452.400636215, 1598452.400636215, 304499.7924187316], 
processed observation next is [1.0, 0.391304347826087, 0.5185185185185185, 0.84, 1.0, 1.0, 0.643910303379328, 1.0, 1.0, 0.643910303379328, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.5708758573700767, 0.5708758573700767, 0.5855765238821762], 
reward next is 0.4144, 
noisyNet noise sample is [array([0.17861578], dtype=float32), -0.7518871]. 
=============================================
[2019-04-27 19:12:14,230] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1881587e-31 1.0000000e+00 2.0209717e-31 8.4748633e-29 5.3377267e-17], sum to 1.0000
[2019-04-27 19:12:14,239] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0967
[2019-04-27 19:12:14,243] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.6, 89.0, 1.0, 2.0, 0.6742998717215981, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 768496.4179262482, 768496.4179262482, 171333.7039570647], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4662000.0000, 
sim time next is 4662600.0000, 
raw observation next is [25.5, 89.83333333333334, 1.0, 2.0, 0.6689259112879898, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 762368.6945120457, 762368.6945120452, 170343.0084551845], 
processed observation next is [1.0, 1.0, 0.5, 0.8983333333333334, 1.0, 1.0, 0.6058641801047497, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.27227453375430205, 0.2722745337543019, 0.3275827085676625], 
reward next is 0.6724, 
noisyNet noise sample is [array([0.7195273], dtype=float32), -3.1255896]. 
=============================================
[2019-04-27 19:12:16,333] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0108014e-20 1.5500104e-10 2.2597414e-15 2.7202932e-12 1.0000000e+00], sum to 1.0000
[2019-04-27 19:12:16,346] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1388
[2019-04-27 19:12:16,349] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.3, 79.0, 1.0, 2.0, 0.5075817246899222, 1.0, 2.0, 0.5075817246899222, 1.0, 2.0, 0.8080866364890554, 6.9112, 6.9112, 121.94756008, 1736536.238702919, 1736536.238702919, 347325.832589319], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4705200.0000, 
sim time next is 4705800.0000, 
raw observation next is [27.08333333333333, 80.66666666666667, 1.0, 2.0, 0.5492358682277476, 1.0, 2.0, 0.5492358682277476, 1.0, 2.0, 0.8744013895820205, 6.911200000000001, 6.9112, 121.94756008, 1879193.080746049, 1879193.080746049, 368411.2611555055], 
processed observation next is [1.0, 0.4782608695652174, 0.5586419753086418, 0.8066666666666668, 1.0, 1.0, 0.46337603360446145, 1.0, 1.0, 0.46337603360446145, 1.0, 1.0, 0.8430017369775256, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.6711403859807318, 0.6711403859807318, 0.7084831945298182], 
reward next is 0.2915, 
noisyNet noise sample is [array([0.4527545], dtype=float32), -1.2496427]. 
=============================================
[2019-04-27 19:12:17,502] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.4585205e-21 5.3689209e-13 1.0787924e-18 3.4034987e-15 1.0000000e+00], sum to 1.0000
[2019-04-27 19:12:17,510] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1804
[2019-04-27 19:12:17,517] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.66666666666666, 85.66666666666667, 1.0, 2.0, 0.652720153258179, 1.0, 2.0, 0.6397247386055241, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.94756008, 2189177.308963597, 2189177.308963597, 416776.8121834382], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4722000.0000, 
sim time next is 4722600.0000, 
raw observation next is [27.83333333333334, 84.83333333333333, 1.0, 2.0, 0.670634654174378, 1.0, 2.0, 0.6486819890636237, 1.0, 2.0, 0.9977734948820727, 6.911200000000001, 6.9112, 121.94756008, 2219867.669007619, 2219867.669007618, 421442.3204427725], 
processed observation next is [1.0, 0.6521739130434783, 0.58641975308642, 0.8483333333333333, 1.0, 1.0, 0.6078983978266405, 1.0, 1.0, 0.5817642726947901, 1.0, 1.0, 0.9972168686025908, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.7928098817884354, 0.792809881788435, 0.8104660008514857], 
reward next is 0.1895, 
noisyNet noise sample is [array([0.80226374], dtype=float32), -1.0496166]. 
=============================================
[2019-04-27 19:12:22,959] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.3148765e-23 9.9999988e-01 1.2191348e-18 1.6741247e-20 6.6528791e-08], sum to 1.0000
[2019-04-27 19:12:22,968] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5692
[2019-04-27 19:12:22,978] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.05, 92.5, 1.0, 2.0, 0.712452549378589, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 812001.8788365468, 812001.8788365468, 178516.1508990969], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4833000.0000, 
sim time next is 4833600.0000, 
raw observation next is [26.06666666666667, 92.33333333333333, 1.0, 2.0, 0.7102817568412787, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 809526.4599810407, 809526.4599810407, 178100.9504440318], 
processed observation next is [1.0, 0.9565217391304348, 0.5209876543209878, 0.9233333333333333, 1.0, 1.0, 0.6550973295729509, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.28911659285037167, 0.28911659285037167, 0.34250182777698424], 
reward next is 0.6575, 
noisyNet noise sample is [array([-0.77395904], dtype=float32), 1.6496011]. 
=============================================
[2019-04-27 19:12:26,843] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.0226083e-22 1.0000000e+00 4.2631841e-22 1.4326051e-20 1.0063464e-12], sum to 1.0000
[2019-04-27 19:12:26,850] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8313
[2019-04-27 19:12:26,860] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.16666666666666, 85.66666666666667, 1.0, 2.0, 0.591073065808893, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 694801.2615501422, 694801.2615501422, 157526.6407837767], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4940400.0000, 
sim time next is 4941000.0000, 
raw observation next is [24.25, 84.0, 1.0, 2.0, 0.5756852988368716, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 678662.7470945648, 678662.7470945644, 154976.5183225338], 
processed observation next is [1.0, 0.17391304347826086, 0.4537037037037037, 0.84, 1.0, 1.0, 0.49486345099627566, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.24237955253377316, 0.242379552533773, 0.2980317660048727], 
reward next is 0.7020, 
noisyNet noise sample is [array([0.09435699], dtype=float32), 0.8268101]. 
=============================================
[2019-04-27 19:12:26,873] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[46.924946]
 [46.81883 ]
 [46.41682 ]
 [46.34047 ]
 [46.08787 ]], R is [[47.34012985]
 [47.56379318]
 [47.76620483]
 [47.95767212]
 [48.14033508]].
[2019-04-27 19:12:30,109] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7815668e-14 9.9999714e-01 1.1678370e-13 1.9069891e-12 2.8504355e-06], sum to 1.0000
[2019-04-27 19:12:30,113] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0527
[2019-04-27 19:12:30,120] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1745097.175776311 W.
[2019-04-27 19:12:30,123] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.91666666666666, 83.16666666666666, 1.0, 2.0, 0.7651222782165935, 1.0, 1.0, 0.7651222782165935, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1745097.175776311, 1745097.175776312, 329553.1742184447], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4985400.0000, 
sim time next is 4986000.0000, 
raw observation next is [26.7, 84.0, 1.0, 2.0, 0.5135047108665085, 1.0, 2.0, 0.5135047108665085, 1.0, 1.0, 0.8175162233803734, 6.911200000000001, 6.9112, 121.94756008, 1756819.854339303, 1756819.854339303, 350266.7689890116], 
processed observation next is [1.0, 0.7391304347826086, 0.5444444444444444, 0.84, 1.0, 1.0, 0.4208389415077482, 1.0, 1.0, 0.4208389415077482, 1.0, 0.5, 0.7718952792254669, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.6274356622640368, 0.6274356622640368, 0.6735899403634839], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.84477776], dtype=float32), 0.31514645]. 
=============================================
[2019-04-27 19:12:30,130] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[34.09252 ]
 [33.9818  ]
 [34.003548]
 [34.06182 ]
 [33.092957]], R is [[33.70112991]
 [33.36412048]
 [33.34476852]
 [33.35912323]
 [33.39262009]].
[2019-04-27 19:12:39,207] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9997988e-30 1.0000000e+00 1.5882786e-29 1.9912685e-27 3.9821958e-20], sum to 1.0000
[2019-04-27 19:12:39,215] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9581
[2019-04-27 19:12:39,218] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.86666666666667, 90.0, 1.0, 2.0, 0.6277761451416136, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 719901.7097885847, 719901.7097885847, 163142.3549948393], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5181600.0000, 
sim time next is 5182200.0000, 
raw observation next is [24.9, 91.0, 1.0, 2.0, 0.6373573477747546, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 728166.0717597627, 728166.0717597627, 164712.6451915106], 
processed observation next is [0.0, 1.0, 0.47777777777777775, 0.91, 1.0, 1.0, 0.5682825568747079, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2600593113427724, 0.2600593113427724, 0.31675508690675114], 
reward next is 0.6832, 
noisyNet noise sample is [array([-0.10300823], dtype=float32), -0.95226043]. 
=============================================
[2019-04-27 19:12:40,327] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0293802e-31 1.0000000e+00 1.1161450e-33 7.2465498e-31 2.9926783e-22], sum to 1.0000
[2019-04-27 19:12:40,332] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2090
[2019-04-27 19:12:40,336] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 94.0, 1.0, 2.0, 0.6738539681076356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 767987.969070883, 767987.9690708825, 171251.6557780345], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5185200.0000, 
sim time next is 5185800.0000, 
raw observation next is [25.0, 94.0, 1.0, 2.0, 0.6748876867928476, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 769166.6841465341, 769166.6841465341, 171442.7296825626], 
processed observation next is [1.0, 0.0, 0.48148148148148145, 0.94, 1.0, 1.0, 0.6129615318962471, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.27470238719519074, 0.27470238719519074, 0.32969755708185117], 
reward next is 0.6703, 
noisyNet noise sample is [array([-0.8791791], dtype=float32), 0.060526326]. 
=============================================
[2019-04-27 19:12:42,429] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.6017681e-21 1.0000000e+00 1.9808430e-20 3.7084500e-18 1.2561222e-11], sum to 1.0000
[2019-04-27 19:12:42,440] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6299
[2019-04-27 19:12:42,450] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1627497.8295109 W.
[2019-04-27 19:12:42,459] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [24.26666666666667, 92.66666666666667, 1.0, 2.0, 0.4757392887573509, 1.0, 1.0, 0.4757392887573509, 1.0, 1.0, 0.7573924414486628, 6.911199999999999, 6.9112, 121.94756008, 1627497.8295109, 1627497.829510901, 331841.262463359], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5192400.0000, 
sim time next is 5193000.0000, 
raw observation next is [24.2, 93.0, 1.0, 2.0, 0.5374002656199647, 1.0, 2.0, 0.5374002656199647, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 121.9260422443824, 1225310.089331332, 1225310.089331332, 247190.8005077288], 
processed observation next is [1.0, 0.08695652173913043, 0.45185185185185184, 0.93, 1.0, 1.0, 0.4492860304999579, 1.0, 1.0, 0.4492860304999579, 0.0, 0.5, -0.25, 8.881784197001253e-17, 0.0, 0.8094621263552269, 0.43761074618976137, 0.43761074618976137, 0.4753669240533246], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6276529], dtype=float32), -0.43344352]. 
=============================================
[2019-04-27 19:12:42,478] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[47.751755]
 [49.898155]
 [52.6372  ]
 [52.445953]
 [52.468925]], R is [[48.01541901]
 [47.89710999]
 [47.41814041]
 [47.63077164]
 [47.83785248]].
[2019-04-27 19:12:43,159] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-27 19:12:43,161] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 19:12:43,162] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:12:43,162] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 19:12:43,163] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:12:43,163] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 19:12:43,163] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 19:12:43,165] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 19:12:43,166] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:12:43,166] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:12:43,171] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:12:43,187] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run32
[2019-04-27 19:12:43,208] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run32
[2019-04-27 19:12:43,239] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run32
[2019-04-27 19:12:43,239] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run32
[2019-04-27 19:12:43,281] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run32
[2019-04-27 19:13:10,258] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05345927], dtype=float32), 0.01922075]
[2019-04-27 19:13:10,259] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [22.25480139, 90.10501318, 1.0, 2.0, 0.4954194153020948, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 596501.8202779107, 596501.8202779107, 142375.6612832462]
[2019-04-27 19:13:10,261] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 19:13:10,265] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [3.7285340e-34 1.0000000e+00 1.8587654e-35 1.3086345e-33 4.3173275e-27], sampled 0.7353859424138196
[2019-04-27 19:13:26,976] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05345927], dtype=float32), 0.01922075]
[2019-04-27 19:13:26,977] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [25.66666666666666, 85.66666666666667, 1.0, 2.0, 0.8975030479161958, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1023527.331787577, 1023527.331787577, 216928.8855269894]
[2019-04-27 19:13:26,979] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:13:26,980] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [6.4612863e-32 1.0000000e+00 5.4398044e-33 8.7046053e-31 2.1846574e-24], sampled 0.048137471800697496
[2019-04-27 19:13:37,940] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05345927], dtype=float32), 0.01922075]
[2019-04-27 19:13:37,941] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [28.83333333333334, 59.33333333333334, 1.0, 2.0, 0.5632134923403735, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 658231.4706967279, 658231.4706967279, 152631.1490786336]
[2019-04-27 19:13:37,943] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:13:37,948] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [8.9058448e-35 1.0000000e+00 2.9337328e-36 3.8879030e-34 7.7832973e-28], sampled 0.6601275282135314
[2019-04-27 19:14:29,882] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05345927], dtype=float32), 0.01922075]
[2019-04-27 19:14:29,883] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [22.329130035, 82.42456977500001, 1.0, 2.0, 0.4139356280798952, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 507387.2635261158, 507387.2635261163, 130463.7719580811]
[2019-04-27 19:14:29,885] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:14:29,891] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [8.79403395e-35 1.00000000e+00 3.86197211e-36 2.91181614e-34
 1.30576625e-27], sampled 0.19974312780656722
[2019-04-27 19:14:34,097] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8698.4298 2196193671.5136 568.0000
[2019-04-27 19:14:34,146] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8581.4902 2249281939.9837 544.0000
[2019-04-27 19:14:34,147] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8102.6277 2445888571.4259 717.0000
[2019-04-27 19:14:34,181] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8769.0843 2170960966.5954 491.0000
[2019-04-27 19:14:34,204] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8922.5419 2121002022.8314 429.0000
[2019-04-27 19:14:35,217] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 775000, evaluation results [775000.0, 8102.627668315354, 2445888571.4259377, 717.0, 8769.08426359412, 2170960966.5954456, 491.0, 8922.541906655968, 2121002022.83137, 429.0, 8581.490159540337, 2249281939.983741, 544.0, 8698.429795640155, 2196193671.5136123, 568.0]
[2019-04-27 19:14:37,056] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.17479065e-14 9.50633943e-01 1.09471113e-12 2.10691992e-10
 4.93660681e-02], sum to 1.0000
[2019-04-27 19:14:37,058] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7165
[2019-04-27 19:14:37,062] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666667, 69.0, 1.0, 2.0, 0.3069142422690795, 1.0, 2.0, 0.3069142422690795, 1.0, 2.0, 0.4886174691914231, 6.911199999999999, 6.9112, 121.94756008, 1049558.39158998, 1049558.39158998, 258710.5349119265], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5245800.0000, 
sim time next is 5246400.0000, 
raw observation next is [29.53333333333334, 70.0, 1.0, 2.0, 0.6563150537980361, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 747989.2040099626, 747989.2040099626, 168042.1401871684], 
processed observation next is [1.0, 0.7391304347826086, 0.6493827160493829, 0.7, 1.0, 1.0, 0.5908512545214716, 0.0, 0.5, -0.1904761904761905, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2671390014321295, 0.2671390014321295, 0.3231579618984008], 
reward next is 0.6768, 
noisyNet noise sample is [array([0.44935638], dtype=float32), -0.13068028]. 
=============================================
[2019-04-27 19:14:51,200] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.62195341e-31 1.00000000e+00 1.26597445e-33 8.47409837e-34
 7.00633124e-25], sum to 1.0000
[2019-04-27 19:14:51,208] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2314
[2019-04-27 19:14:51,214] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.86666666666667, 90.33333333333334, 1.0, 2.0, 0.6872504383905208, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 783263.6595596975, 783263.6595596975, 173744.0494788332], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5527200.0000, 
sim time next is 5527800.0000, 
raw observation next is [25.85, 90.5, 1.0, 2.0, 0.686649692157125, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 782578.6357681538, 782578.6357681538, 173631.7278215195], 
processed observation next is [1.0, 1.0, 0.5129629629629631, 0.905, 1.0, 1.0, 0.6269639192346727, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2794923699171978, 0.2794923699171978, 0.3339071688875375], 
reward next is 0.6661, 
noisyNet noise sample is [array([-1.3047199], dtype=float32), 1.605401]. 
=============================================
[2019-04-27 19:14:54,442] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7664932e-23 3.7121922e-06 4.0056519e-16 5.3215002e-18 9.9999630e-01], sum to 1.0000
[2019-04-27 19:14:54,451] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4223
[2019-04-27 19:14:54,457] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.6, 89.0, 1.0, 2.0, 0.5558007696990821, 1.0, 2.0, 0.5558007696990821, 1.0, 2.0, 0.8848529265282996, 6.9112, 6.9112, 121.94756008, 1901678.591193011, 1901678.591193011, 371820.0387464544], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5589600.0000, 
sim time next is 5590200.0000, 
raw observation next is [25.3, 90.0, 1.0, 2.0, 0.5431486658409579, 1.0, 2.0, 0.5431486658409579, 1.0, 2.0, 0.8647103651359098, 6.911200000000001, 6.9112, 121.94756008, 1858344.247962247, 1858344.247962247, 365271.3354297631], 
processed observation next is [1.0, 0.6956521739130435, 0.49259259259259264, 0.9, 1.0, 1.0, 0.45612936409637844, 1.0, 1.0, 0.45612936409637844, 1.0, 1.0, 0.8308879564198873, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.6636943742722311, 0.6636943742722311, 0.7024448758264674], 
reward next is 0.2976, 
noisyNet noise sample is [array([0.6422333], dtype=float32), -0.45918772]. 
=============================================
[2019-04-27 19:15:02,362] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1984372e-31], sum to 1.0000
[2019-04-27 19:15:02,372] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1769
[2019-04-27 19:15:02,378] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 75.0, 1.0, 2.0, 0.462231861356514, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 560300.4314810024, 560300.4314810024, 137393.5555438494], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5738400.0000, 
sim time next is 5739000.0000, 
raw observation next is [24.21666666666667, 74.16666666666667, 1.0, 2.0, 0.463979000582295, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 561712.9439515518, 561712.9439515518, 137635.6818684471], 
processed observation next is [0.0, 0.43478260869565216, 0.4524691358024692, 0.7416666666666667, 1.0, 1.0, 0.3618797625979702, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.20061176569698277, 0.20061176569698277, 0.2646840035931675], 
reward next is 0.7353, 
noisyNet noise sample is [array([0.82780826], dtype=float32), 0.19247666]. 
=============================================
[2019-04-27 19:15:02,395] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[71.04014 ]
 [71.044235]
 [71.07207 ]
 [71.11058 ]
 [71.13976 ]], R is [[71.04265594]
 [71.06800842]
 [71.09355927]
 [71.11959839]
 [71.14652252]].
[2019-04-27 19:15:08,503] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3667176e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.2237081e-30], sum to 1.0000
[2019-04-27 19:15:08,511] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0925
[2019-04-27 19:15:08,515] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 80.0, 1.0, 2.0, 0.4094014754183793, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 506245.6168396535, 506245.6168396535, 129928.1624376921], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5871600.0000, 
sim time next is 5872200.0000, 
raw observation next is [21.8, 80.33333333333334, 1.0, 2.0, 0.4080863849965389, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 504980.3473148207, 504980.3473148207, 129749.2962993477], 
processed observation next is [1.0, 1.0, 0.362962962962963, 0.8033333333333335, 1.0, 1.0, 0.2953409345196891, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1803501240410074, 0.1803501240410074, 0.24951787749874557], 
reward next is 0.7505, 
noisyNet noise sample is [array([0.33886507], dtype=float32), -0.80838704]. 
=============================================
[2019-04-27 19:15:11,261] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.5397032e-28 1.0868861e-15 3.6850935e-20 1.3337980e-19 1.0000000e+00], sum to 1.0000
[2019-04-27 19:15:11,272] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9169
[2019-04-27 19:15:11,276] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.35, 61.16666666666667, 1.0, 2.0, 0.3053457807451718, 1.0, 2.0, 0.3053457807451718, 1.0, 2.0, 0.4914027066407255, 6.911199999999999, 6.9112, 121.94756008, 1092418.576959952, 1092418.576959952, 257725.9283206602], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5911800.0000, 
sim time next is 5912400.0000, 
raw observation next is [25.6, 60.33333333333334, 1.0, 2.0, 0.2958052688437598, 1.0, 2.0, 0.2958052688437598, 1.0, 2.0, 0.4761059307271535, 6.911199999999999, 6.9112, 121.94756008, 1058527.739239567, 1058527.739239568, 254058.8911082182], 
processed observation next is [1.0, 0.43478260869565216, 0.5037037037037038, 0.6033333333333334, 1.0, 1.0, 0.16167293909971403, 1.0, 1.0, 0.16167293909971403, 1.0, 1.0, 0.34513241340894185, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.3780456211569882, 0.37804562115698853, 0.4885747905927273], 
reward next is 0.5114, 
noisyNet noise sample is [array([2.2168322], dtype=float32), 0.26650998]. 
=============================================
[2019-04-27 19:15:12,592] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0539919e-24 1.0000000e+00 1.4113073e-23 2.1614212e-25 2.0503431e-13], sum to 1.0000
[2019-04-27 19:15:12,600] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5935
[2019-04-27 19:15:12,608] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.53333333333333, 62.33333333333333, 1.0, 2.0, 0.4828200911512291, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 580937.862328566, 580937.862328566, 140401.6813085079], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5946600.0000, 
sim time next is 5947200.0000, 
raw observation next is [26.4, 63.0, 1.0, 2.0, 0.483918628677897, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 582378.7714281747, 582378.7714281747, 140575.5538847148], 
processed observation next is [1.0, 0.8695652173913043, 0.5333333333333333, 0.63, 1.0, 1.0, 0.38561741509273456, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.20799241836720525, 0.20799241836720525, 0.2703376036244516], 
reward next is 0.7297, 
noisyNet noise sample is [array([0.5838761], dtype=float32), -0.64991516]. 
=============================================
[2019-04-27 19:15:17,086] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.9984285e-32 1.0000000e+00 5.3053536e-30 5.4799783e-34 2.3092263e-18], sum to 1.0000
[2019-04-27 19:15:17,095] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7988
[2019-04-27 19:15:17,100] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.46666666666667, 69.0, 1.0, 2.0, 0.5309067911148111, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 627201.4769939929, 627201.4769939929, 147602.6728688423], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6034800.0000, 
sim time next is 6035400.0000, 
raw observation next is [26.35, 69.5, 1.0, 2.0, 0.5302533657670074, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 626787.686622951, 626787.686622951, 147511.1128162334], 
processed observation next is [1.0, 0.8695652173913043, 0.5314814814814816, 0.695, 1.0, 1.0, 0.44077781638929453, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.22385274522248247, 0.22385274522248247, 0.283675216954295], 
reward next is 0.7163, 
noisyNet noise sample is [array([-1.180961], dtype=float32), 0.23218991]. 
=============================================
[2019-04-27 19:15:23,351] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3078659e-33 1.0000000e+00 1.2853614e-33 5.7609985e-36 4.3792371e-24], sum to 1.0000
[2019-04-27 19:15:23,359] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1177
[2019-04-27 19:15:23,367] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.53333333333333, 90.66666666666667, 1.0, 2.0, 0.5951338627299781, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 712542.3609507101, 712542.3609507097, 158739.4704709978], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6151200.0000, 
sim time next is 6151800.0000, 
raw observation next is [22.46666666666667, 90.83333333333334, 1.0, 2.0, 0.579634204577583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 694574.3016121571, 694574.3016121568, 156084.2239668853], 
processed observation next is [1.0, 0.17391304347826086, 0.3876543209876544, 0.9083333333333334, 1.0, 1.0, 0.49956452925902733, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.24806225057577042, 0.24806225057577028, 0.3001619691670871], 
reward next is 0.6998, 
noisyNet noise sample is [array([-0.01630616], dtype=float32), -1.1018908]. 
=============================================
[2019-04-27 19:15:23,584] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-27 19:15:23,585] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 19:15:23,586] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:15:23,587] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 19:15:23,587] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 19:15:23,588] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:15:23,588] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:15:23,588] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 19:15:23,589] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 19:15:23,592] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:15:23,593] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:15:23,608] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run33
[2019-04-27 19:15:23,629] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run33
[2019-04-27 19:15:23,631] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run33
[2019-04-27 19:15:23,662] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run33
[2019-04-27 19:15:23,707] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run33
[2019-04-27 19:15:42,806] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05237443], dtype=float32), 0.013440593]
[2019-04-27 19:15:42,808] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [18.40560027, 93.62162144666667, 1.0, 2.0, 0.3335936958341764, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 422055.40241127, 422055.4024112695, 119802.6570741]
[2019-04-27 19:15:42,808] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 19:15:42,811] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.8160890e-34 1.0000000e+00 4.0982752e-34 9.0295341e-37 1.6519496e-24], sampled 0.780941998143765
[2019-04-27 19:15:44,571] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05237443], dtype=float32), 0.013440593]
[2019-04-27 19:15:44,571] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [28.69854748666667, 22.24965557, 1.0, 2.0, 0.3219198127626633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 415273.9784603139, 415273.9784603139, 111963.1253398469]
[2019-04-27 19:15:44,573] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:15:44,577] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [4.6993564e-35 1.0000000e+00 4.0044822e-35 8.4581916e-38 2.7504766e-26], sampled 0.5125507521800345
[2019-04-27 19:16:07,515] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05237443], dtype=float32), 0.013440593]
[2019-04-27 19:16:07,516] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [25.33333333333333, 87.33333333333333, 1.0, 2.0, 0.8686065989389393, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 992164.8801069099, 992164.8801069099, 210630.2468146147]
[2019-04-27 19:16:07,519] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:16:07,522] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [3.8208781e-31 1.0000000e+00 1.6561974e-30 1.7130896e-32 1.4693839e-20], sampled 0.03406543840059428
[2019-04-27 19:16:58,792] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05237443], dtype=float32), 0.013440593]
[2019-04-27 19:16:58,793] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [22.9, 45.5, 1.0, 2.0, 0.2855192034348443, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 368306.2084643388, 368306.2084643388, 108366.3010871689]
[2019-04-27 19:16:58,794] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:16:58,797] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [3.3317692e-36 1.0000000e+00 8.4643611e-37 0.0000000e+00 1.3387772e-28], sampled 0.29087205381798575
[2019-04-27 19:17:08,564] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05237443], dtype=float32), 0.013440593]
[2019-04-27 19:17:08,566] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [20.28204587, 96.693622755, 1.0, 2.0, 0.4352045428271998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 532774.3604771952, 532774.3604771952, 133525.7034159161]
[2019-04-27 19:17:08,567] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 19:17:08,571] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [2.5214065e-36 1.0000000e+00 7.8949505e-37 0.0000000e+00 3.5904428e-28], sampled 0.6641249203884353
[2019-04-27 19:17:09,845] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05237443], dtype=float32), 0.013440593]
[2019-04-27 19:17:09,845] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [20.33333333333334, 67.0, 1.0, 2.0, 0.2771816509707825, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 355854.0041662789, 355854.0041662794, 112801.2590856987]
[2019-04-27 19:17:09,846] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:17:09,850] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [4.0152160e-37 1.0000000e+00 6.5939837e-38 0.0000000e+00 7.7362830e-30], sampled 0.5482152980012602
[2019-04-27 19:17:10,858] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05237443], dtype=float32), 0.013440593]
[2019-04-27 19:17:10,859] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [24.65823946, 74.29038510000001, 1.0, 2.0, 0.4611833474303405, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 555537.8184085186, 555537.8184085186, 137122.3809557521]
[2019-04-27 19:17:10,860] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:17:10,864] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [2.3515927e-34 1.0000000e+00 1.1080169e-33 2.4445583e-36 1.5055014e-23], sampled 0.293805647796907
[2019-04-27 19:17:13,361] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05237443], dtype=float32), 0.013440593]
[2019-04-27 19:17:13,363] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [21.66666666666666, 64.5, 1.0, 2.0, 0.3127184567332252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 398058.1877611661, 398058.1877611661, 117157.3960833541]
[2019-04-27 19:17:13,364] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:17:13,365] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.6726878e-36 1.0000000e+00 3.3120117e-37 0.0000000e+00 4.4059021e-29], sampled 0.04976718469221231
[2019-04-27 19:17:14,443] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8642.2306 2257489724.2492 396.0000
[2019-04-27 19:17:14,566] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8932.0067 2126783338.2231 386.0000
[2019-04-27 19:17:14,759] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8795.6688 2177194389.3558 422.0000
[2019-04-27 19:17:14,864] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8179.8298 2460586204.8267 466.0000
[2019-04-27 19:17:14,907] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8726.8397 2207039825.1663 466.0000
[2019-04-27 19:17:15,923] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 800000, evaluation results [800000.0, 8179.829839059693, 2460586204.8267136, 466.0, 8795.66878883081, 2177194389.355764, 422.0, 8932.00672921873, 2126783338.2231429, 386.0, 8642.230618867326, 2257489724.2492266, 396.0, 8726.83966260442, 2207039825.166336, 466.0]
[2019-04-27 19:17:20,670] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.2308165e-36 1.0000000e+00 2.5744037e-37 0.0000000e+00 3.9386096e-28], sum to 1.0000
[2019-04-27 19:17:20,677] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5385
[2019-04-27 19:17:20,683] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.25, 86.0, 1.0, 2.0, 0.4975834271406459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 595132.6421985315, 595132.6421985315, 142576.583015704], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6244200.0000, 
sim time next is 6244800.0000, 
raw observation next is [23.36666666666667, 85.66666666666667, 1.0, 2.0, 0.5012325946223756, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 598730.6114999473, 598730.6114999473, 143122.2114162786], 
processed observation next is [0.0, 0.2608695652173913, 0.4209876543209878, 0.8566666666666667, 1.0, 1.0, 0.40622927931235187, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21383236124998117, 0.21383236124998117, 0.2752350219543819], 
reward next is 0.7248, 
noisyNet noise sample is [array([0.66952527], dtype=float32), -0.36976054]. 
=============================================
[2019-04-27 19:17:21,636] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.5554029e-38 1.0000000e+00 2.7844011e-37 0.0000000e+00 3.4141800e-27], sum to 1.0000
[2019-04-27 19:17:21,648] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7336
[2019-04-27 19:17:21,653] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.43333333333333, 72.33333333333333, 1.0, 2.0, 0.6155978042853035, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 706392.8666301301, 706392.8666301301, 161019.7107148675], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6262800.0000, 
sim time next is 6263400.0000, 
raw observation next is [27.56666666666666, 71.66666666666667, 1.0, 2.0, 0.6173215064595821, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 708046.7071200135, 708046.7071200135, 161305.976863522], 
processed observation next is [0.0, 0.4782608695652174, 0.576543209876543, 0.7166666666666667, 1.0, 1.0, 0.5444303648328358, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.25287382397143343, 0.25287382397143343, 0.3102038016606192], 
reward next is 0.6898, 
noisyNet noise sample is [array([1.8162658], dtype=float32), 1.4513189]. 
=============================================
[2019-04-27 19:17:25,818] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.9454848e-34 1.0000000e+00 1.2590627e-34 2.2728319e-36 7.8439354e-25], sum to 1.0000
[2019-04-27 19:17:25,825] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4598
[2019-04-27 19:17:25,829] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.26666666666667, 69.83333333333333, 1.0, 2.0, 0.6958828980804385, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 793107.2183739879, 793107.2183739879, 175366.353361324], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6346200.0000, 
sim time next is 6346800.0000, 
raw observation next is [29.5, 69.0, 1.0, 2.0, 0.6990324934054838, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 796698.7206400682, 796698.7206400682, 175961.5053106398], 
processed observation next is [0.0, 0.4782608695652174, 0.6481481481481481, 0.69, 1.0, 1.0, 0.6417053492922425, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2845352573714529, 0.2845352573714529, 0.33838751021276886], 
reward next is 0.6616, 
noisyNet noise sample is [array([-0.7844861], dtype=float32), -0.20354299]. 
=============================================
[2019-04-27 19:17:30,095] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.5380179e-19 9.9999988e-01 1.7646478e-15 3.1429052e-18 1.5421803e-07], sum to 1.0000
[2019-04-27 19:17:30,104] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5846
[2019-04-27 19:17:30,115] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2149942.585223527 W.
[2019-04-27 19:17:30,120] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.46666666666667, 82.0, 1.0, 2.0, 0.9424097386539266, 1.0, 2.0, 0.9424097386539266, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 2149942.585223527, 2149942.585223527, 406114.5692602437], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6428400.0000, 
sim time next is 6429000.0000, 
raw observation next is [27.68333333333333, 81.0, 1.0, 2.0, 0.9506543866908966, 1.0, 2.0, 0.9506543866908966, 0.0, 2.0, 0.0, 6.911199999999998, 6.9112, 121.9260426156618, 2168774.14735481, 2168774.147354811, 409940.0786016787], 
processed observation next is [1.0, 0.391304347826087, 0.580864197530864, 0.81, 1.0, 1.0, 0.9412552222510674, 1.0, 1.0, 0.9412552222510674, 0.0, 1.0, -0.25, -1.7763568394002506e-16, 0.0, 0.8094621288201359, 0.7745621954838607, 0.7745621954838611, 0.7883463050032283], 
reward next is 0.2117, 
noisyNet noise sample is [array([0.17425317], dtype=float32), -0.80477124]. 
=============================================
[2019-04-27 19:17:30,131] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[43.25195 ]
 [42.882168]
 [43.484615]
 [44.045113]
 [45.287395]], R is [[42.83463669]
 [42.62530136]
 [42.19904709]
 [41.77705765]
 [41.60333252]].
[2019-04-27 19:17:31,833] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1110486e-31 1.0000000e+00 4.2910667e-30 1.4181920e-33 5.5747967e-19], sum to 1.0000
[2019-04-27 19:17:31,844] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6367
[2019-04-27 19:17:31,848] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.6, 61.0, 1.0, 2.0, 0.6694720807359683, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 762991.4685979268, 762991.4685979268, 170444.9910739642], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6462000.0000, 
sim time next is 6462600.0000, 
raw observation next is [30.53333333333334, 61.5, 1.0, 2.0, 0.6633126857715351, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 755968.1984980047, 755968.1984980042, 169315.1465597744], 
processed observation next is [1.0, 0.8260869565217391, 0.6864197530864199, 0.615, 1.0, 1.0, 0.599181768775637, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.26998864232071595, 0.2699886423207158, 0.32560605107648927], 
reward next is 0.6744, 
noisyNet noise sample is [array([0.07862651], dtype=float32), 1.9036301]. 
=============================================
[2019-04-27 19:17:32,185] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3751968e-28 1.0000000e+00 2.4161076e-27 6.8761513e-29 6.0475172e-16], sum to 1.0000
[2019-04-27 19:17:32,191] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6042
[2019-04-27 19:17:32,195] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.73333333333333, 66.0, 1.0, 2.0, 0.6832778930129592, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 778733.8246306609, 778733.8246306609, 173002.1404880387], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6468000.0000, 
sim time next is 6468600.0000, 
raw observation next is [29.61666666666667, 66.5, 1.0, 2.0, 0.6811570715987896, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 776315.4941990221, 776315.4941990216, 172607.1387931715], 
processed observation next is [1.0, 0.8695652173913043, 0.6524691358024692, 0.665, 1.0, 1.0, 0.6204250852366543, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2772555336425079, 0.2772555336425077, 0.3319368053714837], 
reward next is 0.6681, 
noisyNet noise sample is [array([-0.36249673], dtype=float32), 0.22215065]. 
=============================================
[2019-04-27 19:17:34,744] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.4640665e-10 1.2605904e-02 2.2651523e-07 6.5785257e-09 9.8739386e-01], sum to 1.0000
[2019-04-27 19:17:34,751] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1159
[2019-04-27 19:17:34,755] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.76666666666667, 82.5, 1.0, 2.0, 0.6497108865282749, 1.0, 2.0, 0.6382201052405723, 1.0, 1.0, 0.9977734948820727, 6.9112, 6.9112, 121.94756008, 2184022.064827405, 2184022.064827405, 415999.9092710658], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6515400.0000, 
sim time next is 6516000.0000, 
raw observation next is [27.9, 82.0, 1.0, 2.0, 0.6232008783921269, 1.0, 2.0, 0.6232008783921269, 1.0, 2.0, 0.9921560946359214, 6.911199999999999, 6.9112, 121.94756008, 2132564.129168465, 2132564.129168465, 408161.4588415071], 
processed observation next is [1.0, 0.43478260869565216, 0.5888888888888888, 0.82, 1.0, 1.0, 0.5514296171334844, 1.0, 1.0, 0.5514296171334844, 1.0, 1.0, 0.9901951182949017, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.7616300461315946, 0.7616300461315946, 0.7849258823875136], 
reward next is 0.2151, 
noisyNet noise sample is [array([0.98049563], dtype=float32), 0.07373914]. 
=============================================
[2019-04-27 19:17:34,770] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[27.776932]
 [27.836706]
 [28.242832]
 [29.9673  ]
 [30.54646 ]], R is [[27.63368416]
 [27.35734749]
 [27.08377457]
 [27.04461861]
 [27.03884506]].
[2019-04-27 19:17:39,611] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1612585e-25 1.0000000e+00 5.1867943e-27 4.7100336e-28 8.0391156e-17], sum to 1.0000
[2019-04-27 19:17:39,619] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7766
[2019-04-27 19:17:39,623] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.65, 44.5, 1.0, 2.0, 0.8802682826812785, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.95952821726877, 6.9112, 121.9257709134726, 1130457.107339923, 1105708.795720975, 217110.8827830165], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6618600.0000, 
sim time next is 6619200.0000, 
raw observation next is [27.43333333333333, 42.0, 1.0, 2.0, 0.9161881243447232, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.189545144929097, 6.9112, 121.924875537901, 1290924.568228683, 1148388.340190035, 225323.389598154], 
processed observation next is [1.0, 0.6086956521739131, 0.5716049382716049, 0.42, 1.0, 1.0, 0.9002239575532419, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0278345144929097, 0.0, 0.8094543806374795, 0.46104448865310105, 0.41013869292501254, 0.4333142107656808], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.44612435], dtype=float32), 0.13166167]. 
=============================================
[2019-04-27 19:17:42,523] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 19:17:42,533] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2854
[2019-04-27 19:17:42,539] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.9, 47.0, 1.0, 2.0, 0.2754599226423996, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 355327.2081096893, 355327.2081096888, 110191.0545918001], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6674400.0000, 
sim time next is 6675000.0000, 
raw observation next is [22.91666666666666, 47.33333333333334, 1.0, 2.0, 0.293683707461739, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 378840.6320807011, 378840.6320807011, 113593.2181107872], 
processed observation next is [1.0, 0.2608695652173913, 0.4043209876543208, 0.47333333333333344, 1.0, 1.0, 0.1591472707877845, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.13530022574310754, 0.13530022574310754, 0.21844849636689848], 
reward next is 0.7816, 
noisyNet noise sample is [array([-0.5625532], dtype=float32), 0.15919092]. 
=============================================
[2019-04-27 19:17:42,558] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[71.588104]
 [71.565674]
 [71.561386]
 [71.54385 ]
 [71.50214 ]], R is [[71.67256927]
 [71.74393463]
 [71.81641388]
 [71.88942719]
 [71.96213531]].
[2019-04-27 19:17:43,340] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 19:17:43,347] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0690
[2019-04-27 19:17:43,351] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.25, 76.5, 1.0, 2.0, 0.3322949332809437, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 421581.3066743876, 421581.3066743876, 119645.3467659521], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6741000.0000, 
sim time next is 6741600.0000, 
raw observation next is [20.16666666666666, 76.66666666666666, 1.0, 2.0, 0.3284262198456777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 416940.4669942584, 416940.4669942579, 119148.6061000303], 
processed observation next is [1.0, 0.0, 0.3024691358024689, 0.7666666666666666, 1.0, 1.0, 0.20050740457818775, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.14890730964080656, 0.1489073096408064, 0.22913193480775057], 
reward next is 0.7709, 
noisyNet noise sample is [array([-0.30020976], dtype=float32), -1.3544939]. 
=============================================
[2019-04-27 19:17:48,974] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.4716677e-34 3.2245671e-16 3.2562570e-26 2.7558719e-28 1.0000000e+00], sum to 1.0000
[2019-04-27 19:17:48,981] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8887
[2019-04-27 19:17:48,985] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.03333333333333, 54.66666666666667, 1.0, 2.0, 0.4047490220719222, 1.0, 2.0, 0.4047490220719222, 1.0, 2.0, 0.6454701752310377, 6.911199999999999, 6.9112, 121.94756008, 1407026.094584486, 1407026.094584486, 299249.4787071945], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6798000.0000, 
sim time next is 6798600.0000, 
raw observation next is [28.05, 55.0, 1.0, 2.0, 0.4089940981048802, 1.0, 2.0, 0.4089940981048802, 1.0, 2.0, 0.6520962114854266, 6.9112, 6.9112, 121.94756008, 1419908.881492912, 1419908.881492912, 301127.8568793134], 
processed observation next is [1.0, 0.6956521739130435, 0.5944444444444444, 0.55, 1.0, 1.0, 0.2964215453629526, 1.0, 1.0, 0.2964215453629526, 1.0, 1.0, 0.5651202643567832, 0.0, 0.0, 0.8096049824067558, 0.5071103148188971, 0.5071103148188971, 0.5790920324602181], 
reward next is 0.4209, 
noisyNet noise sample is [array([-0.52417094], dtype=float32), -0.59444666]. 
=============================================
[2019-04-27 19:17:50,107] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.7510669e-23 3.1089814e-05 4.4755418e-20 1.4902694e-20 9.9996889e-01], sum to 1.0000
[2019-04-27 19:17:50,119] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0156
[2019-04-27 19:17:50,124] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.26666666666667, 79.33333333333334, 1.0, 2.0, 0.16, 1.0, 2.0, 0.16, 1.0, 2.0, 0.2513935673393982, 6.9112, 6.9112, 121.94756008, 559163.7485576756, 559163.7485576756, 205588.8611423315], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6825000.0000, 
sim time next is 6825600.0000, 
raw observation next is [23.2, 79.0, 1.0, 2.0, 0.16, 1.0, 2.0, 0.16, 1.0, 2.0, 0.2490412276151582, 6.9112, 6.9112, 121.94756008, 554467.3663567199, 554467.3663567199, 204763.6492948276], 
processed observation next is [0.0, 0.0, 0.4148148148148148, 0.79, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.061301534518947744, 0.0, 0.0, 0.8096049824067558, 0.19802405941311424, 0.19802405941311424, 0.3937762486438992], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0329219], dtype=float32), 0.011701076]. 
=============================================
[2019-04-27 19:17:51,137] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.4022976e-33 1.0000000e+00 2.6546189e-34 2.6290189e-36 5.4791785e-21], sum to 1.0000
[2019-04-27 19:17:51,147] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4443
[2019-04-27 19:17:51,151] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 76.0, 1.0, 2.0, 0.3709541072834819, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 463304.7544802614, 463304.7544802614, 124677.4846053967], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6843600.0000, 
sim time next is 6844200.0000, 
raw observation next is [21.78333333333333, 76.0, 1.0, 2.0, 0.3722997995532878, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 464692.8051991014, 464692.8051991014, 124855.2119521729], 
processed observation next is [0.0, 0.21739130434782608, 0.3623456790123456, 0.76, 1.0, 1.0, 0.2527378566110569, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16596171614253621, 0.16596171614253621, 0.24010617683110175], 
reward next is 0.7599, 
noisyNet noise sample is [array([-1.0829915], dtype=float32), 0.017210213]. 
=============================================
[2019-04-27 19:17:51,249] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.3230101e-35 1.0000000e+00 1.4607213e-35 1.5141199e-35 2.7206167e-22], sum to 1.0000
[2019-04-27 19:17:51,256] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7093
[2019-04-27 19:17:51,261] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.3, 76.0, 1.0, 2.0, 0.3884772406175825, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 481703.4952835998, 481703.4952835998, 127015.8734070949], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6847800.0000, 
sim time next is 6848400.0000, 
raw observation next is [22.4, 76.0, 1.0, 2.0, 0.39191182204633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 485315.3502119426, 485315.3502119426, 127479.3294077478], 
processed observation next is [0.0, 0.2608695652173913, 0.38518518518518513, 0.76, 1.0, 1.0, 0.2760855024361072, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1733269107899795, 0.1733269107899795, 0.24515255655336116], 
reward next is 0.7548, 
noisyNet noise sample is [array([1.0199909], dtype=float32), -0.8309691]. 
=============================================
[2019-04-27 19:17:52,288] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5468314e-34 1.0000000e+00 5.4581089e-36 3.3393783e-36 1.8078479e-21], sum to 1.0000
[2019-04-27 19:17:52,293] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8157
[2019-04-27 19:17:52,297] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.56666666666667, 52.66666666666666, 1.0, 2.0, 0.4398485490748109, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 535763.8865627454, 535763.886562745, 134133.1402160412], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6864000.0000, 
sim time next is 6864600.0000, 
raw observation next is [27.83333333333334, 51.33333333333334, 1.0, 2.0, 0.4392988527950081, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 535171.3183696425, 535171.3183696425, 134054.4089102929], 
processed observation next is [0.0, 0.43478260869565216, 0.58641975308642, 0.5133333333333334, 1.0, 1.0, 0.33249863427977155, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.19113261370344375, 0.19113261370344375, 0.25779694021210176], 
reward next is 0.7422, 
noisyNet noise sample is [array([-0.29062453], dtype=float32), -0.13839933]. 
=============================================
[2019-04-27 19:17:53,374] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.5443084e-31 1.0000000e+00 5.9536938e-30 1.2931529e-31 7.5071313e-16], sum to 1.0000
[2019-04-27 19:17:53,381] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4753
[2019-04-27 19:17:53,386] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.6, 48.5, 1.0, 2.0, 0.5285122496647754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 623533.1086701683, 623533.1086701683, 147181.3234485691], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6877800.0000, 
sim time next is 6878400.0000, 
raw observation next is [30.46666666666667, 48.66666666666666, 1.0, 2.0, 0.5255815608212656, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 621018.083184703, 621018.083184703, 146746.3913731791], 
processed observation next is [0.0, 0.6086956521739131, 0.6839506172839507, 0.4866666666666666, 1.0, 1.0, 0.43521614383484, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.22179217256596537, 0.22179217256596537, 0.2822045987945752], 
reward next is 0.7178, 
noisyNet noise sample is [array([1.1421086], dtype=float32), 0.7194077]. 
=============================================
[2019-04-27 19:17:58,195] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.0363815e-32 1.0000000e+00 2.4103493e-31 2.2495963e-34 9.1474628e-18], sum to 1.0000
[2019-04-27 19:17:58,204] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2114
[2019-04-27 19:17:58,208] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.46666666666667, 51.33333333333333, 1.0, 2.0, 0.4766964286832671, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 575458.7912943689, 575458.7912943689, 139522.027133472], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6979200.0000, 
sim time next is 6979800.0000, 
raw observation next is [28.18333333333333, 52.16666666666667, 1.0, 2.0, 0.473126321463081, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 572006.1422724889, 572006.1422724889, 139002.9050885648], 
processed observation next is [0.0, 0.782608695652174, 0.5993827160493826, 0.5216666666666667, 1.0, 1.0, 0.3727694303131916, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.20428790795446033, 0.20428790795446033, 0.26731327901647073], 
reward next is 0.7327, 
noisyNet noise sample is [array([0.33164084], dtype=float32), -0.3993867]. 
=============================================
[2019-04-27 19:18:01,120] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.3241803e-28 1.0000000e+00 6.9361720e-27 1.0593221e-29 2.9661151e-16], sum to 1.0000
[2019-04-27 19:18:01,128] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3577
[2019-04-27 19:18:01,133] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.56666666666667, 81.0, 1.0, 2.0, 0.4813071136250011, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 578914.2670213263, 578914.2670213263, 140161.1237417037], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7082400.0000, 
sim time next is 7083000.0000, 
raw observation next is [23.55, 80.5, 1.0, 2.0, 0.4777537693339874, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 575476.7522244817, 575476.7522244817, 139642.6183315924], 
processed observation next is [1.0, 1.0, 0.4277777777777778, 0.805, 1.0, 1.0, 0.3782782968261754, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.20552741150874346, 0.20552741150874346, 0.2685434967915239], 
reward next is 0.7315, 
noisyNet noise sample is [array([-1.773511], dtype=float32), 0.6943506]. 
=============================================
[2019-04-27 19:18:01,148] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[59.73769 ]
 [59.71161 ]
 [59.699455]
 [59.675503]
 [59.652016]], R is [[59.90882492]
 [60.04019928]
 [60.16921234]
 [60.29618454]
 [60.42190552]].
[2019-04-27 19:18:01,423] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0389986e-23 1.0000000e+00 8.4057938e-23 2.8495234e-26 9.6993746e-13], sum to 1.0000
[2019-04-27 19:18:01,435] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2419
[2019-04-27 19:18:01,442] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1420561.032844332 W.
[2019-04-27 19:18:01,447] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [23.9, 78.0, 1.0, 2.0, 0.9827778694518211, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.375370062349532, 6.9112, 121.9242047481415, 1420561.032844332, 1182868.056820912, 239698.9637028208], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7059600.0000, 
sim time next is 7060200.0000, 
raw observation next is [23.86666666666667, 78.16666666666667, 1.0, 2.0, 0.3189119030771134, 0.0, 2.0, 0.0, 1.0, 1.0, 0.5135027740979182, 6.9112, 6.9112, 121.9257630897639, 761291.192377355, 761291.192377355, 199250.6574093473], 
processed observation next is [1.0, 0.7391304347826086, 0.4395061728395063, 0.7816666666666667, 1.0, 1.0, 0.1891808369965636, 0.0, 1.0, -0.1904761904761905, 1.0, 0.5, 0.39187846762239775, 0.0, 0.0, 0.8094602730588604, 0.2718897115633411, 0.2718897115633411, 0.38317434117182175], 
reward next is 0.6168, 
noisyNet noise sample is [array([-1.593664], dtype=float32), 0.9947364]. 
=============================================
[2019-04-27 19:18:04,302] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-04-27 19:18:04,305] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 19:18:04,308] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 19:18:04,309] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:18:04,310] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:18:04,310] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 19:18:04,310] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 19:18:04,313] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:18:04,312] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 19:18:04,315] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:18:04,316] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:18:04,332] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run34
[2019-04-27 19:18:04,332] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run34
[2019-04-27 19:18:04,384] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run34
[2019-04-27 19:18:04,415] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run34
[2019-04-27 19:18:04,416] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run34
[2019-04-27 19:18:12,811] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0506026], dtype=float32), 0.016972154]
[2019-04-27 19:18:12,812] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [15.0, 94.0, 1.0, 2.0, 0.5295796563393195, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 683272.873125135, 683272.873125135, 139309.1427436889]
[2019-04-27 19:18:12,813] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:18:12,816] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [2.6024372e-36 1.0000000e+00 5.1763881e-38 0.0000000e+00 3.5704590e-29], sampled 0.9293972532897777
[2019-04-27 19:18:27,363] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0506026], dtype=float32), 0.016972154]
[2019-04-27 19:18:27,364] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [24.53333333333333, 56.66666666666667, 1.0, 2.0, 0.9283891748720788, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.26086351454961, 6.9112, 121.9244223533735, 1340678.678472953, 1161622.085949005, 228137.9952638324]
[2019-04-27 19:18:27,365] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:18:27,369] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [3.4481828e-28 1.0000000e+00 2.5572965e-28 6.4606524e-30 2.8929639e-17], sampled 0.9475120089477681
[2019-04-27 19:18:27,369] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1340678.678472953 W.
[2019-04-27 19:18:36,163] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0506026], dtype=float32), 0.016972154]
[2019-04-27 19:18:36,165] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [25.56666666666667, 87.0, 1.0, 2.0, 0.4613123623389249, 1.0, 2.0, 0.4613123623389249, 1.0, 2.0, 0.7344243047383372, 6.9112, 6.9112, 121.94756008, 1578099.346683071, 1578099.346683071, 324980.7915322585]
[2019-04-27 19:18:36,166] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:18:36,168] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [4.8910421e-24 1.7745394e-09 1.0380900e-18 1.8986755e-20 1.0000000e+00], sampled 0.014743298666529236
[2019-04-27 19:18:37,226] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.0506026], dtype=float32), 0.016972154]
[2019-04-27 19:18:37,227] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [24.76666666666667, 69.66666666666667, 1.0, 2.0, 0.5821466563585972, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 705706.5412356869, 705706.5412356869, 156793.555597813]
[2019-04-27 19:18:37,228] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 19:18:37,232] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [9.020119e-33 1.000000e+00 9.706235e-33 6.049997e-35 5.882159e-21], sampled 0.42880055901536596
[2019-04-27 19:18:58,725] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.0506026], dtype=float32), 0.016972154]
[2019-04-27 19:18:58,726] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [30.6, 70.0, 1.0, 2.0, 0.5420181698678113, 1.0, 2.0, 0.5420181698678113, 1.0, 2.0, 0.862910578728977, 6.9112, 6.9112, 121.94756008, 1854472.322257494, 1854472.322257494, 364690.4038557832]
[2019-04-27 19:18:58,727] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 19:18:58,730] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.5606305e-23 8.7555427e-06 3.5146507e-18 1.0704031e-20 9.9999130e-01], sampled 0.6435890811295756
[2019-04-27 19:19:21,390] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0506026], dtype=float32), 0.016972154]
[2019-04-27 19:19:21,392] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [26.0, 100.0, 1.0, 2.0, 0.7618529438101932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 868336.7691534476, 868336.7691534476, 188190.1498302151]
[2019-04-27 19:19:21,393] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:19:21,396] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.5669096e-28 1.0000000e+00 4.6713681e-27 7.1977930e-29 4.4083083e-13], sampled 0.9220890607067697
[2019-04-27 19:19:25,054] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0506026], dtype=float32), 0.016972154]
[2019-04-27 19:19:25,055] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [28.72677614, 76.65388966, 1.0, 2.0, 0.6711319578195901, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 764884.1598376918, 764884.1598376918, 170755.40047522]
[2019-04-27 19:19:25,058] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:19:25,060] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [2.0892076e-32 1.0000000e+00 1.1905308e-32 1.0860756e-34 7.0250260e-21], sampled 0.35764431047942535
[2019-04-27 19:19:26,670] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0506026], dtype=float32), 0.016972154]
[2019-04-27 19:19:26,673] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [30.7, 39.0, 1.0, 2.0, 0.3980689309077359, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 485081.1015098921, 485081.1015098921, 128135.212733386]
[2019-04-27 19:19:26,675] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:19:26,678] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [5.2348078e-31 1.0000000e+00 1.6993061e-30 6.2070510e-33 1.1826272e-18], sampled 0.9366339378544775
[2019-04-27 19:19:55,023] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0506026], dtype=float32), 0.016972154]
[2019-04-27 19:19:55,024] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [30.42140444, 50.47801065, 1.0, 2.0, 0.6584857702750031, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 777759.8779421104, 777759.8779421104, 169700.5661288321]
[2019-04-27 19:19:55,025] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 19:19:55,028] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [2.0052749e-31 1.0000000e+00 2.3994506e-31 1.8724455e-33 8.6318740e-20], sampled 0.9476685708679755
[2019-04-27 19:19:55,639] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8192.9346 2467580847.0488 416.0000
[2019-04-27 19:19:55,931] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8827.0082 2181781165.2025 372.0000
[2019-04-27 19:19:56,042] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8753.9551 2216285158.2540 383.0000
[2019-04-27 19:19:56,177] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8659.2538 2263324563.5774 339.0000
[2019-04-27 19:19:56,180] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8934.6969 2132305134.0064 357.0000
[2019-04-27 19:19:57,194] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 825000, evaluation results [825000.0, 8192.934589835497, 2467580847.0487647, 416.0, 8827.008234262368, 2181781165.202497, 372.0, 8934.69685959666, 2132305134.0063584, 357.0, 8659.25382852731, 2263324563.577398, 339.0, 8753.955055687244, 2216285158.253983, 383.0]
[2019-04-27 19:20:06,657] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.0439280e-33 1.0000000e+00 2.7260555e-33 1.7818247e-35 3.1524364e-20], sum to 1.0000
[2019-04-27 19:20:06,666] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1103
[2019-04-27 19:20:06,672] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.55, 90.0, 1.0, 2.0, 0.3868702112173324, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 479069.3723013505, 479069.3723013505, 126778.5321868518], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7279800.0000, 
sim time next is 7280400.0000, 
raw observation next is [20.6, 90.0, 1.0, 2.0, 0.3863114255693202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 478042.398132476, 478042.398132476, 126693.4745558437], 
processed observation next is [1.0, 0.2608695652173913, 0.3185185185185186, 0.9, 1.0, 1.0, 0.2694183637730002, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.17072942790445572, 0.17072942790445572, 0.24364129722277633], 
reward next is 0.7564, 
noisyNet noise sample is [array([-0.22069111], dtype=float32), 0.20068696]. 
=============================================
[2019-04-27 19:20:17,081] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.5119556e-35 1.0000000e+00 1.8622102e-32 8.2764342e-35 7.4883570e-19], sum to 1.0000
[2019-04-27 19:20:17,087] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5083
[2019-04-27 19:20:17,092] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.8, 90.0, 1.0, 2.0, 0.4988357179932236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 595573.9374663914, 595573.9374663914, 142734.5847357158], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7502400.0000, 
sim time next is 7503000.0000, 
raw observation next is [22.75, 90.16666666666667, 1.0, 2.0, 0.4975755980044434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 594380.6618371808, 594380.6618371808, 142548.3080495407], 
processed observation next is [0.0, 0.8695652173913043, 0.39814814814814814, 0.9016666666666667, 1.0, 1.0, 0.40187571191005167, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21227880779899316, 0.21227880779899316, 0.27413136163373214], 
reward next is 0.7259, 
noisyNet noise sample is [array([-0.09255968], dtype=float32), 1.8864378]. 
=============================================
[2019-04-27 19:20:17,104] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[74.681854]
 [74.68142 ]
 [74.67703 ]
 [74.6756  ]
 [74.67398 ]], R is [[74.62574005]
 [74.60499573]
 [74.58438873]
 [74.56364441]
 [74.54236603]].
[2019-04-27 19:20:22,976] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.3595073e-28 1.0000000e+00 1.9339759e-27 2.1081766e-29 4.6863669e-17], sum to 1.0000
[2019-04-27 19:20:22,981] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9611
[2019-04-27 19:20:22,988] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.46666666666667, 86.66666666666667, 1.0, 2.0, 0.5108887307068392, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 607464.7010280294, 607464.7010280294, 144545.8838547877], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7597200.0000, 
sim time next is 7597800.0000, 
raw observation next is [23.4, 87.0, 1.0, 2.0, 0.5089571798216583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 605412.4574962032, 605412.4574962032, 144248.5903848878], 
processed observation next is [0.0, 0.9565217391304348, 0.42222222222222217, 0.87, 1.0, 1.0, 0.41542521407340277, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2162187348200726, 0.2162187348200726, 0.27740113535555344], 
reward next is 0.7226, 
noisyNet noise sample is [array([-1.2244678], dtype=float32), -0.32412422]. 
=============================================
[2019-04-27 19:20:23,602] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.2927755e-27 1.0000000e+00 5.2877254e-26 1.3886287e-28 5.3849096e-13], sum to 1.0000
[2019-04-27 19:20:23,607] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1127
[2019-04-27 19:20:23,612] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.55, 85.5, 1.0, 2.0, 0.418535579996165, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 514631.1242548816, 514631.1242548816, 131165.3129437538], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7608600.0000, 
sim time next is 7609200.0000, 
raw observation next is [21.43333333333333, 85.33333333333334, 1.0, 2.0, 0.4133945857303712, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 509334.0629447504, 509334.0629447499, 130453.7105626761], 
processed observation next is [1.0, 0.043478260869565216, 0.3493827160493826, 0.8533333333333334, 1.0, 1.0, 0.3016602211075848, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.18190502248026802, 0.18190502248026783, 0.2508725203128387], 
reward next is 0.7491, 
noisyNet noise sample is [array([0.81685185], dtype=float32), -0.64373916]. 
=============================================
[2019-04-27 19:20:25,523] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.7058629e-26 1.0000000e+00 8.2991041e-27 1.0436435e-28 8.7069250e-17], sum to 1.0000
[2019-04-27 19:20:25,535] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8116
[2019-04-27 19:20:25,540] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1307118.55189039 W.
[2019-04-27 19:20:25,550] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [24.0, 82.0, 1.0, 2.0, 0.9673400387820368, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.212757470413331, 6.9112, 121.9246107076428, 1307118.55189039, 1152695.987708657, 235485.2394792231], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7639200.0000, 
sim time next is 7639800.0000, 
raw observation next is [24.26666666666667, 81.33333333333334, 1.0, 2.0, 0.5529017689848221, 1.0, 1.0, 0.5529017689848221, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9258609893915, 1289114.336734083, 1289114.336734083, 253563.997039411], 
processed observation next is [1.0, 0.43478260869565216, 0.4543209876543211, 0.8133333333333335, 1.0, 1.0, 0.4677402011724073, 1.0, 0.5, 0.4677402011724073, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094609230105791, 0.46039797740502963, 0.46039797740502963, 0.48762307122963655], 
reward next is 0.5124, 
noisyNet noise sample is [array([0.14391515], dtype=float32), 2.3846002]. 
=============================================
[2019-04-27 19:20:26,597] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.9566955e-28 1.0000000e+00 7.8683781e-28 1.8831367e-29 7.1280004e-16], sum to 1.0000
[2019-04-27 19:20:26,604] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4274
[2019-04-27 19:20:26,610] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.4, 68.0, 1.0, 2.0, 0.386755717546658, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 479789.7150615907, 479789.7150615907, 126781.7055370422], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7666800.0000, 
sim time next is 7667400.0000, 
raw observation next is [23.15, 68.5, 1.0, 2.0, 0.3843026552391367, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 477781.2269217097, 477781.2269217097, 126464.4283320933], 
processed observation next is [1.0, 0.7391304347826086, 0.4129629629629629, 0.685, 1.0, 1.0, 0.2670269705227818, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1706361524720392, 0.1706361524720392, 0.24320082371556404], 
reward next is 0.7568, 
noisyNet noise sample is [array([0.13694549], dtype=float32), -0.5956855]. 
=============================================
[2019-04-27 19:20:28,422] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.2878405e-33], sum to 1.0000
[2019-04-27 19:20:28,432] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4846
[2019-04-27 19:20:28,439] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.7, 81.0, 1.0, 2.0, 0.2740630842143383, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 351324.8262137747, 351324.8262137747, 112431.348400904], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7703400.0000, 
sim time next is 7704000.0000, 
raw observation next is [18.6, 83.0, 1.0, 2.0, 0.2797681711734142, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 358145.8304643877, 358145.8304643877, 113113.5344702545], 
processed observation next is [1.0, 0.17391304347826086, 0.2444444444444445, 0.83, 1.0, 1.0, 0.1425811561588264, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.12790922516585276, 0.12790922516585276, 0.21752602782741248], 
reward next is 0.7825, 
noisyNet noise sample is [array([-0.3861452], dtype=float32), 0.2687262]. 
=============================================
[2019-04-27 19:20:28,455] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[70.27731 ]
 [70.4418  ]
 [70.499626]
 [70.63204 ]
 [70.75073 ]], R is [[70.28096008]
 [70.36193085]
 [70.44326782]
 [70.52471161]
 [70.6063385 ]].
[2019-04-27 19:20:29,061] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0204456e-20 1.7887718e-05 2.0381917e-17 2.5988614e-19 9.9998212e-01], sum to 1.0000
[2019-04-27 19:20:29,070] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1313
[2019-04-27 19:20:29,074] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.85, 54.0, 1.0, 2.0, 0.3080380249427342, 1.0, 1.0, 0.3080380249427342, 1.0, 2.0, 0.4931592085074783, 6.9112, 6.9112, 121.94756008, 1087726.384109338, 1087726.384109338, 259030.0043460999], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7734600.0000, 
sim time next is 7735200.0000, 
raw observation next is [28.03333333333333, 53.0, 1.0, 2.0, 0.307624216299528, 1.0, 2.0, 0.307624216299528, 1.0, 2.0, 0.4921813951490994, 6.9112, 6.9112, 121.94756008, 1084047.276647561, 1084047.276647561, 258898.1356317424], 
processed observation next is [1.0, 0.5217391304347826, 0.5938271604938271, 0.53, 1.0, 1.0, 0.17574311464229525, 1.0, 1.0, 0.17574311464229525, 1.0, 1.0, 0.3652267439363742, 0.0, 0.0, 0.8096049824067558, 0.38715974165984324, 0.38715974165984324, 0.49788103006104306], 
reward next is 0.5021, 
noisyNet noise sample is [array([-0.8919948], dtype=float32), 1.3358462]. 
=============================================
[2019-04-27 19:20:37,224] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4771245e-25 1.0000000e+00 1.5738130e-25 1.4667698e-26 7.8699064e-10], sum to 1.0000
[2019-04-27 19:20:37,231] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0501
[2019-04-27 19:20:37,234] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.16666666666667, 80.5, 1.0, 2.0, 0.653913500077533, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 122.0513847076852, 804579.6428085009, 804579.6428085014, 169994.9135008816], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7870200.0000, 
sim time next is 7870800.0000, 
raw observation next is [22.03333333333333, 81.0, 1.0, 2.0, 0.5199099791998066, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 640159.8248122868, 640159.8248122868, 146692.4315807072], 
processed observation next is [1.0, 0.08695652173913043, 0.37160493827160485, 0.81, 1.0, 1.0, 0.42846426095215073, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.228628508861531, 0.228628508861531, 0.2821008299628985], 
reward next is 0.7179, 
noisyNet noise sample is [array([-1.4149704], dtype=float32), -0.9498076]. 
=============================================
[2019-04-27 19:20:39,180] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:20:39,180] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:20:39,221] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res14/Eplus-env-sub_run5
[2019-04-27 19:20:40,550] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:20:40,550] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:20:40,553] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res3/Eplus-env-sub_run5
[2019-04-27 19:20:40,583] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:20:40,583] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:20:40,586] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res11/Eplus-env-sub_run5
[2019-04-27 19:20:41,192] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:20:41,192] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:20:41,194] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res4/Eplus-env-sub_run5
[2019-04-27 19:20:41,239] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:20:41,239] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:20:41,242] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res2/Eplus-env-sub_run5
[2019-04-27 19:20:41,270] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:20:41,271] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:20:41,273] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res12/Eplus-env-sub_run5
[2019-04-27 19:20:41,288] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:20:41,288] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:20:41,289] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:20:41,291] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res15/Eplus-env-sub_run5
[2019-04-27 19:20:41,290] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:20:41,312] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res13/Eplus-env-sub_run5
[2019-04-27 19:20:41,363] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:20:41,364] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:20:41,367] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res16/Eplus-env-sub_run5
[2019-04-27 19:20:41,431] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:20:41,431] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:20:41,431] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:20:41,431] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:20:41,432] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res5/Eplus-env-sub_run5
[2019-04-27 19:20:41,456] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res10/Eplus-env-sub_run5
[2019-04-27 19:20:41,486] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:20:41,487] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:20:41,489] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res17/Eplus-env-sub_run5
[2019-04-27 19:20:41,520] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:20:41,522] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:20:41,523] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res9/Eplus-env-sub_run5
[2019-04-27 19:20:41,550] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:20:41,550] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:20:41,551] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:20:41,551] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:20:41,554] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res6/Eplus-env-sub_run5
[2019-04-27 19:20:41,613] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:20:41,614] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:20:41,618] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res7/Eplus-env-sub_run5
[2019-04-27 19:20:41,655] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res8/Eplus-env-sub_run5
[2019-04-27 19:20:46,279] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-27 19:20:46,281] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 19:20:46,281] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:20:46,281] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 19:20:46,283] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:20:46,283] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 19:20:46,284] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 19:20:46,285] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:20:46,285] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 19:20:46,288] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:20:46,289] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:20:46,309] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run35
[2019-04-27 19:20:46,329] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run35
[2019-04-27 19:20:46,330] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run35
[2019-04-27 19:20:46,330] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run35
[2019-04-27 19:20:46,394] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run35
[2019-04-27 19:21:16,378] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0541951], dtype=float32), 0.013405521]
[2019-04-27 19:21:16,381] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [24.03333333333333, 64.33333333333334, 1.0, 2.0, 0.3845215120783713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 476993.3962980373, 476993.3962980373, 126472.1546998787]
[2019-04-27 19:21:16,383] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:21:16,386] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [7.5045576e-30 1.0000000e+00 5.5334717e-30 3.2648665e-32 4.0409973e-16], sampled 0.9294595337109737
[2019-04-27 19:21:38,670] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0541951], dtype=float32), 0.013405521]
[2019-04-27 19:21:38,671] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [21.4301245, 95.29414401666668, 1.0, 2.0, 0.4545463592784008, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 551265.5017625036, 551265.5017625036, 136246.5189930469]
[2019-04-27 19:21:38,672] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 19:21:38,676] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [5.5122298e-32 1.0000000e+00 3.9879311e-33 2.7418632e-35 1.3566693e-19], sampled 0.20131885778715386
[2019-04-27 19:22:04,070] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0541951], dtype=float32), 0.013405521]
[2019-04-27 19:22:04,071] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [30.03876129, 90.026180245, 1.0, 2.0, 0.8900165867563544, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1014510.475318801, 1014510.475318801, 215261.0484211683]
[2019-04-27 19:22:04,072] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 19:22:04,076] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.4283273e-27 1.0000000e+00 5.7842571e-27 1.5909832e-29 2.9378268e-13], sampled 0.2336285419256623
[2019-04-27 19:22:09,425] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0541951], dtype=float32), 0.013405521]
[2019-04-27 19:22:09,426] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [34.6, 35.33333333333334, 1.0, 2.0, 0.8299019883082926, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 973646.5667197348, 973646.5667197348, 203590.9879977372]
[2019-04-27 19:22:09,428] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:22:09,431] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [9.2275966e-22 9.9564159e-01 2.5487848e-20 1.8328531e-21 4.3584197e-03], sampled 0.6922879748397937
[2019-04-27 19:22:37,791] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8227.7264 2466270470.3247 393.0000
[2019-04-27 19:22:37,891] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8780.5480 2214714361.1450 379.0000
[2019-04-27 19:22:37,893] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8851.5749 2181659671.7661 341.0000
[2019-04-27 19:22:38,084] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8693.5461 2263553558.3198 308.0000
[2019-04-27 19:22:38,117] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8948.0196 2131426860.7507 347.0000
[2019-04-27 19:22:39,129] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 850000, evaluation results [850000.0, 8227.72638508096, 2466270470.324706, 393.0, 8851.574879034992, 2181659671.7660584, 341.0, 8948.019555032428, 2131426860.750716, 347.0, 8693.546068753474, 2263553558.319761, 308.0, 8780.54800046207, 2214714361.1450353, 379.0]
[2019-04-27 19:22:40,398] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.3882517e-32 1.0000000e+00 5.9961273e-32 1.7280731e-33 1.1146490e-15], sum to 1.0000
[2019-04-27 19:22:40,406] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2358
[2019-04-27 19:22:40,410] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 72.0, 1.0, 2.0, 0.4185337261464352, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 514542.7970830505, 514542.7970830505, 131162.9501629376], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 90000.0000, 
sim time next is 90600.0000, 
raw observation next is [23.38333333333334, 72.5, 1.0, 2.0, 0.4173254442210857, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 513265.2796244831, 513265.2796244831, 130994.5022609922], 
processed observation next is [1.0, 0.043478260869565216, 0.42160493827160517, 0.725, 1.0, 1.0, 0.30633981454891157, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1833090284373154, 0.1833090284373154, 0.25191250434806195], 
reward next is 0.7481, 
noisyNet noise sample is [array([-0.26764905], dtype=float32), 0.021004843]. 
=============================================
[2019-04-27 19:22:40,760] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.9513181e-33 1.0000000e+00 1.5850743e-32 3.9257576e-34 4.9726682e-18], sum to 1.0000
[2019-04-27 19:22:40,766] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1188
[2019-04-27 19:22:40,770] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.15, 73.5, 1.0, 2.0, 0.4145553212754803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 510445.7127165366, 510445.7127165361, 130612.0096558674], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 91800.0000, 
sim time next is 92400.0000, 
raw observation next is [23.03333333333333, 74.0, 1.0, 2.0, 0.4140282987794167, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 510096.2735931953, 510096.2735931953, 130544.057390051], 
processed observation next is [1.0, 0.043478260869565216, 0.4086419753086419, 0.74, 1.0, 1.0, 0.3024146414040675, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1821772405689983, 0.1821772405689983, 0.25104626421163656], 
reward next is 0.7490, 
noisyNet noise sample is [array([-0.4009453], dtype=float32), 1.5017004]. 
=============================================
[2019-04-27 19:22:41,216] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.7509927e-25 4.9617512e-08 2.7235801e-19 9.2492616e-24 1.0000000e+00], sum to 1.0000
[2019-04-27 19:22:41,221] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7567
[2019-04-27 19:22:41,226] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [37.4, 14.5, 1.0, 2.0, 0.4753972778178616, 1.0, 2.0, 0.4753972778178616, 1.0, 2.0, 0.7681978629132669, 6.9112, 6.9112, 121.94756008, 1714647.627391503, 1714647.627391503, 331217.9783439146], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 137400.0000, 
sim time next is 138000.0000, 
raw observation next is [37.4, 14.0, 1.0, 2.0, 0.4349170124227323, 1.0, 2.0, 0.4349170124227323, 1.0, 2.0, 0.703874790021767, 6.9112, 6.9112, 121.94756008, 1572563.890031444, 1572563.890031444, 312178.6713226615], 
processed observation next is [1.0, 0.6086956521739131, 0.9407407407407407, 0.14, 1.0, 1.0, 0.32728215764610985, 1.0, 1.0, 0.32728215764610985, 1.0, 1.0, 0.6298434875272088, 0.0, 0.0, 0.8096049824067558, 0.5616299607255156, 0.5616299607255156, 0.6003435986974259], 
reward next is 0.3997, 
noisyNet noise sample is [array([-0.8491917], dtype=float32), -1.1782931]. 
=============================================
[2019-04-27 19:22:41,240] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[66.639275]
 [66.62534 ]
 [66.52671 ]
 [66.41283 ]
 [66.36474 ]], R is [[66.86292267]
 [66.55734253]
 [66.29018402]
 [66.04104614]
 [65.79923248]].
[2019-04-27 19:22:45,752] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 3.718542e-30], sum to 1.0000
[2019-04-27 19:22:45,764] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8740
[2019-04-27 19:22:45,773] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.71666666666667, 63.33333333333334, 1.0, 2.0, 0.2890031201017073, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 368726.3042126098, 368726.3042126098, 114227.5748342838], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 197400.0000, 
sim time next is 198000.0000, 
raw observation next is [21.9, 64.0, 1.0, 2.0, 0.3019142662492375, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 384148.6914048651, 384148.6914048651, 115807.9138636798], 
processed observation next is [0.0, 0.30434782608695654, 0.36666666666666664, 0.64, 1.0, 1.0, 0.16894555505861608, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.13719596121602326, 0.13719596121602326, 0.2227075266609227], 
reward next is 0.7773, 
noisyNet noise sample is [array([-0.41940027], dtype=float32), -0.15972549]. 
=============================================
[2019-04-27 19:22:45,786] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[78.7511 ]
 [78.86473]
 [78.96904]
 [79.04881]
 [79.12689]], R is [[78.60912323]
 [78.60337067]
 [78.60072327]
 [78.60079193]
 [78.60214233]].
[2019-04-27 19:22:49,348] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.4111033e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.7767693e-25], sum to 1.0000
[2019-04-27 19:22:49,365] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6383
[2019-04-27 19:22:49,374] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.9, 50.0, 1.0, 2.0, 0.262818821898978, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 339017.3225872531, 339017.3225872531, 96178.00989858127], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 266400.0000, 
sim time next is 267000.0000, 
raw observation next is [20.81666666666667, 50.16666666666667, 1.0, 2.0, 0.2614181102133714, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 337210.1083365334, 337210.1083365334, 95667.19530007259], 
processed observation next is [0.0, 0.08695652173913043, 0.32654320987654334, 0.5016666666666667, 1.0, 1.0, 0.12073584549210882, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.12043218154876194, 0.12043218154876194, 0.18397537557706267], 
reward next is 0.8160, 
noisyNet noise sample is [array([-0.06736965], dtype=float32), 1.4040463]. 
=============================================
[2019-04-27 19:22:49,387] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[74.52525 ]
 [74.552635]
 [74.56916 ]
 [74.564545]
 [74.59512 ]], R is [[75.08377075]
 [75.14797211]
 [75.21031952]
 [75.27068329]
 [75.32872009]].
[2019-04-27 19:22:50,115] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.9174814e-37 1.0000000e+00 2.1969437e-38 0.0000000e+00 2.7047489e-25], sum to 1.0000
[2019-04-27 19:22:50,122] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5320
[2019-04-27 19:22:50,125] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.73333333333333, 51.83333333333334, 1.0, 2.0, 0.2465436624969217, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 318019.1883397989, 318019.1883397989, 89606.01817534257], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 277800.0000, 
sim time next is 278400.0000, 
raw observation next is [19.96666666666667, 50.66666666666667, 1.0, 2.0, 0.250130347484551, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 322646.6630840523, 322646.6630840519, 90304.28990717261], 
processed observation next is [0.0, 0.21739130434782608, 0.2950617283950618, 0.5066666666666667, 1.0, 1.0, 0.10729803271970355, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.11523095110144727, 0.11523095110144711, 0.17366209597533194], 
reward next is 0.8263, 
noisyNet noise sample is [array([1.7885836], dtype=float32), -1.463122]. 
=============================================
[2019-04-27 19:23:09,638] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0612638e-25 9.9999821e-01 6.7052514e-23 6.2274613e-25 1.8276972e-06], sum to 1.0000
[2019-04-27 19:23:09,643] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1941
[2019-04-27 19:23:09,649] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 43.0, 1.0, 2.0, 0.4006480586228517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 513632.9194004771, 513632.9194004771, 128916.9836750382], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 702000.0000, 
sim time next is 702600.0000, 
raw observation next is [24.53333333333333, 43.83333333333334, 1.0, 2.0, 0.4276211817326542, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 548200.8541776899, 548200.8541776894, 132799.4714406681], 
processed observation next is [1.0, 0.13043478260869565, 0.46419753086419746, 0.4383333333333334, 1.0, 1.0, 0.31859664491982637, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.19578601934917494, 0.19578601934917478, 0.25538359892436174], 
reward next is 0.7446, 
noisyNet noise sample is [array([-0.13394982], dtype=float32), 1.0492245]. 
=============================================
[2019-04-27 19:23:13,262] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2144214e-18 2.8236687e-05 4.2024167e-15 1.2186274e-16 9.9997175e-01], sum to 1.0000
[2019-04-27 19:23:13,270] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7372
[2019-04-27 19:23:13,274] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.43333333333334, 53.0, 1.0, 2.0, 0.2994014282735586, 1.0, 2.0, 0.2994014282735586, 1.0, 2.0, 0.4877053797244185, 6.9112, 6.9112, 121.94756008, 1092362.332361147, 1092362.332361147, 254755.8976399078], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 724200.0000, 
sim time next is 724800.0000, 
raw observation next is [25.56666666666667, 53.0, 1.0, 2.0, 0.2751843467536219, 1.0, 2.0, 0.2751843467536219, 1.0, 2.0, 0.4484699711060123, 6.9112, 6.9112, 121.94756008, 1004559.442536657, 1004559.442536657, 245621.1803544129], 
processed observation next is [1.0, 0.391304347826087, 0.5024691358024692, 0.53, 1.0, 1.0, 0.13712422232574037, 1.0, 1.0, 0.13712422232574037, 1.0, 1.0, 0.3105874638825153, 0.0, 0.0, 0.8096049824067558, 0.3587712294773775, 0.3587712294773775, 0.47234842375848635], 
reward next is 0.5277, 
noisyNet noise sample is [array([-1.0186943], dtype=float32), -0.27799216]. 
=============================================
[2019-04-27 19:23:13,804] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1241142e-34 1.0000000e+00 8.2893505e-29 3.1248014e-36 7.2185495e-15], sum to 1.0000
[2019-04-27 19:23:13,812] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5261
[2019-04-27 19:23:13,815] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.63333333333333, 46.66666666666667, 1.0, 2.0, 0.2984204782003076, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 380646.3701127513, 380646.3701127513, 115380.0484747366], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 781800.0000, 
sim time next is 782400.0000, 
raw observation next is [24.46666666666667, 47.33333333333334, 1.0, 2.0, 0.2998614427290731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 382578.1213842436, 382578.1213842436, 115558.1620484562], 
processed observation next is [0.0, 0.043478260869565216, 0.46172839506172847, 0.47333333333333344, 1.0, 1.0, 0.16650171753461085, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.13663504335151558, 0.13663504335151558, 0.22222723470856962], 
reward next is 0.7778, 
noisyNet noise sample is [array([0.16024445], dtype=float32), -1.3464943]. 
=============================================
[2019-04-27 19:23:16,114] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7481478e-35 1.0000000e+00 1.4779865e-33 0.0000000e+00 2.1067392e-20], sum to 1.0000
[2019-04-27 19:23:16,120] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4301
[2019-04-27 19:23:16,124] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.2, 44.0, 1.0, 2.0, 0.2858875780873585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 367218.219333115, 367218.219333115, 113847.3605886706], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1022400.0000, 
sim time next is 1023000.0000, 
raw observation next is [24.13333333333333, 44.33333333333334, 1.0, 2.0, 0.2844059778755418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 365487.9617538574, 365487.9617538574, 113667.50712506], 
processed observation next is [1.0, 0.8695652173913043, 0.44938271604938257, 0.4433333333333334, 1.0, 1.0, 0.14810235461374022, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.13053141491209191, 0.13053141491209191, 0.21859135985588463], 
reward next is 0.7814, 
noisyNet noise sample is [array([-0.11921987], dtype=float32), -1.783359]. 
=============================================
[2019-04-27 19:23:16,138] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[78.655045]
 [78.44372 ]
 [78.67865 ]
 [79.05605 ]
 [79.47816 ]], R is [[78.27640533]
 [78.27470398]
 [78.27177429]
 [78.26799774]
 [78.26344299]].
[2019-04-27 19:23:19,782] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.2003673e-29 1.0000000e+00 4.5912853e-27 7.2596091e-31 8.0415431e-12], sum to 1.0000
[2019-04-27 19:23:19,792] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2119
[2019-04-27 19:23:19,797] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.83333333333334, 55.0, 1.0, 2.0, 0.4007074542806749, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 496280.7489411772, 496280.7489411772, 128714.1709372753], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 859200.0000, 
sim time next is 859800.0000, 
raw observation next is [25.66666666666666, 56.0, 1.0, 2.0, 0.4029223144603589, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 498866.4382114506, 498866.4382114506, 129023.3136083342], 
processed observation next is [0.0, 0.9565217391304348, 0.5061728395061726, 0.56, 1.0, 1.0, 0.2891932315004273, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.17816658507551808, 0.17816658507551808, 0.2481217569391042], 
reward next is 0.7519, 
noisyNet noise sample is [array([0.15783025], dtype=float32), 1.5597916]. 
=============================================
[2019-04-27 19:23:26,819] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.8039692e-21 4.6862831e-05 1.5632092e-17 4.3493285e-18 9.9995315e-01], sum to 1.0000
[2019-04-27 19:23:26,825] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1278
[2019-04-27 19:23:26,831] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.2, 55.0, 1.0, 2.0, 0.2987038827967417, 1.0, 2.0, 0.2987038827967417, 1.0, 2.0, 0.4860971316875708, 6.9112, 6.9112, 121.94756008, 1088430.108198158, 1088430.108198158, 254545.1476157924], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 993600.0000, 
sim time next is 994200.0000, 
raw observation next is [25.21666666666667, 54.66666666666667, 1.0, 2.0, 0.3248015964270145, 1.0, 2.0, 0.3248015964270145, 1.0, 2.0, 0.5283251094176873, 6.911200000000001, 6.9112, 121.94756008, 1182872.63566382, 1182872.63566382, 264746.2855324714], 
processed observation next is [1.0, 0.5217391304347826, 0.48950617283950626, 0.5466666666666667, 1.0, 1.0, 0.1961923766988268, 1.0, 1.0, 0.1961923766988268, 1.0, 1.0, 0.41040638677210906, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.42245451273707857, 0.42245451273707857, 0.5091274721778296], 
reward next is 0.4909, 
noisyNet noise sample is [array([0.13253988], dtype=float32), 0.56543934]. 
=============================================
[2019-04-27 19:23:27,250] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-27 19:23:27,252] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 19:23:27,253] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 19:23:27,254] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 19:23:27,255] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:23:27,254] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:23:27,255] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:23:27,256] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 19:23:27,256] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 19:23:27,258] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:23:27,259] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:23:27,277] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run36
[2019-04-27 19:23:27,296] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run36
[2019-04-27 19:23:27,329] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run36
[2019-04-27 19:23:27,330] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run36
[2019-04-27 19:23:27,366] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run36
[2019-04-27 19:23:52,013] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05593139], dtype=float32), 0.018955009]
[2019-04-27 19:23:52,015] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [18.5497897, 83.27619315666666, 1.0, 2.0, 0.3213615858413036, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 410889.5156466952, 410889.5156466952, 118256.9343678933]
[2019-04-27 19:23:52,017] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 19:23:52,020] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.4309811e-30 1.0000000e+00 5.5021275e-31 3.8182808e-33 7.0327318e-18], sampled 0.8975681265235217
[2019-04-27 19:23:57,555] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05593139], dtype=float32), 0.018955009]
[2019-04-27 19:23:57,558] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [28.51666666666667, 66.33333333333333, 1.0, 2.0, 0.6024017764336798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 690848.7297404585, 690848.7297404585, 158704.7328579664]
[2019-04-27 19:23:57,558] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:23:57,560] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.7929048e-27 1.0000000e+00 1.1201884e-26 1.8243435e-28 2.5736866e-11], sampled 0.008599824290561164
[2019-04-27 19:24:37,668] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05593139], dtype=float32), 0.018955009]
[2019-04-27 19:24:37,669] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [26.93333333333334, 72.66666666666667, 1.0, 2.0, 0.6360583631835802, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 736161.640030787, 736161.640030787, 164940.0940596635]
[2019-04-27 19:24:37,672] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:24:37,673] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.7870658e-28 1.0000000e+00 2.3011947e-28 3.6225219e-30 7.6628442e-14], sampled 0.3698465572850178
[2019-04-27 19:24:40,821] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05593139], dtype=float32), 0.018955009]
[2019-04-27 19:24:40,822] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [25.0, 100.0, 1.0, 2.0, 0.7102827148683011, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 809527.5524457911, 809527.5524457911, 178100.7487896487]
[2019-04-27 19:24:40,824] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:24:40,827] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [4.1917602e-30 1.0000000e+00 1.7959208e-30 2.3867511e-32 2.9992132e-16], sampled 0.09263717953194539
[2019-04-27 19:24:46,590] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05593139], dtype=float32), 0.018955009]
[2019-04-27 19:24:46,592] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [26.86703467333334, 100.199128325, 1.0, 2.0, 0.7340202242865607, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 836596.5699606641, 836596.5699606641, 182695.8598556915]
[2019-04-27 19:24:46,599] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 19:24:46,602] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [4.6252350e-25 1.0000000e+00 1.4424014e-24 7.1751053e-26 7.2417397e-09], sampled 0.9047724405147458
[2019-04-27 19:25:15,161] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05593139], dtype=float32), 0.018955009]
[2019-04-27 19:25:15,162] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [24.45881233, 90.44783639333335, 1.0, 2.0, 0.5798133822795158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 669630.9112809042, 669630.9112809042, 155068.9727420052]
[2019-04-27 19:25:15,163] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:25:15,166] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [3.2955550e-28 1.0000000e+00 1.5282210e-27 1.6731340e-29 1.3333067e-12], sampled 0.14582759916814592
[2019-04-27 19:25:19,354] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8871.0600 2190359916.2550 275.0000
[2019-04-27 19:25:19,463] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8718.0874 2272909986.9863 234.0000
[2019-04-27 19:25:19,512] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8968.3947 2141798842.1154 279.0000
[2019-04-27 19:25:19,524] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8773.4360 2228419263.5934 322.0000
[2019-04-27 19:25:19,628] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8271.1027 2477662173.9553 286.0000
[2019-04-27 19:25:20,644] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 875000, evaluation results [875000.0, 8271.10274421039, 2477662173.95526, 286.0, 8871.060027269597, 2190359916.2549725, 275.0, 8968.394747909437, 2141798842.115385, 279.0, 8718.087392069658, 2272909986.986291, 234.0, 8773.435973176247, 2228419263.5934353, 322.0]
[2019-04-27 19:25:21,768] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2764891e-31 1.0000000e+00 6.4548106e-31 1.7726353e-31 3.4117288e-14], sum to 1.0000
[2019-04-27 19:25:21,779] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7031
[2019-04-27 19:25:21,783] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.8, 46.0, 1.0, 2.0, 0.281333595264924, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 361536.5445528952, 361536.5445528952, 113297.2900388053], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1026000.0000, 
sim time next is 1026600.0000, 
raw observation next is [23.75, 46.16666666666667, 1.0, 2.0, 0.2820788055220764, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 362526.3363591266, 362526.3363591261, 113386.7509036056], 
processed observation next is [1.0, 0.9130434782608695, 0.4351851851851852, 0.4616666666666667, 1.0, 1.0, 0.14533191133580528, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.12947369155683094, 0.12947369155683075, 0.2180514440453954], 
reward next is 0.7819, 
noisyNet noise sample is [array([-1.7925848], dtype=float32), 0.88429755]. 
=============================================
[2019-04-27 19:25:23,177] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7134614e-33 1.0000000e+00 2.8656229e-34 6.7501056e-36 3.5353993e-20], sum to 1.0000
[2019-04-27 19:25:23,187] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2069
[2019-04-27 19:25:23,191] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.85, 68.33333333333334, 1.0, 2.0, 0.3605622236647261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 459968.7502388999, 459968.7502388999, 123385.4849672178], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1048200.0000, 
sim time next is 1048800.0000, 
raw observation next is [20.8, 68.66666666666667, 1.0, 2.0, 0.3322285972302313, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 423799.8773509867, 423799.8773509867, 119649.923485034], 
processed observation next is [1.0, 0.13043478260869565, 0.32592592592592595, 0.6866666666666668, 1.0, 1.0, 0.2050340443217039, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.15135709905392383, 0.15135709905392383, 0.23009600670198846], 
reward next is 0.7699, 
noisyNet noise sample is [array([-0.6516203], dtype=float32), 0.3018251]. 
=============================================
[2019-04-27 19:25:28,280] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.1990895e-38 1.0000000e+00 9.5385747e-38 0.0000000e+00 1.4616463e-28], sum to 1.0000
[2019-04-27 19:25:28,287] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0032
[2019-04-27 19:25:28,291] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.6, 71.0, 1.0, 2.0, 0.2850040937827987, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 366420.5983225626, 366420.5983225626, 113738.8529575201], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1148400.0000, 
sim time next is 1149000.0000, 
raw observation next is [19.68333333333334, 70.5, 1.0, 2.0, 0.3294628033630741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 423519.5024312892, 423519.5024312892, 119289.9970123308], 
processed observation next is [1.0, 0.30434782608695654, 0.28456790123456815, 0.705, 1.0, 1.0, 0.20174143257508823, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.15125696515403184, 0.15125696515403184, 0.22940384040832845], 
reward next is 0.7706, 
noisyNet noise sample is [array([0.32190487], dtype=float32), -0.85329556]. 
=============================================
[2019-04-27 19:25:28,299] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[71.305405]
 [71.33734 ]
 [71.39952 ]
 [71.46945 ]
 [71.53957 ]], R is [[71.15710449]
 [71.22680664]
 [71.29479218]
 [71.36231995]
 [71.43015289]].
[2019-04-27 19:25:34,756] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.7591704e-30 1.0000000e+00 3.8173916e-26 7.7194793e-30 2.3332687e-09], sum to 1.0000
[2019-04-27 19:25:34,762] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7316
[2019-04-27 19:25:34,766] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 57.0, 1.0, 2.0, 0.4043554964344748, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 497157.7375558148, 497157.7375558144, 129140.3574155762], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1274400.0000, 
sim time next is 1275000.0000, 
raw observation next is [25.66666666666666, 57.83333333333333, 1.0, 2.0, 0.4068676063500931, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 501690.2698713054, 501690.2698713058, 129533.5277865722], 
processed observation next is [1.0, 0.782608695652174, 0.5061728395061726, 0.5783333333333333, 1.0, 1.0, 0.2938900075596347, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.17917509638260906, 0.1791750963826092, 0.2491029380511004], 
reward next is 0.7509, 
noisyNet noise sample is [array([0.37919632], dtype=float32), 0.43290415]. 
=============================================
[2019-04-27 19:25:34,778] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[75.71608]
 [76.02728]
 [75.81519]
 [75.60495]
 [75.34587]], R is [[75.82686615]
 [75.82025146]
 [75.06204987]
 [74.31143188]
 [73.56832123]].
[2019-04-27 19:25:35,860] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.3755162e-33 1.0000000e+00 2.6309103e-32 7.9824251e-33 1.5581897e-20], sum to 1.0000
[2019-04-27 19:25:35,872] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7636
[2019-04-27 19:25:35,876] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.76666666666667, 86.0, 1.0, 2.0, 0.3507572022679769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 441391.2680962301, 441391.2680962301, 122022.5261088725], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1300800.0000, 
sim time next is 1301400.0000, 
raw observation next is [19.7, 86.0, 1.0, 2.0, 0.347122980868833, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 437165.7858235738, 437165.7858235738, 121547.057884526], 
processed observation next is [1.0, 0.043478260869565216, 0.28518518518518515, 0.86, 1.0, 1.0, 0.22276545341527734, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1561306377941335, 0.1561306377941335, 0.23374434208562692], 
reward next is 0.7663, 
noisyNet noise sample is [array([-0.64420664], dtype=float32), -1.5944965]. 
=============================================
[2019-04-27 19:25:47,725] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2756171e-32 1.0000000e+00 4.4771585e-31 8.9189681e-35 7.5067435e-20], sum to 1.0000
[2019-04-27 19:25:47,733] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1960
[2019-04-27 19:25:47,738] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.63333333333333, 21.16666666666667, 1.0, 2.0, 0.4301012811385961, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 532126.7258112546, 532126.7258112546, 132922.7652851475], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1519800.0000, 
sim time next is 1520400.0000, 
raw observation next is [35.46666666666667, 22.33333333333334, 1.0, 2.0, 0.4171244694510586, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 514422.4448261738, 514422.4448261738, 131001.2592148103], 
processed observation next is [0.0, 0.6086956521739131, 0.8691358024691359, 0.22333333333333338, 1.0, 1.0, 0.30610055887030785, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.18372230172363352, 0.18372230172363352, 0.25192549849001983], 
reward next is 0.7481, 
noisyNet noise sample is [array([0.7914155], dtype=float32), 0.6961137]. 
=============================================
[2019-04-27 19:25:49,647] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.0239108e-33 1.0000000e+00 2.6748560e-35 5.9146886e-38 4.7738850e-29], sum to 1.0000
[2019-04-27 19:25:49,648] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5736
[2019-04-27 19:25:49,653] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.13333333333333, 83.33333333333334, 1.0, 2.0, 0.4425424063187426, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 556502.3693397014, 556502.3693397014, 134925.8859638096], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1574400.0000, 
sim time next is 1575000.0000, 
raw observation next is [20.35, 81.5, 1.0, 2.0, 0.4345644071961947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 546626.1992381251, 546626.1992381251, 133746.6121341304], 
processed observation next is [1.0, 0.21739130434782608, 0.3092592592592593, 0.815, 1.0, 1.0, 0.3268623895192794, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.19522364258504468, 0.19522364258504468, 0.25720502333486617], 
reward next is 0.7428, 
noisyNet noise sample is [array([-0.7375521], dtype=float32), -1.4540335]. 
=============================================
[2019-04-27 19:25:49,679] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[57.286747]
 [57.197655]
 [56.973907]
 [56.947784]
 [57.114914]], R is [[57.43948746]
 [57.60562134]
 [57.75623322]
 [57.90294266]
 [58.03107834]].
[2019-04-27 19:25:49,918] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.5754365e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.7662534e-32], sum to 1.0000
[2019-04-27 19:25:49,925] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3290
[2019-04-27 19:25:49,930] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 70.0, 1.0, 2.0, 0.3967305046824257, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 499926.8813498668, 499926.8813498663, 128309.8927267384], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1579200.0000, 
sim time next is 1579800.0000, 
raw observation next is [22.0, 68.5, 1.0, 2.0, 0.4053199415254609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 510852.3910783941, 510852.3910783946, 129526.1445090968], 
processed observation next is [1.0, 0.2608695652173913, 0.37037037037037035, 0.685, 1.0, 1.0, 0.29204754943507255, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.18244728252799788, 0.18244728252799808, 0.24908873944057078], 
reward next is 0.7509, 
noisyNet noise sample is [array([1.4242021], dtype=float32), -1.0375829]. 
=============================================
[2019-04-27 19:25:51,375] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.1340429e-20 1.0000000e+00 9.2219769e-20 6.5274939e-21 3.3449285e-11], sum to 1.0000
[2019-04-27 19:25:51,382] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5070
[2019-04-27 19:25:51,387] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.13333333333334, 55.66666666666667, 1.0, 2.0, 0.9040961416686679, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.06763361983057, 6.9112, 121.9250854549617, 1205875.9312171, 1125768.564860441, 222357.2920624555], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1591800.0000, 
sim time next is 1592400.0000, 
raw observation next is [25.26666666666667, 55.33333333333334, 1.0, 2.0, 0.9353186848739066, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.273085148879311, 6.9112, 121.9244482236833, 1349204.680141592, 1163889.56233228, 229626.4278149557], 
processed observation next is [1.0, 0.43478260869565216, 0.49135802469135814, 0.5533333333333335, 1.0, 1.0, 0.9229984343736983, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.03618851488793107, 0.0, 0.8094515437154495, 0.48185881433628286, 0.4156748436901, 0.4415892842595302], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.6022752], dtype=float32), 0.64458454]. 
=============================================
[2019-04-27 19:26:02,798] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.9129055e-30 1.0000000e+00 3.9455699e-29 1.7633954e-31 5.4179794e-17], sum to 1.0000
[2019-04-27 19:26:02,805] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0257
[2019-04-27 19:26:02,815] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.26666666666667, 92.0, 1.0, 2.0, 0.3408447097305241, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 432897.3540817468, 432897.3540817468, 120760.8380247495], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1822800.0000, 
sim time next is 1823400.0000, 
raw observation next is [18.3, 92.0, 1.0, 2.0, 0.3270998147260561, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 415315.6236874032, 415315.6236874032, 118978.7129720239], 
processed observation next is [1.0, 0.08695652173913043, 0.23333333333333336, 0.92, 1.0, 1.0, 0.19892835086435248, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.14832700845978686, 0.14832700845978686, 0.2288052172538921], 
reward next is 0.7712, 
noisyNet noise sample is [array([0.25751668], dtype=float32), -1.0940511]. 
=============================================
[2019-04-27 19:26:06,948] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.1040341e-30 1.0000000e+00 3.4137763e-27 6.7931100e-30 3.0375451e-15], sum to 1.0000
[2019-04-27 19:26:06,955] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1725
[2019-04-27 19:26:06,961] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.75, 92.0, 1.0, 2.0, 0.3734355042607002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 465733.599980825, 465733.5999808246, 125002.6422449403], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1906200.0000, 
sim time next is 1906800.0000, 
raw observation next is [19.66666666666667, 92.0, 1.0, 2.0, 0.3698207687598741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 461735.2258255981, 461735.2258255981, 124520.6615421738], 
processed observation next is [1.0, 0.043478260869565216, 0.28395061728395077, 0.92, 1.0, 1.0, 0.24978662947604058, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16490543779485647, 0.16490543779485647, 0.23946281065802652], 
reward next is 0.7605, 
noisyNet noise sample is [array([1.990067], dtype=float32), 0.36638138]. 
=============================================
[2019-04-27 19:26:07,008] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.2593265e-29 1.0000000e+00 4.8407249e-30 3.8507370e-34 3.2807095e-17], sum to 1.0000
[2019-04-27 19:26:07,021] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4803
[2019-04-27 19:26:07,027] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.6, 92.0, 1.0, 2.0, 0.3603668629750033, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 450479.5538233489, 450479.5538233494, 123255.7637516863], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1912800.0000, 
sim time next is 1913400.0000, 
raw observation next is [19.6, 92.0, 1.0, 2.0, 0.3589281380241818, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 448686.7308319121, 448686.7308319126, 123063.191087905], 
processed observation next is [1.0, 0.13043478260869565, 0.28148148148148155, 0.92, 1.0, 1.0, 0.23681921193354974, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.16024526101139716, 0.16024526101139736, 0.23665998286135576], 
reward next is 0.7633, 
noisyNet noise sample is [array([-0.28201312], dtype=float32), -0.4364742]. 
=============================================
[2019-04-27 19:26:08,372] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-27 19:26:08,374] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 19:26:08,374] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 19:26:08,375] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:26:08,375] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:26:08,376] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 19:26:08,377] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 19:26:08,379] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 19:26:08,380] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:26:08,378] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:26:08,381] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:26:08,396] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run37
[2019-04-27 19:26:08,397] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run37
[2019-04-27 19:26:08,398] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run37
[2019-04-27 19:26:08,416] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run37
[2019-04-27 19:26:08,472] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run37
[2019-04-27 19:27:22,964] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05632879], dtype=float32), 0.029475141]
[2019-04-27 19:27:22,967] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [31.66666666666667, 54.33333333333334, 1.0, 2.0, 0.4840723350169271, 1.0, 2.0, 0.4840723350169271, 1.0, 2.0, 0.7706589224822782, 6.9112, 6.9112, 121.94756008, 1656031.487492906, 1656031.487492906, 335840.3484915267]
[2019-04-27 19:27:22,968] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 19:27:22,970] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [7.2410455e-25 4.5749388e-08 2.7139039e-19 6.2999043e-22 1.0000000e+00], sampled 0.8360462410117137
[2019-04-27 19:27:43,104] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05632879], dtype=float32), 0.029475141]
[2019-04-27 19:27:43,105] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [23.7, 44.0, 1.0, 2.0, 0.279933059348034, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 361044.3761080891, 361044.3761080891, 113118.5891364424]
[2019-04-27 19:27:43,106] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:27:43,108] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [9.7607743e-34 1.0000000e+00 7.5301218e-34 8.1125065e-37 2.9049345e-21], sampled 0.6380003023414244
[2019-04-27 19:28:00,606] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8855.8792 2178992854.9296 348.0000
[2019-04-27 19:28:00,888] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8243.9506 2462176227.8386 414.0000
[2019-04-27 19:28:00,919] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8684.2203 2259580938.0960 341.0000
[2019-04-27 19:28:00,955] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8961.9042 2131382416.0734 335.0000
[2019-04-27 19:28:00,992] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8773.1041 2210879193.4620 405.0000
[2019-04-27 19:28:02,004] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 900000, evaluation results [900000.0, 8243.950582725762, 2462176227.8386016, 414.0, 8855.879188275832, 2178992854.9295783, 348.0, 8961.904176966442, 2131382416.0734456, 335.0, 8684.220299403323, 2259580938.0959816, 341.0, 8773.104081654146, 2210879193.4620457, 405.0]
[2019-04-27 19:28:10,991] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.9805752e-32 1.0000000e+00 2.5308083e-31 2.2209804e-34 4.6531730e-20], sum to 1.0000
[2019-04-27 19:28:11,001] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5285
[2019-04-27 19:28:11,008] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.4, 70.0, 1.0, 2.0, 0.5769748382516512, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 668040.2305726572, 668040.2305726572, 154667.0797539698], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2114400.0000, 
sim time next is 2115000.0000, 
raw observation next is [27.6, 69.0, 1.0, 2.0, 0.5787328060122909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 669696.610814496, 669696.610814496, 154946.9252769672], 
processed observation next is [0.0, 0.4782608695652174, 0.5777777777777778, 0.69, 1.0, 1.0, 0.49849143572891774, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2391773610051771, 0.2391773610051771, 0.29797485630186], 
reward next is 0.7020, 
noisyNet noise sample is [array([0.42248183], dtype=float32), -1.4730631]. 
=============================================
[2019-04-27 19:28:11,026] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[67.78434]
 [67.76796]
 [67.77311]
 [67.77413]
 [67.771  ]], R is [[67.83297729]
 [67.85720825]
 [67.88194275]
 [67.90740967]
 [67.93418884]].
[2019-04-27 19:28:20,012] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.0491621e-22 9.9998915e-01 6.3841012e-21 2.0620938e-21 1.0790296e-05], sum to 1.0000
[2019-04-27 19:28:20,021] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8038
[2019-04-27 19:28:20,026] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.73333333333333, 96.0, 1.0, 2.0, 0.6788867757876778, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 829392.6976834217, 829392.6976834217, 174500.7403044499], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2276400.0000, 
sim time next is 2277000.0000, 
raw observation next is [20.9, 95.5, 1.0, 2.0, 0.7663074427298493, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999998, 6.9112, 121.9260426156618, 934285.1730905622, 934285.1730905632, 191714.2739000642], 
processed observation next is [1.0, 0.34782608695652173, 0.32962962962962955, 0.955, 1.0, 1.0, 0.721794574678392, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -1.7763568394002506e-16, 0.0, 0.8094621288201359, 0.33367327610377223, 0.33367327610377256, 0.36868129596166194], 
reward next is 0.6313, 
noisyNet noise sample is [array([0.3740054], dtype=float32), 0.95636916]. 
=============================================
[2019-04-27 19:28:20,039] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[62.47071 ]
 [63.15045 ]
 [63.218227]
 [63.170128]
 [63.254765]], R is [[62.25312805]
 [62.29501724]
 [62.40232468]
 [62.51813507]
 [62.62610626]].
[2019-04-27 19:28:24,729] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7304260e-20 9.9998450e-01 6.0644860e-19 1.5814960e-20 1.5541942e-05], sum to 1.0000
[2019-04-27 19:28:24,736] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4640
[2019-04-27 19:28:24,746] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1388887.526665286 W.
[2019-04-27 19:28:24,751] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.8, 37.0, 1.0, 2.0, 0.3879441689106086, 1.0, 1.0, 0.3879441689106086, 1.0, 1.0, 0.624480573316013, 6.9112, 6.9112, 121.94756008, 1388887.526665286, 1388887.526665286, 291536.6229349566], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2381400.0000, 
sim time next is 2382000.0000, 
raw observation next is [30.9, 37.0, 1.0, 2.0, 0.3738438933948932, 1.0, 2.0, 0.3738438933948932, 1.0, 2.0, 0.5999561242432933, 6.9112, 6.9112, 121.94756008, 1329145.199742943, 1329145.199742943, 285647.4535506103], 
processed observation next is [1.0, 0.5652173913043478, 0.7, 0.37, 1.0, 1.0, 0.25457606356534906, 1.0, 1.0, 0.25457606356534906, 1.0, 1.0, 0.49994515530411654, 0.0, 0.0, 0.8096049824067558, 0.47469471419390824, 0.47469471419390824, 0.5493220260588659], 
reward next is 0.4507, 
noisyNet noise sample is [array([-0.5957987], dtype=float32), -0.25600085]. 
=============================================
[2019-04-27 19:28:24,760] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[53.921513]
 [55.364426]
 [55.76672 ]
 [54.31232 ]
 [53.75008 ]], R is [[54.04387665]
 [53.94279099]
 [53.40336227]
 [53.49271011]
 [53.57479095]].
[2019-04-27 19:28:37,844] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 1.1771381e-37 0.0000000e+00 3.5673004e-29], sum to 1.0000
[2019-04-27 19:28:37,850] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1954
[2019-04-27 19:28:37,855] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 93.5, 1.0, 2.0, 0.4656763224426659, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 563920.2214154211, 563920.2214154206, 137897.893802829], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2622600.0000, 
sim time next is 2623200.0000, 
raw observation next is [21.73333333333333, 93.66666666666667, 1.0, 2.0, 0.4720736983277609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 570185.1213199649, 570185.1213199649, 138824.4510055018], 
processed observation next is [0.0, 0.34782608695652173, 0.3604938271604937, 0.9366666666666668, 1.0, 1.0, 0.3715163075330487, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.20363754332855888, 0.20363754332855888, 0.26697009808750344], 
reward next is 0.7330, 
noisyNet noise sample is [array([-0.5545169], dtype=float32), -0.10471514]. 
=============================================
[2019-04-27 19:28:39,473] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 1.3418893e-38 0.0000000e+00 7.5011636e-27], sum to 1.0000
[2019-04-27 19:28:39,481] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3294
[2019-04-27 19:28:39,486] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.5628861965184881, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 658571.8013575623, 658571.8013575628, 152607.7776513331], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2669400.0000, 
sim time next is 2670000.0000, 
raw observation next is [24.0, 89.0, 1.0, 2.0, 0.5667616806254794, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 663101.7677438912, 663101.7677438912, 153256.5875951818], 
processed observation next is [0.0, 0.9130434782608695, 0.4444444444444444, 0.89, 1.0, 1.0, 0.4842400959827135, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.23682205990853258, 0.23682205990853258, 0.2947242069138112], 
reward next is 0.7053, 
noisyNet noise sample is [array([-1.192055], dtype=float32), 0.6791932]. 
=============================================
[2019-04-27 19:28:39,497] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[77.62755 ]
 [77.609764]
 [77.57685 ]
 [77.538895]
 [77.47323 ]], R is [[77.54486847]
 [77.47593689]
 [77.40766907]
 [77.33979034]
 [77.27189636]].
[2019-04-27 19:28:47,606] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.4987408e-34 1.0000000e+00 2.2973437e-36 0.0000000e+00 3.2840310e-30], sum to 1.0000
[2019-04-27 19:28:47,616] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1512
[2019-04-27 19:28:47,620] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 91.0, 1.0, 2.0, 0.5109770216483458, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 611542.9561815572, 611542.9561815572, 144706.5845021327], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2858400.0000, 
sim time next is 2859000.0000, 
raw observation next is [22.41666666666667, 92.5, 1.0, 2.0, 0.977860647871295, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.316422299488847, 6.9112, 122.6799032636202, 1378114.35210626, 1169321.295599735, 238300.2825366607], 
processed observation next is [1.0, 0.08695652173913043, 0.38580246913580263, 0.925, 1.0, 1.0, 0.9736436284182084, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.04052222994888473, 0.0, 0.8144669795627594, 0.4921836971808071, 0.41761474842847685, 0.45826977410896286], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.3079503], dtype=float32), 0.8550602]. 
=============================================
[2019-04-27 19:28:47,641] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[54.682983]
 [54.700817]
 [54.78208 ]
 [54.860184]
 [55.030746]], R is [[51.90879822]
 [52.11142731]
 [52.31044769]
 [52.5058136 ]
 [52.69727707]].
[2019-04-27 19:28:49,008] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.5379641e-35 1.0000000e+00 1.3115304e-37 0.0000000e+00 5.3957836e-33], sum to 1.0000
[2019-04-27 19:28:49,016] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0495
[2019-04-27 19:28:49,024] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.26666666666667, 96.0, 1.0, 2.0, 0.5903269793297162, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 701736.749147076, 701736.7491470756, 157714.3831236465], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2868000.0000, 
sim time next is 2868600.0000, 
raw observation next is [22.33333333333334, 95.0, 1.0, 2.0, 0.5804806670582467, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 690761.6687899482, 690761.6687899482, 156049.294076078], 
processed observation next is [1.0, 0.17391304347826086, 0.38271604938271625, 0.95, 1.0, 1.0, 0.500572222688389, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2467005959964101, 0.2467005959964101, 0.30009479630014996], 
reward next is 0.6999, 
noisyNet noise sample is [array([-0.1330814], dtype=float32), -0.5340399]. 
=============================================
[2019-04-27 19:28:49,769] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-04-27 19:28:49,772] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 19:28:49,773] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:28:49,774] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 19:28:49,774] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 19:28:49,774] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:28:49,775] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 19:28:49,774] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:28:49,777] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:28:49,778] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 19:28:49,779] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:28:49,797] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run38
[2019-04-27 19:28:49,816] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run38
[2019-04-27 19:28:49,836] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run38
[2019-04-27 19:28:49,865] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run38
[2019-04-27 19:28:49,892] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run38
[2019-04-27 19:28:59,039] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06339864], dtype=float32), 0.027317813]
[2019-04-27 19:28:59,041] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [31.1066589, 28.98349391, 1.0, 2.0, 0.3559360404005098, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156353, 445370.8856837213, 445370.8856837213, 122670.6502224607]
[2019-04-27 19:28:59,042] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:28:59,045] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.6089505e-37], sampled 0.21907203220058502
[2019-04-27 19:29:24,309] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06339864], dtype=float32), 0.027317813]
[2019-04-27 19:29:24,311] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [31.66666666666666, 53.0, 1.0, 2.0, 0.8015893310974167, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 913654.0890515696, 913654.0890515696, 196262.8987319667]
[2019-04-27 19:29:24,312] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:29:24,314] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 9.039193e-35], sampled 0.7436759147026538
[2019-04-27 19:29:30,667] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06339864], dtype=float32), 0.027317813]
[2019-04-27 19:29:30,669] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [28.16666666666667, 73.33333333333334, 1.0, 2.0, 0.6741603278348285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 768337.3007012532, 768337.3007012532, 171308.7787005906]
[2019-04-27 19:29:30,671] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:29:30,675] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.1374997e-35], sampled 0.30287681227684893
[2019-04-27 19:29:36,779] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06339864], dtype=float32), 0.027317813]
[2019-04-27 19:29:36,781] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [31.54134974, 37.08416244, 1.0, 2.0, 0.4251939728867709, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 517615.7081722611, 517615.7081722611, 131983.7153546507]
[2019-04-27 19:29:36,783] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:29:36,784] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.69456774879825
[2019-04-27 19:30:34,709] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06339864], dtype=float32), 0.027317813]
[2019-04-27 19:30:34,710] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [19.1, 95.0, 1.0, 2.0, 0.6568069229579003, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 822154.4706793258, 822154.4706793258, 170836.2642611451]
[2019-04-27 19:30:34,712] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:30:34,714] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6974295826852608
[2019-04-27 19:30:42,657] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8580.9105 2249102015.0910 549.0000
[2019-04-27 19:30:42,691] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8919.9315 2120918467.8278 431.0000
[2019-04-27 19:30:42,766] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8097.7759 2445650767.0112 740.0000
[2019-04-27 19:30:42,814] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8766.9455 2170970372.7489 493.0000
[2019-04-27 19:30:42,860] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8697.1187 2195901587.2730 570.0000
[2019-04-27 19:30:43,874] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 925000, evaluation results [925000.0, 8097.7758543582995, 2445650767.011225, 740.0, 8766.945479333901, 2170970372.748868, 493.0, 8919.931510305174, 2120918467.82776, 431.0, 8580.910466030075, 2249102015.090965, 549.0, 8697.118692676946, 2195901587.2729635, 570.0]
[2019-04-27 19:30:48,978] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.1451321e-21 1.6252901e-06 4.3798324e-16 5.7430931e-18 9.9999833e-01], sum to 1.0000
[2019-04-27 19:30:48,986] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9228
[2019-04-27 19:30:48,993] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.33333333333333, 85.66666666666667, 1.0, 2.0, 0.4972776555899487, 1.0, 2.0, 0.4972776555899487, 1.0, 2.0, 0.7916822229017949, 6.9112, 6.9112, 121.94756008, 1701250.44343775, 1701250.44343775, 342254.8827049591], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2972400.0000, 
sim time next is 2973000.0000, 
raw observation next is [27.66666666666667, 84.83333333333333, 1.0, 2.0, 0.507567017276081, 1.0, 2.0, 0.507567017276081, 1.0, 2.0, 0.8080632218072331, 6.9112, 6.9112, 121.94756008, 1736485.87286958, 1736485.87286958, 347318.5535922207], 
processed observation next is [1.0, 0.391304347826087, 0.580246913580247, 0.8483333333333333, 1.0, 1.0, 0.4137702586620012, 1.0, 1.0, 0.4137702586620012, 1.0, 1.0, 0.7600790272590413, 0.0, 0.0, 0.8096049824067558, 0.62017352602485, 0.62017352602485, 0.6679202953696552], 
reward next is 0.3321, 
noisyNet noise sample is [array([0.56083745], dtype=float32), 1.8101022]. 
=============================================
[2019-04-27 19:30:49,012] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[59.501892]
 [59.052967]
 [58.319565]
 [57.81264 ]
 [57.27929 ]], R is [[59.98396683]
 [59.72594452]
 [59.4715538 ]
 [59.24354935]
 [59.04528427]].
[2019-04-27 19:30:51,724] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.3549703e-29 1.0000000e+00 1.2985245e-26 3.6585140e-32 8.7307100e-16], sum to 1.0000
[2019-04-27 19:30:51,732] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6819
[2019-04-27 19:30:51,737] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.93333333333333, 97.33333333333334, 1.0, 2.0, 0.6317090091366381, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 722920.0392548885, 722920.0392548881, 163766.5169099507], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3019200.0000, 
sim time next is 3019800.0000, 
raw observation next is [23.91666666666667, 96.66666666666666, 1.0, 2.0, 0.6232751822900258, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 714885.505488679, 714885.5054886786, 162353.4496920242], 
processed observation next is [1.0, 0.9565217391304348, 0.4413580246913582, 0.9666666666666666, 1.0, 1.0, 0.5515180741547926, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2553162519602425, 0.2553162519602423, 0.31221817248466194], 
reward next is 0.6878, 
noisyNet noise sample is [array([-1.7093735], dtype=float32), -1.0156071]. 
=============================================
[2019-04-27 19:30:51,885] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4005240e-28 1.0000000e+00 6.1225047e-27 8.8083835e-32 3.8996137e-16], sum to 1.0000
[2019-04-27 19:30:51,895] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8352
[2019-04-27 19:30:51,902] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 100.0, 1.0, 2.0, 0.5863049155116016, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 680169.8014152587, 680169.8014152582, 156311.6548578711], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3024000.0000, 
sim time next is 3024600.0000, 
raw observation next is [23.16666666666667, 99.00000000000001, 1.0, 2.0, 0.5872522156250599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 681053.1782313308, 681053.1782313308, 156463.7602176274], 
processed observation next is [1.0, 0.0, 0.4135802469135804, 0.9900000000000001, 1.0, 1.0, 0.5086335900298332, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.24323327793976102, 0.24323327793976102, 0.3008918465723604], 
reward next is 0.6991, 
noisyNet noise sample is [array([-0.7942477], dtype=float32), 0.4192791]. 
=============================================
[2019-04-27 19:31:05,921] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5601873e-36 1.0000000e+00 9.5710871e-38 0.0000000e+00 3.9839248e-29], sum to 1.0000
[2019-04-27 19:31:05,927] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5561
[2019-04-27 19:31:05,931] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 93.33333333333334, 1.0, 2.0, 0.4847058664667073, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 584532.0905157172, 584532.0905157176, 140737.8677213524], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3300000.0000, 
sim time next is 3300600.0000, 
raw observation next is [21.75, 93.16666666666666, 1.0, 2.0, 0.4810126132292717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 580779.0849603487, 580779.0849603487, 140190.0692645859], 
processed observation next is [0.0, 0.17391304347826086, 0.3611111111111111, 0.9316666666666665, 1.0, 1.0, 0.3821578728919901, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2074211017715531, 0.2074211017715531, 0.2695962870472806], 
reward next is 0.7304, 
noisyNet noise sample is [array([-0.29774854], dtype=float32), 0.851786]. 
=============================================
[2019-04-27 19:31:15,369] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.2640557e-16 6.8809925e-07 3.5750171e-13 6.7661309e-15 9.9999928e-01], sum to 1.0000
[2019-04-27 19:31:15,374] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8011
[2019-04-27 19:31:15,380] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.5, 72.33333333333333, 1.0, 2.0, 0.4835394614781903, 1.0, 2.0, 0.4835394614781903, 1.0, 2.0, 0.7698105704541295, 6.911200000000001, 6.9112, 121.94756008, 1654206.817609132, 1654206.817609131, 335583.4901055252], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3498000.0000, 
sim time next is 3498600.0000, 
raw observation next is [28.75, 71.16666666666667, 1.0, 2.0, 0.4901963486680466, 1.0, 2.0, 0.4901963486680466, 1.0, 2.0, 0.780408551660061, 6.9112, 6.9112, 121.94756008, 1677001.652082293, 1677001.652082293, 338803.3429668517], 
processed observation next is [1.0, 0.4782608695652174, 0.6203703703703703, 0.7116666666666667, 1.0, 1.0, 0.39309089127148406, 1.0, 1.0, 0.39309089127148406, 1.0, 1.0, 0.7255106895750763, 0.0, 0.0, 0.8096049824067558, 0.5989291614579618, 0.5989291614579618, 0.6515448903208687], 
reward next is 0.3485, 
noisyNet noise sample is [array([-0.22138813], dtype=float32), 0.058857623]. 
=============================================
[2019-04-27 19:31:16,613] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4114284e-27 1.0000000e+00 2.4102414e-29 3.3358089e-31 3.3021086e-22], sum to 1.0000
[2019-04-27 19:31:16,619] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2102
[2019-04-27 19:31:16,627] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.16666666666666, 78.16666666666667, 1.0, 2.0, 0.4587310381878363, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 558442.2371080577, 558442.2371080577, 136938.0111734343], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3559800.0000, 
sim time next is 3560400.0000, 
raw observation next is [23.2, 76.0, 1.0, 2.0, 0.4516783102535216, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 552176.2378203205, 552176.2378203205, 135946.6862769811], 
processed observation next is [1.0, 0.21739130434782608, 0.4148148148148148, 0.76, 1.0, 1.0, 0.34723608363514485, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.19720579922154305, 0.19720579922154305, 0.2614359351480406], 
reward next is 0.7386, 
noisyNet noise sample is [array([-0.51759297], dtype=float32), 0.5410094]. 
=============================================
[2019-04-27 19:31:22,162] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.2015705e-27 9.9999893e-01 4.9777748e-23 2.6347221e-26 1.1214385e-06], sum to 1.0000
[2019-04-27 19:31:22,170] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1345
[2019-04-27 19:31:22,172] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 85.0, 1.0, 2.0, 0.5273703891640581, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 624215.6525176283, 624215.6525176278, 147078.2825135924], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3625800.0000, 
sim time next is 3626400.0000, 
raw observation next is [23.8, 88.0, 1.0, 2.0, 0.5334410746545954, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 628957.2427147543, 628957.2427147538, 147964.3330091009], 
processed observation next is [1.0, 1.0, 0.43703703703703706, 0.88, 1.0, 1.0, 0.44457270792213743, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.22462758668384084, 0.22462758668384067, 0.28454679424827095], 
reward next is 0.7155, 
noisyNet noise sample is [array([-1.6144155], dtype=float32), 0.13985337]. 
=============================================
[2019-04-27 19:31:23,830] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.8245716e-19 1.0792186e-02 7.9896695e-16 1.1718470e-18 9.8920780e-01], sum to 1.0000
[2019-04-27 19:31:23,838] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6884
[2019-04-27 19:31:23,843] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.3776788861197432, 1.0, 1.0, 0.3776788861197432, 1.0, 1.0, 0.6015751428399413, 6.9112, 6.9112, 121.94756008, 1301058.919218612, 1301058.919218612, 287492.2672365794], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3662400.0000, 
sim time next is 3663000.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.3946071529738662, 1.0, 2.0, 0.3946071529738662, 1.0, 2.0, 0.6283270104375855, 6.911199999999999, 6.9112, 121.94756008, 1353741.303198354, 1353741.303198354, 294754.6121490371], 
processed observation next is [1.0, 0.391304347826087, 0.37037037037037035, 1.0, 1.0, 1.0, 0.27929422973079315, 1.0, 1.0, 0.27929422973079315, 1.0, 1.0, 0.5354087630469819, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.483479036856555, 0.483479036856555, 0.5668357925943021], 
reward next is 0.4332, 
noisyNet noise sample is [array([1.2792526], dtype=float32), 0.42856985]. 
=============================================
[2019-04-27 19:31:23,855] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[57.291836]
 [58.223927]
 [59.84431 ]
 [59.90166 ]
 [60.399147]], R is [[57.06667709]
 [56.94314194]
 [56.37371063]
 [56.39159393]
 [56.22556686]].
[2019-04-27 19:31:30,843] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0600917e-21 9.9996698e-01 2.0248540e-18 6.4323608e-21 3.3065407e-05], sum to 1.0000
[2019-04-27 19:31:30,852] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0356
[2019-04-27 19:31:30,859] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.25, 68.0, 1.0, 2.0, 0.6560465527967784, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 747683.049522394, 747683.049522394, 167990.3211172327], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3796200.0000, 
sim time next is 3796800.0000, 
raw observation next is [29.0, 71.0, 1.0, 2.0, 0.6793277239709042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 774229.5316514578, 774229.5316514578, 172267.9735396351], 
processed observation next is [1.0, 0.9565217391304348, 0.6296296296296297, 0.71, 1.0, 1.0, 0.6182472904415527, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2765105470183778, 0.2765105470183778, 0.3312845644992983], 
reward next is 0.6687, 
noisyNet noise sample is [array([0.84236956], dtype=float32), 1.3494363]. 
=============================================
[2019-04-27 19:31:31,621] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-04-27 19:31:31,623] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 19:31:31,623] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:31:31,624] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 19:31:31,624] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 19:31:31,624] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:31:31,625] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:31:31,625] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 19:31:31,626] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 19:31:31,627] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:31:31,629] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:31:31,645] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run39
[2019-04-27 19:31:31,645] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run39
[2019-04-27 19:31:31,695] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run39
[2019-04-27 19:31:31,718] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run39
[2019-04-27 19:31:31,742] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run39
[2019-04-27 19:31:42,002] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05626551], dtype=float32), 0.02661073]
[2019-04-27 19:31:42,003] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [24.10392647, 61.93319336, 1.0, 2.0, 0.4178875762691012, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 519381.9815578769, 519381.9815578769, 131201.9887787303]
[2019-04-27 19:31:42,004] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:31:42,008] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [2.688318e-35 1.000000e+00 5.072518e-36 0.000000e+00 6.010616e-27], sampled 0.054360680549703666
[2019-04-27 19:31:51,134] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05626551], dtype=float32), 0.02661073]
[2019-04-27 19:31:51,135] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [27.0, 44.66666666666666, 1.0, 2.0, 0.3558368078723035, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 445789.6407316785, 445789.6407316785, 122667.2471067104]
[2019-04-27 19:31:51,137] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:31:51,139] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [3.3685345e-37 1.0000000e+00 5.0516961e-38 0.0000000e+00 7.3844668e-29], sampled 0.1639977179327552
[2019-04-27 19:31:55,220] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05626551], dtype=float32), 0.02661073]
[2019-04-27 19:31:55,222] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [33.3, 31.0, 1.0, 2.0, 0.5719334583483427, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 696207.7017757704, 696207.7017757704, 155132.8353487634]
[2019-04-27 19:31:55,224] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:31:55,227] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [6.5725639e-30 1.0000000e+00 4.6171542e-28 1.8360629e-31 3.7014031e-15], sampled 0.8667601338710474
[2019-04-27 19:32:12,624] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05626551], dtype=float32), 0.02661073]
[2019-04-27 19:32:12,625] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [30.0, 61.66666666666667, 1.0, 2.0, 0.6350683002103553, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 723763.2702647252, 723763.2702647252, 164215.4356376504]
[2019-04-27 19:32:12,626] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 19:32:12,631] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [5.6236996e-30 1.0000000e+00 1.0421318e-28 9.4472048e-32 1.9877818e-15], sampled 0.513721693474036
[2019-04-27 19:32:35,952] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05626551], dtype=float32), 0.02661073]
[2019-04-27 19:32:35,954] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [28.5, 79.0, 1.0, 2.0, 0.7320266982027761, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 834323.2200308705, 834323.2200308705, 182299.160723341]
[2019-04-27 19:32:35,956] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:32:35,958] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [2.5053725e-31 1.0000000e+00 4.7361717e-30 1.6213816e-33 5.0814148e-18], sampled 0.18711713026976673
[2019-04-27 19:32:51,514] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05626551], dtype=float32), 0.02661073]
[2019-04-27 19:32:51,515] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [25.415001165, 104.28859365, 1.0, 2.0, 0.7129045342565453, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 812517.2915501082, 812517.2915501082, 178604.6858114393]
[2019-04-27 19:32:51,516] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:32:51,518] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [7.3167555e-35 1.0000000e+00 2.6947456e-35 1.6610499e-38 1.4355337e-25], sampled 0.2368146016874345
[2019-04-27 19:32:56,792] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05626551], dtype=float32), 0.02661073]
[2019-04-27 19:32:56,795] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [29.08333333333334, 69.83333333333333, 1.0, 2.0, 0.891419907419192, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 1016111.150495412, 1016111.150495411, 215549.8381285023]
[2019-04-27 19:32:56,798] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:32:56,801] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [8.4521899e-31 1.0000000e+00 3.0053535e-30 2.8808705e-33 2.2204381e-19], sampled 0.14800858976713427
[2019-04-27 19:33:22,969] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8799.2828 2173468491.5678 433.0000
[2019-04-27 19:33:24,268] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8628.7518 2252379099.3756 437.0000
[2019-04-27 19:33:24,519] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8176.3044 2453210333.8772 515.0000
[2019-04-27 19:33:24,548] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8931.3852 2123252233.5683 401.0000
[2019-04-27 19:33:24,609] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8740.0425 2200968814.3588 478.0000
[2019-04-27 19:33:25,625] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 950000, evaluation results [950000.0, 8176.304390488527, 2453210333.8771605, 515.0, 8799.282754187321, 2173468491.567847, 433.0, 8931.385195871657, 2123252233.568268, 401.0, 8628.751840683655, 2252379099.37563, 437.0, 8740.042457914875, 2200968814.3588243, 478.0]
[2019-04-27 19:33:25,628] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.5567840e-32 1.0000000e+00 1.0532894e-30 1.9101693e-34 1.3772598e-21], sum to 1.0000
[2019-04-27 19:33:25,629] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7490
[2019-04-27 19:33:25,642] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.9, 56.66666666666667, 1.0, 2.0, 0.7121213315838378, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 811624.1808841641, 811624.1808841637, 178456.2235433882], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3844200.0000, 
sim time next is 3844800.0000, 
raw observation next is [33.0, 59.0, 1.0, 2.0, 0.7447858466969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 848873.4393030676, 848873.4393030676, 184803.1683238671], 
processed observation next is [0.0, 0.5217391304347826, 0.7777777777777778, 0.59, 1.0, 1.0, 0.696173627020119, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3031690854653813, 0.3031690854653813, 0.35539070831512903], 
reward next is 0.6446, 
noisyNet noise sample is [array([1.4344704], dtype=float32), -0.23825997]. 
=============================================
[2019-04-27 19:33:30,519] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.8304703e-34 1.0000000e+00 4.9864401e-33 2.9957295e-36 1.2706643e-23], sum to 1.0000
[2019-04-27 19:33:30,529] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7482
[2019-04-27 19:33:30,531] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.14419755e-32 1.00000000e+00 5.74624239e-31 1.40928126e-35
 1.46181393e-20], sum to 1.0000
[2019-04-27 19:33:30,534] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.4, 95.33333333333334, 1.0, 2.0, 0.7137214916315797, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 813448.8948747708, 813448.8948747708, 178757.7593073488], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3904800.0000, 
sim time next is 3905400.0000, 
raw observation next is [25.25, 95.66666666666666, 1.0, 2.0, 0.7087103723454543, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 807734.5688099293, 807734.5688099293, 177798.9833638772], 
processed observation next is [0.0, 0.17391304347826086, 0.49074074074074076, 0.9566666666666666, 1.0, 1.0, 0.6532266337445884, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2884766317178319, 0.2884766317178319, 0.34192112185361], 
reward next is 0.6581, 
noisyNet noise sample is [array([-0.10839229], dtype=float32), 0.16023606]. 
=============================================
[2019-04-27 19:33:30,539] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1146
[2019-04-27 19:33:30,544] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.53333333333333, 90.66666666666666, 1.0, 2.0, 0.4530378715168578, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 551902.0393278743, 551902.0393278743, 136094.7042626833], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4128000.0000, 
sim time next is 4128600.0000, 
raw observation next is [21.41666666666666, 91.33333333333334, 1.0, 2.0, 0.4512580291794585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 550032.176836872, 550032.176836872, 135837.2553001434], 
processed observation next is [1.0, 0.782608695652174, 0.3487654320987652, 0.9133333333333334, 1.0, 1.0, 0.34673574902316495, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.19644006315602572, 0.19644006315602572, 0.26122549096181424], 
reward next is 0.7388, 
noisyNet noise sample is [array([-1.3456129], dtype=float32), 0.1633026]. 
=============================================
[2019-04-27 19:33:32,765] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7959560e-29 1.0000000e+00 2.1316097e-31 2.2365828e-33 3.5429348e-19], sum to 1.0000
[2019-04-27 19:33:32,774] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6201
[2019-04-27 19:33:32,781] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.8132753416285047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 926981.8970524683, 926981.8970524683, 198705.8855138547], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3945000.0000, 
sim time next is 3945600.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.8147435318747369, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 928656.3734865106, 928656.3734865101, 199012.7886979493], 
processed observation next is [0.0, 0.6956521739130435, 0.7037037037037037, 0.7, 1.0, 1.0, 0.779456585565163, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.33166299053089665, 0.3316629905308965, 0.38271690134221015], 
reward next is 0.6173, 
noisyNet noise sample is [array([0.6471902], dtype=float32), 1.3094382]. 
=============================================
[2019-04-27 19:33:34,573] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8338926e-24 1.0000000e+00 1.0867775e-24 6.7719265e-28 1.5438074e-19], sum to 1.0000
[2019-04-27 19:33:34,575] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1729
[2019-04-27 19:33:34,580] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.06666666666666, 90.33333333333334, 1.0, 2.0, 0.9789674646407135, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.015080254160394, 6.9112, 121.9256174277778, 1169212.013267027, 1116016.223933372, 235684.6229513599], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3987600.0000, 
sim time next is 3988200.0000, 
raw observation next is [25.03333333333333, 90.66666666666667, 1.0, 2.0, 0.9987660016817451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.137057535118476, 6.9112, 121.9249765483912, 1254307.849370747, 1138649.615745837, 240423.0820969787], 
processed observation next is [1.0, 0.13043478260869565, 0.482716049382716, 0.9066666666666667, 1.0, 1.0, 0.9985309543830299, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.022585753511847616, 0.0, 0.8094550512420909, 0.4479670890609811, 0.4066605770520847, 0.4623520809557283], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.74299866], dtype=float32), -1.3386483]. 
=============================================
[2019-04-27 19:33:37,381] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.8846295e-31 1.0000000e+00 2.7107136e-29 6.4490449e-33 7.5129718e-17], sum to 1.0000
[2019-04-27 19:33:37,391] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4827
[2019-04-27 19:33:37,398] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.6498676266762586, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 740637.6497661115, 740637.6497661115, 166871.5419484665], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4039200.0000, 
sim time next is 4039800.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.6578733540462628, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 749766.0363493364, 749766.0363493364, 168322.8979557669], 
processed observation next is [1.0, 0.782608695652174, 0.5185185185185185, 0.89, 1.0, 1.0, 0.5927063738645986, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.26777358441047727, 0.26777358441047727, 0.3236978806841671], 
reward next is 0.6763, 
noisyNet noise sample is [array([0.31315953], dtype=float32), -0.92389977]. 
=============================================
[2019-04-27 19:33:44,157] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.7099235e-19 9.9970371e-01 4.7692964e-17 1.0640348e-19 2.9626992e-04], sum to 1.0000
[2019-04-27 19:33:44,167] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0984
[2019-04-27 19:33:44,176] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1467982.23793755 W.
[2019-04-27 19:33:44,182] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.9, 75.0, 1.0, 2.0, 0.4291535836730998, 1.0, 1.0, 0.4291535836730998, 1.0, 1.0, 0.683226481764041, 6.9112, 6.9112, 121.94756008, 1467982.23793755, 1467982.23793755, 310088.4043750617], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4177800.0000, 
sim time next is 4178400.0000, 
raw observation next is [26.26666666666667, 74.66666666666666, 1.0, 2.0, 0.7849108186367729, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9956491300543112, 6.911199999999999, 6.9112, 121.9260425293398, 1611731.315364786, 1611731.315364786, 332915.0498768575], 
processed observation next is [1.0, 0.34782608695652173, 0.5283950617283951, 0.7466666666666666, 1.0, 1.0, 0.743941450758063, 0.0, 0.5, -0.1904761904761905, 1.0, 1.0, 0.9945614125678889, -8.881784197001253e-17, 0.0, 0.8094621282470476, 0.575618326915995, 0.575618326915995, 0.6402212497631875], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.6709248], dtype=float32), 0.021790393]. 
=============================================
[2019-04-27 19:33:45,028] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.4152243e-27 2.5007635e-10 9.8492641e-20 6.3337054e-24 1.0000000e+00], sum to 1.0000
[2019-04-27 19:33:45,035] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8291
[2019-04-27 19:33:45,039] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.26666666666667, 48.33333333333333, 1.0, 2.0, 0.5174253821172681, 1.0, 2.0, 0.5174253821172681, 1.0, 2.0, 0.823758059698931, 6.911200000000001, 6.9112, 121.94756008, 1770246.677674014, 1770246.677674013, 352223.953429078], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4189200.0000, 
sim time next is 4189800.0000, 
raw observation next is [31.63333333333333, 47.16666666666667, 1.0, 2.0, 0.5281013996964082, 1.0, 2.0, 0.5281013996964082, 1.0, 2.0, 0.840754627378541, 6.9112, 6.9112, 121.94756008, 1806809.042421052, 1806809.042421052, 357595.5804317477], 
processed observation next is [1.0, 0.4782608695652174, 0.7271604938271603, 0.47166666666666673, 1.0, 1.0, 0.4382159520195336, 1.0, 1.0, 0.4382159520195336, 1.0, 1.0, 0.8009432842231762, 0.0, 0.0, 0.8096049824067558, 0.6452889437218042, 0.6452889437218042, 0.6876838085225918], 
reward next is 0.3123, 
noisyNet noise sample is [array([0.6585832], dtype=float32), 0.6276706]. 
=============================================
[2019-04-27 19:33:45,794] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0061185e-21 5.6070428e-07 1.0978657e-15 2.5142232e-19 9.9999940e-01], sum to 1.0000
[2019-04-27 19:33:45,805] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7433
[2019-04-27 19:33:45,810] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.16666666666667, 29.66666666666666, 1.0, 2.0, 0.4710884850747521, 1.0, 2.0, 0.4710884850747521, 1.0, 2.0, 0.7517676325713074, 6.911199999999999, 6.9112, 121.94756008, 1643739.955865728, 1643739.955865728, 329729.0597521876], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4200000.0000, 
sim time next is 4200600.0000, 
raw observation next is [34.13333333333333, 30.33333333333334, 1.0, 2.0, 0.4850453268354784, 1.0, 2.0, 0.4850453268354784, 1.0, 2.0, 0.7734518004117942, 6.9112, 6.9112, 121.94756008, 1685546.152402918, 1685546.152402918, 336422.3641960257], 
processed observation next is [1.0, 0.6086956521739131, 0.8197530864197531, 0.3033333333333334, 1.0, 1.0, 0.3869587224231886, 1.0, 1.0, 0.3869587224231886, 1.0, 1.0, 0.7168147505147426, 0.0, 0.0, 0.8096049824067558, 0.6019807687153279, 0.6019807687153279, 0.6469660849923571], 
reward next is 0.3530, 
noisyNet noise sample is [array([1.014538], dtype=float32), -1.7793391]. 
=============================================
[2019-04-27 19:33:57,321] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.3887415e-28 1.0000000e+00 3.6244920e-26 6.6991291e-30 4.6881979e-14], sum to 1.0000
[2019-04-27 19:33:57,328] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3680
[2019-04-27 19:33:57,332] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.7202465173565736, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000002, 6.9112, 121.9260426156618, 820889.6366378198, 820889.6366378189, 180015.8593238865], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4460400.0000, 
sim time next is 4461000.0000, 
raw observation next is [30.0, 70.0, 1.0, 2.0, 0.7153051986637711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 815254.8512396502, 815254.8512396502, 179065.3487878282], 
processed observation next is [0.0, 0.6521739130434783, 0.6666666666666666, 0.7, 1.0, 1.0, 0.6610776174568703, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2911624468713036, 0.2911624468713036, 0.3443564399765927], 
reward next is 0.6556, 
noisyNet noise sample is [array([-1.3383398], dtype=float32), 1.1208982]. 
=============================================
[2019-04-27 19:33:57,356] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[63.20397 ]
 [63.24858 ]
 [63.30055 ]
 [63.351315]
 [63.395718]], R is [[63.22203064]
 [63.24362564]
 [63.27044296]
 [63.30223083]
 [63.33896255]].
[2019-04-27 19:33:59,665] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.5096620e-27 1.0000000e+00 4.1232157e-23 3.6154688e-29 1.9269222e-11], sum to 1.0000
[2019-04-27 19:33:59,675] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0069
[2019-04-27 19:33:59,680] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.03333333333333, 83.66666666666666, 1.0, 2.0, 0.7065642282954179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 805287.2709330095, 805287.2709330095, 177390.4616102606], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4481400.0000, 
sim time next is 4482000.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.7077329746065462, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 806620.0185117348, 806620.0185117348, 177613.2096852566], 
processed observation next is [0.0, 0.9130434782608695, 0.5555555555555556, 0.84, 1.0, 1.0, 0.652063065007793, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2880785780399053, 0.2880785780399053, 0.3415638647793396], 
reward next is 0.6584, 
noisyNet noise sample is [array([0.2866114], dtype=float32), 1.5967411]. 
=============================================
[2019-04-27 19:33:59,698] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[62.222725]
 [62.257927]
 [62.291508]
 [62.318523]
 [62.378376]], R is [[62.22356415]
 [62.26019287]
 [62.2966423 ]
 [62.33261108]
 [62.36815262]].
[2019-04-27 19:34:00,522] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.5100797e-34 1.0000000e+00 4.2298568e-32 3.6845291e-36 4.0937148e-18], sum to 1.0000
[2019-04-27 19:34:00,531] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6456
[2019-04-27 19:34:00,536] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.33333333333334, 94.0, 1.0, 2.0, 0.6395833491530266, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 731993.8838087041, 731993.8838087041, 165174.2496656236], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4495200.0000, 
sim time next is 4495800.0000, 
raw observation next is [24.16666666666666, 94.0, 1.0, 2.0, 0.6288931645902913, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 722440.5611392765, 722440.5611392765, 163401.3491077898], 
processed observation next is [0.0, 0.0, 0.45061728395061706, 0.94, 1.0, 1.0, 0.5582061483217754, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2580144861211702, 0.2580144861211702, 0.31423336366882654], 
reward next is 0.6858, 
noisyNet noise sample is [array([-0.37026292], dtype=float32), -0.554792]. 
=============================================
[2019-04-27 19:34:01,460] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.3890623e-34 1.0000000e+00 1.5623449e-34 2.5142689e-37 3.3061185e-21], sum to 1.0000
[2019-04-27 19:34:01,470] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4510
[2019-04-27 19:34:01,477] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.3, 91.0, 1.0, 2.0, 0.5488769193708751, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 647231.3156147987, 647231.3156147987, 150494.5620277276], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4510800.0000, 
sim time next is 4511400.0000, 
raw observation next is [23.25, 92.5, 1.0, 2.0, 0.5501047569572993, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 647856.7210228491, 647856.7210228491, 150664.1431056346], 
processed observation next is [0.0, 0.21739130434782608, 0.4166666666666667, 0.925, 1.0, 1.0, 0.46441042494916585, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.23137740036530324, 0.23137740036530324, 0.289738736741605], 
reward next is 0.7103, 
noisyNet noise sample is [array([-0.0040646], dtype=float32), -0.8521923]. 
=============================================
[2019-04-27 19:34:04,560] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.9299290e-21 5.5145174e-06 2.8378916e-17 4.1370954e-20 9.9999452e-01], sum to 1.0000
[2019-04-27 19:34:04,566] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3059
[2019-04-27 19:34:04,572] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.98333333333333, 94.0, 1.0, 2.0, 0.3486081691063527, 1.0, 2.0, 0.3486081691063527, 1.0, 2.0, 0.554995558592761, 6.9112, 6.9112, 121.94756008, 1192250.542324491, 1192250.542324491, 275302.1976125505], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4787400.0000, 
sim time next is 4788000.0000, 
raw observation next is [24.0, 94.0, 1.0, 2.0, 0.3666490406147352, 1.0, 2.0, 0.3666490406147352, 1.0, 2.0, 0.583717213584272, 6.911200000000001, 6.9112, 121.94756008, 1254001.363157182, 1254001.363157182, 282780.9619380271], 
processed observation next is [1.0, 0.43478260869565216, 0.4444444444444444, 0.94, 1.0, 1.0, 0.24601076263658953, 1.0, 1.0, 0.24601076263658953, 1.0, 1.0, 0.47964651698033994, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.44785762969899356, 0.44785762969899356, 0.5438095421885137], 
reward next is 0.4562, 
noisyNet noise sample is [array([-0.53525096], dtype=float32), 1.238113]. 
=============================================
[2019-04-27 19:34:04,593] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[60.985016]
 [60.924393]
 [60.808266]
 [60.51598 ]
 [60.443115]], R is [[60.74075317]
 [60.60391998]
 [60.46930695]
 [60.32382584]
 [60.16090012]].
[2019-04-27 19:34:08,863] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.2468245e-28 1.0000000e+00 3.1174846e-25 1.8832230e-29 3.0459985e-11], sum to 1.0000
[2019-04-27 19:34:08,870] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8315
[2019-04-27 19:34:08,874] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.66666666666667, 82.33333333333334, 1.0, 2.0, 0.6717368064195233, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 765573.8455898293, 765573.8455898293, 170861.0484092745], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4650600.0000, 
sim time next is 4651200.0000, 
raw observation next is [26.6, 82.0, 1.0, 2.0, 0.6670791849859876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 760262.9538084806, 760262.9538084806, 170003.581616455], 
processed observation next is [1.0, 0.8695652173913043, 0.5407407407407407, 0.82, 1.0, 1.0, 0.60366569641189, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2715224835030288, 0.2715224835030288, 0.32692996464702884], 
reward next is 0.6731, 
noisyNet noise sample is [array([1.3470018], dtype=float32), -0.96279866]. 
=============================================
[2019-04-27 19:34:09,599] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.1881322e-22 9.9996543e-01 5.8467911e-20 1.7254862e-23 3.4616060e-05], sum to 1.0000
[2019-04-27 19:34:09,605] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1827
[2019-04-27 19:34:09,610] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.5, 89.83333333333334, 1.0, 2.0, 0.6689259112879898, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 762368.6945120457, 762368.6945120452, 170343.0084551845], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4662600.0000, 
sim time next is 4663200.0000, 
raw observation next is [25.4, 90.66666666666667, 1.0, 2.0, 0.6661350936927317, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 759186.4503031863, 759186.4503031863, 169830.5236998207], 
processed observation next is [1.0, 1.0, 0.49629629629629624, 0.9066666666666667, 1.0, 1.0, 0.6025417782056329, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2711380179654237, 0.2711380179654237, 0.3265971609611937], 
reward next is 0.6734, 
noisyNet noise sample is [array([0.95338714], dtype=float32), 1.3611808]. 
=============================================
[2019-04-27 19:34:10,031] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.6802209e-29 1.0000000e+00 9.6350606e-28 1.9090576e-31 1.3874617e-16], sum to 1.0000
[2019-04-27 19:34:10,039] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2017
[2019-04-27 19:34:10,045] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.2, 94.66666666666667, 1.0, 2.0, 0.8931513664360335, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.925881215065, 1021878.542009406, 1021878.542009406, 216140.8153070965], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4674000.0000, 
sim time next is 4674600.0000, 
raw observation next is [23.9, 94.5, 1.0, 2.0, 0.7307740568547058, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260425664464, 842156.4405466687, 842156.4405466687, 182518.2293335609], 
processed observation next is [1.0, 0.08695652173913043, 0.4407407407407407, 0.945, 1.0, 1.0, 0.6794929248270306, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621284933968, 0.300770157338096, 0.300770157338096, 0.3509965948722325], 
reward next is 0.6490, 
noisyNet noise sample is [array([0.5723661], dtype=float32), 0.49063653]. 
=============================================
[2019-04-27 19:34:12,863] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5230406e-28 1.0000000e+00 7.1205608e-29 2.9707806e-31 1.1030128e-15], sum to 1.0000
[2019-04-27 19:34:12,872] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0364
[2019-04-27 19:34:12,876] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.03333333333333, 93.66666666666667, 1.0, 2.0, 0.6885567323147725, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 794751.7437175903, 794751.7437175903, 174484.7017349176], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4762200.0000, 
sim time next is 4762800.0000, 
raw observation next is [24.0, 94.0, 1.0, 2.0, 0.6850679220476161, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 790593.1290413323, 790593.1290413323, 173822.8758559487], 
processed observation next is [1.0, 0.13043478260869565, 0.4444444444444444, 0.94, 1.0, 1.0, 0.6250808595804954, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.28235468894333293, 0.28235468894333293, 0.3342747612614398], 
reward next is 0.6657, 
noisyNet noise sample is [array([-0.43127385], dtype=float32), 0.9564134]. 
=============================================
[2019-04-27 19:34:13,515] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-27 19:34:13,517] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 19:34:13,517] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:34:13,518] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 19:34:13,518] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 19:34:13,518] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 19:34:13,519] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:34:13,519] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 19:34:13,521] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:34:13,520] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:34:13,524] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:34:13,538] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run40
[2019-04-27 19:34:13,538] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run40
[2019-04-27 19:34:13,573] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run40
[2019-04-27 19:34:13,592] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run40
[2019-04-27 19:34:13,617] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run40
[2019-04-27 19:34:53,842] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05413865], dtype=float32), 0.024769925]
[2019-04-27 19:34:53,846] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [28.66666666666667, 70.0, 1.0, 2.0, 0.647870844082316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 738360.8712802774, 738360.8712802774, 166509.8861774889]
[2019-04-27 19:34:53,849] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 19:34:53,852] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.1228949e-29 1.0000000e+00 8.2997017e-28 3.1280412e-31 7.4880540e-16], sampled 0.7011093012909996
[2019-04-27 19:35:20,527] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05413865], dtype=float32), 0.024769925]
[2019-04-27 19:35:20,529] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [22.657478715, 78.29263190500001, 1.0, 2.0, 0.4026505144607417, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 495947.8165154997, 495947.8165154997, 128923.6654559948]
[2019-04-27 19:35:20,532] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:35:20,534] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [3.9891786e-33 1.0000000e+00 1.0741363e-32 2.6917750e-36 7.4087267e-23], sampled 0.19077168375074205
[2019-04-27 19:35:27,677] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05413865], dtype=float32), 0.024769925]
[2019-04-27 19:35:27,678] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [23.2, 98.66666666666666, 1.0, 2.0, 0.7138707661991073, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 827531.2491712276, 827531.2491712276, 179476.0645774766]
[2019-04-27 19:35:27,681] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:35:27,685] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [5.2652494e-31 1.0000000e+00 8.0812735e-31 1.0900734e-33 2.8670195e-21], sampled 0.2612029798270793
[2019-04-27 19:36:05,458] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8633.5095 2252571610.3062 432.0000
[2019-04-27 19:36:05,610] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8929.6665 2123413505.4254 405.0000
[2019-04-27 19:36:05,627] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8733.7092 2201497631.2928 483.0000
[2019-04-27 19:36:05,767] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8795.2128 2173363115.9390 442.0000
[2019-04-27 19:36:05,970] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8180.0649 2453354669.4825 513.0000
[2019-04-27 19:36:06,985] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 975000, evaluation results [975000.0, 8180.064935427575, 2453354669.482452, 513.0, 8795.212753338692, 2173363115.9389505, 442.0, 8929.66648354114, 2123413505.425444, 405.0, 8633.509525252868, 2252571610.306219, 432.0, 8733.709244234406, 2201497631.2927885, 483.0]
[2019-04-27 19:36:08,208] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.5895036e-33 1.0000000e+00 2.1753108e-31 1.0826176e-34 1.9329556e-22], sum to 1.0000
[2019-04-27 19:36:08,223] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0047
[2019-04-27 19:36:08,232] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 94.0, 1.0, 2.0, 0.7820151006977825, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 891330.3442817368, 891330.3442817368, 192258.9681117187], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4816800.0000, 
sim time next is 4817400.0000, 
raw observation next is [27.0, 94.00000000000001, 1.0, 2.0, 0.7938015909375321, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 904772.3580829006, 904772.3580829006, 194669.4562564862], 
processed observation next is [1.0, 0.782608695652174, 0.5555555555555556, 0.9400000000000002, 1.0, 1.0, 0.754525703497062, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.32313298502960736, 0.32313298502960736, 0.37436433895478116], 
reward next is 0.6256, 
noisyNet noise sample is [array([0.47317976], dtype=float32), -0.79121464]. 
=============================================
[2019-04-27 19:36:11,867] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.36778922e-25 9.99999881e-01 1.75808454e-21 5.24720140e-24
 1.05476715e-07], sum to 1.0000
[2019-04-27 19:36:11,878] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6000
[2019-04-27 19:36:11,883] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.06666666666667, 99.16666666666667, 1.0, 2.0, 0.7046540058245051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 803109.0064817888, 803109.0064817888, 177027.8323441885], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4842600.0000, 
sim time next is 4843200.0000, 
raw observation next is [25.13333333333333, 98.33333333333334, 1.0, 2.0, 0.7031167994577994, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 801356.104164444, 801356.104164444, 176735.6157864665], 
processed observation next is [1.0, 0.043478260869565216, 0.4864197530864196, 0.9833333333333334, 1.0, 1.0, 0.646567618402142, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.28619860863015856, 0.28619860863015856, 0.33987618420474325], 
reward next is 0.6601, 
noisyNet noise sample is [array([0.7803442], dtype=float32), -1.0566075]. 
=============================================
[2019-04-27 19:36:12,293] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.5969715e-19 1.6855993e-05 1.4885163e-15 4.6152734e-16 9.9998319e-01], sum to 1.0000
[2019-04-27 19:36:12,300] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6793
[2019-04-27 19:36:12,306] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.06666666666667, 94.16666666666667, 1.0, 2.0, 0.2952986280443909, 1.0, 2.0, 0.2952986280443909, 1.0, 2.0, 0.4701250330515738, 6.911199999999999, 6.9112, 121.94756008, 1009810.163970135, 1009810.163970136, 254260.5803320204], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4848600.0000, 
sim time next is 4849200.0000, 
raw observation next is [25.0, 94.0, 1.0, 2.0, 0.2888552814945229, 1.0, 2.0, 0.2888552814945229, 1.0, 2.0, 0.4598670154990368, 6.911199999999999, 6.9112, 121.94756008, 987762.1452276299, 987762.1452276304, 251824.5488198617], 
processed observation next is [1.0, 0.13043478260869565, 0.48148148148148145, 0.94, 1.0, 1.0, 0.15339914463633678, 1.0, 1.0, 0.15339914463633678, 1.0, 1.0, 0.324833769373796, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.35277219472415355, 0.3527721947241537, 0.48427797849973403], 
reward next is 0.5157, 
noisyNet noise sample is [array([1.0052445], dtype=float32), 0.48051846]. 
=============================================
[2019-04-27 19:36:18,520] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.7517905e-32 1.0000000e+00 1.9471010e-34 9.6077094e-37 2.0335569e-28], sum to 1.0000
[2019-04-27 19:36:18,525] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8934
[2019-04-27 19:36:18,528] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 100.0, 1.0, 2.0, 0.5755096133797039, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 668463.7569617374, 668463.756961737, 154516.2886701329], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5011200.0000, 
sim time next is 5011800.0000, 
raw observation next is [23.0, 100.0, 1.0, 2.0, 0.5777342261666353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 670528.9499301063, 670528.9499301063, 154868.8205309012], 
processed observation next is [0.0, 0.0, 0.4074074074074074, 1.0, 1.0, 1.0, 0.49730265019837533, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.23947462497503796, 0.23947462497503796, 0.2978246548671177], 
reward next is 0.7022, 
noisyNet noise sample is [array([0.5970332], dtype=float32), 0.84309554]. 
=============================================
[2019-04-27 19:36:18,733] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.12193920e-23 1.00000000e+00 6.71375407e-25 1.10607405e-26
 4.51169845e-19], sum to 1.0000
[2019-04-27 19:36:18,739] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7388
[2019-04-27 19:36:18,748] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1405080.122777089 W.
[2019-04-27 19:36:18,751] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.6161721085381506, 1.0, 1.0, 0.6161721085381506, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9256910126495, 1405080.122777089, 1405080.12277709, 273648.6837316154], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4971600.0000, 
sim time next is 4972200.0000, 
raw observation next is [25.7, 90.0, 1.0, 2.0, 0.5885729779772448, 1.0, 2.0, 0.5885729779772448, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260425084473, 1342089.657193357, 1342089.657193358, 264134.2214352393], 
processed observation next is [1.0, 0.5652173913043478, 0.5074074074074074, 0.9, 1.0, 1.0, 0.5102059261633867, 1.0, 1.0, 0.5102059261633867, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621281083432, 0.4793177347119132, 0.4793177347119136, 0.5079504258369987], 
reward next is 0.4920, 
noisyNet noise sample is [array([-0.17447287], dtype=float32), 0.67972726]. 
=============================================
[2019-04-27 19:36:22,936] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.3659690e-35 1.0000000e+00 1.8638936e-33 6.9035626e-37 2.8893264e-23], sum to 1.0000
[2019-04-27 19:36:22,944] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1785
[2019-04-27 19:36:22,947] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.66666666666667, 85.66666666666667, 1.0, 2.0, 0.6838817278954192, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 779422.3669198233, 779422.3669198233, 173115.5118854976], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5044800.0000, 
sim time next is 5045400.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.6900441506625967, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 786449.304510588, 786449.304510588, 174268.4481445762], 
processed observation next is [0.0, 0.391304347826087, 0.5555555555555556, 0.84, 1.0, 1.0, 0.6310049412649961, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2808747516109243, 0.2808747516109243, 0.3351316310472619], 
reward next is 0.6649, 
noisyNet noise sample is [array([-0.4156512], dtype=float32), -0.12577303]. 
=============================================
[2019-04-27 19:36:37,152] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.8274875e-34 1.0000000e+00 1.6752493e-35 9.2276984e-38 1.1122178e-33], sum to 1.0000
[2019-04-27 19:36:37,158] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4236
[2019-04-27 19:36:37,163] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.23333333333334, 68.66666666666667, 1.0, 2.0, 0.6043491172497751, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 691231.0408281235, 691231.040828123, 158950.8855260064], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5336400.0000, 
sim time next is 5337000.0000, 
raw observation next is [28.2, 69.0, 1.0, 2.0, 0.609643488261585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 696915.5413460811, 696915.5413460811, 159850.5127158439], 
processed observation next is [1.0, 0.782608695652174, 0.6, 0.69, 1.0, 1.0, 0.5352898669780773, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.24889840762360038, 0.24889840762360038, 0.3074048321458537], 
reward next is 0.6926, 
noisyNet noise sample is [array([-2.0565813], dtype=float32), 0.37024775]. 
=============================================
[2019-04-27 19:36:37,174] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[46.39647 ]
 [45.829144]
 [45.12491 ]
 [44.03715 ]
 [42.115612]], R is [[47.10162735]
 [47.32493591]
 [47.54718781]
 [47.7677803 ]
 [47.98831558]].
[2019-04-27 19:36:37,410] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.6999840e-25 1.0000000e+00 9.5296044e-28 5.0423428e-29 3.4225977e-23], sum to 1.0000
[2019-04-27 19:36:37,416] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0515
[2019-04-27 19:36:37,421] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.4, 67.5, 1.0, 2.0, 0.5748088970257436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 657763.7460299103, 657763.7460299103, 153933.7905142614], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5333400.0000, 
sim time next is 5334000.0000, 
raw observation next is [28.36666666666667, 67.66666666666666, 1.0, 2.0, 0.5815999679483186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 665724.5852571533, 665724.5852571533, 155086.868884908], 
processed observation next is [1.0, 0.7391304347826086, 0.606172839506173, 0.6766666666666665, 1.0, 1.0, 0.5019047237479983, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2377587804489833, 0.2377587804489833, 0.2982439786248231], 
reward next is 0.7018, 
noisyNet noise sample is [array([-0.8633699], dtype=float32), -0.5017253]. 
=============================================
[2019-04-27 19:36:37,440] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[42.267006]
 [39.033363]
 [37.36123 ]
 [34.84186 ]
 [34.644264]], R is [[44.93757248]
 [45.19216919]
 [44.74024963]
 [44.29284668]
 [43.84991837]].
[2019-04-27 19:36:38,400] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.5577651e-22 1.0000000e+00 2.9668998e-21 5.6174160e-23 1.1425997e-12], sum to 1.0000
[2019-04-27 19:36:38,406] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3417
[2019-04-27 19:36:38,413] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1649945.408211243 W.
[2019-04-27 19:36:38,420] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.2, 78.66666666666667, 1.0, 2.0, 0.4822949623720504, 1.0, 2.0, 0.4822949623720504, 1.0, 1.0, 0.7678292873466486, 6.9112, 6.9112, 121.94756008, 1649945.408211243, 1649945.408211243, 334984.2109491321], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5570400.0000, 
sim time next is 5571000.0000, 
raw observation next is [27.4, 78.5, 1.0, 2.0, 0.7463934181994031, 1.0, 2.0, 0.7463934181994031, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1702339.535882675, 1702339.535882675, 322101.0161935107], 
processed observation next is [1.0, 0.4782608695652174, 0.5703703703703703, 0.785, 1.0, 1.0, 0.698087402618337, 1.0, 1.0, 0.698087402618337, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.6079784056723839, 0.6079784056723839, 0.6194250311413667], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.00563931], dtype=float32), -1.382466]. 
=============================================
[2019-04-27 19:36:38,434] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[45.136272]
 [45.964584]
 [46.65051 ]
 [46.514877]
 [46.19872 ]], R is [[44.88037109]
 [44.43156815]
 [44.39645386]
 [44.33588791]
 [44.3218956 ]].
[2019-04-27 19:36:40,939] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.0070633e-11 2.5148896e-04 8.4417462e-10 1.5597443e-10 9.9974853e-01], sum to 1.0000
[2019-04-27 19:36:40,945] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5143
[2019-04-27 19:36:40,948] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.0, 76.0, 1.0, 2.0, 0.551838640497281, 1.0, 2.0, 0.551838640497281, 1.0, 2.0, 0.8785450878014927, 6.9112, 6.9112, 121.94756008, 1888107.794663325, 1888107.794663325, 369759.9459241719], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5396400.0000, 
sim time next is 5397000.0000, 
raw observation next is [27.83333333333334, 77.33333333333334, 1.0, 2.0, 0.5389142092301485, 1.0, 2.0, 0.5389142092301485, 1.0, 2.0, 0.8579689723048773, 6.9112, 6.9112, 121.94756008, 1843841.406527126, 1843841.406527126, 363098.9152479119], 
processed observation next is [1.0, 0.4782608695652174, 0.58641975308642, 0.7733333333333334, 1.0, 1.0, 0.4510883443216054, 1.0, 1.0, 0.4510883443216054, 1.0, 1.0, 0.8224612153810967, 0.0, 0.0, 0.8096049824067558, 0.6585147880454021, 0.6585147880454021, 0.6982671447075228], 
reward next is 0.3017, 
noisyNet noise sample is [array([0.55065393], dtype=float32), 0.3634839]. 
=============================================
[2019-04-27 19:36:40,967] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[29.782072]
 [29.423058]
 [29.0371  ]
 [29.073158]
 [28.430578]], R is [[30.0553627 ]
 [30.04373169]
 [30.03495598]
 [30.03842735]
 [29.73804283]].
[2019-04-27 19:36:42,993] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.4250769e-32 1.0000000e+00 8.4700907e-34 4.5384345e-35 1.0080956e-25], sum to 1.0000
[2019-04-27 19:36:43,001] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9474
[2019-04-27 19:36:43,005] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.1, 85.0, 1.0, 2.0, 0.781851595967815, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 891143.8754305083, 891143.8754305083, 192224.6056568392], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5434200.0000, 
sim time next is 5434800.0000, 
raw observation next is [28.03333333333333, 85.33333333333333, 1.0, 2.0, 0.7804858316940452, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 889586.2923229545, 889586.292322954, 191946.7917800127], 
processed observation next is [1.0, 0.9130434782608695, 0.5938271604938271, 0.8533333333333333, 1.0, 1.0, 0.7386736091595776, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.3177093901153409, 0.3177093901153407, 0.3691284457307936], 
reward next is 0.6309, 
noisyNet noise sample is [array([1.7602181], dtype=float32), -0.4884082]. 
=============================================
[2019-04-27 19:36:47,588] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.1075895e-30 1.0000000e+00 1.0745648e-28 4.9443147e-33 2.4087530e-21], sum to 1.0000
[2019-04-27 19:36:47,593] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9684
[2019-04-27 19:36:47,598] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.4, 93.0, 1.0, 2.0, 0.8667590108540993, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 987982.581787345, 987982.581787345, 210116.2081275475], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5540400.0000, 
sim time next is 5541000.0000, 
raw observation next is [25.4, 93.0, 1.0, 2.0, 0.8142160512704711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 928054.7794752461, 928054.7794752456, 198891.8406937736], 
processed observation next is [1.0, 0.13043478260869565, 0.49629629629629624, 0.93, 1.0, 1.0, 0.7788286324648466, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.3314481355268736, 0.3314481355268734, 0.3824843090264877], 
reward next is 0.6175, 
noisyNet noise sample is [array([0.6001958], dtype=float32), 1.6983284]. 
=============================================
[2019-04-27 19:36:47,612] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[57.08348 ]
 [56.521877]
 [55.684586]
 [54.46729 ]
 [55.30975 ]], R is [[58.09060287]
 [58.10562897]
 [58.10889816]
 [57.52780914]
 [56.95252991]].
[2019-04-27 19:36:50,861] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.9100903e-36 1.0000000e+00 1.6549868e-35 2.3586855e-38 2.8923187e-28], sum to 1.0000
[2019-04-27 19:36:50,868] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8919
[2019-04-27 19:36:50,874] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.68333333333333, 93.16666666666667, 1.0, 2.0, 0.6975119125361281, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 794964.7911373801, 794964.7911373801, 175673.7941017581], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5602200.0000, 
sim time next is 5602800.0000, 
raw observation next is [25.56666666666667, 93.33333333333334, 1.0, 2.0, 0.6937312972677965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 790653.7448559483, 790653.7448559483, 174960.5993993781], 
processed observation next is [1.0, 0.8695652173913043, 0.5024691358024692, 0.9333333333333335, 1.0, 1.0, 0.6353944015092815, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.28237633744855295, 0.28237633744855295, 0.33646269115265015], 
reward next is 0.6635, 
noisyNet noise sample is [array([-0.13716036], dtype=float32), -0.6452265]. 
=============================================
[2019-04-27 19:36:55,343] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-27 19:36:55,344] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 19:36:55,345] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 19:36:55,347] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:36:55,347] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:36:55,349] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 19:36:55,350] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:36:55,353] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 19:36:55,353] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:36:55,354] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 19:36:55,354] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:36:55,371] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run41
[2019-04-27 19:36:55,377] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run41
[2019-04-27 19:36:55,446] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run41
[2019-04-27 19:36:55,506] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run41
[2019-04-27 19:36:55,577] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run41
[2019-04-27 19:37:10,281] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.0395604], dtype=float32), 0.022352438]
[2019-04-27 19:37:10,282] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [29.0, 26.33333333333334, 1.0, 2.0, 0.3265780782745935, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 419120.3570071484, 419120.3570071484, 118922.3121347209]
[2019-04-27 19:37:10,284] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 19:37:10,286] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.7163897e-34], sampled 0.7053240806360476
[2019-04-27 19:37:31,284] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0395604], dtype=float32), 0.022352438]
[2019-04-27 19:37:31,286] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [31.226971215, 47.850279875, 1.0, 2.0, 0.5472745387327924, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156612, 640769.2612809701, 640769.2612809701, 150039.5436458483]
[2019-04-27 19:37:31,287] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 19:37:31,291] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.5343607e-34 1.0000000e+00 4.5992801e-34 4.2274103e-37 5.6874406e-26], sampled 0.9441550171656053
[2019-04-27 19:37:35,923] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0395604], dtype=float32), 0.022352438]
[2019-04-27 19:37:35,924] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [30.29078322, 35.88812823, 1.0, 2.0, 0.4198716166696324, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 517662.8963651118, 517662.8963651118, 131392.0953518875]
[2019-04-27 19:37:35,925] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 19:37:35,928] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [4.3464694e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.9224279e-32], sampled 0.3903907515971061
[2019-04-27 19:37:57,374] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0395604], dtype=float32), 0.022352438]
[2019-04-27 19:37:57,375] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [31.15, 50.66666666666667, 1.0, 2.0, 0.5509732618648228, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 639795.6786626767, 639795.6786626762, 150417.1661172249]
[2019-04-27 19:37:57,376] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:37:57,380] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.5267091e-38 1.0000000e+00 2.5566007e-38 0.0000000e+00 6.9010855e-30], sampled 0.6509431242771192
[2019-04-27 19:38:07,962] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0395604], dtype=float32), 0.022352438]
[2019-04-27 19:38:07,964] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [21.70397205, 95.0129614, 1.0, 2.0, 0.4920817760504118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 591025.5335140678, 591025.5335140678, 141803.1408642163]
[2019-04-27 19:38:07,966] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:38:07,969] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [2.1474529e-37 1.0000000e+00 8.8933721e-38 0.0000000e+00 1.0122586e-30], sampled 0.010619016525066582
[2019-04-27 19:38:15,074] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0395604], dtype=float32), 0.022352438]
[2019-04-27 19:38:15,076] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [26.67840480666667, 70.121196445, 1.0, 2.0, 0.5777302121049896, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 674927.4459493145, 674927.445949314, 155062.9293706361]
[2019-04-27 19:38:15,078] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:38:15,080] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.0761484e-35 1.0000000e+00 1.6184756e-35 2.6462619e-38 7.9829973e-27], sampled 0.40841787655015793
[2019-04-27 19:38:48,813] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8112.9529 2446541672.2179 676.0000
[2019-04-27 19:38:49,182] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8588.9384 2249579125.5197 528.0000
[2019-04-27 19:38:49,185] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8769.3904 2171045707.4665 489.0000
[2019-04-27 19:38:49,190] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8699.5864 2196693903.6830 562.0000
[2019-04-27 19:38:49,408] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8920.5508 2121025023.8922 428.0000
[2019-04-27 19:38:50,424] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1000000, evaluation results [1000000.0, 8112.952912677882, 2446541672.2178807, 676.0, 8769.39039749133, 2171045707.4664536, 489.0, 8920.550791622534, 2121025023.8921907, 428.0, 8588.938372996648, 2249579125.519696, 528.0, 8699.586365213834, 2196693903.682955, 562.0]
[2019-04-27 19:38:52,041] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.2990269e-36], sum to 1.0000
[2019-04-27 19:38:52,050] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4219
[2019-04-27 19:38:52,064] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.1, 96.33333333333334, 1.0, 2.0, 0.4654199867372461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 564507.6823005414, 564507.6823005414, 137886.9658407118], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5719200.0000, 
sim time next is 5719800.0000, 
raw observation next is [21.05, 96.16666666666666, 1.0, 2.0, 0.4619950577188193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 561004.0369393249, 561004.0369393249, 137388.3885545093], 
processed observation next is [0.0, 0.17391304347826086, 0.3351851851851852, 0.9616666666666666, 1.0, 1.0, 0.3595179258557373, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.20035858462118747, 0.20035858462118747, 0.26420843952790246], 
reward next is 0.7358, 
noisyNet noise sample is [array([-1.3388402], dtype=float32), 0.8212386]. 
=============================================
[2019-04-27 19:39:00,774] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.6511062e-34 1.0000000e+00 7.2317331e-35 1.9347345e-36 8.5552462e-24], sum to 1.0000
[2019-04-27 19:39:00,782] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1666
[2019-04-27 19:39:00,788] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.5, 85.0, 1.0, 2.0, 0.3982850215165286, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 503273.1625400921, 503273.1625400921, 128545.4962429288], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5887200.0000, 
sim time next is 5887800.0000, 
raw observation next is [19.45, 85.0, 1.0, 2.0, 0.3823483505483523, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 483465.3454825889, 483465.3454825885, 126324.6352164814], 
processed observation next is [1.0, 0.13043478260869565, 0.2759259259259259, 0.85, 1.0, 1.0, 0.26470041731946703, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.17266619481521034, 0.17266619481521017, 0.24293199080092578], 
reward next is 0.7571, 
noisyNet noise sample is [array([-1.4403421], dtype=float32), 0.058483023]. 
=============================================
[2019-04-27 19:39:01,961] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6226596e-29 1.0000000e+00 1.6490248e-28 6.8607906e-31 1.4004431e-14], sum to 1.0000
[2019-04-27 19:39:01,969] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6845
[2019-04-27 19:39:01,976] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.6, 83.0, 1.0, 2.0, 0.4546960617917437, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 553105.8715461107, 553105.8715461107, 136318.8663338169], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5962800.0000, 
sim time next is 5963400.0000, 
raw observation next is [22.6, 82.5, 1.0, 2.0, 0.4513168465665986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 549577.7700985565, 549577.7700985565, 135830.4857293374], 
processed observation next is [1.0, 0.0, 0.39259259259259266, 0.825, 1.0, 1.0, 0.3468057697221412, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.19627777503519875, 0.19627777503519875, 0.2612124725564181], 
reward next is 0.7388, 
noisyNet noise sample is [array([-1.2667651], dtype=float32), -0.61859876]. 
=============================================
[2019-04-27 19:39:15,659] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.7271076e-14 6.5428793e-02 1.9734147e-11 1.0798383e-13 9.3457115e-01], sum to 1.0000
[2019-04-27 19:39:15,670] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2022
[2019-04-27 19:39:15,676] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.4, 67.0, 1.0, 2.0, 0.4565085846919805, 1.0, 1.0, 0.4565085846919805, 1.0, 2.0, 0.7267765342762859, 6.911200000000001, 6.9112, 121.94756008, 1561649.391927104, 1561649.391927104, 322719.9040974648], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6174000.0000, 
sim time next is 6174600.0000, 
raw observation next is [27.53333333333333, 66.33333333333334, 1.0, 2.0, 0.5299971782143098, 1.0, 2.0, 0.5299971782143098, 1.0, 2.0, 0.8437727685202363, 6.911200000000001, 6.9112, 121.94756008, 1813301.708379077, 1813301.708379076, 358555.890709327], 
processed observation next is [1.0, 0.4782608695652174, 0.5753086419753086, 0.6633333333333334, 1.0, 1.0, 0.44047283120751163, 1.0, 1.0, 0.44047283120751163, 1.0, 1.0, 0.8047159606502954, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.6476077529925275, 0.6476077529925272, 0.6895305590563982], 
reward next is 0.3105, 
noisyNet noise sample is [array([0.7385847], dtype=float32), 0.22896723]. 
=============================================
[2019-04-27 19:39:15,906] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.9343931e-16 9.9847597e-01 6.2957303e-14 7.4593210e-16 1.5240643e-03], sum to 1.0000
[2019-04-27 19:39:15,913] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2872
[2019-04-27 19:39:15,916] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 61.0, 1.0, 2.0, 0.3376007133003787, 1.0, 2.0, 0.3376007133003787, 1.0, 2.0, 0.537471330461842, 6.911199999999999, 6.9112, 121.94756008, 1154576.346160889, 1154576.34616089, 270827.9454452133], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6179400.0000, 
sim time next is 6180000.0000, 
raw observation next is [28.73333333333333, 60.33333333333333, 1.0, 2.0, 1.02, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.529822273427602, 6.9112, 121.92377067721, 1496869.442453083, 1180085.449608038, 246548.508645292], 
processed observation next is [1.0, 0.5217391304347826, 0.619753086419753, 0.6033333333333333, 1.0, 1.0, 1.0238095238095237, 0.0, 0.5, -0.1904761904761905, 0.0, 0.5, -0.25, 0.061862227342760166, 0.0, 0.8094470455114655, 0.5345962294475296, 0.4214590891457278, 0.4741317473947923], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.361691], dtype=float32), -1.2101411]. 
=============================================
[2019-04-27 19:39:15,933] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[42.940563]
 [42.121346]
 [41.571884]
 [41.11889 ]
 [40.677246]], R is [[43.52876663]
 [43.57265472]
 [43.59783554]
 [43.52967834]
 [43.41598129]].
[2019-04-27 19:39:16,765] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2785642e-18 3.5643334e-06 4.2446220e-14 5.1064665e-17 9.9999642e-01], sum to 1.0000
[2019-04-27 19:39:16,774] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9776
[2019-04-27 19:39:16,779] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.83333333333334, 54.66666666666667, 1.0, 2.0, 0.5194331589308676, 1.0, 2.0, 0.5194331589308676, 1.0, 2.0, 0.8269545057749074, 6.911199999999999, 6.9112, 121.94756008, 1777122.63568189, 1777122.63568189, 353229.4520443017], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6186000.0000, 
sim time next is 6186600.0000, 
raw observation next is [29.85, 54.5, 1.0, 2.0, 0.5268331021449881, 1.0, 2.0, 0.5268331021449881, 1.0, 2.0, 0.8387354563711126, 6.911200000000001, 6.9112, 121.94756008, 1802465.400841739, 1802465.400841738, 356954.2078568199], 
processed observation next is [1.0, 0.6086956521739131, 0.6611111111111112, 0.545, 1.0, 1.0, 0.4367060739821287, 1.0, 1.0, 0.4367060739821287, 1.0, 1.0, 0.7984193204638906, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.6437376431577639, 0.6437376431577636, 0.6864503997246537], 
reward next is 0.3135, 
noisyNet noise sample is [array([-1.1416234], dtype=float32), 1.156413]. 
=============================================
[2019-04-27 19:39:22,487] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.8658796e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.6547316e-29], sum to 1.0000
[2019-04-27 19:39:22,495] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7113
[2019-04-27 19:39:22,499] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 84.0, 1.0, 2.0, 0.5676207622377275, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 663858.251045723, 663858.251045723, 153390.069447801], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6307200.0000, 
sim time next is 6307800.0000, 
raw observation next is [24.66666666666666, 84.33333333333333, 1.0, 2.0, 0.5676646365629173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 663814.5876128008, 663814.5876128008, 153393.3632568787], 
processed observation next is [0.0, 0.0, 0.4691358024691356, 0.8433333333333333, 1.0, 1.0, 0.4853150435272825, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.23707663843314314, 0.23707663843314314, 0.29498723703245905], 
reward next is 0.7050, 
noisyNet noise sample is [array([1.3402228], dtype=float32), -0.25531426]. 
=============================================
[2019-04-27 19:39:25,084] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3600294e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8089475e-33], sum to 1.0000
[2019-04-27 19:39:25,091] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9659
[2019-04-27 19:39:25,097] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.83333333333334, 67.0, 1.0, 2.0, 0.6985180834798587, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 796112.134795099, 796112.134795099, 175864.0993321772], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6349200.0000, 
sim time next is 6349800.0000, 
raw observation next is [29.91666666666666, 66.5, 1.0, 2.0, 0.6994415040383968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 797165.1191299822, 797165.1191299822, 176038.6784514979], 
processed observation next is [0.0, 0.4782608695652174, 0.66358024691358, 0.665, 1.0, 1.0, 0.6421922667123771, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.28470182826070795, 0.28470182826070795, 0.33853592009903444], 
reward next is 0.6615, 
noisyNet noise sample is [array([-2.060336], dtype=float32), -2.760167]. 
=============================================
[2019-04-27 19:39:29,997] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.3288881e-25 3.0058955e-13 1.8937052e-21 5.0798567e-21 1.0000000e+00], sum to 1.0000
[2019-04-27 19:39:30,007] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5452
[2019-04-27 19:39:30,012] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.5, 55.0, 1.0, 2.0, 0.6178643140446697, 1.0, 2.0, 0.6178643140446697, 1.0, 2.0, 0.9836601103950021, 6.911199999999999, 6.9112, 121.94756008, 2114281.040083725, 2114281.040083725, 405194.8722818734], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6453600.0000, 
sim time next is 6454200.0000, 
raw observation next is [31.45, 55.0, 1.0, 2.0, 0.6181023643260529, 1.0, 2.0, 0.6181023643260529, 1.0, 2.0, 0.9840390941957208, 6.9112, 6.9112, 121.94756008, 2115096.593247633, 2115096.593247633, 405326.8777518157], 
processed observation next is [1.0, 0.6956521739130435, 0.7203703703703703, 0.55, 1.0, 1.0, 0.5453599575310154, 1.0, 1.0, 0.5453599575310154, 1.0, 1.0, 0.9800488677446508, 0.0, 0.0, 0.8096049824067558, 0.7553916404455832, 0.7553916404455832, 0.7794747649073379], 
reward next is 0.2205, 
noisyNet noise sample is [array([1.0318705], dtype=float32), 0.57779557]. 
=============================================
[2019-04-27 19:39:31,932] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.2703171e-31 1.0000000e+00 6.7148093e-31 3.9175893e-34 4.7260903e-27], sum to 1.0000
[2019-04-27 19:39:31,944] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5698
[2019-04-27 19:39:31,947] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.46666666666667, 84.33333333333333, 1.0, 2.0, 0.9724708545281541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.975057375932082, 6.9112, 121.9257931018817, 1141290.562281011, 1108589.945216129, 234147.2466572598], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6495000.0000, 
sim time next is 6495600.0000, 
raw observation next is [26.43333333333333, 84.66666666666667, 1.0, 2.0, 0.9904861571974659, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.086043825605604, 6.9112, 121.9252101760082, 1218718.969493683, 1129183.914716933, 238435.8808617688], 
processed observation next is [1.0, 0.17391304347826086, 0.5345679012345678, 0.8466666666666667, 1.0, 1.0, 0.9886739966636499, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.01748438256056044, 0.0, 0.8094566022865123, 0.43525677481917247, 0.40327996954176176, 0.45853054011878613], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.36291072], dtype=float32), 0.6142729]. 
=============================================
[2019-04-27 19:39:38,556] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-27 19:39:38,560] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 19:39:38,560] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:39:38,561] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 19:39:38,563] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:39:38,564] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 19:39:38,564] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 19:39:38,567] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:39:38,569] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:39:38,568] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 19:39:38,572] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:39:38,576] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run42
[2019-04-27 19:39:38,618] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run42
[2019-04-27 19:39:38,619] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run42
[2019-04-27 19:39:38,660] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run42
[2019-04-27 19:39:38,680] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run42
[2019-04-27 19:39:44,445] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.02941445], dtype=float32), 0.019362722]
[2019-04-27 19:39:44,446] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [28.7, 31.5, 1.0, 2.0, 0.526529710815023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 668335.9496856063, 668335.9496856063, 148123.4887108688]
[2019-04-27 19:39:44,447] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 19:39:44,450] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [2.8865681e-26 1.0000000e+00 1.3011596e-23 1.3697637e-26 1.9005719e-11], sampled 0.4951302254456148
[2019-04-27 19:39:48,076] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02941445], dtype=float32), 0.019362722]
[2019-04-27 19:39:48,078] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [34.66666666666667, 18.33333333333333, 1.0, 2.0, 0.3972785039767801, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 500194.3293158085, 500194.3293158085, 128381.3262110493]
[2019-04-27 19:39:48,079] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:39:48,082] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [4.3117799e-35 1.0000000e+00 6.4623151e-34 3.5239893e-38 3.9930659e-23], sampled 0.36030467755205786
[2019-04-27 19:40:30,137] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02941445], dtype=float32), 0.019362722]
[2019-04-27 19:40:30,138] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [22.33333333333334, 98.66666666666666, 1.0, 2.0, 0.5459288755152403, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 644139.7031130373, 644139.7031130373, 150024.2544401179]
[2019-04-27 19:40:30,139] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:40:30,142] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [5.5280593e-34 1.0000000e+00 3.4719305e-34 8.3408298e-37 6.0095646e-26], sampled 0.42932872009079814
[2019-04-27 19:40:39,530] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02941445], dtype=float32), 0.019362722]
[2019-04-27 19:40:39,532] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [26.304000515, 53.05283794833333, 1.0, 2.0, 0.4536452162349587, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 557891.989097215, 557891.989097215, 136329.6001076262]
[2019-04-27 19:40:39,533] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 19:40:39,535] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [3.5014232e-31 1.0000000e+00 8.1992802e-29 3.1331870e-32 2.9318944e-16], sampled 0.6271240862843882
[2019-04-27 19:40:44,935] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02941445], dtype=float32), 0.019362722]
[2019-04-27 19:40:44,935] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [22.47363526333334, 113.57839677, 1.0, 2.0, 0.81282274284731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 954351.9202348242, 954351.9202348242, 199995.8575098212]
[2019-04-27 19:40:44,937] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 19:40:44,940] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.2828210e-30 1.0000000e+00 2.5886033e-30 9.8418000e-33 1.3281717e-20], sampled 0.08551574929502026
[2019-04-27 19:40:59,827] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.02941445], dtype=float32), 0.019362722]
[2019-04-27 19:40:59,828] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [34.83333333333334, 32.33333333333334, 1.0, 2.0, 0.4925155210411678, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 584568.397081871, 584568.397081871, 141616.4874105921]
[2019-04-27 19:40:59,829] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:40:59,835] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.0898202e-32 1.0000000e+00 6.0019660e-31 2.1381296e-34 7.9992437e-20], sampled 0.077072553115073
[2019-04-27 19:41:05,392] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.02941445], dtype=float32), 0.019362722]
[2019-04-27 19:41:05,393] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [26.33333333333333, 87.0, 1.0, 2.0, 0.4286169639042158, 1.0, 2.0, 0.4286169639042158, 1.0, 2.0, 0.6823721656154919, 6.9112, 6.9112, 121.94756008, 1466144.895184303, 1466144.895184303, 309844.7520141749]
[2019-04-27 19:41:05,395] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:41:05,397] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [6.2943756e-19 9.0986609e-01 1.0167889e-14 5.2133087e-18 9.0133891e-02], sampled 0.16705406256385802
[2019-04-27 19:41:05,398] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1466144.895184303 W.
[2019-04-27 19:41:05,474] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02941445], dtype=float32), 0.019362722]
[2019-04-27 19:41:05,478] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [23.45483521833334, 92.6506374, 1.0, 2.0, 0.9634917257180929, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.107991297310551, 6.9112, 121.9252494083497, 1234029.919631097, 1133255.842005973, 233880.6426791234]
[2019-04-27 19:41:05,479] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 19:41:05,480] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [2.1919237e-28 1.0000000e+00 8.1091714e-28 6.5031816e-30 2.7649631e-18], sampled 0.38318122999620496
[2019-04-27 19:41:05,595] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02941445], dtype=float32), 0.019362722]
[2019-04-27 19:41:05,596] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [29.9, 53.83333333333333, 1.0, 2.0, 0.5566555073910262, 1.0, 2.0, 0.5566555073910262, 1.0, 2.0, 0.8862136967707377, 6.911199999999999, 6.9112, 121.94756008, 1904606.203552595, 1904606.203552596, 372265.5681403356]
[2019-04-27 19:41:05,596] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:41:05,599] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [8.6292926e-23 6.3739980e-10 2.0418001e-17 7.3603985e-20 1.0000000e+00], sampled 0.9735102307923672
[2019-04-27 19:41:19,625] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02941445], dtype=float32), 0.019362722]
[2019-04-27 19:41:19,627] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [19.78333333333333, 94.83333333333334, 1.0, 2.0, 0.4425041905097714, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 549211.2350413625, 549211.2350413625, 134788.4135644976]
[2019-04-27 19:41:19,628] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:41:19,631] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.7157823e-34 1.0000000e+00 1.4103416e-34 1.8769614e-37 2.1044917e-26], sampled 0.2575835954829272
[2019-04-27 19:41:19,638] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.02941445], dtype=float32), 0.019362722]
[2019-04-27 19:41:19,640] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [24.83333333333334, 90.66666666666666, 1.0, 2.0, 0.6183284500146817, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 707789.1738052649, 707789.1738052649, 161413.4100607122]
[2019-04-27 19:41:19,641] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:41:19,644] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [5.3128703e-29 1.0000000e+00 3.6766134e-27 3.2066109e-30 6.0593169e-16], sampled 0.5757577746727257
[2019-04-27 19:41:30,298] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8688.4803 2260553834.9943 327.0000
[2019-04-27 19:41:30,456] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8235.7607 2462356629.1257 411.0000
[2019-04-27 19:41:30,515] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8947.9473 2129346825.6187 359.0000
[2019-04-27 19:41:30,547] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8849.1856 2178715897.4631 354.0000
[2019-04-27 19:41:30,605] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8781.1282 2211958254.6023 392.0000
[2019-04-27 19:41:31,623] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 1025000, evaluation results [1025000.0, 8235.760741151433, 2462356629.1257315, 411.0, 8849.18562374012, 2178715897.46311, 354.0, 8947.94731459435, 2129346825.6186836, 359.0, 8688.480260259092, 2260553834.994326, 327.0, 8781.128183018403, 2211958254.602307, 392.0]
[2019-04-27 19:41:35,462] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2574097e-21 7.9553747e-01 2.4885012e-17 1.4864048e-18 2.0446247e-01], sum to 1.0000
[2019-04-27 19:41:35,470] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1823
[2019-04-27 19:41:35,476] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 37.0, 1.0, 2.0, 0.7120968927433495, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 905593.5176126278, 905593.5176126278, 181628.7581254059], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6692400.0000, 
sim time next is 6693000.0000, 
raw observation next is [27.15, 36.5, 1.0, 2.0, 0.7999602192573273, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1016978.123732565, 1016978.123732565, 199628.7747214955], 
processed observation next is [1.0, 0.4782608695652174, 0.561111111111111, 0.365, 1.0, 1.0, 0.7618574038777706, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3632064727616304, 0.3632064727616304, 0.3839014898490298], 
reward next is 0.6161, 
noisyNet noise sample is [array([1.0976585], dtype=float32), -0.9783981]. 
=============================================
[2019-04-27 19:41:35,489] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[75.759285]
 [75.98638 ]
 [75.96312 ]
 [75.85575 ]
 [75.77336 ]], R is [[75.39440155]
 [75.29116821]
 [75.21021271]
 [75.13243103]
 [75.05548096]].
[2019-04-27 19:41:39,683] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5108807e-38], sum to 1.0000
[2019-04-27 19:41:39,693] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2505
[2019-04-27 19:41:39,704] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.76666666666667, 74.5, 1.0, 2.0, 0.3183162235655951, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 407020.2715480354, 407020.2715480354, 117870.1760861498], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6761400.0000, 
sim time next is 6762000.0000, 
raw observation next is [20.03333333333333, 73.0, 1.0, 2.0, 0.3064229197519042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 391567.6815191884, 391567.6815191884, 116374.0205811919], 
processed observation next is [1.0, 0.2608695652173913, 0.2975308641975308, 0.73, 1.0, 1.0, 0.1743129997046479, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.13984560054256728, 0.13984560054256728, 0.22379619342536905], 
reward next is 0.7762, 
noisyNet noise sample is [array([-1.6864299], dtype=float32), 1.1750298]. 
=============================================
[2019-04-27 19:41:39,717] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[68.810135]
 [68.89884 ]
 [68.97478 ]
 [69.060646]
 [69.151054]], R is [[68.73990631]
 [68.82582855]
 [68.91772461]
 [69.00894165]
 [69.09960175]].
[2019-04-27 19:41:40,722] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.5915108e-23 1.2652862e-07 4.2910990e-15 2.0118831e-20 9.9999988e-01], sum to 1.0000
[2019-04-27 19:41:40,731] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6633
[2019-04-27 19:41:40,739] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.0, 49.33333333333333, 1.0, 2.0, 0.2749111940306788, 1.0, 2.0, 0.2749111940306788, 1.0, 2.0, 0.4460056509667431, 6.911200000000001, 6.9112, 121.94756008, 997381.3068133513, 997381.3068133509, 245778.6537887002], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6784800.0000, 
sim time next is 6785400.0000, 
raw observation next is [27.1, 49.16666666666667, 1.0, 2.0, 0.2886983671763256, 1.0, 2.0, 0.2886983671763256, 1.0, 2.0, 0.4674645502091853, 6.9112, 6.9112, 121.94756008, 1044336.096141957, 1044336.096141957, 251032.7722984911], 
processed observation next is [1.0, 0.5217391304347826, 0.5592592592592593, 0.4916666666666667, 1.0, 1.0, 0.15321234187657806, 1.0, 1.0, 0.15321234187657806, 1.0, 1.0, 0.3343306877614816, 0.0, 0.0, 0.8096049824067558, 0.37297717719355605, 0.37297717719355605, 0.4827553313432521], 
reward next is 0.5172, 
noisyNet noise sample is [array([1.913885], dtype=float32), 1.3786215]. 
=============================================
[2019-04-27 19:41:42,508] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8951804e-35 1.0000000e+00 1.6896676e-31 1.5755222e-36 2.6313105e-18], sum to 1.0000
[2019-04-27 19:41:42,517] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1147
[2019-04-27 19:41:42,520] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.48333333333333, 53.33333333333334, 1.0, 2.0, 0.4907255136696769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 588635.1604891283, 588635.1604891283, 141565.7191472813], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6887400.0000, 
sim time next is 6888000.0000, 
raw observation next is [28.36666666666667, 53.66666666666667, 1.0, 2.0, 0.4885838245297905, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 586483.3477726496, 586483.3477726501, 141247.1449567288], 
processed observation next is [0.0, 0.7391304347826086, 0.606172839506173, 0.5366666666666667, 1.0, 1.0, 0.3911712196783221, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.20945833849023202, 0.20945833849023218, 0.27162912491678615], 
reward next is 0.7284, 
noisyNet noise sample is [array([0.73817956], dtype=float32), -0.23678397]. 
=============================================
[2019-04-27 19:41:42,530] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[76.06243 ]
 [76.09986 ]
 [76.10067 ]
 [76.095215]
 [76.092674]], R is [[76.0029068 ]
 [75.97064209]
 [75.93805695]
 [75.90493011]
 [75.87130737]].
[2019-04-27 19:41:43,872] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.3990031e-34 1.0000000e+00 5.7158987e-38 0.0000000e+00 5.8084911e-27], sum to 1.0000
[2019-04-27 19:41:43,880] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7627
[2019-04-27 19:41:43,884] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 76.0, 1.0, 2.0, 0.3954570203674297, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 489044.6079551654, 489044.6079551654, 127959.5200885413], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6849000.0000, 
sim time next is 6849600.0000, 
raw observation next is [22.6, 76.0, 1.0, 2.0, 0.3991177470981507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 492897.8268782223, 492897.8268782223, 128457.3392356372], 
processed observation next is [0.0, 0.2608695652173913, 0.39259259259259266, 0.76, 1.0, 1.0, 0.28466398464065557, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1760349381707937, 0.1760349381707937, 0.24703334468391772], 
reward next is 0.7530, 
noisyNet noise sample is [array([-1.0560175], dtype=float32), -0.96381164]. 
=============================================
[2019-04-27 19:41:45,707] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.9614993e-33 1.0000000e+00 1.8185052e-30 1.5946499e-33 4.3992446e-17], sum to 1.0000
[2019-04-27 19:41:45,715] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4515
[2019-04-27 19:41:45,720] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.9, 55.0, 1.0, 2.0, 0.4809355447515657, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 578971.3012254849, 578971.3012254846, 140121.0225739165], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6890400.0000, 
sim time next is 6891000.0000, 
raw observation next is [27.76666666666667, 55.33333333333334, 1.0, 2.0, 0.4811274720479713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 579730.905084478, 579730.905084478, 140168.4107579232], 
processed observation next is [0.0, 0.782608695652174, 0.5839506172839507, 0.5533333333333335, 1.0, 1.0, 0.38229460958091827, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.207046751815885, 0.207046751815885, 0.26955463607292923], 
reward next is 0.7304, 
noisyNet noise sample is [array([1.5633618], dtype=float32), -0.045887116]. 
=============================================
[2019-04-27 19:41:45,747] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[70.451866]
 [70.49374 ]
 [70.52905 ]
 [70.56253 ]
 [70.590485]], R is [[70.40184021]
 [70.42835999]
 [70.45406342]
 [70.47896576]
 [70.50308228]].
[2019-04-27 19:41:46,928] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2845315e-29 1.0000000e+00 1.9455054e-26 3.3286398e-31 6.5687729e-17], sum to 1.0000
[2019-04-27 19:41:46,936] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6966
[2019-04-27 19:41:46,939] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.35, 70.5, 1.0, 2.0, 0.4077139911472971, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 503827.325988433, 503827.325988433, 129680.2138327827], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6911400.0000, 
sim time next is 6912000.0000, 
raw observation next is [23.3, 71.0, 1.0, 2.0, 0.4089761553230605, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 505205.383233199, 505205.383233199, 129855.4867964328], 
processed observation next is [0.0, 0.0, 0.41851851851851857, 0.71, 1.0, 1.0, 0.29640018490840536, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1804304940118568, 0.1804304940118568, 0.24972208999314], 
reward next is 0.7503, 
noisyNet noise sample is [array([-1.3692373], dtype=float32), -0.7222097]. 
=============================================
[2019-04-27 19:41:46,945] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[64.66967]
 [64.72045]
 [64.77585]
 [64.8265 ]
 [64.87522]], R is [[64.69239807]
 [64.79608917]
 [64.89907837]
 [65.00131226]
 [65.10273743]].
[2019-04-27 19:41:48,034] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.0133312e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.4707992e-27], sum to 1.0000
[2019-04-27 19:41:48,039] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2619
[2019-04-27 19:41:48,045] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.2, 86.33333333333333, 1.0, 2.0, 0.4130009339265889, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 509836.2584876765, 509836.2584876765, 130421.7928790147], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6929400.0000, 
sim time next is 6930000.0000, 
raw observation next is [21.1, 87.0, 1.0, 2.0, 0.4116994529863343, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 508342.2465766961, 508342.2465766961, 130238.4346384613], 
processed observation next is [0.0, 0.21739130434782608, 0.3370370370370371, 0.87, 1.0, 1.0, 0.2996422059361123, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.18155080234882004, 0.18155080234882004, 0.2504585281508871], 
reward next is 0.7495, 
noisyNet noise sample is [array([-1.1099688], dtype=float32), -0.22621895]. 
=============================================
[2019-04-27 19:41:48,056] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[70.78498 ]
 [70.79929 ]
 [70.82037 ]
 [70.842384]
 [70.870125]], R is [[70.80413818]
 [70.84529114]
 [70.88568878]
 [70.92539978]
 [70.9644165 ]].
[2019-04-27 19:41:49,556] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.8273382e-37 1.0000000e+00 3.0038444e-35 0.0000000e+00 9.5204990e-25], sum to 1.0000
[2019-04-27 19:41:49,559] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9383
[2019-04-27 19:41:49,563] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 51.0, 1.0, 2.0, 0.5792332855147203, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 671967.4252264837, 671967.4252264837, 155108.8989273303], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6967200.0000, 
sim time next is 6967800.0000, 
raw observation next is [31.0, 50.5, 1.0, 2.0, 0.5736049632336135, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 666804.776432595, 666804.776432595, 154219.4835527587], 
processed observation next is [0.0, 0.6521739130434783, 0.7037037037037037, 0.505, 1.0, 1.0, 0.49238686099239704, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.23814456301164105, 0.23814456301164105, 0.2965759299091513], 
reward next is 0.7034, 
noisyNet noise sample is [array([-1.0040885], dtype=float32), 0.50702655]. 
=============================================
[2019-04-27 19:41:51,843] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.2289577e-30 1.0000000e+00 4.7711224e-27 2.8053676e-33 2.8992833e-17], sum to 1.0000
[2019-04-27 19:41:51,850] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9452
[2019-04-27 19:41:51,856] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.03333333333333, 80.33333333333334, 1.0, 2.0, 0.7180566210836745, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 885128.0000193475, 885128.0000193475, 182300.423758556], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7006800.0000, 
sim time next is 7007400.0000, 
raw observation next is [21.95, 81.0, 1.0, 2.0, 0.6150164738385212, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 758144.7574072037, 758144.7574072032, 162923.9996031015], 
processed observation next is [1.0, 0.08695652173913043, 0.36851851851851847, 0.81, 1.0, 1.0, 0.5416862783791919, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.270765984788287, 0.27076598478828684, 0.31331538385211827], 
reward next is 0.6867, 
noisyNet noise sample is [array([1.4400274], dtype=float32), 0.09411958]. 
=============================================
[2019-04-27 19:42:00,162] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.175581e-38 1.000000e+00 8.324428e-37 0.000000e+00 3.670201e-24], sum to 1.0000
[2019-04-27 19:42:00,169] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7492
[2019-04-27 19:42:00,173] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.8, 92.33333333333334, 1.0, 2.0, 0.3739705320141695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 466068.3401501114, 466068.3401501114, 125069.2829520061], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7173600.0000, 
sim time next is 7174200.0000, 
raw observation next is [19.8, 92.66666666666666, 1.0, 2.0, 0.3766445018125332, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 469153.8831494225, 469153.8831494225, 125429.7907497607], 
processed observation next is [1.0, 0.0, 0.2888888888888889, 0.9266666666666665, 1.0, 1.0, 0.25791012120539664, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16755495826765088, 0.16755495826765088, 0.2412111360572321], 
reward next is 0.7588, 
noisyNet noise sample is [array([0.8603225], dtype=float32), -1.3445643]. 
=============================================
[2019-04-27 19:42:11,035] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.3110479e-27 2.9689285e-12 9.1923683e-20 1.0398109e-24 1.0000000e+00], sum to 1.0000
[2019-04-27 19:42:11,043] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5325
[2019-04-27 19:42:11,050] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [20.3, 93.83333333333334, 1.0, 2.0, 0.2803966227659373, 1.0, 2.0, 0.2803966227659373, 1.0, 2.0, 0.4538704010480744, 6.9112, 6.9112, 121.94756008, 1013745.582960589, 1013745.582960589, 247941.3969064464], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7387800.0000, 
sim time next is 7388400.0000, 
raw observation next is [20.4, 93.66666666666667, 1.0, 2.0, 0.20694233969289, 1.0, 2.0, 0.20694233969289, 1.0, 2.0, 0.33515350969802, 6.9112, 6.9112, 121.94756008, 748692.3515844641, 748692.3515844641, 222081.3208996677], 
processed observation next is [1.0, 0.5217391304347826, 0.31111111111111106, 0.9366666666666668, 1.0, 1.0, 0.05588373772963095, 1.0, 1.0, 0.05588373772963095, 1.0, 1.0, 0.16894188712252498, 0.0, 0.0, 0.8096049824067558, 0.26739012556588004, 0.26739012556588004, 0.42707946326859175], 
reward next is 0.5729, 
noisyNet noise sample is [array([0.7083753], dtype=float32), 1.7434407]. 
=============================================
[2019-04-27 19:42:13,760] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.542897e-37 1.000000e+00 9.983047e-37 0.000000e+00 3.835573e-29], sum to 1.0000
[2019-04-27 19:42:13,770] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4935
[2019-04-27 19:42:13,775] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.8, 93.0, 1.0, 2.0, 0.3767346783778851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 468924.6951272137, 468924.6951272133, 125435.2420299281], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7437600.0000, 
sim time next is 7438200.0000, 
raw observation next is [19.75, 93.16666666666667, 1.0, 2.0, 0.3758188166950627, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 467949.8384064146, 467949.8384064146, 125313.2757674624], 
processed observation next is [0.0, 0.08695652173913043, 0.28703703703703703, 0.9316666666666668, 1.0, 1.0, 0.2569271627322175, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16712494228800523, 0.16712494228800523, 0.24098706878358153], 
reward next is 0.7590, 
noisyNet noise sample is [array([0.11488531], dtype=float32), -0.12887496]. 
=============================================
[2019-04-27 19:42:20,003] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-27 19:42:20,005] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 19:42:20,006] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:42:20,007] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 19:42:20,008] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 19:42:20,009] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 19:42:20,010] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 19:42:20,011] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:42:20,013] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:42:20,009] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:42:20,016] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:42:20,036] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run43
[2019-04-27 19:42:20,057] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run43
[2019-04-27 19:42:20,058] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run43
[2019-04-27 19:42:20,059] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run43
[2019-04-27 19:42:20,103] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run43
[2019-04-27 19:42:22,186] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03758708], dtype=float32), 0.015500772]
[2019-04-27 19:42:22,188] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [25.3, 62.0, 1.0, 2.0, 0.4233578746239968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 519330.9272859811, 519330.9272859811, 131829.3802225213]
[2019-04-27 19:42:22,190] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:42:22,192] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.5543605e-35 1.0000000e+00 2.9692366e-33 1.1617527e-36 3.6608239e-21], sampled 0.5670416683175935
[2019-04-27 19:42:35,624] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.03758708], dtype=float32), 0.015500772]
[2019-04-27 19:42:35,625] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [17.16666666666667, 89.66666666666667, 1.0, 2.0, 0.2918279477404159, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 375549.7242090321, 375549.7242090317, 114565.5160576615]
[2019-04-27 19:42:35,627] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 19:42:35,630] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.3413223e-31], sampled 0.7916906586673917
[2019-04-27 19:42:41,207] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03758708], dtype=float32), 0.015500772]
[2019-04-27 19:42:41,211] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [33.99044392666666, 30.97558193, 1.0, 2.0, 0.4652035192814015, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 559647.4164719558, 559647.4164719554, 137703.3312538354]
[2019-04-27 19:42:41,212] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:42:41,219] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.2777129e-37 1.0000000e+00 1.7446893e-36 0.0000000e+00 3.6667965e-26], sampled 0.248147856663811
[2019-04-27 19:42:42,480] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03758708], dtype=float32), 0.015500772]
[2019-04-27 19:42:42,481] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [24.76666666666667, 56.33333333333333, 1.0, 2.0, 0.9333411047356869, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.261553164076012, 6.9112, 121.9245553140606, 1341159.528135065, 1161749.582457129, 229166.6599388816]
[2019-04-27 19:42:42,481] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:42:42,484] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [7.4063269e-30 1.0000000e+00 5.0034683e-28 1.6337829e-30 4.6016187e-17], sampled 0.6587845103370902
[2019-04-27 19:42:42,488] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1341159.528135065 W.
[2019-04-27 19:42:48,220] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03758708], dtype=float32), 0.015500772]
[2019-04-27 19:42:48,221] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [20.199670465, 85.94025817666666, 1.0, 2.0, 0.3455586774688054, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 433282.6382315413, 433282.6382315413, 121313.07910875]
[2019-04-27 19:42:48,222] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:42:48,225] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 1.967553e-33], sampled 0.8275257901691956
[2019-04-27 19:43:13,994] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03758708], dtype=float32), 0.015500772]
[2019-04-27 19:43:13,995] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [26.45309849, 64.129283025, 1.0, 2.0, 0.5013183345923394, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 600386.7622287908, 600386.7622287908, 143191.3350277829]
[2019-04-27 19:43:13,998] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:43:14,001] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0345259e-29], sampled 0.7916340650667588
[2019-04-27 19:43:32,324] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03758708], dtype=float32), 0.015500772]
[2019-04-27 19:43:32,324] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [25.0, 100.0, 1.0, 2.0, 0.7107305992485454, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 810038.2875812339, 810038.2875812339, 178186.340450628]
[2019-04-27 19:43:32,325] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:43:32,328] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [4.4911520e-38 1.0000000e+00 2.5069902e-37 0.0000000e+00 5.9426268e-28], sampled 0.60526634200901
[2019-04-27 19:43:37,348] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.03758708], dtype=float32), 0.015500772]
[2019-04-27 19:43:37,349] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [28.19196148833333, 83.10829095, 1.0, 2.0, 0.3377019904227062, 1.0, 2.0, 0.3377019904227062, 1.0, 2.0, 0.537632566938953, 6.9112, 6.9112, 121.94756008, 1154922.969513799, 1154922.969513799, 270868.8049801839]
[2019-04-27 19:43:37,351] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 19:43:37,355] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [8.9533879e-25 5.5641636e-02 2.5532274e-19 5.8614006e-22 9.4435841e-01], sampled 0.392887066879008
[2019-04-27 19:44:11,462] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8669.7920 2257296251.1458 362.0000
[2019-04-27 19:44:11,503] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8815.8550 2175172085.5711 406.0000
[2019-04-27 19:44:11,714] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8939.8627 2127037558.5608 375.0000
[2019-04-27 19:44:11,834] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8197.2111 2460137519.7457 464.0000
[2019-04-27 19:44:11,931] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8760.7198 2206460694.1018 429.0000
[2019-04-27 19:44:12,950] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 1050000, evaluation results [1050000.0, 8197.21108003004, 2460137519.7456546, 464.0, 8815.855032692547, 2175172085.5710506, 406.0, 8939.862661799027, 2127037558.560807, 375.0, 8669.792009295614, 2257296251.145754, 362.0, 8760.719750405606, 2206460694.1018167, 429.0]
[2019-04-27 19:44:14,905] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.8129296e-32 1.0000000e+00 4.2875651e-30 4.6253549e-34 4.7292113e-19], sum to 1.0000
[2019-04-27 19:44:14,912] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1784
[2019-04-27 19:44:14,917] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.7, 85.33333333333333, 1.0, 2.0, 0.5157446209856511, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 612699.841394145, 612699.841394145, 145298.9343371062], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7594800.0000, 
sim time next is 7595400.0000, 
raw observation next is [23.65, 85.66666666666667, 1.0, 2.0, 0.5154528694282259, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 612403.2816684177, 612403.2816684177, 145254.2658743413], 
processed observation next is [0.0, 0.9130434782608695, 0.4314814814814814, 0.8566666666666667, 1.0, 1.0, 0.42315817789074506, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21871545773872061, 0.21871545773872061, 0.27933512668142557], 
reward next is 0.7207, 
noisyNet noise sample is [array([-1.8570173], dtype=float32), -0.15398277]. 
=============================================
[2019-04-27 19:44:19,262] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.4287553e-29 1.0000000e+00 5.2657220e-27 7.8837190e-31 9.8314208e-15], sum to 1.0000
[2019-04-27 19:44:19,273] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9693
[2019-04-27 19:44:19,283] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.9, 78.0, 1.0, 2.0, 0.3163479656018406, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 402209.4503635289, 402209.4503635289, 117612.6525677301], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7678800.0000, 
sim time next is 7679400.0000, 
raw observation next is [19.86666666666667, 78.66666666666667, 1.0, 2.0, 0.3188066569046101, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 405087.1125240113, 405087.1125240109, 117922.4783106525], 
processed observation next is [1.0, 0.9130434782608695, 0.2913580246913582, 0.7866666666666667, 1.0, 1.0, 0.18905554393405963, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.14467396875857547, 0.14467396875857533, 0.2267739967512548], 
reward next is 0.7732, 
noisyNet noise sample is [array([-0.39169893], dtype=float32), -0.7357441]. 
=============================================
[2019-04-27 19:44:19,718] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.8543886e-32 1.0000000e+00 8.3321056e-32 1.0339871e-34 4.5439871e-16], sum to 1.0000
[2019-04-27 19:44:19,729] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2909
[2019-04-27 19:44:19,737] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.95, 76.0, 1.0, 2.0, 0.3071061236381439, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 391310.1829491871, 391310.1829491871, 116456.0591385079], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7677000.0000, 
sim time next is 7677600.0000, 
raw observation next is [19.93333333333333, 76.66666666666667, 1.0, 2.0, 0.3098348451782043, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 394503.4325814692, 394503.4325814692, 116796.0253044473], 
processed observation next is [1.0, 0.8695652173913043, 0.293827160493827, 0.7666666666666667, 1.0, 1.0, 0.17837481568833846, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1408940830648104, 0.1408940830648104, 0.22460774097009098], 
reward next is 0.7754, 
noisyNet noise sample is [array([1.4638478], dtype=float32), 0.19217761]. 
=============================================
[2019-04-27 19:44:23,143] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:44:23,144] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:44:23,183] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res14/Eplus-env-sub_run6
[2019-04-27 19:44:24,400] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.7826058e-35 1.0000000e+00 9.9235321e-33 7.9898687e-38 1.9108830e-27], sum to 1.0000
[2019-04-27 19:44:24,408] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6864
[2019-04-27 19:44:24,411] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.11666666666667, 58.0, 1.0, 2.0, 0.3520785537122555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 446602.3152498361, 446602.3152498361, 122237.3794916017], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7798200.0000, 
sim time next is 7798800.0000, 
raw observation next is [23.33333333333334, 57.0, 1.0, 2.0, 0.3194812387424169, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 405081.6681245176, 405081.6681245171, 118001.4888134094], 
processed observation next is [1.0, 0.2608695652173913, 0.4197530864197533, 0.57, 1.0, 1.0, 0.18985861755049632, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.14467202433018486, 0.14467202433018467, 0.2269259400257873], 
reward next is 0.7731, 
noisyNet noise sample is [array([-1.674684], dtype=float32), -1.8992385]. 
=============================================
[2019-04-27 19:44:27,228] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2971898e-23 7.2107209e-11 5.1008944e-18 4.8942360e-23 1.0000000e+00], sum to 1.0000
[2019-04-27 19:44:27,236] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9518
[2019-04-27 19:44:27,242] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 34.5, 1.0, 2.0, 0.3576443056951841, 1.0, 2.0, 0.3576443056951841, 1.0, 2.0, 0.5756582869964976, 6.911199999999999, 6.9112, 121.94756008, 1280097.886610474, 1280097.886610474, 278690.131511135], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7831800.0000, 
sim time next is 7832400.0000, 
raw observation next is [31.0, 34.0, 1.0, 2.0, 0.3672928049367152, 1.0, 2.0, 0.3672928049367152, 1.0, 2.0, 0.5916278434481692, 6.911199999999999, 6.9112, 121.94756008, 1316654.704515028, 1316654.704515028, 282686.9127580108], 
processed observation next is [1.0, 0.6521739130434783, 0.7037037037037037, 0.34, 1.0, 1.0, 0.24677714873418474, 1.0, 1.0, 0.24677714873418474, 1.0, 1.0, 0.48953480431021146, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.47023382304108147, 0.47023382304108147, 0.54362867838079], 
reward next is 0.4564, 
noisyNet noise sample is [array([1.1884266], dtype=float32), 0.09555596]. 
=============================================
[2019-04-27 19:44:31,049] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:44:31,050] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:44:31,105] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res3/Eplus-env-sub_run6
[2019-04-27 19:44:32,009] A3C_AGENT_WORKER-Thread-19 INFO:Local step 66500, global step 1059702: loss 0.0072
[2019-04-27 19:44:32,009] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 66500, global step 1059702: learning rate 0.0001
[2019-04-27 19:44:32,929] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:44:32,929] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:44:32,951] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res11/Eplus-env-sub_run6
[2019-04-27 19:44:33,225] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:44:33,225] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:44:33,236] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res4/Eplus-env-sub_run6
[2019-04-27 19:44:33,352] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:44:33,352] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:44:33,363] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res15/Eplus-env-sub_run6
[2019-04-27 19:44:33,528] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:44:33,529] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:44:33,538] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res2/Eplus-env-sub_run6
[2019-04-27 19:44:33,558] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:44:33,558] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:44:33,571] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res12/Eplus-env-sub_run6
[2019-04-27 19:44:33,572] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:44:33,572] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:44:33,590] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:44:33,592] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:44:33,603] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res7/Eplus-env-sub_run6
[2019-04-27 19:44:33,633] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:44:33,634] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:44:33,634] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res16/Eplus-env-sub_run6
[2019-04-27 19:44:33,685] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res6/Eplus-env-sub_run6
[2019-04-27 19:44:33,713] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:44:33,713] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:44:33,714] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:44:33,714] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:44:33,721] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res10/Eplus-env-sub_run6
[2019-04-27 19:44:33,743] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:44:33,744] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:44:33,752] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res17/Eplus-env-sub_run6
[2019-04-27 19:44:33,747] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res13/Eplus-env-sub_run6
[2019-04-27 19:44:33,806] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:44:33,807] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:44:33,810] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res5/Eplus-env-sub_run6
[2019-04-27 19:44:33,841] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:44:33,841] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:44:33,850] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 19:44:33,851] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:44:33,853] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res8/Eplus-env-sub_run6
[2019-04-27 19:44:33,889] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res9/Eplus-env-sub_run6
[2019-04-27 19:44:34,095] A3C_AGENT_WORKER-Thread-3 INFO:Local step 66500, global step 1060629: loss 0.1886
[2019-04-27 19:44:34,096] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 66500, global step 1060629: learning rate 0.0001
[2019-04-27 19:44:39,959] A3C_AGENT_WORKER-Thread-19 INFO:Local step 67000, global step 1063267: loss 0.0341
[2019-04-27 19:44:39,961] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 67000, global step 1063267: learning rate 0.0001
[2019-04-27 19:44:40,981] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2001689e-27 1.0000000e+00 1.2631046e-25 3.8574364e-29 1.6099234e-19], sum to 1.0000
[2019-04-27 19:44:40,990] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5756
[2019-04-27 19:44:40,993] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.56666666666666, 30.66666666666667, 1.0, 2.0, 0.511434006213826, 1.0, 2.0, 0.511434006213826, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1214580.63925585, 1214580.63925585, 241086.5294196213], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 130800.0000, 
sim time next is 131400.0000, 
raw observation next is [33.95, 29.0, 1.0, 2.0, 0.9385765833354407, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.120395266200753, 6.9112, 121.9251232001478, 1242683.524046575, 1135557.657908034, 229374.9182874765], 
processed observation next is [1.0, 0.5217391304347826, 0.8129629629629631, 0.29, 1.0, 1.0, 0.9268768849231437, 0.0, 0.5, -0.1904761904761905, 0.0, 1.0, -0.25, 0.020919526620075325, 0.0, 0.8094560248572477, 0.4438155443023482, 0.40555630639572643, 0.44110561209130095], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.21973172], dtype=float32), 0.72232854]. 
=============================================
[2019-04-27 19:44:42,239] A3C_AGENT_WORKER-Thread-16 INFO:Local step 66500, global step 1064470: loss 0.3027
[2019-04-27 19:44:42,241] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 66500, global step 1064470: learning rate 0.0001
[2019-04-27 19:44:42,705] A3C_AGENT_WORKER-Thread-17 INFO:Local step 66500, global step 1064708: loss 1.1667
[2019-04-27 19:44:42,709] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 66500, global step 1064711: learning rate 0.0001
[2019-04-27 19:44:42,752] A3C_AGENT_WORKER-Thread-9 INFO:Local step 66500, global step 1064731: loss 0.0440
[2019-04-27 19:44:42,755] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 66500, global step 1064732: learning rate 0.0001
[2019-04-27 19:44:42,825] A3C_AGENT_WORKER-Thread-20 INFO:Local step 66500, global step 1064770: loss 0.0486
[2019-04-27 19:44:42,828] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 66500, global step 1064772: learning rate 0.0001
[2019-04-27 19:44:42,877] A3C_AGENT_WORKER-Thread-2 INFO:Local step 66500, global step 1064797: loss 0.0510
[2019-04-27 19:44:42,880] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 66500, global step 1064798: learning rate 0.0001
[2019-04-27 19:44:43,200] A3C_AGENT_WORKER-Thread-12 INFO:Local step 66500, global step 1064960: loss 3.3873
[2019-04-27 19:44:43,204] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 66500, global step 1064961: learning rate 0.0001
[2019-04-27 19:44:43,262] A3C_AGENT_WORKER-Thread-15 INFO:Local step 66500, global step 1064995: loss 3.6323
[2019-04-27 19:44:43,264] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 66500, global step 1064996: learning rate 0.0001
[2019-04-27 19:44:43,277] A3C_AGENT_WORKER-Thread-18 INFO:Local step 66500, global step 1065001: loss 3.8771
[2019-04-27 19:44:43,279] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 66500, global step 1065002: learning rate 0.0001
[2019-04-27 19:44:43,308] A3C_AGENT_WORKER-Thread-10 INFO:Local step 66500, global step 1065018: loss 3.6444
[2019-04-27 19:44:43,309] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 66500, global step 1065018: learning rate 0.0001
[2019-04-27 19:44:43,395] A3C_AGENT_WORKER-Thread-11 INFO:Local step 66500, global step 1065062: loss 4.1526
[2019-04-27 19:44:43,396] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 66500, global step 1065062: learning rate 0.0001
[2019-04-27 19:44:43,476] A3C_AGENT_WORKER-Thread-13 INFO:Local step 66500, global step 1065105: loss 3.0172
[2019-04-27 19:44:43,477] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 66500, global step 1065105: learning rate 0.0001
[2019-04-27 19:44:43,498] A3C_AGENT_WORKER-Thread-22 INFO:Local step 66500, global step 1065115: loss 2.8754
[2019-04-27 19:44:43,502] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 66500, global step 1065115: learning rate 0.0001
[2019-04-27 19:44:43,539] A3C_AGENT_WORKER-Thread-21 INFO:Local step 66500, global step 1065136: loss 0.5530
[2019-04-27 19:44:43,541] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 66500, global step 1065136: learning rate 0.0001
[2019-04-27 19:44:43,686] A3C_AGENT_WORKER-Thread-14 INFO:Local step 66500, global step 1065211: loss 0.9179
[2019-04-27 19:44:43,691] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 66500, global step 1065213: learning rate 0.0001
[2019-04-27 19:44:47,766] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4723030e-33 1.0000000e+00 2.8896440e-32 6.6703663e-37 4.6291936e-23], sum to 1.0000
[2019-04-27 19:44:47,777] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7114
[2019-04-27 19:44:47,783] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 50.0, 1.0, 2.0, 0.3393705592997774, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 428690.5862372747, 428690.5862372747, 120546.4194723115], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 606000.0000, 
sim time next is 606600.0000, 
raw observation next is [24.85, 50.5, 1.0, 2.0, 0.3378176251789944, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 426939.3126653464, 426939.3126653464, 120346.3386367489], 
processed observation next is [1.0, 0.0, 0.475925925925926, 0.505, 1.0, 1.0, 0.21168764902261236, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.15247832595190944, 0.15247832595190944, 0.2314352666091325], 
reward next is 0.7686, 
noisyNet noise sample is [array([-0.15000616], dtype=float32), -0.4817635]. 
=============================================
[2019-04-27 19:44:48,078] A3C_AGENT_WORKER-Thread-3 INFO:Local step 67000, global step 1067481: loss 0.0223
[2019-04-27 19:44:48,079] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 67000, global step 1067481: learning rate 0.0001
[2019-04-27 19:44:52,746] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.2089930e-38 1.0000000e+00 1.1800515e-37 0.0000000e+00 9.9530043e-30], sum to 1.0000
[2019-04-27 19:44:52,752] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8557
[2019-04-27 19:44:52,754] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.73333333333333, 44.5, 1.0, 2.0, 0.2909835684954484, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 374909.372492536, 374909.372492536, 114458.8833006356], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 341400.0000, 
sim time next is 342000.0000, 
raw observation next is [23.6, 45.0, 1.0, 2.0, 0.2912203712318286, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 375294.9561756047, 375294.9561756047, 114487.0245443953], 
processed observation next is [0.0, 1.0, 0.4296296296296297, 0.45, 1.0, 1.0, 0.1562147276569388, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.13403391291985883, 0.13403391291985883, 0.22016735489306788], 
reward next is 0.7798, 
noisyNet noise sample is [array([0.8838579], dtype=float32), 0.43759057]. 
=============================================
[2019-04-27 19:44:52,764] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[70.87861]
 [70.93071]
 [70.97996]
 [71.02394]
 [71.07267]], R is [[70.88827515]
 [70.95927429]
 [71.0296402 ]
 [71.09901428]
 [71.16737366]].
[2019-04-27 19:44:55,540] A3C_AGENT_WORKER-Thread-19 INFO:Local step 67500, global step 1071399: loss 3.1886
[2019-04-27 19:44:55,542] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 67500, global step 1071399: learning rate 0.0001
[2019-04-27 19:44:57,625] A3C_AGENT_WORKER-Thread-16 INFO:Local step 67000, global step 1072498: loss 0.0194
[2019-04-27 19:44:57,627] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 67000, global step 1072498: learning rate 0.0001
[2019-04-27 19:44:58,074] A3C_AGENT_WORKER-Thread-20 INFO:Local step 67000, global step 1072731: loss 0.0179
[2019-04-27 19:44:58,075] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 67000, global step 1072731: learning rate 0.0001
[2019-04-27 19:44:58,092] A3C_AGENT_WORKER-Thread-17 INFO:Local step 67000, global step 1072744: loss 0.0384
[2019-04-27 19:44:58,095] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 67000, global step 1072745: learning rate 0.0001
[2019-04-27 19:44:58,133] A3C_AGENT_WORKER-Thread-9 INFO:Local step 67000, global step 1072765: loss 0.0194
[2019-04-27 19:44:58,137] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 67000, global step 1072765: learning rate 0.0001
[2019-04-27 19:44:58,192] A3C_AGENT_WORKER-Thread-2 INFO:Local step 67000, global step 1072793: loss 0.0198
[2019-04-27 19:44:58,193] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 67000, global step 1072794: learning rate 0.0001
[2019-04-27 19:44:58,367] A3C_AGENT_WORKER-Thread-18 INFO:Local step 67000, global step 1072888: loss 0.0713
[2019-04-27 19:44:58,369] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 67000, global step 1072888: learning rate 0.0001
[2019-04-27 19:44:58,489] A3C_AGENT_WORKER-Thread-12 INFO:Local step 67000, global step 1072951: loss 0.0389
[2019-04-27 19:44:58,491] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 67000, global step 1072953: learning rate 0.0001
[2019-04-27 19:44:58,608] A3C_AGENT_WORKER-Thread-10 INFO:Local step 67000, global step 1073013: loss 0.0218
[2019-04-27 19:44:58,609] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 67000, global step 1073014: learning rate 0.0001
[2019-04-27 19:44:58,620] A3C_AGENT_WORKER-Thread-15 INFO:Local step 67000, global step 1073016: loss 0.0317
[2019-04-27 19:44:58,622] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 67000, global step 1073017: learning rate 0.0001
[2019-04-27 19:44:58,703] A3C_AGENT_WORKER-Thread-11 INFO:Local step 67000, global step 1073059: loss 0.0200
[2019-04-27 19:44:58,704] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 67000, global step 1073059: learning rate 0.0001
[2019-04-27 19:44:58,772] A3C_AGENT_WORKER-Thread-13 INFO:Local step 67000, global step 1073094: loss 0.0179
[2019-04-27 19:44:58,778] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 67000, global step 1073094: learning rate 0.0001
[2019-04-27 19:44:58,779] A3C_AGENT_WORKER-Thread-22 INFO:Local step 67000, global step 1073095: loss 0.0183
[2019-04-27 19:44:58,783] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 67000, global step 1073095: learning rate 0.0001
[2019-04-27 19:44:58,858] A3C_AGENT_WORKER-Thread-21 INFO:Local step 67000, global step 1073141: loss 0.0227
[2019-04-27 19:44:58,862] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 67000, global step 1073141: learning rate 0.0001
[2019-04-27 19:44:58,903] A3C_AGENT_WORKER-Thread-14 INFO:Local step 67000, global step 1073161: loss 0.0169
[2019-04-27 19:44:58,906] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 67000, global step 1073161: learning rate 0.0001
[2019-04-27 19:44:59,633] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.0579710e-25 2.7118789e-09 3.0285303e-16 1.0046622e-20 1.0000000e+00], sum to 1.0000
[2019-04-27 19:44:59,647] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0656
[2019-04-27 19:44:59,652] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.05, 40.5, 1.0, 2.0, 0.3259177213586403, 1.0, 2.0, 0.3259177213586403, 1.0, 2.0, 0.5309710956718745, 6.9112, 6.9112, 121.94756008, 1189390.805070221, 1189390.805070221, 265098.5309411635], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 469800.0000, 
sim time next is 470400.0000, 
raw observation next is [28.36666666666667, 39.66666666666667, 1.0, 2.0, 0.3183914708757304, 1.0, 2.0, 0.3183914708757304, 1.0, 2.0, 0.518224437450791, 6.911200000000001, 6.9112, 121.94756008, 1160486.218971726, 1160486.218971725, 262175.0846132672], 
processed observation next is [1.0, 0.43478260869565216, 0.606172839506173, 0.3966666666666667, 1.0, 1.0, 0.18856127485205998, 1.0, 1.0, 0.18856127485205998, 1.0, 1.0, 0.3977805468134888, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.4144593639184736, 0.41445936391847327, 0.5041828550255139], 
reward next is 0.4958, 
noisyNet noise sample is [array([0.87230825], dtype=float32), -1.6203448]. 
=============================================
[2019-04-27 19:45:02,391] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-04-27 19:45:02,394] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 19:45:02,395] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 19:45:02,395] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:45:02,396] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:45:02,396] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 19:45:02,397] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 19:45:02,399] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:45:02,398] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 19:45:02,400] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:45:02,401] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:45:02,423] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run44
[2019-04-27 19:45:02,423] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run44
[2019-04-27 19:45:02,424] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run44
[2019-04-27 19:45:02,499] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run44
[2019-04-27 19:45:02,519] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run44
[2019-04-27 19:45:09,308] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03772137], dtype=float32), 0.006471356]
[2019-04-27 19:45:09,309] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [25.83333333333334, 46.33333333333333, 1.0, 2.0, 0.8330627348986528, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1050252.506727442, 1050252.506727442, 206667.0511668756]
[2019-04-27 19:45:09,312] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:45:09,315] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.7288776e-33 1.0000000e+00 6.8778167e-33 1.9118085e-35 3.5931904e-24], sampled 0.360339894072803
[2019-04-27 19:45:30,408] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03772137], dtype=float32), 0.006471356]
[2019-04-27 19:45:30,409] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [20.51516779, 86.22172189, 1.0, 2.0, 0.3582663327218465, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 447220.4463996302, 447220.4463996302, 122963.3112506872]
[2019-04-27 19:45:30,411] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:45:30,413] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7188863575893618
[2019-04-27 19:46:23,549] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03772137], dtype=float32), 0.006471356]
[2019-04-27 19:46:23,550] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [25.52568885666667, 87.92505795833333, 1.0, 2.0, 0.6004577298018886, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 684919.3451953443, 684919.3451953443, 158187.9036155079]
[2019-04-27 19:46:23,551] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:46:23,554] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.5239060e-36 1.0000000e+00 5.3643777e-35 1.8961393e-38 1.2060355e-25], sampled 0.7248924300003632
[2019-04-27 19:46:30,374] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03772137], dtype=float32), 0.006471356]
[2019-04-27 19:46:30,375] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [29.95, 62.16666666666667, 1.0, 2.0, 0.6545726849322197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 746002.4946121693, 746002.4946121693, 167721.0501352041]
[2019-04-27 19:46:30,380] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:46:30,384] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [6.4772157e-36 1.0000000e+00 2.1807163e-34 1.4259573e-37 2.3668052e-24], sampled 0.4557400808382075
[2019-04-27 19:46:53,903] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8812.7058 2174521861.9094 415.0000
[2019-04-27 19:46:53,922] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8180.6520 2455921723.6576 498.0000
[2019-04-27 19:46:54,068] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8664.2044 2255698187.1233 382.0000
[2019-04-27 19:46:54,098] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8761.0273 2204348893.0187 445.0000
[2019-04-27 19:46:54,200] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8934.6163 2125400956.0850 386.0000
[2019-04-27 19:46:55,217] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 1075000, evaluation results [1075000.0, 8180.652022076617, 2455921723.657595, 498.0, 8812.705827261534, 2174521861.9094067, 415.0, 8934.616291145066, 2125400956.0850298, 386.0, 8664.204386778883, 2255698187.1233387, 382.0, 8761.027256234765, 2204348893.0187097, 445.0]
[2019-04-27 19:46:56,383] A3C_AGENT_WORKER-Thread-3 INFO:Local step 67500, global step 1075552: loss 0.7898
[2019-04-27 19:46:56,385] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 67500, global step 1075552: learning rate 0.0001
[2019-04-27 19:46:59,360] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.5951711e-26 9.9999464e-01 3.7496057e-21 6.1956846e-26 5.3416234e-06], sum to 1.0000
[2019-04-27 19:46:59,369] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5338
[2019-04-27 19:46:59,376] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333334, 44.0, 1.0, 2.0, 0.3586482503421252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 450508.1208305991, 450508.1208305991, 123061.5723934116], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 598800.0000, 
sim time next is 599400.0000, 
raw observation next is [26.65, 44.5, 1.0, 2.0, 0.3556900625969951, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 447108.1961736394, 447108.1961736389, 122671.1236002641], 
processed observation next is [1.0, 0.9565217391304348, 0.5425925925925925, 0.445, 1.0, 1.0, 0.23296436023451797, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.15968149863344264, 0.15968149863344247, 0.2359060069235848], 
reward next is 0.7641, 
noisyNet noise sample is [array([0.37452438], dtype=float32), -0.066091575]. 
=============================================
[2019-04-27 19:47:04,507] A3C_AGENT_WORKER-Thread-19 INFO:Local step 68000, global step 1079384: loss 0.2536
[2019-04-27 19:47:04,509] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 68000, global step 1079384: learning rate 0.0001
[2019-04-27 19:47:06,630] A3C_AGENT_WORKER-Thread-16 INFO:Local step 67500, global step 1080502: loss 2.3653
[2019-04-27 19:47:06,631] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 67500, global step 1080503: learning rate 0.0001
[2019-04-27 19:47:07,109] A3C_AGENT_WORKER-Thread-17 INFO:Local step 67500, global step 1080753: loss 4.3251
[2019-04-27 19:47:07,112] A3C_AGENT_WORKER-Thread-20 INFO:Local step 67500, global step 1080754: loss 6.1731
[2019-04-27 19:47:07,112] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 67500, global step 1080754: learning rate 0.0001
[2019-04-27 19:47:07,114] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 67500, global step 1080756: learning rate 0.0001
[2019-04-27 19:47:07,147] A3C_AGENT_WORKER-Thread-9 INFO:Local step 67500, global step 1080771: loss 6.1133
[2019-04-27 19:47:07,147] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 67500, global step 1080771: learning rate 0.0001
[2019-04-27 19:47:07,220] A3C_AGENT_WORKER-Thread-2 INFO:Local step 67500, global step 1080809: loss 8.2756
[2019-04-27 19:47:07,221] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 67500, global step 1080809: learning rate 0.0001
[2019-04-27 19:47:07,352] A3C_AGENT_WORKER-Thread-18 INFO:Local step 67500, global step 1080878: loss 6.2873
[2019-04-27 19:47:07,355] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 67500, global step 1080878: learning rate 0.0001
[2019-04-27 19:47:07,489] A3C_AGENT_WORKER-Thread-12 INFO:Local step 67500, global step 1080948: loss 5.7552
[2019-04-27 19:47:07,495] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 67500, global step 1080951: learning rate 0.0001
[2019-04-27 19:47:07,590] A3C_AGENT_WORKER-Thread-15 INFO:Local step 67500, global step 1081007: loss 3.3396
[2019-04-27 19:47:07,592] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 67500, global step 1081007: learning rate 0.0001
[2019-04-27 19:47:07,598] A3C_AGENT_WORKER-Thread-11 INFO:Local step 67500, global step 1081009: loss -0.0511
[2019-04-27 19:47:07,600] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 67500, global step 1081009: learning rate 0.0001
[2019-04-27 19:47:07,689] A3C_AGENT_WORKER-Thread-10 INFO:Local step 67500, global step 1081059: loss 6.1949
[2019-04-27 19:47:07,691] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 67500, global step 1081060: learning rate 0.0001
[2019-04-27 19:47:07,703] A3C_AGENT_WORKER-Thread-13 INFO:Local step 67500, global step 1081066: loss 2.8940
[2019-04-27 19:47:07,705] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 67500, global step 1081066: learning rate 0.0001
[2019-04-27 19:47:07,741] A3C_AGENT_WORKER-Thread-22 INFO:Local step 67500, global step 1081085: loss 4.2223
[2019-04-27 19:47:07,743] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 67500, global step 1081085: learning rate 0.0001
[2019-04-27 19:47:07,874] A3C_AGENT_WORKER-Thread-14 INFO:Local step 67500, global step 1081154: loss 4.4938
[2019-04-27 19:47:07,876] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 67500, global step 1081154: learning rate 0.0001
[2019-04-27 19:47:07,887] A3C_AGENT_WORKER-Thread-21 INFO:Local step 67500, global step 1081160: loss 5.4205
[2019-04-27 19:47:07,889] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 67500, global step 1081160: learning rate 0.0001
[2019-04-27 19:47:11,777] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.6922999e-35 1.0000000e+00 1.2407415e-31 8.1656879e-38 6.5692285e-23], sum to 1.0000
[2019-04-27 19:47:11,785] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1863
[2019-04-27 19:47:11,791] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.25, 46.83333333333334, 1.0, 2.0, 0.3929010347650196, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 488175.4217870397, 488175.4217870397, 127653.1244679115], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 853800.0000, 
sim time next is 854400.0000, 
raw observation next is [27.1, 47.66666666666667, 1.0, 2.0, 0.3954460906278995, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 491146.1886505549, 491146.1886505549, 128005.1723881358], 
processed observation next is [0.0, 0.9130434782608695, 0.5592592592592593, 0.47666666666666674, 1.0, 1.0, 0.28029296503321366, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1754093530894839, 0.1754093530894839, 0.24616379305410732], 
reward next is 0.7538, 
noisyNet noise sample is [array([-0.08781805], dtype=float32), -0.27505147]. 
=============================================
[2019-04-27 19:47:12,458] A3C_AGENT_WORKER-Thread-3 INFO:Local step 68000, global step 1083473: loss 0.2766
[2019-04-27 19:47:12,461] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 68000, global step 1083473: learning rate 0.0001
[2019-04-27 19:47:18,540] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 19:47:18,546] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3584
[2019-04-27 19:47:18,553] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.75, 60.5, 1.0, 2.0, 0.2595513449419527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 334777.3806723494, 334777.3806723494, 110710.4662628514], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 970200.0000, 
sim time next is 970800.0000, 
raw observation next is [20.86666666666667, 60.33333333333334, 1.0, 2.0, 0.2745177893365329, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 353894.1764660525, 353894.176466052, 112473.4190715814], 
processed observation next is [1.0, 0.21739130434782608, 0.3283950617283952, 0.6033333333333334, 1.0, 1.0, 0.13633070159111063, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.12639077730930445, 0.1263907773093043, 0.21629503667611807], 
reward next is 0.7837, 
noisyNet noise sample is [array([-1.9903682], dtype=float32), -0.98186064]. 
=============================================
[2019-04-27 19:47:19,519] A3C_AGENT_WORKER-Thread-19 INFO:Local step 68500, global step 1087352: loss 0.3011
[2019-04-27 19:47:19,522] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 68500, global step 1087353: learning rate 0.0001
[2019-04-27 19:47:21,757] A3C_AGENT_WORKER-Thread-16 INFO:Local step 68000, global step 1088520: loss 0.1678
[2019-04-27 19:47:21,762] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 68000, global step 1088521: learning rate 0.0001
[2019-04-27 19:47:22,019] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.5002783e-38 1.0000000e+00 9.9681297e-37 0.0000000e+00 1.6427266e-29], sum to 1.0000
[2019-04-27 19:47:22,026] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5407
[2019-04-27 19:47:22,030] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 62.5, 1.0, 2.0, 0.2948440895375151, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 376676.6291069136, 376676.6291069136, 114941.7890576327], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1042200.0000, 
sim time next is 1042800.0000, 
raw observation next is [21.5, 63.33333333333333, 1.0, 2.0, 0.2963253524574443, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 378461.6701397922, 378461.6701397922, 115123.4635584812], 
processed observation next is [1.0, 0.043478260869565216, 0.35185185185185186, 0.6333333333333333, 1.0, 1.0, 0.16229208625886227, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.13516488219278294, 0.13516488219278294, 0.22139127607400233], 
reward next is 0.7786, 
noisyNet noise sample is [array([-0.50289536], dtype=float32), 1.4110005]. 
=============================================
[2019-04-27 19:47:22,117] A3C_AGENT_WORKER-Thread-17 INFO:Local step 68000, global step 1088705: loss 0.2567
[2019-04-27 19:47:22,119] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 68000, global step 1088706: learning rate 0.0001
[2019-04-27 19:47:22,167] A3C_AGENT_WORKER-Thread-2 INFO:Local step 68000, global step 1088733: loss 0.3786
[2019-04-27 19:47:22,172] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 68000, global step 1088735: learning rate 0.0001
[2019-04-27 19:47:22,202] A3C_AGENT_WORKER-Thread-20 INFO:Local step 68000, global step 1088753: loss 0.3889
[2019-04-27 19:47:22,203] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 68000, global step 1088753: learning rate 0.0001
[2019-04-27 19:47:22,242] A3C_AGENT_WORKER-Thread-9 INFO:Local step 68000, global step 1088774: loss 0.4852
[2019-04-27 19:47:22,245] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 68000, global step 1088776: learning rate 0.0001
[2019-04-27 19:47:22,484] A3C_AGENT_WORKER-Thread-18 INFO:Local step 68000, global step 1088905: loss 0.3210
[2019-04-27 19:47:22,485] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 68000, global step 1088906: learning rate 0.0001
[2019-04-27 19:47:22,602] A3C_AGENT_WORKER-Thread-11 INFO:Local step 68000, global step 1088972: loss 0.2775
[2019-04-27 19:47:22,604] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 68000, global step 1088972: learning rate 0.0001
[2019-04-27 19:47:22,626] A3C_AGENT_WORKER-Thread-12 INFO:Local step 68000, global step 1088986: loss 0.2382
[2019-04-27 19:47:22,627] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 68000, global step 1088986: learning rate 0.0001
[2019-04-27 19:47:22,682] A3C_AGENT_WORKER-Thread-15 INFO:Local step 68000, global step 1089012: loss 0.2272
[2019-04-27 19:47:22,683] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 68000, global step 1089012: learning rate 0.0001
[2019-04-27 19:47:22,723] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.4948888e-36], sum to 1.0000
[2019-04-27 19:47:22,732] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2959
[2019-04-27 19:47:22,734] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.46666666666667, 70.66666666666667, 1.0, 2.0, 0.3356264452395879, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 428296.3629871624, 428296.362987162, 120090.8060287826], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1052400.0000, 
sim time next is 1053000.0000, 
raw observation next is [20.4, 71.0, 1.0, 2.0, 0.3341257590100143, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 426468.4853504596, 426468.4853504596, 119896.2784113687], 
processed observation next is [1.0, 0.17391304347826086, 0.31111111111111106, 0.71, 1.0, 1.0, 0.20729257025001702, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.15231017333944985, 0.15231017333944985, 0.23056976617570904], 
reward next is 0.7694, 
noisyNet noise sample is [array([2.9186177], dtype=float32), 0.9262828]. 
=============================================
[2019-04-27 19:47:22,745] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[68.85558]
 [68.96562]
 [69.2588 ]
 [69.37121]
 [69.48453]], R is [[68.86940765]
 [68.9497757 ]
 [69.02476501]
 [69.1074295 ]
 [69.18955231]].
[2019-04-27 19:47:22,815] A3C_AGENT_WORKER-Thread-13 INFO:Local step 68000, global step 1089084: loss 0.1785
[2019-04-27 19:47:22,817] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 68000, global step 1089084: learning rate 0.0001
[2019-04-27 19:47:22,845] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.9116766e-30 1.0000000e+00 1.4924333e-26 5.4993575e-31 4.2720487e-18], sum to 1.0000
[2019-04-27 19:47:22,852] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8486
[2019-04-27 19:47:22,855] A3C_AGENT_WORKER-Thread-10 INFO:Local step 68000, global step 1089101: loss 0.1682
[2019-04-27 19:47:22,857] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 68000, global step 1089101: learning rate 0.0001
[2019-04-27 19:47:22,858] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.4, 55.33333333333334, 1.0, 2.0, 0.5985596647207719, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 759693.7211377056, 759693.7211377051, 160393.8681826965], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1070400.0000, 
sim time next is 1071000.0000, 
raw observation next is [23.5, 55.0, 1.0, 2.0, 0.6034466180397021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 765630.8559611051, 765630.8559611051, 161257.6547895322], 
processed observation next is [1.0, 0.391304347826087, 0.42592592592592593, 0.55, 1.0, 1.0, 0.5279126405234549, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2734395914146804, 0.2734395914146804, 0.3101108745952542], 
reward next is 0.6899, 
noisyNet noise sample is [array([-0.56820196], dtype=float32), 1.1896807]. 
=============================================
[2019-04-27 19:47:22,869] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[64.39634 ]
 [64.477104]
 [64.79609 ]
 [65.203026]
 [65.46174 ]], R is [[64.35746765]
 [64.40544128]
 [64.44332886]
 [64.49182129]
 [64.55281067]].
[2019-04-27 19:47:22,896] A3C_AGENT_WORKER-Thread-22 INFO:Local step 68000, global step 1089118: loss 0.1657
[2019-04-27 19:47:22,898] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 68000, global step 1089119: learning rate 0.0001
[2019-04-27 19:47:22,988] A3C_AGENT_WORKER-Thread-21 INFO:Local step 68000, global step 1089169: loss 0.1837
[2019-04-27 19:47:22,990] A3C_AGENT_WORKER-Thread-14 INFO:Local step 68000, global step 1089169: loss 0.2706
[2019-04-27 19:47:22,992] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 68000, global step 1089169: learning rate 0.0001
[2019-04-27 19:47:22,993] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 68000, global step 1089169: learning rate 0.0001
[2019-04-27 19:47:28,090] A3C_AGENT_WORKER-Thread-3 INFO:Local step 68500, global step 1091473: loss 0.1353
[2019-04-27 19:47:28,092] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 68500, global step 1091473: learning rate 0.0001
[2019-04-27 19:47:35,298] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 19:47:35,307] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9506
[2019-04-27 19:47:35,310] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.98333333333333, 86.0, 1.0, 2.0, 0.3614184378642232, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 453589.4943546316, 453589.4943546316, 123426.8805597462], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1299000.0000, 
sim time next is 1299600.0000, 
raw observation next is [19.9, 86.0, 1.0, 2.0, 0.3587415359463513, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 450686.5870899601, 450686.5870899605, 123074.9545565197], 
processed observation next is [1.0, 0.043478260869565216, 0.2925925925925925, 0.86, 1.0, 1.0, 0.23659706660279914, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.16095949538927146, 0.16095949538927162, 0.23668260491638404], 
reward next is 0.7633, 
noisyNet noise sample is [array([0.02186024], dtype=float32), 0.75796384]. 
=============================================
[2019-04-27 19:47:35,322] A3C_AGENT_WORKER-Thread-19 INFO:Local step 69000, global step 1095266: loss 0.0127
[2019-04-27 19:47:35,324] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 69000, global step 1095266: learning rate 0.0001
[2019-04-27 19:47:36,945] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 19:47:36,951] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6818
[2019-04-27 19:47:36,955] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.63333333333333, 82.66666666666667, 1.0, 2.0, 0.3647044103958714, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 456783.0438562738, 456783.0438562743, 123854.3163785518], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1320000.0000, 
sim time next is 1320600.0000, 
raw observation next is [20.86666666666667, 81.83333333333333, 1.0, 2.0, 0.3695342025741535, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 462107.2071783562, 462107.2071783566, 124495.2287621833], 
processed observation next is [1.0, 0.2608695652173913, 0.3283950617283952, 0.8183333333333332, 1.0, 1.0, 0.24944547925494462, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.16503828827798436, 0.1650382882779845, 0.2394139014657371], 
reward next is 0.7606, 
noisyNet noise sample is [array([0.7292046], dtype=float32), 0.31117776]. 
=============================================
[2019-04-27 19:47:37,383] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 19:47:37,391] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3717
[2019-04-27 19:47:37,397] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.1, 81.0, 1.0, 2.0, 0.3677332372885538, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 459138.6487449835, 459138.6487449835, 124238.1400699036], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1321200.0000, 
sim time next is 1321800.0000, 
raw observation next is [21.33333333333334, 80.0, 1.0, 2.0, 0.3923893187806749, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 489247.2450425317, 489247.2450425317, 127616.2431529367], 
processed observation next is [1.0, 0.30434782608695654, 0.3456790123456792, 0.8, 1.0, 1.0, 0.2766539509293749, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.17473115894376132, 0.17473115894376132, 0.24541585221718595], 
reward next is 0.7546, 
noisyNet noise sample is [array([0.8966164], dtype=float32), -1.6596037]. 
=============================================
[2019-04-27 19:47:37,776] A3C_AGENT_WORKER-Thread-16 INFO:Local step 68500, global step 1096516: loss 0.0812
[2019-04-27 19:47:37,778] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 68500, global step 1096516: learning rate 0.0001
[2019-04-27 19:47:38,104] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.9953410e-36 1.0000000e+00 6.4783588e-34 5.7060761e-37 4.8487446e-23], sum to 1.0000
[2019-04-27 19:47:38,117] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7645
[2019-04-27 19:47:38,124] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.55, 67.5, 1.0, 2.0, 0.4163875744128235, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 516003.2330807519, 516003.2330807514, 130953.5173396881], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1549800.0000, 
sim time next is 1550400.0000, 
raw observation next is [23.53333333333333, 66.66666666666666, 1.0, 2.0, 0.4107015315999783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 509926.3337581342, 509926.3337581342, 130159.9861910687], 
processed observation next is [0.0, 0.9565217391304348, 0.4271604938271604, 0.6666666666666665, 1.0, 1.0, 0.2984542042856885, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.18211654777076222, 0.18211654777076222, 0.2503076657520552], 
reward next is 0.7497, 
noisyNet noise sample is [array([-0.16842687], dtype=float32), -1.6584392]. 
=============================================
[2019-04-27 19:47:38,235] A3C_AGENT_WORKER-Thread-17 INFO:Local step 68500, global step 1096755: loss 1.3359
[2019-04-27 19:47:38,238] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 68500, global step 1096755: learning rate 0.0001
[2019-04-27 19:47:38,280] A3C_AGENT_WORKER-Thread-2 INFO:Local step 68500, global step 1096777: loss -22.2773
[2019-04-27 19:47:38,283] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 68500, global step 1096780: learning rate 0.0001
[2019-04-27 19:47:38,308] A3C_AGENT_WORKER-Thread-9 INFO:Local step 68500, global step 1096791: loss -117.6821
[2019-04-27 19:47:38,313] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 68500, global step 1096791: learning rate 0.0001
[2019-04-27 19:47:38,346] A3C_AGENT_WORKER-Thread-20 INFO:Local step 68500, global step 1096810: loss 1.0049
[2019-04-27 19:47:38,348] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 68500, global step 1096811: learning rate 0.0001
[2019-04-27 19:47:38,473] A3C_AGENT_WORKER-Thread-18 INFO:Local step 68500, global step 1096876: loss 0.0467
[2019-04-27 19:47:38,474] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 68500, global step 1096876: learning rate 0.0001
[2019-04-27 19:47:38,507] A3C_AGENT_WORKER-Thread-11 INFO:Local step 68500, global step 1096893: loss 0.0265
[2019-04-27 19:47:38,510] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 68500, global step 1096893: learning rate 0.0001
[2019-04-27 19:47:38,596] A3C_AGENT_WORKER-Thread-12 INFO:Local step 68500, global step 1096944: loss 49.0852
[2019-04-27 19:47:38,597] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 68500, global step 1096945: learning rate 0.0001
[2019-04-27 19:47:38,696] A3C_AGENT_WORKER-Thread-15 INFO:Local step 68500, global step 1096998: loss 0.0769
[2019-04-27 19:47:38,698] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 68500, global step 1096998: learning rate 0.0001
[2019-04-27 19:47:38,803] A3C_AGENT_WORKER-Thread-22 INFO:Local step 68500, global step 1097054: loss 0.0382
[2019-04-27 19:47:38,810] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 68500, global step 1097056: learning rate 0.0001
[2019-04-27 19:47:38,879] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.5891663e-27 2.2543391e-11 4.1805829e-22 2.6748100e-24 1.0000000e+00], sum to 1.0000
[2019-04-27 19:47:38,880] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7511
[2019-04-27 19:47:38,885] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.98333333333333, 28.33333333333334, 1.0, 2.0, 0.37517725888384, 1.0, 2.0, 0.37517725888384, 1.0, 2.0, 0.6117059488538791, 6.9112, 6.9112, 121.94756008, 1370694.092785976, 1370694.092785976, 285328.5117109562], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1356600.0000, 
sim time next is 1357200.0000, 
raw observation next is [31.0, 28.0, 1.0, 2.0, 0.3684718759336908, 1.0, 2.0, 0.3684718759336908, 1.0, 2.0, 0.6011502012669445, 6.9112, 6.9112, 121.94756008, 1347226.884721697, 1347226.884721697, 282451.0699089104], 
processed observation next is [1.0, 0.7391304347826086, 0.7037037037037037, 0.28, 1.0, 1.0, 0.24818080468296524, 1.0, 1.0, 0.24818080468296524, 1.0, 1.0, 0.5014377515836806, 0.0, 0.0, 0.8096049824067558, 0.4811524588291775, 0.4811524588291775, 0.5431751344402123], 
reward next is 0.4568, 
noisyNet noise sample is [array([-0.3977943], dtype=float32), -0.63601124]. 
=============================================
[2019-04-27 19:47:38,887] A3C_AGENT_WORKER-Thread-13 INFO:Local step 68500, global step 1097094: loss 30.4135
[2019-04-27 19:47:38,890] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 68500, global step 1097096: learning rate 0.0001
[2019-04-27 19:47:38,918] A3C_AGENT_WORKER-Thread-10 INFO:Local step 68500, global step 1097109: loss 0.0471
[2019-04-27 19:47:38,923] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 68500, global step 1097111: learning rate 0.0001
[2019-04-27 19:47:39,078] A3C_AGENT_WORKER-Thread-21 INFO:Local step 68500, global step 1097194: loss 0.4422
[2019-04-27 19:47:39,081] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 68500, global step 1097195: learning rate 0.0001
[2019-04-27 19:47:39,114] A3C_AGENT_WORKER-Thread-14 INFO:Local step 68500, global step 1097212: loss 0.1487
[2019-04-27 19:47:39,116] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 68500, global step 1097212: learning rate 0.0001
[2019-04-27 19:47:39,953] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.5695359e-30 1.0000000e+00 1.2068949e-27 1.9270652e-30 1.9031181e-15], sum to 1.0000
[2019-04-27 19:47:39,960] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2219
[2019-04-27 19:47:39,965] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.08333333333334, 67.0, 1.0, 2.0, 0.3430470416954682, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 433109.2388413184, 433109.2388413184, 121024.7425471307], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1381800.0000, 
sim time next is 1382400.0000, 
raw observation next is [22.0, 67.0, 1.0, 2.0, 0.3395088317936186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 429022.0081478792, 429022.0081478792, 120566.18890507], 
processed observation next is [0.0, 0.0, 0.37037037037037035, 0.67, 1.0, 1.0, 0.21370099023049838, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.15322214576709972, 0.15322214576709972, 0.23185805558667308], 
reward next is 0.7681, 
noisyNet noise sample is [array([0.24478976], dtype=float32), 0.019108944]. 
=============================================
[2019-04-27 19:47:43,511] A3C_AGENT_WORKER-Thread-3 INFO:Local step 69000, global step 1099521: loss 0.2442
[2019-04-27 19:47:43,513] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 69000, global step 1099522: learning rate 0.0001
[2019-04-27 19:47:44,427] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-04-27 19:47:44,428] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 19:47:44,429] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 19:47:44,430] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:47:44,431] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:47:44,431] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 19:47:44,432] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:47:44,433] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 19:47:44,435] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:47:44,435] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 19:47:44,436] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:47:44,455] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run45
[2019-04-27 19:47:44,477] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run45
[2019-04-27 19:47:44,479] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run45
[2019-04-27 19:47:44,512] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run45
[2019-04-27 19:47:44,543] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run45
[2019-04-27 19:47:50,067] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04135628], dtype=float32), 0.0025051148]
[2019-04-27 19:47:50,069] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [20.82398780666667, 56.12506744500001, 1.0, 2.0, 0.2558076635282051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 329971.3917525537, 329971.3917525537, 102475.9309787331]
[2019-04-27 19:47:50,070] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:47:50,074] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.540990897919225
[2019-04-27 19:47:58,302] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04135628], dtype=float32), 0.0025051148]
[2019-04-27 19:47:58,303] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [19.66666666666667, 59.0, 1.0, 2.0, 0.2229367713906073, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 287562.709931296, 287562.709931296, 91658.54452025022]
[2019-04-27 19:47:58,305] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 19:47:58,308] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.1543456917591649
[2019-04-27 19:48:14,595] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04135628], dtype=float32), 0.0025051148]
[2019-04-27 19:48:14,597] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [31.8, 38.66666666666667, 1.0, 2.0, 0.6125101699015677, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 737998.0832924923, 737998.0832924923, 161957.3138689009]
[2019-04-27 19:48:14,600] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:48:14,603] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [8.4818152e-37 1.0000000e+00 1.0743601e-35 0.0000000e+00 4.1126666e-27], sampled 0.6283651511211462
[2019-04-27 19:48:22,856] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04135628], dtype=float32), 0.0025051148]
[2019-04-27 19:48:22,857] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [24.66666666666666, 89.0, 1.0, 2.0, 0.5974080983985894, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9510931488740095, 6.911200000000001, 6.9112, 121.9256167661982, 1362253.802312081, 1362253.80231208, 292851.485559107]
[2019-04-27 19:48:22,858] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:48:22,860] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [5.6507894e-32 1.0000000e+00 2.0978508e-30 2.7765155e-33 1.0661493e-21], sampled 0.6671502886215596
[2019-04-27 19:48:22,863] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1362253.802312081 W.
[2019-04-27 19:48:43,435] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04135628], dtype=float32), 0.0025051148]
[2019-04-27 19:48:43,436] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [37.23445698833333, 35.98644228416666, 1.0, 2.0, 0.6131080136357313, 1.0, 2.0, 0.6131080136357313, 1.0, 2.0, 0.9760879252420822, 6.911199999999999, 6.9112, 121.94756008, 2097986.252840552, 2097986.252840552, 402563.7591187353]
[2019-04-27 19:48:43,438] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:48:43,441] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [2.1338529e-33 9.6489788e-25 1.4649489e-23 1.3445870e-27 1.0000000e+00], sampled 0.6363833477097427
[2019-04-27 19:48:46,170] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04135628], dtype=float32), 0.0025051148]
[2019-04-27 19:48:46,172] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [26.651833335, 78.990998925, 1.0, 2.0, 0.5929983379770837, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 680220.7672178983, 680220.7672178978, 157096.5464750405]
[2019-04-27 19:48:46,173] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 19:48:46,177] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.7393588e-35], sampled 0.4985756786482106
[2019-04-27 19:48:47,017] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04135628], dtype=float32), 0.0025051148]
[2019-04-27 19:48:47,019] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [31.99548457833334, 62.42967392333333, 1.0, 2.0, 0.7192919883754867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 819801.1454372347, 819801.1454372347, 179834.7102951105]
[2019-04-27 19:48:47,019] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:48:47,022] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [3.4712235e-37 1.0000000e+00 5.2138491e-36 0.0000000e+00 2.0707811e-27], sampled 0.11311786067087326
[2019-04-27 19:48:55,232] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04135628], dtype=float32), 0.0025051148]
[2019-04-27 19:48:55,234] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [26.80740066, 85.308893, 1.0, 2.0, 0.7006101952524426, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 798497.789758176, 798497.789758176, 176259.8694874218]
[2019-04-27 19:48:55,235] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:48:55,238] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.3179394e-31], sampled 0.4199001319309219
[2019-04-27 19:48:57,490] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04135628], dtype=float32), 0.0025051148]
[2019-04-27 19:48:57,492] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [22.187814255, 98.53516254499999, 1.0, 2.0, 0.5481972353626713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 650352.7012998586, 650352.7012998586, 150539.2913199909]
[2019-04-27 19:48:57,494] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:48:57,499] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8143717e-34], sampled 0.2905290417990196
[2019-04-27 19:49:00,070] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04135628], dtype=float32), 0.0025051148]
[2019-04-27 19:49:00,072] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [27.16666666666666, 80.16666666666667, 1.0, 2.0, 0.5421932043630487, 1.0, 2.0, 0.5421932043630487, 1.0, 2.0, 0.8631892393458707, 6.9112, 6.9112, 121.94756008, 1855071.810744067, 1855071.810744067, 364780.3041788157]
[2019-04-27 19:49:00,073] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:49:00,075] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.1328818e-22 2.8786910e-06 2.8458250e-16 7.9988850e-20 9.9999714e-01], sampled 0.42653117785721884
[2019-04-27 19:49:35,190] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04135628], dtype=float32), 0.0025051148]
[2019-04-27 19:49:35,191] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [20.2, 87.5, 1.0, 2.0, 0.396583633784572, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 495147.9034393002, 495147.9034393002, 128216.2464715692]
[2019-04-27 19:49:35,194] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:49:35,198] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.23303824927220718
[2019-04-27 19:49:35,486] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8813.5737 2174130576.9274 416.0000
[2019-04-27 19:49:35,572] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8932.5694 2125153171.8500 388.0000
[2019-04-27 19:49:35,871] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8647.6250 2254071854.5626 410.0000
[2019-04-27 19:49:35,887] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8170.5916 2453998030.3180 522.0000
[2019-04-27 19:49:35,890] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8752.1475 2203803601.9005 456.0000
[2019-04-27 19:49:36,906] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 1100000, evaluation results [1100000.0, 8170.591584264188, 2453998030.318033, 522.0, 8813.573722391575, 2174130576.9273667, 416.0, 8932.56944496791, 2125153171.8499854, 388.0, 8647.625049357592, 2254071854.5626135, 410.0, 8752.147469114116, 2203803601.9004726, 456.0]
[2019-04-27 19:49:37,092] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 19:49:37,097] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3497
[2019-04-27 19:49:37,103] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.4, 63.0, 1.0, 2.0, 0.3326222642079921, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 421497.0749684218, 421497.0749684214, 119683.5262104917], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1479600.0000, 
sim time next is 1480200.0000, 
raw observation next is [22.3, 63.83333333333333, 1.0, 2.0, 0.3346001137098937, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 423835.8588279076, 423835.8588279076, 119938.1200779544], 
processed observation next is [0.0, 0.13043478260869565, 0.38148148148148153, 0.6383333333333333, 1.0, 1.0, 0.2078572782260639, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.15136994958139557, 0.15136994958139557, 0.23065023091914308], 
reward next is 0.7693, 
noisyNet noise sample is [array([-0.22891638], dtype=float32), 1.0590378]. 
=============================================
[2019-04-27 19:49:37,919] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.9416317e-37 1.0000000e+00 7.3060698e-36 0.0000000e+00 1.3745004e-34], sum to 1.0000
[2019-04-27 19:49:37,927] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6327
[2019-04-27 19:49:37,931] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.2, 58.0, 1.0, 2.0, 0.3941100691738396, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 488804.5264293347, 488804.5264293347, 127803.4189010594], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1494000.0000, 
sim time next is 1494600.0000, 
raw observation next is [25.51666666666667, 56.83333333333334, 1.0, 2.0, 0.3983873913752171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 493460.5977980687, 493460.5977980682, 128388.7979225982], 
processed observation next is [0.0, 0.30434782608695654, 0.5006172839506173, 0.5683333333333335, 1.0, 1.0, 0.28379451354192514, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.17623592778502453, 0.17623592778502437, 0.24690153446653498], 
reward next is 0.7531, 
noisyNet noise sample is [array([-0.2846354], dtype=float32), -0.2567927]. 
=============================================
[2019-04-27 19:49:43,885] A3C_AGENT_WORKER-Thread-19 INFO:Local step 69500, global step 1103322: loss 0.8809
[2019-04-27 19:49:43,890] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 69500, global step 1103322: learning rate 0.0001
[2019-04-27 19:49:43,997] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2200110e-23 1.0000000e+00 3.0277389e-20 8.7576261e-24 1.0118032e-11], sum to 1.0000
[2019-04-27 19:49:44,006] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5550
[2019-04-27 19:49:44,016] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1340678.678472953 W.
[2019-04-27 19:49:44,022] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [24.53333333333333, 56.66666666666667, 1.0, 2.0, 0.9283891748720788, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.26086351454961, 6.9112, 121.9244223533735, 1340678.678472953, 1161622.085949005, 228137.9952638324], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1588800.0000, 
sim time next is 1589400.0000, 
raw observation next is [24.65, 56.5, 1.0, 2.0, 0.5013900880312536, 1.0, 1.0, 0.5013900880312536, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9258320196252, 1233639.370607976, 1233639.370607976, 239326.8686936322], 
processed observation next is [1.0, 0.391304347826087, 0.46851851851851845, 0.565, 1.0, 1.0, 0.4064167714657781, 1.0, 0.5, 0.4064167714657781, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094607306814574, 0.4405854895028486, 0.4405854895028486, 0.460243978256985], 
reward next is 0.5398, 
noisyNet noise sample is [array([-0.6307201], dtype=float32), 0.6084988]. 
=============================================
[2019-04-27 19:49:46,528] A3C_AGENT_WORKER-Thread-16 INFO:Local step 69000, global step 1104567: loss 0.0701
[2019-04-27 19:49:46,533] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 69000, global step 1104567: learning rate 0.0001
[2019-04-27 19:49:46,949] A3C_AGENT_WORKER-Thread-2 INFO:Local step 69000, global step 1104770: loss 0.0579
[2019-04-27 19:49:46,950] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 69000, global step 1104770: learning rate 0.0001
[2019-04-27 19:49:46,979] A3C_AGENT_WORKER-Thread-9 INFO:Local step 69000, global step 1104786: loss 0.0538
[2019-04-27 19:49:46,985] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 69000, global step 1104787: learning rate 0.0001
[2019-04-27 19:49:47,050] A3C_AGENT_WORKER-Thread-20 INFO:Local step 69000, global step 1104815: loss 0.0586
[2019-04-27 19:49:47,054] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 69000, global step 1104817: learning rate 0.0001
[2019-04-27 19:49:47,092] A3C_AGENT_WORKER-Thread-17 INFO:Local step 69000, global step 1104833: loss 0.0797
[2019-04-27 19:49:47,095] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 69000, global step 1104833: learning rate 0.0001
[2019-04-27 19:49:47,099] A3C_AGENT_WORKER-Thread-18 INFO:Local step 69000, global step 1104835: loss 0.1256
[2019-04-27 19:49:47,105] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 69000, global step 1104839: learning rate 0.0001
[2019-04-27 19:49:47,268] A3C_AGENT_WORKER-Thread-11 INFO:Local step 69000, global step 1104897: loss 0.0297
[2019-04-27 19:49:47,274] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 69000, global step 1104899: learning rate 0.0001
[2019-04-27 19:49:47,352] A3C_AGENT_WORKER-Thread-12 INFO:Local step 69000, global step 1104933: loss 0.0354
[2019-04-27 19:49:47,355] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 69000, global step 1104933: learning rate 0.0001
[2019-04-27 19:49:47,447] A3C_AGENT_WORKER-Thread-15 INFO:Local step 69000, global step 1104963: loss 0.0062
[2019-04-27 19:49:47,449] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 69000, global step 1104963: learning rate 0.0001
[2019-04-27 19:49:47,563] A3C_AGENT_WORKER-Thread-22 INFO:Local step 69000, global step 1105014: loss 0.0739
[2019-04-27 19:49:47,569] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 69000, global step 1105015: learning rate 0.0001
[2019-04-27 19:49:47,696] A3C_AGENT_WORKER-Thread-13 INFO:Local step 69000, global step 1105079: loss 0.0236
[2019-04-27 19:49:47,700] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 69000, global step 1105079: learning rate 0.0001
[2019-04-27 19:49:47,739] A3C_AGENT_WORKER-Thread-10 INFO:Local step 69000, global step 1105092: loss 0.0683
[2019-04-27 19:49:47,741] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 69000, global step 1105093: learning rate 0.0001
[2019-04-27 19:49:47,931] A3C_AGENT_WORKER-Thread-14 INFO:Local step 69000, global step 1105168: loss 0.0035
[2019-04-27 19:49:47,933] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 69000, global step 1105168: learning rate 0.0001
[2019-04-27 19:49:48,058] A3C_AGENT_WORKER-Thread-21 INFO:Local step 69000, global step 1105228: loss 0.0538
[2019-04-27 19:49:48,062] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 69000, global step 1105229: learning rate 0.0001
[2019-04-27 19:49:52,509] A3C_AGENT_WORKER-Thread-3 INFO:Local step 69500, global step 1107569: loss 0.6315
[2019-04-27 19:49:52,511] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 69500, global step 1107569: learning rate 0.0001
[2019-04-27 19:49:58,260] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.5631630e-24 9.9999964e-01 2.6582023e-21 6.3017670e-25 3.0862972e-07], sum to 1.0000
[2019-04-27 19:49:58,269] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9727
[2019-04-27 19:49:58,277] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.81666666666667, 83.66666666666666, 1.0, 2.0, 0.7742189989889957, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 951331.3442675844, 951331.3442675844, 193570.2054619326], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1864200.0000, 
sim time next is 1864800.0000, 
raw observation next is [21.8, 84.0, 1.0, 2.0, 0.7446317800083466, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 914628.1381730991, 914628.1381730991, 187512.5707800933], 
processed observation next is [1.0, 0.6086956521739131, 0.362962962962963, 0.84, 1.0, 1.0, 0.6959902142956508, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3266529064903925, 0.3266529064903925, 0.36060109765402554], 
reward next is 0.6394, 
noisyNet noise sample is [array([-0.24925777], dtype=float32), 0.65277284]. 
=============================================
[2019-04-27 19:49:59,433] A3C_AGENT_WORKER-Thread-19 INFO:Local step 70000, global step 1111215: loss 0.1130
[2019-04-27 19:49:59,435] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 70000, global step 1111215: learning rate 0.0001
[2019-04-27 19:50:01,950] A3C_AGENT_WORKER-Thread-16 INFO:Local step 69500, global step 1112537: loss 0.0206
[2019-04-27 19:50:01,952] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 69500, global step 1112537: learning rate 0.0001
[2019-04-27 19:50:02,392] A3C_AGENT_WORKER-Thread-18 INFO:Local step 69500, global step 1112761: loss 0.6483
[2019-04-27 19:50:02,395] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 69500, global step 1112761: learning rate 0.0001
[2019-04-27 19:50:02,490] A3C_AGENT_WORKER-Thread-2 INFO:Local step 69500, global step 1112814: loss 0.5398
[2019-04-27 19:50:02,492] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 69500, global step 1112814: learning rate 0.0001
[2019-04-27 19:50:02,522] A3C_AGENT_WORKER-Thread-20 INFO:Local step 69500, global step 1112829: loss 0.2119
[2019-04-27 19:50:02,525] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 69500, global step 1112830: learning rate 0.0001
[2019-04-27 19:50:02,547] A3C_AGENT_WORKER-Thread-17 INFO:Local step 69500, global step 1112843: loss 0.4178
[2019-04-27 19:50:02,550] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 69500, global step 1112843: learning rate 0.0001
[2019-04-27 19:50:02,636] A3C_AGENT_WORKER-Thread-9 INFO:Local step 69500, global step 1112891: loss 0.3417
[2019-04-27 19:50:02,637] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 69500, global step 1112891: learning rate 0.0001
[2019-04-27 19:50:02,684] A3C_AGENT_WORKER-Thread-11 INFO:Local step 69500, global step 1112914: loss 0.4108
[2019-04-27 19:50:02,685] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 69500, global step 1112914: learning rate 0.0001
[2019-04-27 19:50:02,750] A3C_AGENT_WORKER-Thread-15 INFO:Local step 69500, global step 1112949: loss 0.2540
[2019-04-27 19:50:02,752] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 69500, global step 1112949: learning rate 0.0001
[2019-04-27 19:50:02,771] A3C_AGENT_WORKER-Thread-12 INFO:Local step 69500, global step 1112961: loss 0.0868
[2019-04-27 19:50:02,774] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 69500, global step 1112962: learning rate 0.0001
[2019-04-27 19:50:02,912] A3C_AGENT_WORKER-Thread-22 INFO:Local step 69500, global step 1113033: loss 0.5472
[2019-04-27 19:50:02,914] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 69500, global step 1113034: learning rate 0.0001
[2019-04-27 19:50:02,941] A3C_AGENT_WORKER-Thread-13 INFO:Local step 69500, global step 1113049: loss 0.3796
[2019-04-27 19:50:02,943] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 69500, global step 1113049: learning rate 0.0001
[2019-04-27 19:50:02,996] A3C_AGENT_WORKER-Thread-10 INFO:Local step 69500, global step 1113077: loss 0.1644
[2019-04-27 19:50:03,000] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 69500, global step 1113077: learning rate 0.0001
[2019-04-27 19:50:03,074] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.9476918e-34 1.0000000e+00 1.2830099e-33 2.3605678e-37 4.1341281e-24], sum to 1.0000
[2019-04-27 19:50:03,075] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6888
[2019-04-27 19:50:03,080] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.73333333333333, 67.0, 1.0, 2.0, 0.5692185734439653, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 661537.5250217642, 661537.5250217642, 153474.2059970188], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1966800.0000, 
sim time next is 1967400.0000, 
raw observation next is [27.6, 68.0, 1.0, 2.0, 0.5714357918383279, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 663548.7602385518, 663548.7602385518, 153821.3051132063], 
processed observation next is [1.0, 0.782608695652174, 0.5777777777777778, 0.68, 1.0, 1.0, 0.48980451409324754, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.23698170008519706, 0.23698170008519706, 0.29581020214078135], 
reward next is 0.7042, 
noisyNet noise sample is [array([-0.9002949], dtype=float32), -0.97071064]. 
=============================================
[2019-04-27 19:50:03,149] A3C_AGENT_WORKER-Thread-14 INFO:Local step 69500, global step 1113157: loss 0.0658
[2019-04-27 19:50:03,151] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 69500, global step 1113158: learning rate 0.0001
[2019-04-27 19:50:03,229] A3C_AGENT_WORKER-Thread-21 INFO:Local step 69500, global step 1113196: loss 0.0556
[2019-04-27 19:50:03,231] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 69500, global step 1113196: learning rate 0.0001
[2019-04-27 19:50:07,748] A3C_AGENT_WORKER-Thread-3 INFO:Local step 70000, global step 1115581: loss 0.0872
[2019-04-27 19:50:07,751] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 70000, global step 1115582: learning rate 0.0001
[2019-04-27 19:50:12,001] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.4748513e-32 1.0000000e+00 9.0262643e-29 1.7587869e-31 6.4374748e-16], sum to 1.0000
[2019-04-27 19:50:12,008] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0415
[2019-04-27 19:50:12,018] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.9, 47.0, 1.0, 2.0, 0.37879048434143, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 472393.7393056062, 472393.7393056062, 125734.9420301856], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2503200.0000, 
sim time next is 2503800.0000, 
raw observation next is [26.8, 47.5, 1.0, 2.0, 0.3789824441734111, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 472583.400942294, 472583.400942294, 125760.3394696272], 
processed observation next is [1.0, 1.0, 0.5481481481481482, 0.475, 1.0, 1.0, 0.2606933859207275, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16877978605081928, 0.16877978605081928, 0.24184680667236], 
reward next is 0.7582, 
noisyNet noise sample is [array([-0.7484493], dtype=float32), 0.084483124]. 
=============================================
[2019-04-27 19:50:12,807] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.9434269e-35 1.0000000e+00 3.2867957e-33 1.5433826e-35 7.9566852e-23], sum to 1.0000
[2019-04-27 19:50:12,816] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4491
[2019-04-27 19:50:12,820] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.03333333333333, 78.0, 1.0, 2.0, 0.5837010496313119, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 676337.5188218128, 676337.5188218132, 155830.6876802297], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2155200.0000, 
sim time next is 2155800.0000, 
raw observation next is [25.86666666666667, 78.5, 1.0, 2.0, 0.5788280871580881, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 671910.7123257823, 671910.7123257823, 155059.0090658607], 
processed observation next is [0.0, 0.9565217391304348, 0.5135802469135804, 0.785, 1.0, 1.0, 0.49860486566439055, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.23996811154492223, 0.23996811154492223, 0.29819040204973213], 
reward next is 0.7018, 
noisyNet noise sample is [array([-0.00658567], dtype=float32), 0.99819654]. 
=============================================
[2019-04-27 19:50:13,919] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.6171298e-28 1.0000000e+00 1.6697271e-24 7.0624227e-30 3.5408156e-14], sum to 1.0000
[2019-04-27 19:50:13,926] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7020
[2019-04-27 19:50:13,930] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.96666666666667, 89.66666666666667, 1.0, 2.0, 0.6240771214773022, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 729262.8886767506, 729262.8886767506, 163121.8741699757], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2173200.0000, 
sim time next is 2173800.0000, 
raw observation next is [23.93333333333333, 89.83333333333333, 1.0, 2.0, 0.6131154880258671, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 716624.3122771784, 716624.3122771784, 161191.4838229008], 
processed observation next is [1.0, 0.13043478260869565, 0.4419753086419752, 0.8983333333333333, 1.0, 1.0, 0.5394232000307941, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.25593725438470655, 0.25593725438470655, 0.3099836227363477], 
reward next is 0.6900, 
noisyNet noise sample is [array([-0.93629193], dtype=float32), -0.9964143]. 
=============================================
[2019-04-27 19:50:14,508] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.8151360e-33 1.0000000e+00 3.1782326e-32 2.0968225e-34 8.1794368e-23], sum to 1.0000
[2019-04-27 19:50:14,516] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9595
[2019-04-27 19:50:14,518] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.03333333333333, 89.0, 1.0, 2.0, 0.6677488403913536, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 780747.9770508385, 780747.9770508385, 171071.3506785375], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2182200.0000, 
sim time next is 2182800.0000, 
raw observation next is [24.06666666666667, 89.0, 1.0, 2.0, 0.600877259537118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 702135.375749168, 702135.375749168, 159044.5051388009], 
processed observation next is [1.0, 0.2608695652173913, 0.4469135802469137, 0.89, 1.0, 1.0, 0.524853880401331, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.25076263419613143, 0.25076263419613143, 0.3058548175746171], 
reward next is 0.6941, 
noisyNet noise sample is [array([0.9210593], dtype=float32), 0.551063]. 
=============================================
[2019-04-27 19:50:14,640] A3C_AGENT_WORKER-Thread-19 INFO:Local step 70500, global step 1119196: loss 0.1122
[2019-04-27 19:50:14,642] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 70500, global step 1119197: learning rate 0.0001
[2019-04-27 19:50:15,026] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.5954216e-34 1.0000000e+00 1.6698604e-32 1.7401487e-37 4.7337767e-27], sum to 1.0000
[2019-04-27 19:50:15,038] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1421
[2019-04-27 19:50:15,043] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.45, 88.16666666666667, 1.0, 2.0, 0.6806306591690354, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 790928.2355039264, 790928.2355039264, 173251.2279477488], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2188200.0000, 
sim time next is 2188800.0000, 
raw observation next is [24.5, 88.0, 1.0, 2.0, 0.6287499981691207, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 730225.2175244266, 730225.2175244266, 163751.7885507291], 
processed observation next is [1.0, 0.34782608695652173, 0.46296296296296297, 0.88, 1.0, 1.0, 0.558035712106096, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2607947205444381, 0.2607947205444381, 0.31490728567447906], 
reward next is 0.6851, 
noisyNet noise sample is [array([-0.97269195], dtype=float32), -0.04981304]. 
=============================================
[2019-04-27 19:50:17,341] A3C_AGENT_WORKER-Thread-16 INFO:Local step 70000, global step 1120571: loss 0.3601
[2019-04-27 19:50:17,347] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 70000, global step 1120571: learning rate 0.0001
[2019-04-27 19:50:17,702] A3C_AGENT_WORKER-Thread-18 INFO:Local step 70000, global step 1120759: loss 0.0450
[2019-04-27 19:50:17,703] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 70000, global step 1120759: learning rate 0.0001
[2019-04-27 19:50:17,831] A3C_AGENT_WORKER-Thread-17 INFO:Local step 70000, global step 1120826: loss 0.0698
[2019-04-27 19:50:17,834] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 70000, global step 1120826: learning rate 0.0001
[2019-04-27 19:50:17,895] A3C_AGENT_WORKER-Thread-20 INFO:Local step 70000, global step 1120859: loss 0.1503
[2019-04-27 19:50:17,897] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 70000, global step 1120859: learning rate 0.0001
[2019-04-27 19:50:17,926] A3C_AGENT_WORKER-Thread-2 INFO:Local step 70000, global step 1120875: loss 0.1735
[2019-04-27 19:50:17,927] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 70000, global step 1120875: learning rate 0.0001
[2019-04-27 19:50:17,980] A3C_AGENT_WORKER-Thread-9 INFO:Local step 70000, global step 1120906: loss 0.1021
[2019-04-27 19:50:17,983] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 70000, global step 1120906: learning rate 0.0001
[2019-04-27 19:50:18,019] A3C_AGENT_WORKER-Thread-15 INFO:Local step 70000, global step 1120924: loss 0.1002
[2019-04-27 19:50:18,021] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 70000, global step 1120924: learning rate 0.0001
[2019-04-27 19:50:18,042] A3C_AGENT_WORKER-Thread-12 INFO:Local step 70000, global step 1120936: loss 0.0723
[2019-04-27 19:50:18,043] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 70000, global step 1120936: learning rate 0.0001
[2019-04-27 19:50:18,056] A3C_AGENT_WORKER-Thread-11 INFO:Local step 70000, global step 1120940: loss 0.0457
[2019-04-27 19:50:18,058] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 70000, global step 1120940: learning rate 0.0001
[2019-04-27 19:50:18,144] A3C_AGENT_WORKER-Thread-13 INFO:Local step 70000, global step 1120989: loss 0.0513
[2019-04-27 19:50:18,147] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 70000, global step 1120989: learning rate 0.0001
[2019-04-27 19:50:18,226] A3C_AGENT_WORKER-Thread-22 INFO:Local step 70000, global step 1121027: loss 0.0467
[2019-04-27 19:50:18,229] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 70000, global step 1121028: learning rate 0.0001
[2019-04-27 19:50:18,309] A3C_AGENT_WORKER-Thread-14 INFO:Local step 70000, global step 1121075: loss 0.0390
[2019-04-27 19:50:18,312] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 70000, global step 1121075: learning rate 0.0001
[2019-04-27 19:50:18,379] A3C_AGENT_WORKER-Thread-10 INFO:Local step 70000, global step 1121108: loss 0.0362
[2019-04-27 19:50:18,381] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 70000, global step 1121109: learning rate 0.0001
[2019-04-27 19:50:18,489] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [8.6886252e-36 1.0000000e+00 5.6448128e-35 5.7379256e-36 3.6691315e-27], sum to 1.0000
[2019-04-27 19:50:18,497] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3926
[2019-04-27 19:50:18,503] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 98.0, 1.0, 2.0, 0.5690891194874756, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 662573.5889974631, 662573.5889974636, 153505.3937153124], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2253600.0000, 
sim time next is 2254200.0000, 
raw observation next is [22.95, 98.0, 1.0, 2.0, 0.7833593560642439, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 913933.7270088828, 913933.7270088828, 193582.0684582038], 
processed observation next is [1.0, 0.08695652173913043, 0.4055555555555555, 0.98, 1.0, 1.0, 0.7420944715050523, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.32640490250317244, 0.32640490250317244, 0.37227320857346885], 
reward next is 0.6277, 
noisyNet noise sample is [array([-1.4922183], dtype=float32), 1.7248987]. 
=============================================
[2019-04-27 19:50:18,581] A3C_AGENT_WORKER-Thread-21 INFO:Local step 70000, global step 1121218: loss 0.0413
[2019-04-27 19:50:18,583] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 70000, global step 1121218: learning rate 0.0001
[2019-04-27 19:50:23,096] A3C_AGENT_WORKER-Thread-3 INFO:Local step 70500, global step 1123600: loss 0.2032
[2019-04-27 19:50:23,098] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 70500, global step 1123600: learning rate 0.0001
[2019-04-27 19:50:23,787] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.5472023e-25 3.1431516e-14 5.2042991e-18 2.8665479e-22 1.0000000e+00], sum to 1.0000
[2019-04-27 19:50:23,795] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0950
[2019-04-27 19:50:23,798] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.8, 39.83333333333334, 1.0, 2.0, 0.3138759622690205, 1.0, 2.0, 0.3138759622690205, 1.0, 2.0, 0.5096091653789186, 6.9112, 6.9112, 121.94756008, 1140114.055989719, 1140114.055989719, 260547.3071979409], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2369400.0000, 
sim time next is 2370000.0000, 
raw observation next is [28.9, 39.66666666666667, 1.0, 2.0, 0.3184594275107809, 1.0, 2.0, 0.3184594275107809, 1.0, 2.0, 0.5165973453517133, 6.9112, 6.9112, 121.94756008, 1155298.152013949, 1155298.152013949, 262396.3665587115], 
processed observation next is [1.0, 0.43478260869565216, 0.6259259259259259, 0.3966666666666667, 1.0, 1.0, 0.18864217560807248, 1.0, 1.0, 0.18864217560807248, 1.0, 1.0, 0.3957466816896416, 0.0, 0.0, 0.8096049824067558, 0.4126064828621246, 0.4126064828621246, 0.5046083972282913], 
reward next is 0.4954, 
noisyNet noise sample is [array([-0.12677585], dtype=float32), -0.49743128]. 
=============================================
[2019-04-27 19:50:23,812] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[60.08367 ]
 [60.752914]
 [61.290558]
 [62.086315]
 [62.038357]], R is [[59.56769562]
 [59.47096634]
 [59.38245392]
 [59.29531479]
 [59.28773499]].
[2019-04-27 19:50:25,782] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-27 19:50:25,783] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 19:50:25,783] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 19:50:25,784] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 19:50:25,787] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 19:50:25,787] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:50:25,788] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:50:25,784] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:50:25,791] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:50:25,790] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 19:50:25,795] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:50:25,816] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run46
[2019-04-27 19:50:25,841] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run46
[2019-04-27 19:50:25,842] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run46
[2019-04-27 19:50:25,890] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run46
[2019-04-27 19:50:25,890] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run46
[2019-04-27 19:50:27,092] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03269195], dtype=float32), 0.00066682213]
[2019-04-27 19:50:27,094] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [31.0, 18.0, 1.0, 2.0, 0.6196574202396279, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 798621.8010394695, 798621.8010394695, 164172.8577602204]
[2019-04-27 19:50:27,095] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:50:27,097] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [2.31759601e-30 1.00000000e+00 8.39878127e-29 1.02479164e-32
 8.75023124e-20], sampled 0.011419292757883004
[2019-04-27 19:50:31,576] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03269195], dtype=float32), 0.00066682213]
[2019-04-27 19:50:31,577] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [27.95, 30.5, 1.0, 2.0, 0.825086479965565, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426154913, 1055009.078818609, 1055009.078818609, 205060.5286455875]
[2019-04-27 19:50:31,578] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:50:31,579] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [8.4856766e-27 1.0000000e+00 4.0046859e-25 1.6112799e-28 4.4029069e-16], sampled 0.7765830948911949
[2019-04-27 19:50:33,097] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.03269195], dtype=float32), 0.00066682213]
[2019-04-27 19:50:33,099] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [17.0, 83.0, 1.0, 2.0, 0.2377137231545761, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 306627.0826161238, 306627.0826161238, 98331.03460345413]
[2019-04-27 19:50:33,101] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 19:50:33,103] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [3.65365297e-36 1.00000000e+00 2.21473424e-36 0.00000000e+00
 1.35378205e-30], sampled 0.619865223456912
[2019-04-27 19:50:44,251] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.03269195], dtype=float32), 0.00066682213]
[2019-04-27 19:50:44,252] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [30.33333333333334, 29.66666666666666, 1.0, 2.0, 0.3558868136221633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 448548.3664857254, 448548.366485725, 122713.8661609316]
[2019-04-27 19:50:44,254] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 19:50:44,257] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [6.2489326e-34 1.0000000e+00 2.0066284e-32 4.8474156e-37 3.2105755e-23], sampled 0.8922198249902487
[2019-04-27 19:50:45,085] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03269195], dtype=float32), 0.00066682213]
[2019-04-27 19:50:45,086] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [29.2, 37.0, 1.0, 2.0, 0.3580874203103792, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 446999.9934159174, 446999.993415917, 122939.0234884339]
[2019-04-27 19:50:45,088] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:50:45,090] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [3.9417237e-34 1.0000000e+00 7.6565604e-33 2.5710668e-37 6.2570506e-24], sampled 0.763131574406773
[2019-04-27 19:50:46,015] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03269195], dtype=float32), 0.00066682213]
[2019-04-27 19:50:46,015] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [30.2, 40.0, 1.0, 2.0, 0.4336259919293123, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 530746.7942384988, 530746.7942384983, 133292.7381806915]
[2019-04-27 19:50:46,016] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:50:46,018] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [5.4737052e-36 1.0000000e+00 6.4308316e-36 0.0000000e+00 1.8314759e-29], sampled 0.5703591450960711
[2019-04-27 19:52:05,889] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.03269195], dtype=float32), 0.00066682213]
[2019-04-27 19:52:05,889] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [20.40625171666667, 94.22107208666667, 1.0, 2.0, 0.4131655597248141, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 508489.8203284385, 508489.8203284385, 130406.7249792123]
[2019-04-27 19:52:05,892] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 19:52:05,895] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [2.8211899e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.2409799e-34], sampled 0.9894815007005398
[2019-04-27 19:52:14,333] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03269195], dtype=float32), 0.00066682213]
[2019-04-27 19:52:14,335] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [28.15805404, 54.97592407, 1.0, 2.0, 0.8065625881366506, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 968167.9594286663, 968167.9594286658, 199575.4987977315]
[2019-04-27 19:52:14,336] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:52:14,341] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [6.1687904e-32 1.0000000e+00 1.0656437e-31 3.9193361e-35 7.4889342e-25], sampled 0.23412959730255223
[2019-04-27 19:52:16,541] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8929.0857 2125202462.6616 390.0000
[2019-04-27 19:52:16,847] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8810.4642 2175284675.9310 409.0000
[2019-04-27 19:52:16,892] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8657.8279 2255749852.9698 382.0000
[2019-04-27 19:52:16,893] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8192.1815 2456767579.0022 486.0000
[2019-04-27 19:52:16,981] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8750.6984 2204167468.6359 451.0000
[2019-04-27 19:52:17,997] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1125000, evaluation results [1125000.0, 8192.18146305892, 2456767579.0022163, 486.0, 8810.464233109326, 2175284675.931027, 409.0, 8929.085745906672, 2125202462.6616194, 390.0, 8657.827898474412, 2255749852.969785, 382.0, 8750.698436929833, 2204167468.6359324, 451.0]
[2019-04-27 19:52:22,496] A3C_AGENT_WORKER-Thread-19 INFO:Local step 71000, global step 1127145: loss 0.2994
[2019-04-27 19:52:22,498] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 71000, global step 1127145: learning rate 0.0001
[2019-04-27 19:52:25,730] A3C_AGENT_WORKER-Thread-16 INFO:Local step 70500, global step 1128648: loss -158.7498
[2019-04-27 19:52:25,733] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 70500, global step 1128649: learning rate 0.0001
[2019-04-27 19:52:26,046] A3C_AGENT_WORKER-Thread-17 INFO:Local step 70500, global step 1128800: loss 21.8416
[2019-04-27 19:52:26,048] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 70500, global step 1128801: learning rate 0.0001
[2019-04-27 19:52:26,071] A3C_AGENT_WORKER-Thread-18 INFO:Local step 70500, global step 1128809: loss 24.6718
[2019-04-27 19:52:26,072] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 70500, global step 1128809: learning rate 0.0001
[2019-04-27 19:52:26,222] A3C_AGENT_WORKER-Thread-2 INFO:Local step 70500, global step 1128881: loss 7.5540
[2019-04-27 19:52:26,225] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 70500, global step 1128881: learning rate 0.0001
[2019-04-27 19:52:26,237] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2587038e-33 1.0000000e+00 2.1051577e-34 1.1351871e-36 1.3009313e-23], sum to 1.0000
[2019-04-27 19:52:26,239] A3C_AGENT_WORKER-Thread-9 INFO:Local step 70500, global step 1128889: loss 0.0592
[2019-04-27 19:52:26,241] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 70500, global step 1128889: learning rate 0.0001
[2019-04-27 19:52:26,244] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9401
[2019-04-27 19:52:26,249] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.91666666666667, 84.83333333333333, 1.0, 2.0, 0.6449019684677452, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 734975.7019669173, 734975.7019669173, 165973.6906181919], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2758200.0000, 
sim time next is 2758800.0000, 
raw observation next is [25.83333333333334, 85.66666666666667, 1.0, 2.0, 0.6523135559985128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 743426.5651325795, 743426.5651325795, 167310.7446097803], 
processed observation next is [0.0, 0.9565217391304348, 0.5123456790123458, 0.8566666666666667, 1.0, 1.0, 0.5860875666648961, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2655094875473498, 0.2655094875473498, 0.32175143194188516], 
reward next is 0.6782, 
noisyNet noise sample is [array([0.33180073], dtype=float32), 0.35380074]. 
=============================================
[2019-04-27 19:52:26,258] A3C_AGENT_WORKER-Thread-20 INFO:Local step 70500, global step 1128895: loss 14.5867
[2019-04-27 19:52:26,260] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 70500, global step 1128895: learning rate 0.0001
[2019-04-27 19:52:26,330] A3C_AGENT_WORKER-Thread-15 INFO:Local step 70500, global step 1128926: loss -3.0285
[2019-04-27 19:52:26,333] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 70500, global step 1128926: learning rate 0.0001
[2019-04-27 19:52:26,391] A3C_AGENT_WORKER-Thread-12 INFO:Local step 70500, global step 1128959: loss -6.1029
[2019-04-27 19:52:26,394] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 70500, global step 1128960: learning rate 0.0001
[2019-04-27 19:52:26,436] A3C_AGENT_WORKER-Thread-11 INFO:Local step 70500, global step 1128975: loss 40.4952
[2019-04-27 19:52:26,439] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 70500, global step 1128976: learning rate 0.0001
[2019-04-27 19:52:26,456] A3C_AGENT_WORKER-Thread-13 INFO:Local step 70500, global step 1128983: loss -0.0067
[2019-04-27 19:52:26,458] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 70500, global step 1128984: learning rate 0.0001
[2019-04-27 19:52:26,609] A3C_AGENT_WORKER-Thread-22 INFO:Local step 70500, global step 1129057: loss 30.2200
[2019-04-27 19:52:26,611] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 70500, global step 1129057: learning rate 0.0001
[2019-04-27 19:52:26,703] A3C_AGENT_WORKER-Thread-14 INFO:Local step 70500, global step 1129102: loss 7.5253
[2019-04-27 19:52:26,705] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 70500, global step 1129103: learning rate 0.0001
[2019-04-27 19:52:26,750] A3C_AGENT_WORKER-Thread-10 INFO:Local step 70500, global step 1129124: loss -100.5642
[2019-04-27 19:52:26,752] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 70500, global step 1129124: learning rate 0.0001
[2019-04-27 19:52:26,820] A3C_AGENT_WORKER-Thread-21 INFO:Local step 70500, global step 1129153: loss 26.9452
[2019-04-27 19:52:26,822] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 70500, global step 1129154: learning rate 0.0001
[2019-04-27 19:52:31,666] A3C_AGENT_WORKER-Thread-3 INFO:Local step 71000, global step 1131447: loss 0.0755
[2019-04-27 19:52:31,668] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 71000, global step 1131447: learning rate 0.0001
[2019-04-27 19:52:37,295] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.19310165e-36 1.00000000e+00 1.27681760e-35 0.00000000e+00
 2.59062976e-29], sum to 1.0000
[2019-04-27 19:52:37,301] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7665
[2019-04-27 19:52:37,310] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.5, 89.0, 1.0, 2.0, 0.6587284075184734, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 750741.0021109901, 750741.0021109901, 168476.170627899], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2761200.0000, 
sim time next is 2761800.0000, 
raw observation next is [25.41666666666666, 89.83333333333334, 1.0, 2.0, 0.66134077416968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 753719.7334922912, 753719.7334922912, 168952.7045336798], 
processed observation next is [0.0, 1.0, 0.49691358024691334, 0.8983333333333334, 1.0, 1.0, 0.5968342549639047, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2691856191043897, 0.2691856191043897, 0.32490904718015345], 
reward next is 0.6751, 
noisyNet noise sample is [array([-0.7323838], dtype=float32), -0.11126396]. 
=============================================
[2019-04-27 19:52:37,811] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.3735000e-21 1.0000000e+00 2.9612619e-18 3.0279696e-20 9.3682673e-10], sum to 1.0000
[2019-04-27 19:52:37,820] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2603
[2019-04-27 19:52:37,826] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1697377.876826156 W.
[2019-04-27 19:52:37,830] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.0, 86.5, 1.0, 2.0, 0.7442200358480128, 1.0, 2.0, 0.7442200358480128, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1697377.876826156, 1697377.876826156, 321244.8367954451], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2971800.0000, 
sim time next is 2972400.0000, 
raw observation next is [27.33333333333333, 85.66666666666667, 1.0, 2.0, 0.7459150813609384, 1.0, 2.0, 0.7459150813609384, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 1701247.528558022, 1701247.528558022, 321913.1916422373], 
processed observation next is [1.0, 0.391304347826087, 0.5679012345679011, 0.8566666666666667, 1.0, 1.0, 0.6975179540011172, 1.0, 1.0, 0.6975179540011172, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.6075884030564365, 0.6075884030564365, 0.6190638300812256], 
reward next is 0.3809, 
noisyNet noise sample is [array([-0.25936902], dtype=float32), -0.67530406]. 
=============================================
[2019-04-27 19:52:38,019] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.9757858e-36 1.0000000e+00 4.8463963e-38 0.0000000e+00 7.7257937e-35], sum to 1.0000
[2019-04-27 19:52:38,027] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7658
[2019-04-27 19:52:38,033] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 89.0, 1.0, 2.0, 0.7089815670881964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 821656.8948250046, 821656.8948250046, 178525.4621613864], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2788200.0000, 
sim time next is 2788800.0000, 
raw observation next is [24.66666666666666, 89.0, 1.0, 2.0, 0.7113500533430229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 821502.9155606232, 821502.9155606232, 178840.8700506967], 
processed observation next is [1.0, 0.2608695652173913, 0.4691358024691356, 0.89, 1.0, 1.0, 0.6563691111226463, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.29339389841450825, 0.29339389841450825, 0.3439247500974937], 
reward next is 0.6561, 
noisyNet noise sample is [array([-1.1570705], dtype=float32), 0.4759254]. 
=============================================
[2019-04-27 19:52:39,136] A3C_AGENT_WORKER-Thread-19 INFO:Local step 71500, global step 1135363: loss 144.6107
[2019-04-27 19:52:39,137] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 71500, global step 1135363: learning rate 0.0001
[2019-04-27 19:52:41,406] A3C_AGENT_WORKER-Thread-16 INFO:Local step 71000, global step 1136530: loss 0.1018
[2019-04-27 19:52:41,409] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 71000, global step 1136532: learning rate 0.0001
[2019-04-27 19:52:41,902] A3C_AGENT_WORKER-Thread-18 INFO:Local step 71000, global step 1136787: loss 0.0715
[2019-04-27 19:52:41,906] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 71000, global step 1136788: learning rate 0.0001
[2019-04-27 19:52:41,942] A3C_AGENT_WORKER-Thread-17 INFO:Local step 71000, global step 1136807: loss 0.0775
[2019-04-27 19:52:41,950] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 71000, global step 1136809: learning rate 0.0001
[2019-04-27 19:52:42,085] A3C_AGENT_WORKER-Thread-12 INFO:Local step 71000, global step 1136881: loss 0.0728
[2019-04-27 19:52:42,090] A3C_AGENT_WORKER-Thread-9 INFO:Local step 71000, global step 1136883: loss 0.0663
[2019-04-27 19:52:42,091] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 71000, global step 1136883: learning rate 0.0001
[2019-04-27 19:52:42,094] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 71000, global step 1136885: learning rate 0.0001
[2019-04-27 19:52:42,130] A3C_AGENT_WORKER-Thread-2 INFO:Local step 71000, global step 1136905: loss 0.0964
[2019-04-27 19:52:42,133] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 71000, global step 1136905: learning rate 0.0001
[2019-04-27 19:52:42,165] A3C_AGENT_WORKER-Thread-11 INFO:Local step 71000, global step 1136923: loss 0.1721
[2019-04-27 19:52:42,168] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 71000, global step 1136923: learning rate 0.0001
[2019-04-27 19:52:42,181] A3C_AGENT_WORKER-Thread-20 INFO:Local step 71000, global step 1136931: loss 0.1847
[2019-04-27 19:52:42,183] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 71000, global step 1136933: learning rate 0.0001
[2019-04-27 19:52:42,201] A3C_AGENT_WORKER-Thread-15 INFO:Local step 71000, global step 1136941: loss 0.2478
[2019-04-27 19:52:42,205] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 71000, global step 1136941: learning rate 0.0001
[2019-04-27 19:52:42,280] A3C_AGENT_WORKER-Thread-13 INFO:Local step 71000, global step 1136979: loss 0.3852
[2019-04-27 19:52:42,282] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 71000, global step 1136979: learning rate 0.0001
[2019-04-27 19:52:42,374] A3C_AGENT_WORKER-Thread-22 INFO:Local step 71000, global step 1137030: loss 0.3338
[2019-04-27 19:52:42,376] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 71000, global step 1137030: learning rate 0.0001
[2019-04-27 19:52:42,448] A3C_AGENT_WORKER-Thread-14 INFO:Local step 71000, global step 1137067: loss 0.4171
[2019-04-27 19:52:42,450] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 71000, global step 1137067: learning rate 0.0001
[2019-04-27 19:52:42,533] A3C_AGENT_WORKER-Thread-10 INFO:Local step 71000, global step 1137114: loss 0.2284
[2019-04-27 19:52:42,534] A3C_AGENT_WORKER-Thread-21 INFO:Local step 71000, global step 1137114: loss 0.2863
[2019-04-27 19:52:42,536] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 71000, global step 1137116: learning rate 0.0001
[2019-04-27 19:52:42,539] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 71000, global step 1137118: learning rate 0.0001
[2019-04-27 19:52:46,041] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 19:52:46,048] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9890
[2019-04-27 19:52:46,056] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.6852241463638109, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 780953.1052891882, 780953.1052891882, 173365.1066106339], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2928600.0000, 
sim time next is 2929200.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.6852007309650351, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 780926.4050572189, 780926.4050572184, 173360.7346072997], 
processed observation next is [1.0, 0.9130434782608695, 0.5185185185185185, 0.89, 1.0, 1.0, 0.6252389654345656, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.27890228752043533, 0.27890228752043517, 0.33338602809096096], 
reward next is 0.6666, 
noisyNet noise sample is [array([0.9176645], dtype=float32), 0.2618486]. 
=============================================
[2019-04-27 19:52:46,452] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 19:52:46,462] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6000
[2019-04-27 19:52:46,468] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.2, 90.0, 1.0, 2.0, 0.6460838069726847, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 736323.2552639624, 736323.2552639624, 166186.2659664574], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2935800.0000, 
sim time next is 2936400.0000, 
raw observation next is [25.16666666666667, 90.33333333333334, 1.0, 2.0, 0.6465684997179202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 736875.9112433739, 736875.9112433739, 166273.5592879111], 
processed observation next is [1.0, 1.0, 0.4876543209876545, 0.9033333333333334, 1.0, 1.0, 0.579248213949905, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.263169968301205, 0.263169968301205, 0.31975684478444444], 
reward next is 0.6802, 
noisyNet noise sample is [array([0.20455629], dtype=float32), 0.7267487]. 
=============================================
[2019-04-27 19:52:46,670] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 19:52:46,675] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9459
[2019-04-27 19:52:46,679] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.33333333333334, 88.66666666666667, 1.0, 2.0, 0.3412133683757362, 1.0, 1.0, 0.3412133683757362, 1.0, 1.0, 0.543222794997773, 6.911199999999999, 6.9112, 121.94756008, 1166940.840790101, 1166940.840790101, 272288.9723157343], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2946000.0000, 
sim time next is 2946600.0000, 
raw observation next is [25.25, 90.0, 1.0, 2.0, 0.8900683796423061, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.926042512486, 1014569.551983506, 1014569.551983506, 215246.0904863918], 
processed observation next is [1.0, 0.08695652173913043, 0.49074074074074076, 0.9, 1.0, 1.0, 0.8691290233836978, 0.0, 0.5, -0.1904761904761905, 0.0, 0.5, -0.25, 0.0, 0.0, 0.809462128135156, 0.36234626856553787, 0.36234626856553787, 0.4139347893969073], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5577768], dtype=float32), -0.7539574]. 
=============================================
[2019-04-27 19:52:47,629] A3C_AGENT_WORKER-Thread-3 INFO:Local step 71500, global step 1139709: loss 163.2273
[2019-04-27 19:52:47,630] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 71500, global step 1139709: learning rate 0.0001
[2019-04-27 19:52:50,467] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.7656487e-33 1.0000000e+00 7.9270762e-33 3.6992321e-36 7.2239562e-28], sum to 1.0000
[2019-04-27 19:52:50,474] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4067
[2019-04-27 19:52:50,479] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.81666666666667, 94.00000000000001, 1.0, 2.0, 0.7282028666346291, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 842697.089597497, 842697.0895974966, 182188.3509094492], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3384600.0000, 
sim time next is 3385200.0000, 
raw observation next is [23.63333333333333, 94.0, 1.0, 2.0, 0.6893794756759507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 800864.2462435955, 800864.2462435955, 174885.149717705], 
processed observation next is [1.0, 0.17391304347826086, 0.430864197530864, 0.94, 1.0, 1.0, 0.6302136615189889, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2860229450869984, 0.2860229450869984, 0.3363175956109712], 
reward next is 0.6637, 
noisyNet noise sample is [array([-1.6925377], dtype=float32), -1.352869]. 
=============================================
[2019-04-27 19:52:54,009] A3C_AGENT_WORKER-Thread-19 INFO:Local step 72000, global step 1143052: loss 0.0729
[2019-04-27 19:52:54,010] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 72000, global step 1143052: learning rate 0.0001
[2019-04-27 19:52:56,727] A3C_AGENT_WORKER-Thread-16 INFO:Local step 71500, global step 1144494: loss 0.7325
[2019-04-27 19:52:56,729] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 71500, global step 1144494: learning rate 0.0001
[2019-04-27 19:52:57,301] A3C_AGENT_WORKER-Thread-18 INFO:Local step 71500, global step 1144795: loss 2.6067
[2019-04-27 19:52:57,303] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 71500, global step 1144795: learning rate 0.0001
[2019-04-27 19:52:57,435] A3C_AGENT_WORKER-Thread-11 INFO:Local step 71500, global step 1144867: loss 3.6531
[2019-04-27 19:52:57,437] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 71500, global step 1144867: learning rate 0.0001
[2019-04-27 19:52:57,460] A3C_AGENT_WORKER-Thread-17 INFO:Local step 71500, global step 1144878: loss 2.3657
[2019-04-27 19:52:57,463] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 71500, global step 1144878: learning rate 0.0001
[2019-04-27 19:52:57,505] A3C_AGENT_WORKER-Thread-12 INFO:Local step 71500, global step 1144903: loss 1.8894
[2019-04-27 19:52:57,512] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 71500, global step 1144906: learning rate 0.0001
[2019-04-27 19:52:57,521] A3C_AGENT_WORKER-Thread-15 INFO:Local step 71500, global step 1144910: loss 1.9212
[2019-04-27 19:52:57,522] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 71500, global step 1144911: learning rate 0.0001
[2019-04-27 19:52:57,527] A3C_AGENT_WORKER-Thread-9 INFO:Local step 71500, global step 1144914: loss 1.7913
[2019-04-27 19:52:57,530] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 71500, global step 1144915: learning rate 0.0001
[2019-04-27 19:52:57,562] A3C_AGENT_WORKER-Thread-20 INFO:Local step 71500, global step 1144931: loss 1.2889
[2019-04-27 19:52:57,569] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 71500, global step 1144932: learning rate 0.0001
[2019-04-27 19:52:57,638] A3C_AGENT_WORKER-Thread-2 INFO:Local step 71500, global step 1144969: loss -2.0171
[2019-04-27 19:52:57,639] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 71500, global step 1144969: learning rate 0.0001
[2019-04-27 19:52:57,690] A3C_AGENT_WORKER-Thread-13 INFO:Local step 71500, global step 1144995: loss -9.8182
[2019-04-27 19:52:57,694] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 71500, global step 1144995: learning rate 0.0001
[2019-04-27 19:52:57,745] A3C_AGENT_WORKER-Thread-22 INFO:Local step 71500, global step 1145022: loss 9.6945
[2019-04-27 19:52:57,748] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 71500, global step 1145023: learning rate 0.0001
[2019-04-27 19:52:57,859] A3C_AGENT_WORKER-Thread-14 INFO:Local step 71500, global step 1145082: loss 26.4100
[2019-04-27 19:52:57,860] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 71500, global step 1145082: learning rate 0.0001
[2019-04-27 19:52:57,890] A3C_AGENT_WORKER-Thread-10 INFO:Local step 71500, global step 1145096: loss -27.5379
[2019-04-27 19:52:57,891] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 71500, global step 1145097: learning rate 0.0001
[2019-04-27 19:52:58,021] A3C_AGENT_WORKER-Thread-21 INFO:Local step 71500, global step 1145166: loss 17.9500
[2019-04-27 19:52:58,022] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 71500, global step 1145166: learning rate 0.0001
[2019-04-27 19:52:59,945] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2829465e-36 1.0000000e+00 5.5015288e-35 0.0000000e+00 1.9069717e-28], sum to 1.0000
[2019-04-27 19:52:59,955] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7857
[2019-04-27 19:52:59,960] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 68.0, 1.0, 2.0, 0.512037171124954, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 611036.1802359482, 611036.1802359482, 144810.7120844044], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3199200.0000, 
sim time next is 3199800.0000, 
raw observation next is [26.0, 66.5, 1.0, 2.0, 0.4999643946697936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 599139.4748597388, 599139.4748597384, 142991.5188850824], 
processed observation next is [0.0, 0.0, 0.5185185185185185, 0.665, 1.0, 1.0, 0.40471951746403995, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.21397838387847815, 0.21397838387847798, 0.27498369016362], 
reward next is 0.7250, 
noisyNet noise sample is [array([0.01008038], dtype=float32), 0.89352846]. 
=============================================
[2019-04-27 19:53:00,456] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4218875e-24 3.4401108e-16 5.5141617e-18 1.1148284e-21 1.0000000e+00], sum to 1.0000
[2019-04-27 19:53:00,465] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7237
[2019-04-27 19:53:00,469] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 59.0, 1.0, 2.0, 0.6481025573640121, 1.0, 2.0, 0.6374159406584409, 1.0, 2.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.94756008, 2181266.81130098, 2181266.811300981, 415585.489437338], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3412800.0000, 
sim time next is 3413400.0000, 
raw observation next is [31.0, 59.0, 1.0, 2.0, 0.5434917009062725, 1.0, 2.0, 0.5434917009062725, 1.0, 2.0, 0.8652564881317629, 6.9112, 6.9112, 121.94756008, 1859519.139399637, 1859519.139399637, 365447.7486000143], 
processed observation next is [1.0, 0.5217391304347826, 0.7037037037037037, 0.59, 1.0, 1.0, 0.4565377391741339, 1.0, 1.0, 0.4565377391741339, 1.0, 1.0, 0.8315706101647036, 0.0, 0.0, 0.8096049824067558, 0.6641139783570132, 0.6641139783570132, 0.7027841319231044], 
reward next is 0.2972, 
noisyNet noise sample is [array([-0.7684749], dtype=float32), 1.2568759]. 
=============================================
[2019-04-27 19:53:02,666] A3C_AGENT_WORKER-Thread-3 INFO:Local step 72000, global step 1147604: loss 0.6325
[2019-04-27 19:53:02,667] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 72000, global step 1147604: learning rate 0.0001
[2019-04-27 19:53:03,332] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 1.8979359e-36 0.0000000e+00 9.2200351e-29], sum to 1.0000
[2019-04-27 19:53:03,339] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1178
[2019-04-27 19:53:03,347] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 49.0, 1.0, 2.0, 0.5663411452420506, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 662021.3457446035, 662021.3457446035, 153160.3021056365], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3261600.0000, 
sim time next is 3262200.0000, 
raw observation next is [30.56666666666667, 51.0, 1.0, 2.0, 0.5622422927515968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 657859.371571572, 657859.371571572, 152501.9857479914], 
processed observation next is [0.0, 0.782608695652174, 0.6876543209876544, 0.51, 1.0, 1.0, 0.47885987232332944, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.23494977556127572, 0.23494977556127572, 0.29327304951536803], 
reward next is 0.7067, 
noisyNet noise sample is [array([1.689821], dtype=float32), 0.44324502]. 
=============================================
[2019-04-27 19:53:07,315] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-27 19:53:07,316] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 19:53:07,320] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 19:53:07,321] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 19:53:07,323] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 19:53:07,324] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:53:07,322] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:53:07,325] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:53:07,324] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:53:07,324] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 19:53:07,329] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:53:07,343] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run47
[2019-04-27 19:53:07,365] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run47
[2019-04-27 19:53:07,366] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run47
[2019-04-27 19:53:07,403] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run47
[2019-04-27 19:53:07,403] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run47
[2019-04-27 19:53:36,735] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03695719], dtype=float32), -0.0016354914]
[2019-04-27 19:53:36,736] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [20.5, 86.0, 1.0, 2.0, 0.3704923056948934, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 461867.9117584236, 461867.9117584236, 124598.3837980456]
[2019-04-27 19:53:36,736] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:53:36,739] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2730808194699752
[2019-04-27 19:53:38,255] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.03695719], dtype=float32), -0.0016354914]
[2019-04-27 19:53:38,257] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [34.25, 35.16666666666667, 1.0, 2.0, 0.562727121265654, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8974944381046914, 6.911199999999999, 6.9112, 121.9260426156618, 1304799.654482569, 1304799.65448257, 279281.7444034835]
[2019-04-27 19:53:38,258] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 19:53:38,261] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [4.6837133e-28 1.0000000e+00 5.8590548e-26 3.6726286e-29 1.4534485e-16], sampled 0.7986633231090885
[2019-04-27 19:53:38,262] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1304799.654482569 W.
[2019-04-27 19:53:47,727] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03695719], dtype=float32), -0.0016354914]
[2019-04-27 19:53:47,728] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [24.81939497, 91.89003604333334, 1.0, 2.0, 0.6022207509662372, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 688773.3521909461, 688773.3521909456, 158582.6701571965]
[2019-04-27 19:53:47,729] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:53:47,731] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.0233044e-36], sampled 0.7220484686475092
[2019-04-27 19:54:11,339] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.03695719], dtype=float32), -0.0016354914]
[2019-04-27 19:54:11,340] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [28.06530605333334, 72.14367885666667, 1.0, 2.0, 0.7157067250261221, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 815712.7264311489, 815712.7264311489, 179134.8704463956]
[2019-04-27 19:54:11,340] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 19:54:11,345] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [3.9709340e-38 1.0000000e+00 1.2150049e-37 0.0000000e+00 3.4817752e-32], sampled 0.45914045700430783
[2019-04-27 19:54:57,812] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8104.6190 2446378282.1575 698.0000
[2019-04-27 19:54:57,985] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8584.1165 2249548893.9982 535.0000
[2019-04-27 19:54:58,124] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8698.5163 2196657418.4911 560.0000
[2019-04-27 19:54:58,145] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8920.5476 2121143439.6831 426.0000
[2019-04-27 19:54:58,158] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8769.4040 2171291782.3510 485.0000
[2019-04-27 19:54:59,174] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 1150000, evaluation results [1150000.0, 8104.6190133458695, 2446378282.1574893, 698.0, 8769.403954575257, 2171291782.350977, 485.0, 8920.54755277632, 2121143439.6831424, 426.0, 8584.116462685264, 2249548893.9982214, 535.0, 8698.516283723779, 2196657418.491122, 560.0]
[2019-04-27 19:55:01,715] A3C_AGENT_WORKER-Thread-19 INFO:Local step 72500, global step 1151199: loss 0.4128
[2019-04-27 19:55:01,719] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 72500, global step 1151201: learning rate 0.0001
[2019-04-27 19:55:02,912] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.5231526e-14 2.2566224e-08 1.7120234e-11 1.6169347e-14 1.0000000e+00], sum to 1.0000
[2019-04-27 19:55:02,922] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8845
[2019-04-27 19:55:02,926] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.75, 60.33333333333334, 1.0, 2.0, 0.6318307569306695, 1.0, 2.0, 0.6292800404417693, 1.0, 2.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.94756008, 2153391.816776877, 2153391.816776878, 411424.1674105898], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3412200.0000, 
sim time next is 3412800.0000, 
raw observation next is [31.0, 59.0, 1.0, 2.0, 0.6481025573640121, 1.0, 2.0, 0.6374159406584409, 1.0, 2.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.94756008, 2181266.81130098, 2181266.811300981, 415585.489437338], 
processed observation next is [1.0, 0.5217391304347826, 0.7037037037037037, 0.59, 1.0, 1.0, 0.5810744730523953, 1.0, 1.0, 0.5683523103076676, 1.0, 1.0, 0.9972168686025908, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.7790238611789214, 0.7790238611789218, 0.7992028643025731], 
reward next is 0.2008, 
noisyNet noise sample is [array([-0.10270542], dtype=float32), -1.0544152]. 
=============================================
[2019-04-27 19:55:04,355] A3C_AGENT_WORKER-Thread-16 INFO:Local step 72000, global step 1152477: loss 1.0435
[2019-04-27 19:55:04,357] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 72000, global step 1152477: learning rate 0.0001
[2019-04-27 19:55:05,053] A3C_AGENT_WORKER-Thread-18 INFO:Local step 72000, global step 1152811: loss 1.2267
[2019-04-27 19:55:05,055] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 72000, global step 1152811: learning rate 0.0001
[2019-04-27 19:55:05,117] A3C_AGENT_WORKER-Thread-15 INFO:Local step 72000, global step 1152836: loss 1.5714
[2019-04-27 19:55:05,122] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 72000, global step 1152837: learning rate 0.0001
[2019-04-27 19:55:05,178] A3C_AGENT_WORKER-Thread-11 INFO:Local step 72000, global step 1152864: loss 1.9501
[2019-04-27 19:55:05,181] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 72000, global step 1152864: learning rate 0.0001
[2019-04-27 19:55:05,239] A3C_AGENT_WORKER-Thread-9 INFO:Local step 72000, global step 1152894: loss 2.2030
[2019-04-27 19:55:05,241] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 72000, global step 1152894: learning rate 0.0001
[2019-04-27 19:55:05,262] A3C_AGENT_WORKER-Thread-17 INFO:Local step 72000, global step 1152905: loss 2.2096
[2019-04-27 19:55:05,265] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 72000, global step 1152905: learning rate 0.0001
[2019-04-27 19:55:05,293] A3C_AGENT_WORKER-Thread-20 INFO:Local step 72000, global step 1152918: loss 2.3377
[2019-04-27 19:55:05,296] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 72000, global step 1152919: learning rate 0.0001
[2019-04-27 19:55:05,364] A3C_AGENT_WORKER-Thread-12 INFO:Local step 72000, global step 1152950: loss 2.2240
[2019-04-27 19:55:05,368] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 72000, global step 1152952: learning rate 0.0001
[2019-04-27 19:55:05,432] A3C_AGENT_WORKER-Thread-2 INFO:Local step 72000, global step 1152982: loss 1.8468
[2019-04-27 19:55:05,438] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 72000, global step 1152984: learning rate 0.0001
[2019-04-27 19:55:05,446] A3C_AGENT_WORKER-Thread-13 INFO:Local step 72000, global step 1152986: loss 2.1853
[2019-04-27 19:55:05,450] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 72000, global step 1152987: learning rate 0.0001
[2019-04-27 19:55:05,497] A3C_AGENT_WORKER-Thread-22 INFO:Local step 72000, global step 1153012: loss 2.1937
[2019-04-27 19:55:05,501] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 72000, global step 1153012: learning rate 0.0001
[2019-04-27 19:55:05,523] A3C_AGENT_WORKER-Thread-10 INFO:Local step 72000, global step 1153020: loss 1.8873
[2019-04-27 19:55:05,526] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 72000, global step 1153022: learning rate 0.0001
[2019-04-27 19:55:05,669] A3C_AGENT_WORKER-Thread-14 INFO:Local step 72000, global step 1153090: loss 1.3830
[2019-04-27 19:55:05,673] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 72000, global step 1153093: learning rate 0.0001
[2019-04-27 19:55:05,780] A3C_AGENT_WORKER-Thread-21 INFO:Local step 72000, global step 1153145: loss 1.0981
[2019-04-27 19:55:05,782] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 72000, global step 1153145: learning rate 0.0001
[2019-04-27 19:55:07,864] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.0164166e-16 5.5368082e-06 4.7667982e-14 9.4580472e-16 9.9999452e-01], sum to 1.0000
[2019-04-27 19:55:07,875] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5352
[2019-04-27 19:55:07,881] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.75, 71.16666666666667, 1.0, 2.0, 0.4901963486680466, 1.0, 2.0, 0.4901963486680466, 1.0, 2.0, 0.780408551660061, 6.9112, 6.9112, 121.94756008, 1677001.652082293, 1677001.652082293, 338803.3429668517], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3498600.0000, 
sim time next is 3499200.0000, 
raw observation next is [29.0, 70.0, 1.0, 2.0, 0.5320343250457115, 1.0, 2.0, 0.5320343250457115, 1.0, 2.0, 0.8470159726210681, 6.911200000000001, 6.9112, 121.94756008, 1820278.585408029, 1820278.585408029, 359589.9793161841], 
processed observation next is [1.0, 0.5217391304347826, 0.6296296296296297, 0.7, 1.0, 1.0, 0.44289800600679935, 1.0, 1.0, 0.44289800600679935, 1.0, 1.0, 0.808769965776335, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.6500994947885818, 0.6500994947885818, 0.6915191909926618], 
reward next is 0.3085, 
noisyNet noise sample is [array([-0.03779225], dtype=float32), -0.082097284]. 
=============================================
[2019-04-27 19:55:10,972] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.9451120e-31 1.0000000e+00 3.6348136e-30 9.5631097e-35 2.4141068e-21], sum to 1.0000
[2019-04-27 19:55:10,982] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0274
[2019-04-27 19:55:10,992] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.26666666666667, 96.16666666666667, 1.0, 2.0, 0.5505198162067277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 655155.4545426711, 655155.4545426711, 151002.4004097214], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3552600.0000, 
sim time next is 3553200.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.5556354714146452, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 658815.4632071634, 658815.4632071629, 151759.4836401415], 
processed observation next is [1.0, 0.13043478260869565, 0.37037037037037035, 1.0, 1.0, 1.0, 0.47099460882695854, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2352912368597012, 0.23529123685970105, 0.291845160846426], 
reward next is 0.7082, 
noisyNet noise sample is [array([0.5774723], dtype=float32), -0.1906391]. 
=============================================
[2019-04-27 19:55:11,307] A3C_AGENT_WORKER-Thread-3 INFO:Local step 72500, global step 1155701: loss 0.3860
[2019-04-27 19:55:11,313] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 72500, global step 1155703: learning rate 0.0001
[2019-04-27 19:55:18,213] A3C_AGENT_WORKER-Thread-19 INFO:Local step 73000, global step 1159131: loss 1.1811
[2019-04-27 19:55:18,216] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 73000, global step 1159131: learning rate 0.0001
[2019-04-27 19:55:20,760] A3C_AGENT_WORKER-Thread-16 INFO:Local step 72500, global step 1160473: loss 0.1808
[2019-04-27 19:55:20,763] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 72500, global step 1160473: learning rate 0.0001
[2019-04-27 19:55:20,906] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.5797655e-30 1.0000000e+00 5.5866900e-29 6.9657951e-33 1.4817858e-18], sum to 1.0000
[2019-04-27 19:55:20,918] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4304
[2019-04-27 19:55:20,922] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 100.0, 1.0, 2.0, 0.7014627358198725, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 799469.9520095333, 799469.9520095333, 176417.2662065039], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3739800.0000, 
sim time next is 3740400.0000, 
raw observation next is [24.0, 100.0, 1.0, 2.0, 0.677500949363094, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 772146.5087235766, 772146.5087235766, 171925.4094873653], 
processed observation next is [1.0, 0.30434782608695654, 0.4444444444444444, 1.0, 1.0, 1.0, 0.6160725587655881, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2757666102584202, 0.2757666102584202, 0.33062578747570254], 
reward next is 0.6694, 
noisyNet noise sample is [array([-0.56149], dtype=float32), 0.02259747]. 
=============================================
[2019-04-27 19:55:21,376] A3C_AGENT_WORKER-Thread-18 INFO:Local step 72500, global step 1160787: loss 0.4840
[2019-04-27 19:55:21,377] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 72500, global step 1160787: learning rate 0.0001
[2019-04-27 19:55:21,508] A3C_AGENT_WORKER-Thread-11 INFO:Local step 72500, global step 1160853: loss 0.0647
[2019-04-27 19:55:21,510] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 72500, global step 1160853: learning rate 0.0001
[2019-04-27 19:55:21,579] A3C_AGENT_WORKER-Thread-15 INFO:Local step 72500, global step 1160890: loss 0.1178
[2019-04-27 19:55:21,581] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 72500, global step 1160890: learning rate 0.0001
[2019-04-27 19:55:21,630] A3C_AGENT_WORKER-Thread-17 INFO:Local step 72500, global step 1160914: loss 0.1362
[2019-04-27 19:55:21,633] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 72500, global step 1160914: learning rate 0.0001
[2019-04-27 19:55:21,642] A3C_AGENT_WORKER-Thread-2 INFO:Local step 72500, global step 1160918: loss 0.3219
[2019-04-27 19:55:21,643] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 72500, global step 1160918: learning rate 0.0001
[2019-04-27 19:55:21,700] A3C_AGENT_WORKER-Thread-12 INFO:Local step 72500, global step 1160952: loss 0.2596
[2019-04-27 19:55:21,701] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 72500, global step 1160952: learning rate 0.0001
[2019-04-27 19:55:21,722] A3C_AGENT_WORKER-Thread-20 INFO:Local step 72500, global step 1160964: loss 0.3159
[2019-04-27 19:55:21,726] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 72500, global step 1160965: learning rate 0.0001
[2019-04-27 19:55:21,734] A3C_AGENT_WORKER-Thread-9 INFO:Local step 72500, global step 1160970: loss 0.2743
[2019-04-27 19:55:21,735] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 72500, global step 1160970: learning rate 0.0001
[2019-04-27 19:55:21,746] A3C_AGENT_WORKER-Thread-13 INFO:Local step 72500, global step 1160974: loss 0.2593
[2019-04-27 19:55:21,749] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 72500, global step 1160975: learning rate 0.0001
[2019-04-27 19:55:21,823] A3C_AGENT_WORKER-Thread-10 INFO:Local step 72500, global step 1161012: loss 0.7430
[2019-04-27 19:55:21,826] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 72500, global step 1161012: learning rate 0.0001
[2019-04-27 19:55:21,884] A3C_AGENT_WORKER-Thread-22 INFO:Local step 72500, global step 1161048: loss 0.8188
[2019-04-27 19:55:21,887] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 72500, global step 1161049: learning rate 0.0001
[2019-04-27 19:55:21,937] A3C_AGENT_WORKER-Thread-21 INFO:Local step 72500, global step 1161069: loss 0.7943
[2019-04-27 19:55:21,939] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 72500, global step 1161071: learning rate 0.0001
[2019-04-27 19:55:22,058] A3C_AGENT_WORKER-Thread-14 INFO:Local step 72500, global step 1161136: loss 0.6411
[2019-04-27 19:55:22,058] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 72500, global step 1161136: learning rate 0.0001
[2019-04-27 19:55:27,015] A3C_AGENT_WORKER-Thread-3 INFO:Local step 73000, global step 1163702: loss 0.0829
[2019-04-27 19:55:27,016] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 73000, global step 1163702: learning rate 0.0001
[2019-04-27 19:55:27,147] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.0343021e-36 1.0000000e+00 5.0846839e-36 0.0000000e+00 1.2529146e-28], sum to 1.0000
[2019-04-27 19:55:27,155] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6474
[2019-04-27 19:55:27,162] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.98333333333333, 49.33333333333334, 1.0, 2.0, 0.7093941609613229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 808514.3104822761, 808514.3104822757, 177931.6629825271], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3859800.0000, 
sim time next is 3860400.0000, 
raw observation next is [33.96666666666667, 48.66666666666667, 1.0, 2.0, 0.6875673180855572, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 783624.9939572404, 783624.9939572404, 173805.0944261228], 
processed observation next is [0.0, 0.6956521739130435, 0.8135802469135803, 0.4866666666666667, 1.0, 1.0, 0.6280563310542346, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.279866069270443, 0.279866069270443, 0.3342405662040823], 
reward next is 0.6658, 
noisyNet noise sample is [array([-0.56964433], dtype=float32), -1.5868236]. 
=============================================
[2019-04-27 19:55:33,706] A3C_AGENT_WORKER-Thread-19 INFO:Local step 73500, global step 1167224: loss 0.3483
[2019-04-27 19:55:33,710] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 73500, global step 1167225: learning rate 0.0001
[2019-04-27 19:55:36,014] A3C_AGENT_WORKER-Thread-16 INFO:Local step 73000, global step 1168453: loss 0.4775
[2019-04-27 19:55:36,015] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 73000, global step 1168453: learning rate 0.0001
[2019-04-27 19:55:36,252] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.7731948e-18 1.3690573e-09 2.7882681e-16 2.4516500e-18 1.0000000e+00], sum to 1.0000
[2019-04-27 19:55:36,260] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9528
[2019-04-27 19:55:36,267] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.75, 79.0, 1.0, 2.0, 0.4986688638398493, 1.0, 2.0, 0.4986688638398493, 1.0, 2.0, 0.7938970717441263, 6.911199999999999, 6.9112, 121.94756008, 1706014.488644921, 1706014.488644921, 342936.1768736436], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4035000.0000, 
sim time next is 4035600.0000, 
raw observation next is [26.7, 78.0, 1.0, 2.0, 0.471467024785587, 1.0, 2.0, 0.471467024785587, 1.0, 2.0, 0.7505908580676907, 6.9112, 6.9112, 121.94756008, 1612869.28059733, 1612869.28059733, 329802.000877006], 
processed observation next is [1.0, 0.7391304347826086, 0.5444444444444444, 0.78, 1.0, 1.0, 0.3707940771256988, 1.0, 1.0, 0.3707940771256988, 1.0, 1.0, 0.6882385725846132, 0.0, 0.0, 0.8096049824067558, 0.576024743070475, 0.576024743070475, 0.6342346170711654], 
reward next is 0.3658, 
noisyNet noise sample is [array([0.50932735], dtype=float32), 0.40360716]. 
=============================================
[2019-04-27 19:55:36,677] A3C_AGENT_WORKER-Thread-11 INFO:Local step 73000, global step 1168804: loss 0.1706
[2019-04-27 19:55:36,679] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 73000, global step 1168804: learning rate 0.0001
[2019-04-27 19:55:36,754] A3C_AGENT_WORKER-Thread-17 INFO:Local step 73000, global step 1168839: loss 0.1676
[2019-04-27 19:55:36,757] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 73000, global step 1168840: learning rate 0.0001
[2019-04-27 19:55:36,796] A3C_AGENT_WORKER-Thread-18 INFO:Local step 73000, global step 1168863: loss 0.2318
[2019-04-27 19:55:36,797] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 73000, global step 1168863: learning rate 0.0001
[2019-04-27 19:55:36,830] A3C_AGENT_WORKER-Thread-15 INFO:Local step 73000, global step 1168882: loss 0.2651
[2019-04-27 19:55:36,834] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 73000, global step 1168882: learning rate 0.0001
[2019-04-27 19:55:36,855] A3C_AGENT_WORKER-Thread-2 INFO:Local step 73000, global step 1168894: loss 0.2060
[2019-04-27 19:55:36,855] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 73000, global step 1168894: learning rate 0.0001
[2019-04-27 19:55:36,859] A3C_AGENT_WORKER-Thread-12 INFO:Local step 73000, global step 1168896: loss 0.2178
[2019-04-27 19:55:36,860] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 73000, global step 1168896: learning rate 0.0001
[2019-04-27 19:55:37,002] A3C_AGENT_WORKER-Thread-9 INFO:Local step 73000, global step 1168969: loss 0.1688
[2019-04-27 19:55:37,008] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 73000, global step 1168971: learning rate 0.0001
[2019-04-27 19:55:37,034] A3C_AGENT_WORKER-Thread-20 INFO:Local step 73000, global step 1168985: loss 0.1348
[2019-04-27 19:55:37,035] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 73000, global step 1168985: learning rate 0.0001
[2019-04-27 19:55:37,089] A3C_AGENT_WORKER-Thread-22 INFO:Local step 73000, global step 1169014: loss 0.1359
[2019-04-27 19:55:37,091] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 73000, global step 1169014: learning rate 0.0001
[2019-04-27 19:55:37,106] A3C_AGENT_WORKER-Thread-13 INFO:Local step 73000, global step 1169022: loss 0.1120
[2019-04-27 19:55:37,109] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 73000, global step 1169022: learning rate 0.0001
[2019-04-27 19:55:37,193] A3C_AGENT_WORKER-Thread-10 INFO:Local step 73000, global step 1169063: loss 0.0522
[2019-04-27 19:55:37,196] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 73000, global step 1169065: learning rate 0.0001
[2019-04-27 19:55:37,212] A3C_AGENT_WORKER-Thread-14 INFO:Local step 73000, global step 1169071: loss 0.2836
[2019-04-27 19:55:37,213] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 73000, global step 1169072: learning rate 0.0001
[2019-04-27 19:55:37,326] A3C_AGENT_WORKER-Thread-21 INFO:Local step 73000, global step 1169134: loss 0.0337
[2019-04-27 19:55:37,329] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 73000, global step 1169135: learning rate 0.0001
[2019-04-27 19:55:39,053] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.4773121e-28 1.6575187e-22 9.7622806e-23 7.1630212e-25 1.0000000e+00], sum to 1.0000
[2019-04-27 19:55:39,061] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9813
[2019-04-27 19:55:39,067] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.25, 68.33333333333334, 1.0, 2.0, 0.4520246393236958, 1.0, 2.0, 0.4520246393236958, 1.0, 2.0, 0.7196379472180711, 6.911200000000001, 6.9112, 121.94756008, 1546294.982896526, 1546294.982896525, 320621.0469793997], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4101000.0000, 
sim time next is 4101600.0000, 
raw observation next is [27.4, 68.66666666666667, 1.0, 2.0, 0.4539971289272494, 1.0, 2.0, 0.4539971289272494, 1.0, 2.0, 0.7227782149064299, 6.911200000000001, 6.9112, 121.94756008, 1553049.356477857, 1553049.356477857, 321542.9668029878], 
processed observation next is [1.0, 0.4782608695652174, 0.5703703703703703, 0.6866666666666668, 1.0, 1.0, 0.3499965820562493, 1.0, 1.0, 0.3499965820562493, 1.0, 1.0, 0.6534727686330374, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.5546604844563775, 0.5546604844563775, 0.618351859236515], 
reward next is 0.3816, 
noisyNet noise sample is [array([-0.01704042], dtype=float32), 1.81647]. 
=============================================
[2019-04-27 19:55:39,619] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.1442395e-21 6.9193451e-10 1.5972389e-16 5.1488080e-19 1.0000000e+00], sum to 1.0000
[2019-04-27 19:55:39,629] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6011
[2019-04-27 19:55:39,634] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.9, 67.0, 1.0, 2.0, 0.5676706541978753, 1.0, 2.0, 0.5676706541978753, 1.0, 2.0, 0.9037501692255282, 6.9112, 6.9112, 121.94756008, 1942335.689603683, 1942335.689603683, 378042.4808428687], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4109400.0000, 
sim time next is 4110000.0000, 
raw observation next is [28.86666666666667, 66.0, 1.0, 2.0, 0.5696431233607608, 1.0, 2.0, 0.5696431233607608, 1.0, 2.0, 0.9068904043716779, 6.9112, 6.9112, 121.94756008, 1949092.031778807, 1949092.031778807, 379083.8599617617], 
processed observation next is [1.0, 0.5652173913043478, 0.6246913580246916, 0.66, 1.0, 1.0, 0.4876703849532867, 1.0, 1.0, 0.4876703849532867, 1.0, 1.0, 0.8836130054645974, 0.0, 0.0, 0.8096049824067558, 0.6961042970638597, 0.6961042970638597, 0.7290074230033878], 
reward next is 0.2710, 
noisyNet noise sample is [array([-0.91541797], dtype=float32), -0.5274571]. 
=============================================
[2019-04-27 19:55:39,660] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[49.371037]
 [49.02482 ]
 [48.936855]
 [48.88045 ]
 [48.792816]], R is [[49.52135468]
 [49.29913712]
 [49.09195328]
 [48.95121765]
 [48.85450745]].
[2019-04-27 19:55:42,352] A3C_AGENT_WORKER-Thread-3 INFO:Local step 73500, global step 1171729: loss 9.7048
[2019-04-27 19:55:42,355] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 73500, global step 1171729: learning rate 0.0001
[2019-04-27 19:55:46,753] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.5259061e-34 1.0000000e+00 2.2187255e-36 6.5977582e-38 2.7860792e-30], sum to 1.0000
[2019-04-27 19:55:46,764] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1790
[2019-04-27 19:55:46,768] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.2, 72.5, 1.0, 2.0, 0.6798872608837635, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 837406.283050732, 837406.283050732, 174884.6415126255], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4242600.0000, 
sim time next is 4243200.0000, 
raw observation next is [22.8, 74.33333333333334, 1.0, 2.0, 0.5843962243930172, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 721161.3012016342, 721161.3012016342, 157548.1954942732], 
processed observation next is [1.0, 0.08695652173913043, 0.4, 0.7433333333333334, 1.0, 1.0, 0.5052336004678776, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.25755760757201224, 0.25755760757201224, 0.30297729902744847], 
reward next is 0.6970, 
noisyNet noise sample is [array([-1.7099227], dtype=float32), -0.028816307]. 
=============================================
[2019-04-27 19:55:48,581] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-27 19:55:48,582] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 19:55:48,582] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:55:48,583] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 19:55:48,584] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:55:48,585] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 19:55:48,585] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 19:55:48,585] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:55:48,586] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:55:48,587] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 19:55:48,589] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:55:48,610] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run48
[2019-04-27 19:55:48,611] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run48
[2019-04-27 19:55:48,611] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run48
[2019-04-27 19:55:48,611] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run48
[2019-04-27 19:55:48,655] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run48
[2019-04-27 19:56:03,833] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04813195], dtype=float32), 0.0024344025]
[2019-04-27 19:56:03,834] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [37.0, 16.16666666666667, 1.0, 2.0, 0.6946697737147572, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 865051.810357057, 865051.810357057, 177948.5679284399]
[2019-04-27 19:56:03,836] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:56:03,838] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [2.6098159e-24 9.9999857e-01 1.2478327e-21 1.3817187e-23 1.3744158e-06], sampled 0.9215095681532018
[2019-04-27 19:56:03,947] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04813195], dtype=float32), 0.0024344025]
[2019-04-27 19:56:03,949] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [21.55, 65.0, 1.0, 2.0, 0.6423283072089581, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 817289.4652583299, 817289.4652583294, 168317.3520425776]
[2019-04-27 19:56:03,950] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:56:03,953] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.8790547e-28 1.0000000e+00 1.5943681e-28 5.4190391e-30 1.5983093e-19], sampled 0.9696681330971858
[2019-04-27 19:56:07,699] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04813195], dtype=float32), 0.0024344025]
[2019-04-27 19:56:07,700] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [28.271050695, 31.802607015, 1.0, 2.0, 0.3355813872979118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 426967.322955514, 426967.3229555135, 120078.9091206797]
[2019-04-27 19:56:07,702] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:56:07,705] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [5.9049287e-30 1.0000000e+00 8.5869392e-29 3.3960008e-31 1.2719442e-16], sampled 0.2545930713152289
[2019-04-27 19:56:21,710] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04813195], dtype=float32), 0.0024344025]
[2019-04-27 19:56:21,710] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [34.0, 41.0, 1.0, 2.0, 0.7210361146572022, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 830314.0569242812, 830314.0569242812, 180593.254923237]
[2019-04-27 19:56:21,712] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:56:21,714] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [3.6384676e-27 1.0000000e+00 7.0745175e-26 1.3850901e-27 1.0999005e-13], sampled 0.38271545018794306
[2019-04-27 19:56:25,341] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04813195], dtype=float32), 0.0024344025]
[2019-04-27 19:56:25,342] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [33.7, 46.0, 1.0, 2.0, 0.6444713297979563, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 734484.6806802172, 734484.6806802177, 165897.6475875616]
[2019-04-27 19:56:25,344] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:56:25,347] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.8832516e-29 1.0000000e+00 1.5224249e-28 2.4388467e-30 5.2637202e-17], sampled 0.631852987828786
[2019-04-27 19:56:34,358] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04813195], dtype=float32), 0.0024344025]
[2019-04-27 19:56:34,360] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [30.33333333333334, 76.0, 1.0, 2.0, 0.7995612094717149, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000002, 6.9112, 121.9260426156618, 911341.0550418189, 911341.0550418181, 195859.3482100465]
[2019-04-27 19:56:34,360] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:56:34,364] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [3.2455300e-26 1.0000000e+00 7.8427431e-24 6.4545807e-26 5.9575513e-09], sampled 0.25051699362587243
[2019-04-27 19:56:36,482] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04813195], dtype=float32), 0.0024344025]
[2019-04-27 19:56:36,483] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [25.0, 78.5, 1.0, 2.0, 0.5486124186746155, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 648276.7967985752, 648276.7967985752, 150506.079054339]
[2019-04-27 19:56:36,485] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 19:56:36,487] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [3.2758809e-33 1.0000000e+00 5.1548807e-34 1.2426146e-35 2.4981735e-26], sampled 0.3494996586375232
[2019-04-27 19:56:42,866] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04813195], dtype=float32), 0.0024344025]
[2019-04-27 19:56:42,866] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [31.0, 65.66666666666667, 1.0, 2.0, 0.7303228802341011, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 832380.2488116848, 832380.2488116848, 181968.3183629829]
[2019-04-27 19:56:42,867] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:56:42,869] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [6.8338481e-27 1.0000000e+00 6.8733332e-25 6.6087778e-27 6.0860886e-11], sampled 0.11990794889354617
[2019-04-27 19:56:54,531] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04813195], dtype=float32), 0.0024344025]
[2019-04-27 19:56:54,532] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [24.0, 89.0, 1.0, 2.0, 0.550410493510599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 643991.0305513463, 643991.0305513458, 150537.0835296109]
[2019-04-27 19:56:54,532] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 19:56:54,535] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.4773068e-30 1.0000000e+00 2.6005794e-30 3.5118952e-32 6.0358891e-20], sampled 0.30183264429277434
[2019-04-27 19:56:57,561] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04813195], dtype=float32), 0.0024344025]
[2019-04-27 19:56:57,562] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [27.636233215, 81.630672505, 1.0, 2.0, 0.675217370432876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 769542.6118300912, 769542.6118300912, 171508.0535026581]
[2019-04-27 19:56:57,563] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:56:57,565] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [2.1129279e-28 1.0000000e+00 1.1345472e-27 1.5030637e-29 1.8628304e-15], sampled 0.20857340862060292
[2019-04-27 19:57:15,470] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04813195], dtype=float32), 0.0024344025]
[2019-04-27 19:57:15,471] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [21.66666666666667, 90.33333333333334, 1.0, 2.0, 0.4395134931282084, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 534965.7596522585, 534965.759652258, 134072.3358145971]
[2019-04-27 19:57:15,472] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 19:57:15,478] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.4256324e-35 1.0000000e+00 8.9446500e-37 1.7925734e-38 6.1654558e-30], sampled 0.26782856415771295
[2019-04-27 19:57:37,136] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04813195], dtype=float32), 0.0024344025]
[2019-04-27 19:57:37,137] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [21.08333333333333, 61.16666666666667, 1.0, 2.0, 0.24678781437096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 317180.7989267517, 317180.7989267517, 109249.9454894592]
[2019-04-27 19:57:37,140] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 19:57:37,142] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [9.9384350e-30 1.0000000e+00 3.4445919e-29 1.4783377e-31 1.5299137e-18], sampled 0.7155051742518624
[2019-04-27 19:57:39,151] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8765.3745 2211927049.1373 409.0000
[2019-04-27 19:57:39,169] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8235.3929 2460339992.8444 435.0000
[2019-04-27 19:57:39,170] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8949.4450 2130004661.5044 351.0000
[2019-04-27 19:57:39,250] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8843.1918 2179875490.6831 353.0000
[2019-04-27 19:57:39,334] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8689.1595 2259476720.1810 338.0000
[2019-04-27 19:57:40,352] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 1175000, evaluation results [1175000.0, 8235.392912102348, 2460339992.844368, 435.0, 8843.191845699888, 2179875490.6830564, 353.0, 8949.44498174011, 2130004661.5044043, 351.0, 8689.159505350317, 2259476720.180962, 338.0, 8765.374491263023, 2211927049.1373262, 409.0]
[2019-04-27 19:57:40,785] A3C_AGENT_WORKER-Thread-19 INFO:Local step 74000, global step 1175214: loss 4.9855
[2019-04-27 19:57:40,789] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 74000, global step 1175214: learning rate 0.0001
[2019-04-27 19:57:43,043] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.6784778e-22 1.0000000e+00 1.9684478e-24 1.7534558e-22 6.5036215e-17], sum to 1.0000
[2019-04-27 19:57:43,056] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4054
[2019-04-27 19:57:43,063] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1521916.836901416 W.
[2019-04-27 19:57:43,067] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [23.9, 95.0, 1.0, 2.0, 1.02, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.587467659765186, 6.9112, 121.9233223230012, 1521916.836901416, 1175615.077497092, 246301.5754806661], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4696200.0000, 
sim time next is 4696800.0000, 
raw observation next is [23.6, 96.66666666666666, 1.0, 2.0, 0.5959144745961168, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9487152511365419, 6.911199999999999, 6.9112, 121.9256353590084, 1358844.910232587, 1358844.910232587, 292272.2388961862], 
processed observation next is [1.0, 0.34782608695652173, 0.4296296296296297, 0.9666666666666666, 1.0, 1.0, 0.5189458030906152, 0.0, 1.0, -0.1904761904761905, 1.0, 0.5, 0.9358940639206772, -8.881784197001253e-17, 0.0, 0.8094594250594744, 0.48530175365449535, 0.48530175365449535, 0.5620619978772811], 
reward next is 0.4379, 
noisyNet noise sample is [array([-0.09403741], dtype=float32), -2.5396783]. 
=============================================
[2019-04-27 19:57:43,636] A3C_AGENT_WORKER-Thread-16 INFO:Local step 73500, global step 1176562: loss 0.5272
[2019-04-27 19:57:43,637] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 73500, global step 1176562: learning rate 0.0001
[2019-04-27 19:57:44,100] A3C_AGENT_WORKER-Thread-18 INFO:Local step 73500, global step 1176772: loss 0.5519
[2019-04-27 19:57:44,104] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 73500, global step 1176775: learning rate 0.0001
[2019-04-27 19:57:44,184] A3C_AGENT_WORKER-Thread-17 INFO:Local step 73500, global step 1176818: loss 0.6851
[2019-04-27 19:57:44,187] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 73500, global step 1176819: learning rate 0.0001
[2019-04-27 19:57:44,207] A3C_AGENT_WORKER-Thread-11 INFO:Local step 73500, global step 1176828: loss 0.3758
[2019-04-27 19:57:44,209] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 73500, global step 1176829: learning rate 0.0001
[2019-04-27 19:57:44,255] A3C_AGENT_WORKER-Thread-2 INFO:Local step 73500, global step 1176844: loss -24.6745
[2019-04-27 19:57:44,260] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 73500, global step 1176845: learning rate 0.0001
[2019-04-27 19:57:44,365] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.7230892e-15 9.9999976e-01 1.2987880e-13 5.1312485e-15 2.7123647e-07], sum to 1.0000
[2019-04-27 19:57:44,371] A3C_AGENT_WORKER-Thread-15 INFO:Local step 73500, global step 1176904: loss 0.2174
[2019-04-27 19:57:44,374] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 73500, global step 1176904: learning rate 0.0001
[2019-04-27 19:57:44,375] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5747
[2019-04-27 19:57:44,382] A3C_AGENT_WORKER-Thread-12 INFO:Local step 73500, global step 1176908: loss -61.2291
[2019-04-27 19:57:44,383] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1918477.911541455 W.
[2019-04-27 19:57:44,384] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 73500, global step 1176908: learning rate 0.0001
[2019-04-27 19:57:44,388] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.4, 74.66666666666667, 1.0, 2.0, 0.5607054152078416, 1.0, 2.0, 0.5607054152078416, 1.0, 2.0, 0.8926612819113972, 6.911200000000001, 6.9112, 121.94756008, 1918477.911541455, 1918477.911541455, 374381.9340374481], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4357200.0000, 
sim time next is 4357800.0000, 
raw observation next is [28.6, 72.5, 1.0, 2.0, 1.02, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9977734948820727, 7.10121199094849, 6.9112, 121.9251565208312, 1975481.275222186, 1878178.859925822, 383451.8303535489], 
processed observation next is [1.0, 0.43478260869565216, 0.6148148148148148, 0.725, 1.0, 1.0, 1.0238095238095237, 0.0, 0.5, -0.1904761904761905, 1.0, 1.0, 0.9972168686025908, 0.019001199094848965, 0.0, 0.8094562460719344, 0.7055290268650665, 0.6707781642592221, 0.7374073660645172], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.29310626], dtype=float32), 0.17666164]. 
=============================================
[2019-04-27 19:57:44,468] A3C_AGENT_WORKER-Thread-20 INFO:Local step 73500, global step 1176954: loss -40.7013
[2019-04-27 19:57:44,470] A3C_AGENT_WORKER-Thread-22 INFO:Local step 73500, global step 1176954: loss -24.4252
[2019-04-27 19:57:44,470] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 73500, global step 1176954: learning rate 0.0001
[2019-04-27 19:57:44,472] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 73500, global step 1176956: learning rate 0.0001
[2019-04-27 19:57:44,604] A3C_AGENT_WORKER-Thread-10 INFO:Local step 73500, global step 1177017: loss -192.4742
[2019-04-27 19:57:44,606] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 73500, global step 1177017: learning rate 0.0001
[2019-04-27 19:57:44,638] A3C_AGENT_WORKER-Thread-9 INFO:Local step 73500, global step 1177032: loss -39.5125
[2019-04-27 19:57:44,641] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 73500, global step 1177033: learning rate 0.0001
[2019-04-27 19:57:44,702] A3C_AGENT_WORKER-Thread-13 INFO:Local step 73500, global step 1177060: loss -99.1648
[2019-04-27 19:57:44,704] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 73500, global step 1177061: learning rate 0.0001
[2019-04-27 19:57:44,724] A3C_AGENT_WORKER-Thread-14 INFO:Local step 73500, global step 1177070: loss -37.4120
[2019-04-27 19:57:44,727] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 73500, global step 1177070: learning rate 0.0001
[2019-04-27 19:57:44,849] A3C_AGENT_WORKER-Thread-21 INFO:Local step 73500, global step 1177128: loss -15.9505
[2019-04-27 19:57:44,850] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 73500, global step 1177129: learning rate 0.0001
[2019-04-27 19:57:47,341] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 2.906597e-36], sum to 1.0000
[2019-04-27 19:57:47,349] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4490
[2019-04-27 19:57:47,353] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.06666666666667, 78.33333333333334, 1.0, 2.0, 0.5553034366926217, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 654210.4289146812, 654210.4289146812, 151534.5712762493], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4401600.0000, 
sim time next is 4402200.0000, 
raw observation next is [24.78333333333333, 78.16666666666666, 1.0, 2.0, 0.5385271867271654, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 638057.4717540786, 638057.4717540786, 148915.8895508468], 
processed observation next is [1.0, 0.9565217391304348, 0.4734567901234567, 0.7816666666666666, 1.0, 1.0, 0.45062760324662543, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.22787766848359953, 0.22787766848359953, 0.2863767106747054], 
reward next is 0.7136, 
noisyNet noise sample is [array([0.9694727], dtype=float32), -0.057535462]. 
=============================================
[2019-04-27 19:57:49,264] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 19:57:49,275] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0572
[2019-04-27 19:57:49,281] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.33333333333334, 82.33333333333334, 1.0, 2.0, 0.6381782678915837, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 727309.2639811118, 727309.2639811118, 164769.3508700177], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4440000.0000, 
sim time next is 4440600.0000, 
raw observation next is [26.5, 81.5, 1.0, 2.0, 0.64254036287586, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 732282.9643369211, 732282.9643369206, 165550.1397304996], 
processed observation next is [0.0, 0.391304347826087, 0.5370370370370371, 0.815, 1.0, 1.0, 0.5744528129474524, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.261529630120329, 0.2615296301203288, 0.31836565332788386], 
reward next is 0.6816, 
noisyNet noise sample is [array([-0.39302868], dtype=float32), -1.5364009]. 
=============================================
[2019-04-27 19:57:50,409] A3C_AGENT_WORKER-Thread-3 INFO:Local step 74000, global step 1179743: loss 0.0907
[2019-04-27 19:57:50,415] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 74000, global step 1179744: learning rate 0.0001
[2019-04-27 19:57:50,768] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 2.687676e-32], sum to 1.0000
[2019-04-27 19:57:50,777] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3343
[2019-04-27 19:57:50,780] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.728133080311069, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 829883.0889152735, 829883.0889152735, 181541.7028957919], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4462200.0000, 
sim time next is 4462800.0000, 
raw observation next is [30.0, 70.0, 1.0, 2.0, 0.7333141580214746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 835791.3950273665, 835791.3950273665, 182549.9663078595], 
processed observation next is [0.0, 0.6521739130434783, 0.6666666666666666, 0.7, 1.0, 1.0, 0.6825168547874697, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.298496926795488, 0.298496926795488, 0.3510576275151144], 
reward next is 0.6489, 
noisyNet noise sample is [array([-0.5832961], dtype=float32), 1.2465957]. 
=============================================
[2019-04-27 19:57:56,451] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 9.713897e-34], sum to 1.0000
[2019-04-27 19:57:56,461] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0644
[2019-04-27 19:57:56,467] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 94.0, 1.0, 2.0, 0.5809474561304158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 675712.2408315028, 675712.2408315028, 155478.8704360882], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4563000.0000, 
sim time next is 4563600.0000, 
raw observation next is [23.46666666666667, 94.0, 1.0, 2.0, 0.5734449492247621, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 668836.6896821251, 668836.6896821251, 154290.9621920363], 
processed observation next is [0.0, 0.8260869565217391, 0.42469135802469143, 0.94, 1.0, 1.0, 0.49219636812471673, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2388702463150447, 0.2388702463150447, 0.29671338883083903], 
reward next is 0.7033, 
noisyNet noise sample is [array([0.1986345], dtype=float32), 0.10538657]. 
=============================================
[2019-04-27 19:57:57,327] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.0153887e-35 1.0000000e+00 1.8804443e-37 0.0000000e+00 1.9305278e-33], sum to 1.0000
[2019-04-27 19:57:57,336] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5310
[2019-04-27 19:57:57,341] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 100.0, 1.0, 2.0, 0.4767870266231637, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 575292.6595478867, 575292.6595478867, 139526.9669840808], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4582800.0000, 
sim time next is 4583400.0000, 
raw observation next is [21.1, 99.66666666666667, 1.0, 2.0, 0.4775865532115207, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 575808.052026434, 575808.052026434, 139634.9541864309], 
processed observation next is [1.0, 0.043478260869565216, 0.3370370370370371, 0.9966666666666667, 1.0, 1.0, 0.3780792300137152, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.20564573286658358, 0.20564573286658358, 0.26852875805082865], 
reward next is 0.7315, 
noisyNet noise sample is [array([-0.43686602], dtype=float32), 0.5936147]. 
=============================================
[2019-04-27 19:57:57,948] A3C_AGENT_WORKER-Thread-19 INFO:Local step 74500, global step 1183336: loss 12.8024
[2019-04-27 19:57:57,950] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 74500, global step 1183336: learning rate 0.0001
[2019-04-27 19:57:59,297] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4605304e-22 6.5411371e-11 6.6667645e-19 1.5127747e-20 1.0000000e+00], sum to 1.0000
[2019-04-27 19:57:59,306] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8588
[2019-04-27 19:57:59,310] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.5, 72.5, 1.0, 2.0, 0.3980698858582377, 1.0, 2.0, 0.3980698858582377, 1.0, 2.0, 0.6337402225174168, 6.9112, 6.9112, 121.94756008, 1361561.432242368, 1361561.432242368, 296237.6690647618], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4620600.0000, 
sim time next is 4621200.0000, 
raw observation next is [27.66666666666666, 73.0, 1.0, 2.0, 0.4137980092441405, 1.0, 2.0, 0.4137980092441405, 1.0, 2.0, 0.6587799071769921, 6.911199999999999, 6.9112, 121.94756008, 1415407.732447332, 1415407.732447333, 303179.1693813886], 
processed observation next is [1.0, 0.4782608695652174, 0.5802469135802467, 0.73, 1.0, 1.0, 0.3021404871954054, 1.0, 1.0, 0.3021404871954054, 1.0, 1.0, 0.57347488397124, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.5055027615883328, 0.5055027615883332, 0.583036864194978], 
reward next is 0.4170, 
noisyNet noise sample is [array([-0.48871675], dtype=float32), -0.35788825]. 
=============================================
[2019-04-27 19:58:00,262] A3C_AGENT_WORKER-Thread-16 INFO:Local step 74000, global step 1184541: loss 0.1142
[2019-04-27 19:58:00,265] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 74000, global step 1184541: learning rate 0.0001
[2019-04-27 19:58:00,553] A3C_AGENT_WORKER-Thread-11 INFO:Local step 74000, global step 1184695: loss 7.7350
[2019-04-27 19:58:00,554] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 74000, global step 1184695: learning rate 0.0001
[2019-04-27 19:58:00,681] A3C_AGENT_WORKER-Thread-18 INFO:Local step 74000, global step 1184764: loss 10.5231
[2019-04-27 19:58:00,684] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 74000, global step 1184765: learning rate 0.0001
[2019-04-27 19:58:00,818] A3C_AGENT_WORKER-Thread-2 INFO:Local step 74000, global step 1184838: loss 3.9622
[2019-04-27 19:58:00,821] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 74000, global step 1184839: learning rate 0.0001
[2019-04-27 19:58:00,840] A3C_AGENT_WORKER-Thread-17 INFO:Local step 74000, global step 1184849: loss 3.0206
[2019-04-27 19:58:00,843] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 74000, global step 1184851: learning rate 0.0001
[2019-04-27 19:58:00,872] A3C_AGENT_WORKER-Thread-12 INFO:Local step 74000, global step 1184861: loss 0.2315
[2019-04-27 19:58:00,874] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 74000, global step 1184861: learning rate 0.0001
[2019-04-27 19:58:00,973] A3C_AGENT_WORKER-Thread-15 INFO:Local step 74000, global step 1184918: loss 0.1279
[2019-04-27 19:58:00,974] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 74000, global step 1184919: learning rate 0.0001
[2019-04-27 19:58:01,014] A3C_AGENT_WORKER-Thread-22 INFO:Local step 74000, global step 1184937: loss 0.0171
[2019-04-27 19:58:01,016] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 74000, global step 1184937: learning rate 0.0001
[2019-04-27 19:58:01,056] A3C_AGENT_WORKER-Thread-20 INFO:Local step 74000, global step 1184954: loss 0.1949
[2019-04-27 19:58:01,057] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 74000, global step 1184955: learning rate 0.0001
[2019-04-27 19:58:01,070] A3C_AGENT_WORKER-Thread-10 INFO:Local step 74000, global step 1184962: loss 0.3700
[2019-04-27 19:58:01,074] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 74000, global step 1184963: learning rate 0.0001
[2019-04-27 19:58:01,162] A3C_AGENT_WORKER-Thread-9 INFO:Local step 74000, global step 1185014: loss 0.8718
[2019-04-27 19:58:01,164] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 74000, global step 1185014: learning rate 0.0001
[2019-04-27 19:58:01,279] A3C_AGENT_WORKER-Thread-13 INFO:Local step 74000, global step 1185074: loss 0.5036
[2019-04-27 19:58:01,281] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 74000, global step 1185074: learning rate 0.0001
[2019-04-27 19:58:01,326] A3C_AGENT_WORKER-Thread-14 INFO:Local step 74000, global step 1185097: loss 0.4297
[2019-04-27 19:58:01,328] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 74000, global step 1185097: learning rate 0.0001
[2019-04-27 19:58:01,407] A3C_AGENT_WORKER-Thread-21 INFO:Local step 74000, global step 1185138: loss 0.4199
[2019-04-27 19:58:01,409] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 74000, global step 1185139: learning rate 0.0001
[2019-04-27 19:58:06,497] A3C_AGENT_WORKER-Thread-3 INFO:Local step 74500, global step 1187784: loss 10.1996
[2019-04-27 19:58:06,500] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 74500, global step 1187786: learning rate 0.0001
[2019-04-27 19:58:07,464] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.5563696e-18 3.0447070e-05 7.0144337e-14 2.4707496e-16 9.9996960e-01], sum to 1.0000
[2019-04-27 19:58:07,475] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4485
[2019-04-27 19:58:07,480] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.9, 93.83333333333334, 1.0, 2.0, 0.4179928393512709, 1.0, 1.0, 0.4179928393512709, 1.0, 2.0, 0.6654582133236226, 6.911200000000001, 6.9112, 121.94756008, 1429769.658047425, 1429769.658047424, 305053.6737872803], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4783800.0000, 
sim time next is 4784400.0000, 
raw observation next is [23.9, 94.0, 1.0, 2.0, 0.4204083422127254, 1.0, 2.0, 0.4204083422127254, 1.0, 2.0, 0.6693037725464943, 6.9112, 6.9112, 121.94756008, 1438039.786753516, 1438039.786753516, 306137.486973296], 
processed observation next is [1.0, 0.391304347826087, 0.4407407407407407, 0.94, 1.0, 1.0, 0.3100099312056254, 1.0, 1.0, 0.3100099312056254, 1.0, 1.0, 0.5866297156831178, 0.0, 0.0, 0.8096049824067558, 0.5135856381262557, 0.5135856381262557, 0.5887259364871077], 
reward next is 0.4113, 
noisyNet noise sample is [array([0.13584131], dtype=float32), -0.38086075]. 
=============================================
[2019-04-27 19:58:08,066] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.6308391e-33 2.5896247e-27 1.6574694e-28 1.5687330e-29 1.0000000e+00], sum to 1.0000
[2019-04-27 19:58:08,073] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0306
[2019-04-27 19:58:08,077] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.26666666666667, 88.16666666666667, 1.0, 2.0, 0.496457319270491, 1.0, 2.0, 0.496457319270491, 1.0, 2.0, 0.7903762207647295, 6.9112, 6.9112, 121.94756008, 1698441.300558795, 1698441.300558795, 341853.6445901535], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4810200.0000, 
sim time next is 4810800.0000, 
raw observation next is [27.53333333333334, 87.33333333333334, 1.0, 2.0, 0.5388499162153455, 1.0, 2.0, 0.5388499162153455, 1.0, 2.0, 0.8578666157314336, 6.9112, 6.9112, 121.94756008, 1843621.207384442, 1843621.207384442, 363066.0054746807], 
processed observation next is [1.0, 0.6956521739130435, 0.5753086419753088, 0.8733333333333334, 1.0, 1.0, 0.45101180501826843, 1.0, 1.0, 0.45101180501826843, 1.0, 1.0, 0.8223332696642921, 0.0, 0.0, 0.8096049824067558, 0.6584361454944435, 0.6584361454944435, 0.6982038566820783], 
reward next is 0.3018, 
noisyNet noise sample is [array([0.4446706], dtype=float32), 0.9174989]. 
=============================================
[2019-04-27 19:58:08,627] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0368729e-18 9.9678349e-01 1.5312541e-17 1.2289279e-19 3.2165442e-03], sum to 1.0000
[2019-04-27 19:58:08,636] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2013
[2019-04-27 19:58:08,641] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.03333333333333, 93.33333333333334, 1.0, 2.0, 0.7907957156200198, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 901344.2568705454, 901344.2568705454, 194052.0992878359], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4821000.0000, 
sim time next is 4821600.0000, 
raw observation next is [27.06666666666667, 92.66666666666667, 1.0, 2.0, 0.7855354031976082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 895345.0817297838, 895345.0817297833, 192975.7273210877], 
processed observation next is [1.0, 0.8260869565217391, 0.5580246913580248, 0.9266666666666667, 1.0, 1.0, 0.7446850038066763, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.31976610061777994, 0.31976610061777977, 0.3711071679251687], 
reward next is 0.6289, 
noisyNet noise sample is [array([-0.68802017], dtype=float32), -0.0028708142]. 
=============================================
[2019-04-27 19:58:13,224] A3C_AGENT_WORKER-Thread-19 INFO:Local step 75000, global step 1191312: loss 0.4622
[2019-04-27 19:58:13,228] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 75000, global step 1191312: learning rate 0.0001
[2019-04-27 19:58:13,853] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7674807e-34 1.0000000e+00 1.4076143e-37 0.0000000e+00 1.2804716e-34], sum to 1.0000
[2019-04-27 19:58:13,858] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8147
[2019-04-27 19:58:13,868] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.43333333333333, 85.16666666666667, 1.0, 2.0, 0.776154960661893, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 884647.1801359899, 884647.1801359894, 191075.9996043923], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4902600.0000, 
sim time next is 4903200.0000, 
raw observation next is [29.0, 89.0, 1.0, 2.0, 0.8032810717761752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 915583.4923511527, 915583.4923511527, 196632.9749180524], 
processed observation next is [1.0, 0.782608695652174, 0.6296296296296297, 0.89, 1.0, 1.0, 0.7658107997335419, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.326994104411126, 0.326994104411126, 0.37814033638087], 
reward next is 0.6219, 
noisyNet noise sample is [array([-0.4593551], dtype=float32), -0.64067364]. 
=============================================
[2019-04-27 19:58:15,380] A3C_AGENT_WORKER-Thread-16 INFO:Local step 74500, global step 1192423: loss 13.6506
[2019-04-27 19:58:15,382] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 74500, global step 1192423: learning rate 0.0001
[2019-04-27 19:58:15,902] A3C_AGENT_WORKER-Thread-18 INFO:Local step 74500, global step 1192705: loss 14.4441
[2019-04-27 19:58:15,907] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 74500, global step 1192706: learning rate 0.0001
[2019-04-27 19:58:16,064] A3C_AGENT_WORKER-Thread-11 INFO:Local step 74500, global step 1192787: loss 12.3461
[2019-04-27 19:58:16,065] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 74500, global step 1192787: learning rate 0.0001
[2019-04-27 19:58:16,113] A3C_AGENT_WORKER-Thread-17 INFO:Local step 74500, global step 1192805: loss 12.0911
[2019-04-27 19:58:16,115] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 74500, global step 1192805: learning rate 0.0001
[2019-04-27 19:58:16,218] A3C_AGENT_WORKER-Thread-2 INFO:Local step 74500, global step 1192866: loss 12.9590
[2019-04-27 19:58:16,220] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 74500, global step 1192866: learning rate 0.0001
[2019-04-27 19:58:16,254] A3C_AGENT_WORKER-Thread-15 INFO:Local step 74500, global step 1192880: loss 13.0677
[2019-04-27 19:58:16,257] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 74500, global step 1192881: learning rate 0.0001
[2019-04-27 19:58:16,337] A3C_AGENT_WORKER-Thread-22 INFO:Local step 74500, global step 1192923: loss 13.9937
[2019-04-27 19:58:16,338] A3C_AGENT_WORKER-Thread-12 INFO:Local step 74500, global step 1192924: loss 13.1310
[2019-04-27 19:58:16,339] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 74500, global step 1192924: learning rate 0.0001
[2019-04-27 19:58:16,341] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 74500, global step 1192924: learning rate 0.0001
[2019-04-27 19:58:16,446] A3C_AGENT_WORKER-Thread-10 INFO:Local step 74500, global step 1192981: loss 13.0687
[2019-04-27 19:58:16,449] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 74500, global step 1192981: learning rate 0.0001
[2019-04-27 19:58:16,481] A3C_AGENT_WORKER-Thread-9 INFO:Local step 74500, global step 1192997: loss 12.5376
[2019-04-27 19:58:16,484] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 74500, global step 1192997: learning rate 0.0001
[2019-04-27 19:58:16,484] A3C_AGENT_WORKER-Thread-20 INFO:Local step 74500, global step 1192997: loss 13.1514
[2019-04-27 19:58:16,489] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 74500, global step 1192999: learning rate 0.0001
[2019-04-27 19:58:16,526] A3C_AGENT_WORKER-Thread-14 INFO:Local step 74500, global step 1193019: loss 13.3252
[2019-04-27 19:58:16,527] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 74500, global step 1193019: learning rate 0.0001
[2019-04-27 19:58:16,651] A3C_AGENT_WORKER-Thread-13 INFO:Local step 74500, global step 1193085: loss 12.5987
[2019-04-27 19:58:16,652] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 74500, global step 1193086: learning rate 0.0001
[2019-04-27 19:58:16,822] A3C_AGENT_WORKER-Thread-21 INFO:Local step 74500, global step 1193176: loss 10.7527
[2019-04-27 19:58:16,824] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 74500, global step 1193176: learning rate 0.0001
[2019-04-27 19:58:21,616] A3C_AGENT_WORKER-Thread-3 INFO:Local step 75000, global step 1195685: loss 0.0167
[2019-04-27 19:58:21,618] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 75000, global step 1195685: learning rate 0.0001
[2019-04-27 19:58:25,146] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.922683e-37 1.000000e+00 0.000000e+00 0.000000e+00 6.760458e-38], sum to 1.0000
[2019-04-27 19:58:25,157] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0813
[2019-04-27 19:58:25,165] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.85, 96.5, 1.0, 2.0, 0.727627503505801, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 829306.5505850486, 829306.5505850482, 181443.0918796339], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5124600.0000, 
sim time next is 5125200.0000, 
raw observation next is [26.13333333333333, 95.33333333333334, 1.0, 2.0, 0.7367783672500761, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 839741.8743177919, 839741.8743177919, 183226.9877760185], 
processed observation next is [0.0, 0.30434782608695654, 0.5234567901234567, 0.9533333333333335, 1.0, 1.0, 0.6866409133929476, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.29990781225635427, 0.29990781225635427, 0.35235959187695864], 
reward next is 0.6476, 
noisyNet noise sample is [array([-0.47510782], dtype=float32), 0.7444142]. 
=============================================
[2019-04-27 19:58:28,971] A3C_AGENT_WORKER-Thread-19 INFO:Local step 75500, global step 1199509: loss 0.0583
[2019-04-27 19:58:28,976] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 75500, global step 1199510: learning rate 0.0001
[2019-04-27 19:58:29,908] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-27 19:58:29,910] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 19:58:29,910] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 19:58:29,911] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:58:29,911] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:58:29,913] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 19:58:29,915] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 19:58:29,915] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:58:29,915] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 19:58:29,916] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:58:29,918] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 19:58:29,937] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run49
[2019-04-27 19:58:29,938] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run49
[2019-04-27 19:58:29,979] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run49
[2019-04-27 19:58:29,980] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run49
[2019-04-27 19:58:30,009] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run49
[2019-04-27 19:59:43,723] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03517532], dtype=float32), -0.005814255]
[2019-04-27 19:59:43,724] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [27.23333333333333, 83.33333333333334, 1.0, 2.0, 0.7228037982545928, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 823805.8236599652, 823805.8236599652, 180506.3591590478]
[2019-04-27 19:59:43,729] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:59:43,733] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [2.9717040e-35 1.0000000e+00 1.2583741e-37 0.0000000e+00 4.9255007e-36], sampled 0.21523539034437356
[2019-04-27 19:59:45,582] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03517532], dtype=float32), -0.005814255]
[2019-04-27 19:59:45,583] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [29.6, 78.0, 1.0, 2.0, 0.8019902882322919, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 914111.3738386043, 914111.3738386043, 196359.3821118289]
[2019-04-27 19:59:45,585] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 19:59:45,589] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [8.7348563e-35 1.0000000e+00 1.0164190e-36 0.0000000e+00 3.7640036e-34], sampled 0.2708879417578519
[2019-04-27 19:59:48,429] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03517532], dtype=float32), -0.005814255]
[2019-04-27 19:59:48,430] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [29.09220769833334, 71.95492143333333, 1.0, 2.0, 0.7022968676019847, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 800421.123695223, 800421.123695223, 176580.3122851602]
[2019-04-27 19:59:48,432] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 19:59:48,434] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [9.6829582e-36 1.0000000e+00 7.0215086e-38 0.0000000e+00 7.5280462e-36], sampled 0.2475703640704453
[2019-04-27 19:59:56,615] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03517532], dtype=float32), -0.005814255]
[2019-04-27 19:59:56,617] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [30.1, 61.33333333333334, 1.0, 2.0, 0.6464475325952437, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 736737.9821689208, 736737.9821689213, 166252.7112708152]
[2019-04-27 19:59:56,620] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 19:59:56,621] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [2.4374070e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.2384008e-37], sampled 0.16763310093867578
[2019-04-27 20:00:04,414] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.03517532], dtype=float32), -0.005814255]
[2019-04-27 20:00:04,415] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [20.07403033, 78.39338895, 1.0, 2.0, 0.3352990939235838, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 424730.1530745637, 424730.1530745637, 120028.5818281266]
[2019-04-27 20:00:04,419] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 20:00:04,424] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.46964393536470683
[2019-04-27 20:00:19,597] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8766.9702 2170957530.8983 493.0000
[2019-04-27 20:00:19,912] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8921.3103 2120862644.6037 431.0000
[2019-04-27 20:00:20,103] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8097.2566 2445685795.0946 744.0000
[2019-04-27 20:00:20,130] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8696.5369 2196204097.2692 570.0000
[2019-04-27 20:00:20,227] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8579.8243 2249031978.5433 554.0000
[2019-04-27 20:00:21,244] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 1200000, evaluation results [1200000.0, 8097.256590303452, 2445685795.094597, 744.0, 8766.970175200424, 2170957530.898276, 493.0, 8921.310297729184, 2120862644.60367, 431.0, 8579.824315579977, 2249031978.54326, 554.0, 8696.53694268413, 2196204097.269227, 570.0]
[2019-04-27 20:00:22,294] A3C_AGENT_WORKER-Thread-16 INFO:Local step 75000, global step 1200491: loss 1.0434
[2019-04-27 20:00:22,297] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 75000, global step 1200491: learning rate 0.0001
[2019-04-27 20:00:22,846] A3C_AGENT_WORKER-Thread-11 INFO:Local step 75000, global step 1200753: loss 0.2547
[2019-04-27 20:00:22,849] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 75000, global step 1200754: learning rate 0.0001
[2019-04-27 20:00:22,878] A3C_AGENT_WORKER-Thread-17 INFO:Local step 75000, global step 1200768: loss 0.1174
[2019-04-27 20:00:22,881] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 75000, global step 1200769: learning rate 0.0001
[2019-04-27 20:00:22,957] A3C_AGENT_WORKER-Thread-18 INFO:Local step 75000, global step 1200812: loss 0.1458
[2019-04-27 20:00:22,959] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 75000, global step 1200812: learning rate 0.0001
[2019-04-27 20:00:22,964] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.9400340e-28 1.0000000e+00 1.9389187e-26 2.5743611e-32 2.4956463e-21], sum to 1.0000
[2019-04-27 20:00:22,974] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6320
[2019-04-27 20:00:22,977] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.9, 87.0, 1.0, 2.0, 0.7198512778382713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 820438.9275181234, 820438.9275181234, 179937.7420888281], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5257800.0000, 
sim time next is 5258400.0000, 
raw observation next is [26.8, 87.33333333333334, 1.0, 2.0, 0.7171056983264646, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 817308.0289711765, 817308.0289711765, 179408.9724934152], 
processed observation next is [1.0, 0.8695652173913043, 0.5481481481481482, 0.8733333333333334, 1.0, 1.0, 0.6632210694362675, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.291895724632563, 0.291895724632563, 0.34501725479502926], 
reward next is 0.6550, 
noisyNet noise sample is [array([0.30501053], dtype=float32), -0.51344305]. 
=============================================
[2019-04-27 20:00:23,017] A3C_AGENT_WORKER-Thread-12 INFO:Local step 75000, global step 1200836: loss 0.0239
[2019-04-27 20:00:23,019] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 75000, global step 1200837: learning rate 0.0001
[2019-04-27 20:00:23,073] A3C_AGENT_WORKER-Thread-22 INFO:Local step 75000, global step 1200865: loss 0.1658
[2019-04-27 20:00:23,082] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 75000, global step 1200870: learning rate 0.0001
[2019-04-27 20:00:23,118] A3C_AGENT_WORKER-Thread-15 INFO:Local step 75000, global step 1200889: loss 0.2233
[2019-04-27 20:00:23,119] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 75000, global step 1200889: learning rate 0.0001
[2019-04-27 20:00:23,173] A3C_AGENT_WORKER-Thread-9 INFO:Local step 75000, global step 1200911: loss 0.2100
[2019-04-27 20:00:23,177] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 75000, global step 1200912: learning rate 0.0001
[2019-04-27 20:00:23,180] A3C_AGENT_WORKER-Thread-2 INFO:Local step 75000, global step 1200913: loss 0.1300
[2019-04-27 20:00:23,185] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 75000, global step 1200913: learning rate 0.0001
[2019-04-27 20:00:23,241] A3C_AGENT_WORKER-Thread-10 INFO:Local step 75000, global step 1200943: loss 0.1071
[2019-04-27 20:00:23,244] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 75000, global step 1200944: learning rate 0.0001
[2019-04-27 20:00:23,318] A3C_AGENT_WORKER-Thread-20 INFO:Local step 75000, global step 1200977: loss 0.0665
[2019-04-27 20:00:23,318] A3C_AGENT_WORKER-Thread-14 INFO:Local step 75000, global step 1200977: loss 0.0173
[2019-04-27 20:00:23,319] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 75000, global step 1200977: learning rate 0.0001
[2019-04-27 20:00:23,324] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 75000, global step 1200978: learning rate 0.0001
[2019-04-27 20:00:23,505] A3C_AGENT_WORKER-Thread-13 INFO:Local step 75000, global step 1201067: loss 0.0125
[2019-04-27 20:00:23,507] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 75000, global step 1201067: learning rate 0.0001
[2019-04-27 20:00:23,809] A3C_AGENT_WORKER-Thread-21 INFO:Local step 75000, global step 1201212: loss 0.0679
[2019-04-27 20:00:23,814] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 75000, global step 1201213: learning rate 0.0001
[2019-04-27 20:00:29,206] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.9469111e-34 1.0000000e+00 3.6566845e-33 9.2001518e-36 3.2507555e-28], sum to 1.0000
[2019-04-27 20:00:29,214] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3874
[2019-04-27 20:00:29,221] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.3, 91.0, 1.0, 2.0, 0.6259090137355, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 724014.4993251206, 724014.4993251206, 163111.4576653071], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5377200.0000, 
sim time next is 5377800.0000, 
raw observation next is [24.35, 91.0, 1.0, 2.0, 0.6467919221254247, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 747343.3467945544, 747343.3467945544, 166815.3077044935], 
processed observation next is [1.0, 0.21739130434782608, 0.4574074074074075, 0.91, 1.0, 1.0, 0.5795141930064579, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2669083381409123, 0.2669083381409123, 0.32079866866248746], 
reward next is 0.6792, 
noisyNet noise sample is [array([-2.6226754], dtype=float32), 0.44401652]. 
=============================================
[2019-04-27 20:00:29,274] A3C_AGENT_WORKER-Thread-3 INFO:Local step 75500, global step 1203820: loss 0.1831
[2019-04-27 20:00:29,276] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 75500, global step 1203820: learning rate 0.0001
[2019-04-27 20:00:30,413] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.4854728e-20 4.1688150e-03 8.8992024e-16 4.7711996e-17 9.9583119e-01], sum to 1.0000
[2019-04-27 20:00:30,423] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1993
[2019-04-27 20:00:30,429] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.08333333333333, 85.16666666666667, 1.0, 2.0, 0.4384351645304218, 1.0, 2.0, 0.4384351645304218, 1.0, 2.0, 0.6980030607688827, 6.911200000000001, 6.9112, 121.94756008, 1499762.318660958, 1499762.318660958, 314327.9136529238], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5389800.0000, 
sim time next is 5390400.0000, 
raw observation next is [26.26666666666667, 84.33333333333334, 1.0, 2.0, 0.4338099062061622, 1.0, 2.0, 0.4338099062061622, 1.0, 2.0, 0.6906394988825142, 6.9112, 6.9112, 121.94756008, 1483925.295311446, 1483925.295311446, 312209.2974092858], 
processed observation next is [1.0, 0.391304347826087, 0.5283950617283951, 0.8433333333333334, 1.0, 1.0, 0.325964174054955, 1.0, 1.0, 0.325964174054955, 1.0, 1.0, 0.6132993736031427, 0.0, 0.0, 0.8096049824067558, 0.5299733197540879, 0.5299733197540879, 0.6004024950178573], 
reward next is 0.3996, 
noisyNet noise sample is [array([-0.50307703], dtype=float32), -0.41172156]. 
=============================================
[2019-04-27 20:00:32,428] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.8894458e-19 4.8509497e-05 7.1791777e-15 5.1220364e-18 9.9995148e-01], sum to 1.0000
[2019-04-27 20:00:32,436] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9618
[2019-04-27 20:00:32,442] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.86666666666667, 73.0, 1.0, 2.0, 0.2306332437491231, 1.0, 2.0, 0.2306332437491231, 1.0, 2.0, 0.3671756352489687, 6.911199999999999, 6.9112, 121.94756008, 788565.1420881279, 788565.1420881284, 230862.9566582472], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5419200.0000, 
sim time next is 5419800.0000, 
raw observation next is [29.8, 73.5, 1.0, 2.0, 0.2276283740303019, 1.0, 2.0, 0.2276283740303019, 1.0, 2.0, 0.3623917847948307, 6.9112, 6.9112, 121.94756008, 778285.8854734759, 778285.8854734759, 229832.5025556266], 
processed observation next is [1.0, 0.7391304347826086, 0.6592592592592593, 0.735, 1.0, 1.0, 0.08050996908369273, 1.0, 1.0, 0.08050996908369273, 1.0, 1.0, 0.20298973099353834, 0.0, 0.0, 0.8096049824067558, 0.27795924481195566, 0.27795924481195566, 0.44198558183774345], 
reward next is 0.5580, 
noisyNet noise sample is [array([-1.2595488], dtype=float32), -0.42912924]. 
=============================================
[2019-04-27 20:00:33,798] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2780746e-28 1.0000000e+00 1.4442869e-28 1.3711770e-31 4.9538385e-22], sum to 1.0000
[2019-04-27 20:00:33,806] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5716
[2019-04-27 20:00:33,816] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1841620.939071087 W.
[2019-04-27 20:00:33,821] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.28333333333333, 93.16666666666667, 1.0, 2.0, 1.02, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.222737286188517, 6.9112, 123.1484431547214, 1841620.939071087, 1163263.084350454, 245772.1654744472], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5451000.0000, 
sim time next is 5451600.0000, 
raw observation next is [26.26666666666667, 93.33333333333334, 1.0, 2.0, 0.6253715170681324, 1.0, 1.0, 0.6253715170681324, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9252528083467, 1426077.411062768, 1426077.411062768, 276879.4045863046], 
processed observation next is [1.0, 0.08695652173913043, 0.5283950617283951, 0.9333333333333335, 1.0, 1.0, 0.5540137107953957, 1.0, 0.5, 0.5540137107953957, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094568853209054, 0.5093133610938457, 0.5093133610938457, 0.5324603934352012], 
reward next is 0.4675, 
noisyNet noise sample is [array([1.5458412], dtype=float32), 0.0309985]. 
=============================================
[2019-04-27 20:00:36,878] A3C_AGENT_WORKER-Thread-19 INFO:Local step 76000, global step 1207419: loss 6.6027
[2019-04-27 20:00:36,882] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 76000, global step 1207419: learning rate 0.0001
[2019-04-27 20:00:39,298] A3C_AGENT_WORKER-Thread-16 INFO:Local step 75500, global step 1208523: loss 0.0722
[2019-04-27 20:00:39,302] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 75500, global step 1208523: learning rate 0.0001
[2019-04-27 20:00:39,759] A3C_AGENT_WORKER-Thread-11 INFO:Local step 75500, global step 1208720: loss 0.1109
[2019-04-27 20:00:39,762] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 75500, global step 1208720: learning rate 0.0001
[2019-04-27 20:00:39,836] A3C_AGENT_WORKER-Thread-17 INFO:Local step 75500, global step 1208758: loss 0.1065
[2019-04-27 20:00:39,840] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 75500, global step 1208763: learning rate 0.0001
[2019-04-27 20:00:39,868] A3C_AGENT_WORKER-Thread-18 INFO:Local step 75500, global step 1208775: loss 0.1381
[2019-04-27 20:00:39,869] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 75500, global step 1208776: learning rate 0.0001
[2019-04-27 20:00:40,057] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.7373752e-35 1.0000000e+00 2.0257397e-33 1.5037359e-35 1.8663407e-26], sum to 1.0000
[2019-04-27 20:00:40,065] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6953
[2019-04-27 20:00:40,068] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.3, 90.5, 1.0, 2.0, 0.6657803393039041, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 758781.9407214485, 758781.9407214485, 169764.8773232345], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5550600.0000, 
sim time next is 5551200.0000, 
raw observation next is [25.3, 90.0, 1.0, 2.0, 0.6574176297603279, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 749246.4020143951, 749246.4020143951, 168237.2808901216], 
processed observation next is [1.0, 0.2608695652173913, 0.49259259259259264, 0.9, 1.0, 1.0, 0.5921638449527713, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.26758800071942684, 0.26758800071942684, 0.3235332324810031], 
reward next is 0.6765, 
noisyNet noise sample is [array([0.10094101], dtype=float32), 1.6067178]. 
=============================================
[2019-04-27 20:00:40,082] A3C_AGENT_WORKER-Thread-22 INFO:Local step 75500, global step 1208884: loss 0.0546
[2019-04-27 20:00:40,084] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 75500, global step 1208884: learning rate 0.0001
[2019-04-27 20:00:40,091] A3C_AGENT_WORKER-Thread-12 INFO:Local step 75500, global step 1208887: loss 0.0492
[2019-04-27 20:00:40,098] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 75500, global step 1208889: learning rate 0.0001
[2019-04-27 20:00:40,141] A3C_AGENT_WORKER-Thread-9 INFO:Local step 75500, global step 1208918: loss 0.0699
[2019-04-27 20:00:40,143] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 75500, global step 1208918: learning rate 0.0001
[2019-04-27 20:00:40,184] A3C_AGENT_WORKER-Thread-15 INFO:Local step 75500, global step 1208935: loss 0.1119
[2019-04-27 20:00:40,187] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 75500, global step 1208937: learning rate 0.0001
[2019-04-27 20:00:40,193] A3C_AGENT_WORKER-Thread-2 INFO:Local step 75500, global step 1208939: loss 0.0720
[2019-04-27 20:00:40,197] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 75500, global step 1208940: learning rate 0.0001
[2019-04-27 20:00:40,264] A3C_AGENT_WORKER-Thread-14 INFO:Local step 75500, global step 1208981: loss 0.0606
[2019-04-27 20:00:40,268] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 75500, global step 1208983: learning rate 0.0001
[2019-04-27 20:00:40,286] A3C_AGENT_WORKER-Thread-20 INFO:Local step 75500, global step 1208993: loss 0.0805
[2019-04-27 20:00:40,288] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 75500, global step 1208993: learning rate 0.0001
[2019-04-27 20:00:40,292] A3C_AGENT_WORKER-Thread-10 INFO:Local step 75500, global step 1208996: loss 0.0728
[2019-04-27 20:00:40,292] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 75500, global step 1208996: learning rate 0.0001
[2019-04-27 20:00:40,347] A3C_AGENT_WORKER-Thread-13 INFO:Local step 75500, global step 1209020: loss 0.0663
[2019-04-27 20:00:40,348] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 75500, global step 1209020: learning rate 0.0001
[2019-04-27 20:00:40,430] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1380239e-34 1.0000000e+00 3.2904842e-33 1.9517608e-37 9.2750706e-28], sum to 1.0000
[2019-04-27 20:00:40,439] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1502
[2019-04-27 20:00:40,442] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.4, 85.0, 1.0, 2.0, 0.681950313956874, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 783596.0413554873, 783596.0413554873, 173072.651087924], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5558400.0000, 
sim time next is 5559000.0000, 
raw observation next is [25.48333333333333, 84.66666666666667, 1.0, 2.0, 0.6636929628448025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 762427.0367269114, 762427.0367269114, 169683.47952109], 
processed observation next is [1.0, 0.34782608695652173, 0.4993827160493826, 0.8466666666666667, 1.0, 1.0, 0.5996344795771458, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2722953702596112, 0.2722953702596112, 0.32631438369440385], 
reward next is 0.6737, 
noisyNet noise sample is [array([1.3125533], dtype=float32), 2.789737]. 
=============================================
[2019-04-27 20:00:40,456] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[68.31603 ]
 [68.246864]
 [68.35207 ]
 [68.45934 ]
 [68.60258 ]], R is [[68.54918671]
 [68.5308609 ]
 [68.49937439]
 [68.46648407]
 [68.4372406 ]].
[2019-04-27 20:00:40,715] A3C_AGENT_WORKER-Thread-21 INFO:Local step 75500, global step 1209218: loss 0.0554
[2019-04-27 20:00:40,716] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 75500, global step 1209219: learning rate 0.0001
[2019-04-27 20:00:42,469] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.7665733e-31 1.0000000e+00 4.8251120e-29 3.7831364e-33 1.1549375e-16], sum to 1.0000
[2019-04-27 20:00:42,478] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0949
[2019-04-27 20:00:42,484] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.46666666666667, 91.83333333333333, 1.0, 2.0, 0.6645476616719772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 757376.3786191136, 757376.3786191131, 169540.6733111292], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5597400.0000, 
sim time next is 5598000.0000, 
raw observation next is [25.5, 92.0, 1.0, 2.0, 0.6713782580853789, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 765165.0065467341, 765165.0065467341, 170795.9375357359], 
processed observation next is [1.0, 0.8260869565217391, 0.5, 0.92, 1.0, 1.0, 0.608783640577832, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.27327321662383364, 0.27327321662383364, 0.32845372603026135], 
reward next is 0.6715, 
noisyNet noise sample is [array([0.6236124], dtype=float32), -0.05959461]. 
=============================================
[2019-04-27 20:00:42,499] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[72.37641 ]
 [73.109604]
 [73.34793 ]
 [73.95858 ]
 [74.34967 ]], R is [[72.05453491]
 [72.00794983]
 [71.96625519]
 [71.92510223]
 [71.88497162]].
[2019-04-27 20:00:45,324] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.8094808e-35 1.0000000e+00 1.7309158e-34 3.0267309e-37 2.0842527e-28], sum to 1.0000
[2019-04-27 20:00:45,330] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1173
[2019-04-27 20:00:45,334] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.8, 84.66666666666667, 1.0, 2.0, 0.6860411942090732, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999997, 6.9112, 121.9260426156618, 781884.7734126083, 781884.7734126096, 173518.5770050304], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5653200.0000, 
sim time next is 5653800.0000, 
raw observation next is [26.95, 84.0, 1.0, 2.0, 0.6879696794476735, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 784083.8023960066, 784083.8023960066, 173879.508576239], 
processed observation next is [0.0, 0.43478260869565216, 0.5537037037037037, 0.84, 1.0, 1.0, 0.6285353326758018, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2800299294271452, 0.2800299294271452, 0.33438367033892114], 
reward next is 0.6656, 
noisyNet noise sample is [array([-0.63812506], dtype=float32), 1.1594087]. 
=============================================
[2019-04-27 20:00:45,379] A3C_AGENT_WORKER-Thread-3 INFO:Local step 76000, global step 1211655: loss 0.0436
[2019-04-27 20:00:45,381] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 76000, global step 1211657: learning rate 0.0001
[2019-04-27 20:00:52,547] A3C_AGENT_WORKER-Thread-19 INFO:Local step 76500, global step 1215405: loss 0.1036
[2019-04-27 20:00:52,549] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 76500, global step 1215405: learning rate 0.0001
[2019-04-27 20:00:54,720] A3C_AGENT_WORKER-Thread-16 INFO:Local step 76000, global step 1216533: loss 0.0714
[2019-04-27 20:00:54,722] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 76000, global step 1216534: learning rate 0.0001
[2019-04-27 20:00:55,015] A3C_AGENT_WORKER-Thread-11 INFO:Local step 76000, global step 1216686: loss 4.0140
[2019-04-27 20:00:55,017] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 76000, global step 1216686: learning rate 0.0001
[2019-04-27 20:00:55,157] A3C_AGENT_WORKER-Thread-17 INFO:Local step 76000, global step 1216759: loss 3.4447
[2019-04-27 20:00:55,160] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 76000, global step 1216761: learning rate 0.0001
[2019-04-27 20:00:55,181] A3C_AGENT_WORKER-Thread-18 INFO:Local step 76000, global step 1216774: loss 3.5451
[2019-04-27 20:00:55,183] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 76000, global step 1216774: learning rate 0.0001
[2019-04-27 20:00:55,311] A3C_AGENT_WORKER-Thread-12 INFO:Local step 76000, global step 1216835: loss 1.2695
[2019-04-27 20:00:55,313] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 76000, global step 1216835: learning rate 0.0001
[2019-04-27 20:00:55,411] A3C_AGENT_WORKER-Thread-9 INFO:Local step 76000, global step 1216888: loss 0.3004
[2019-04-27 20:00:55,415] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 76000, global step 1216888: learning rate 0.0001
[2019-04-27 20:00:55,483] A3C_AGENT_WORKER-Thread-2 INFO:Local step 76000, global step 1216924: loss 0.1816
[2019-04-27 20:00:55,486] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 76000, global step 1216925: learning rate 0.0001
[2019-04-27 20:00:55,547] A3C_AGENT_WORKER-Thread-22 INFO:Local step 76000, global step 1216958: loss 0.0338
[2019-04-27 20:00:55,548] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 76000, global step 1216958: learning rate 0.0001
[2019-04-27 20:00:55,559] A3C_AGENT_WORKER-Thread-15 INFO:Local step 76000, global step 1216961: loss 0.0246
[2019-04-27 20:00:55,561] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 76000, global step 1216961: learning rate 0.0001
[2019-04-27 20:00:55,568] A3C_AGENT_WORKER-Thread-10 INFO:Local step 76000, global step 1216966: loss 0.0478
[2019-04-27 20:00:55,573] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 76000, global step 1216970: learning rate 0.0001
[2019-04-27 20:00:55,672] A3C_AGENT_WORKER-Thread-14 INFO:Local step 76000, global step 1217021: loss 0.0746
[2019-04-27 20:00:55,673] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 76000, global step 1217021: learning rate 0.0001
[2019-04-27 20:00:55,689] A3C_AGENT_WORKER-Thread-20 INFO:Local step 76000, global step 1217032: loss 0.1203
[2019-04-27 20:00:55,690] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 76000, global step 1217032: learning rate 0.0001
[2019-04-27 20:00:55,739] A3C_AGENT_WORKER-Thread-13 INFO:Local step 76000, global step 1217055: loss 0.0666
[2019-04-27 20:00:55,743] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 76000, global step 1217056: learning rate 0.0001
[2019-04-27 20:00:55,893] A3C_AGENT_WORKER-Thread-21 INFO:Local step 76000, global step 1217132: loss 0.1759
[2019-04-27 20:00:55,895] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 76000, global step 1217133: learning rate 0.0001
[2019-04-27 20:01:01,010] A3C_AGENT_WORKER-Thread-3 INFO:Local step 76500, global step 1219821: loss 0.4906
[2019-04-27 20:01:01,013] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 76500, global step 1219821: learning rate 0.0001
[2019-04-27 20:01:01,904] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1786947e-35 1.0000000e+00 1.1220950e-35 9.3092298e-38 2.7808644e-31], sum to 1.0000
[2019-04-27 20:01:01,913] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0566
[2019-04-27 20:01:01,920] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.28333333333333, 77.66666666666667, 1.0, 2.0, 0.422293968254894, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 521868.143214841, 521868.1432148415, 131772.5963651111], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5986200.0000, 
sim time next is 5986800.0000, 
raw observation next is [22.4, 77.0, 1.0, 2.0, 0.4508983077953888, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 557033.2995638859, 557033.2995638859, 135981.9205735227], 
processed observation next is [1.0, 0.30434782608695654, 0.38518518518518513, 0.77, 1.0, 1.0, 0.3463075092802248, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.19894046412995925, 0.19894046412995925, 0.2615036934106206], 
reward next is 0.7385, 
noisyNet noise sample is [array([-1.2940215], dtype=float32), 0.38951847]. 
=============================================
[2019-04-27 20:01:04,863] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0612403e-27 1.0000000e+00 1.3826058e-24 5.2551237e-28 1.9086257e-11], sum to 1.0000
[2019-04-27 20:01:04,872] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5164
[2019-04-27 20:01:04,878] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 80.0, 1.0, 2.0, 0.688501652681309, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 784690.4063132785, 784690.406313278, 173977.442048638], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6392400.0000, 
sim time next is 6393000.0000, 
raw observation next is [27.1, 80.5, 1.0, 2.0, 0.6891042682750844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 785377.563608391, 785377.563608391, 174090.2286971483], 
processed observation next is [0.0, 1.0, 0.5592592592592593, 0.805, 1.0, 1.0, 0.6298860336608147, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.28049198700299677, 0.28049198700299677, 0.3347889013406698], 
reward next is 0.6652, 
noisyNet noise sample is [array([0.19380301], dtype=float32), 1.628587]. 
=============================================
[2019-04-27 20:01:04,900] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[63.759083]
 [63.75467 ]
 [63.733963]
 [63.73669 ]
 [63.74602 ]], R is [[63.79713058]
 [63.82458878]
 [63.85152435]
 [63.87792206]
 [63.90369415]].
[2019-04-27 20:01:08,023] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.3248756e-18 2.8060622e-05 9.5094589e-16 7.7559995e-18 9.9997199e-01], sum to 1.0000
[2019-04-27 20:01:08,033] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2443
[2019-04-27 20:01:08,037] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.9, 60.0, 1.0, 2.0, 0.4506645066549811, 1.0, 2.0, 0.4506645066549811, 1.0, 2.0, 0.7174725717130487, 6.911199999999999, 6.9112, 121.94756008, 1541637.530860281, 1541637.530860282, 319986.5882399204], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6091200.0000, 
sim time next is 6091800.0000, 
raw observation next is [29.05, 59.33333333333333, 1.0, 2.0, 0.3721033362835554, 1.0, 2.0, 0.3721033362835554, 1.0, 2.0, 0.5924006299230422, 6.911200000000001, 6.9112, 121.94756008, 1272671.465172858, 1272671.465172857, 285077.5999098869], 
processed observation next is [1.0, 0.5217391304347826, 0.6314814814814815, 0.5933333333333333, 1.0, 1.0, 0.25250397176613737, 1.0, 1.0, 0.25250397176613737, 1.0, 1.0, 0.4905007874038027, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.4545255232760207, 0.45452552327602036, 0.5482261536728594], 
reward next is 0.4518, 
noisyNet noise sample is [array([-0.09316765], dtype=float32), -0.8733069]. 
=============================================
[2019-04-27 20:01:08,066] A3C_AGENT_WORKER-Thread-19 INFO:Local step 77000, global step 1223520: loss 2.0101
[2019-04-27 20:01:08,070] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 77000, global step 1223521: learning rate 0.0001
[2019-04-27 20:01:09,999] A3C_AGENT_WORKER-Thread-16 INFO:Local step 76500, global step 1224533: loss 0.1870
[2019-04-27 20:01:10,002] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 76500, global step 1224533: learning rate 0.0001
[2019-04-27 20:01:10,321] A3C_AGENT_WORKER-Thread-11 INFO:Local step 76500, global step 1224697: loss 0.2917
[2019-04-27 20:01:10,322] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 76500, global step 1224697: learning rate 0.0001
[2019-04-27 20:01:10,414] A3C_AGENT_WORKER-Thread-12 INFO:Local step 76500, global step 1224744: loss 0.2813
[2019-04-27 20:01:10,416] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 76500, global step 1224744: learning rate 0.0001
[2019-04-27 20:01:10,426] A3C_AGENT_WORKER-Thread-18 INFO:Local step 76500, global step 1224752: loss 0.2259
[2019-04-27 20:01:10,429] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 76500, global step 1224753: learning rate 0.0001
[2019-04-27 20:01:10,458] A3C_AGENT_WORKER-Thread-17 INFO:Local step 76500, global step 1224773: loss 0.3459
[2019-04-27 20:01:10,461] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 76500, global step 1224773: learning rate 0.0001
[2019-04-27 20:01:10,598] A3C_AGENT_WORKER-Thread-15 INFO:Local step 76500, global step 1224843: loss 0.1909
[2019-04-27 20:01:10,601] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 76500, global step 1224843: learning rate 0.0001
[2019-04-27 20:01:10,794] A3C_AGENT_WORKER-Thread-9 INFO:Local step 76500, global step 1224943: loss 0.0793
[2019-04-27 20:01:10,795] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 76500, global step 1224943: learning rate 0.0001
[2019-04-27 20:01:10,804] A3C_AGENT_WORKER-Thread-2 INFO:Local step 76500, global step 1224946: loss 0.1257
[2019-04-27 20:01:10,805] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 76500, global step 1224946: learning rate 0.0001
[2019-04-27 20:01:10,816] A3C_AGENT_WORKER-Thread-10 INFO:Local step 76500, global step 1224953: loss 0.0591
[2019-04-27 20:01:10,818] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 76500, global step 1224954: learning rate 0.0001
[2019-04-27 20:01:10,897] A3C_AGENT_WORKER-Thread-22 INFO:Local step 76500, global step 1224990: loss 0.0555
[2019-04-27 20:01:10,900] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 76500, global step 1224990: learning rate 0.0001
[2019-04-27 20:01:10,912] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-27 20:01:10,913] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 20:01:10,914] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:01:10,914] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 20:01:10,915] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:01:10,916] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 20:01:10,917] A3C_AGENT_WORKER-Thread-20 INFO:Local step 76500, global step 1225000: loss 0.0715
[2019-04-27 20:01:10,918] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:01:10,919] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 20:01:10,920] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 76500, global step 1225000: learning rate 0.0001
[2019-04-27 20:01:10,917] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 20:01:10,922] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:01:10,923] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:01:10,940] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run50
[2019-04-27 20:01:10,963] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run50
[2019-04-27 20:01:10,963] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run50
[2019-04-27 20:01:11,006] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run50
[2019-04-27 20:01:11,026] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run50
[2019-04-27 20:01:21,016] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04730767], dtype=float32), -0.019001711]
[2019-04-27 20:01:21,018] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [34.441872315, 33.49367033333333, 1.0, 2.0, 0.6487659328439747, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 773390.668596687, 773390.668596687, 168201.3500248611]
[2019-04-27 20:01:21,019] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 20:01:21,022] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [7.1725473e-37 1.0000000e+00 1.8592254e-36 0.0000000e+00 8.4782909e-29], sampled 0.3215822263564576
[2019-04-27 20:01:24,366] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04730767], dtype=float32), -0.019001711]
[2019-04-27 20:01:24,368] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [21.95, 47.5, 1.0, 2.0, 0.2565230901905377, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 330894.5309144075, 330894.5309144075, 99208.61092000733]
[2019-04-27 20:01:24,369] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:01:24,371] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [3.8611010e-37 1.0000000e+00 3.4749861e-37 0.0000000e+00 4.0796282e-30], sampled 0.5977911822530472
[2019-04-27 20:01:48,915] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04730767], dtype=float32), -0.019001711]
[2019-04-27 20:01:48,916] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [24.1, 83.0, 1.0, 2.0, 0.8227178880734974, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 975609.5463264236, 975609.5463264236, 202523.5249907068]
[2019-04-27 20:01:48,918] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:01:48,920] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.2062041e-35 1.0000000e+00 2.5126797e-35 4.7026668e-38 4.1310749e-29], sampled 0.2736247117973376
[2019-04-27 20:01:51,704] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04730767], dtype=float32), -0.019001711]
[2019-04-27 20:01:51,705] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [26.66666666666667, 76.0, 1.0, 2.0, 0.6353802949385727, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 731651.1314198761, 731651.1314198761, 164641.6989084076]
[2019-04-27 20:01:51,707] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:01:51,711] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [3.9340246e-38 1.0000000e+00 4.0371616e-38 0.0000000e+00 2.8441280e-32], sampled 0.6548376904382919
[2019-04-27 20:01:54,300] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04730767], dtype=float32), -0.019001711]
[2019-04-27 20:01:54,301] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [26.65, 60.5, 1.0, 2.0, 0.4686326684413961, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 566450.8974415314, 566450.8974415314, 138313.6200174906]
[2019-04-27 20:01:54,302] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 20:01:54,304] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 1.148656e-36], sampled 0.46056510961808994
[2019-04-27 20:02:27,693] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04730767], dtype=float32), -0.019001711]
[2019-04-27 20:02:27,695] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [25.96762426, 90.62691638, 1.0, 2.0, 0.6681108086718591, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 761439.2681690532, 761439.2681690532, 170195.9549708361]
[2019-04-27 20:02:27,698] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 20:02:27,703] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [8.8975208e-36 1.0000000e+00 2.9640730e-35 2.6422071e-38 2.2758703e-27], sampled 0.5347423994043624
[2019-04-27 20:02:36,441] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04730767], dtype=float32), -0.019001711]
[2019-04-27 20:02:36,443] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [26.78333333333333, 76.33333333333334, 1.0, 2.0, 0.5962316978140423, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 684884.771559851, 684884.771559851, 157695.9872546207]
[2019-04-27 20:02:36,445] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 20:02:36,447] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [8.6524502e-34 1.0000000e+00 2.4166731e-32 3.1840890e-35 8.5462520e-22], sampled 0.5600147653008579
[2019-04-27 20:02:43,268] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04730767], dtype=float32), -0.019001711]
[2019-04-27 20:02:43,271] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [24.96520142333333, 45.21619288, 1.0, 2.0, 0.3097881815788276, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 395171.4476851156, 395171.447685116, 116793.2386965768]
[2019-04-27 20:02:43,271] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 20:02:43,274] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 7.440991e-37], sampled 0.9855576165626746
[2019-04-27 20:02:54,299] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04730767], dtype=float32), -0.019001711]
[2019-04-27 20:02:54,301] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [25.48578329, 68.50158030333333, 1.0, 2.0, 0.4679931542077257, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 563113.092664423, 563113.092664423, 138130.9943514268]
[2019-04-27 20:02:54,302] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 20:02:54,305] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.8786924e-35], sampled 0.38906273763362786
[2019-04-27 20:03:00,346] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8772.2321 2171594019.9867 476.0000
[2019-04-27 20:03:00,777] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8142.1383 2448812193.5863 589.0000
[2019-04-27 20:03:00,831] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8613.0441 2250434832.2255 465.0000
[2019-04-27 20:03:00,845] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8920.4809 2121688257.1293 422.0000
[2019-04-27 20:03:00,850] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8710.7797 2198270604.4017 531.0000
[2019-04-27 20:03:01,867] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 1225000, evaluation results [1225000.0, 8142.13830103889, 2448812193.5863333, 589.0, 8772.232055552528, 2171594019.986724, 476.0, 8920.480880642166, 2121688257.1293294, 422.0, 8613.044126747127, 2250434832.2254725, 465.0, 8710.77968079905, 2198270604.4016557, 531.0]
[2019-04-27 20:03:01,941] A3C_AGENT_WORKER-Thread-14 INFO:Local step 76500, global step 1225040: loss 0.0916
[2019-04-27 20:03:01,947] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 76500, global step 1225040: learning rate 0.0001
[2019-04-27 20:03:02,077] A3C_AGENT_WORKER-Thread-13 INFO:Local step 76500, global step 1225099: loss 0.1043
[2019-04-27 20:03:02,080] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 76500, global step 1225100: learning rate 0.0001
[2019-04-27 20:03:02,297] A3C_AGENT_WORKER-Thread-21 INFO:Local step 76500, global step 1225206: loss 0.0709
[2019-04-27 20:03:02,300] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 76500, global step 1225206: learning rate 0.0001
[2019-04-27 20:03:07,670] A3C_AGENT_WORKER-Thread-3 INFO:Local step 77000, global step 1227763: loss 3.5987
[2019-04-27 20:03:07,672] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 77000, global step 1227763: learning rate 0.0001
[2019-04-27 20:03:10,245] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.3610975e-31 1.0000000e+00 2.4511103e-31 9.5175957e-32 7.1214700e-18], sum to 1.0000
[2019-04-27 20:03:10,256] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0393
[2019-04-27 20:03:10,263] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 72.0, 1.0, 2.0, 0.5880175838920451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 683662.2588360517, 683662.2588360517, 156672.5302765865], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6297000.0000, 
sim time next is 6297600.0000, 
raw observation next is [26.66666666666667, 73.0, 1.0, 2.0, 0.5894172518688824, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 685318.2628575763, 685318.2628575763, 156913.5257397964], 
processed observation next is [0.0, 0.9130434782608695, 0.5432098765432101, 0.73, 1.0, 1.0, 0.5112110141296219, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.24475652244913437, 0.24475652244913437, 0.30175678026883923], 
reward next is 0.6982, 
noisyNet noise sample is [array([-0.23716551], dtype=float32), 0.4087241]. 
=============================================
[2019-04-27 20:03:15,612] A3C_AGENT_WORKER-Thread-19 INFO:Local step 77500, global step 1231460: loss 0.0529
[2019-04-27 20:03:15,612] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 77500, global step 1231460: learning rate 0.0001
[2019-04-27 20:03:16,331] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.3176177e-27 1.0000000e+00 7.6377898e-29 1.5782769e-29 4.3172724e-22], sum to 1.0000
[2019-04-27 20:03:16,338] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8105
[2019-04-27 20:03:16,341] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.9, 92.0, 1.0, 2.0, 0.7964312522410109, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 907771.412252223, 907771.4122522225, 195197.8800778459], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6415200.0000, 
sim time next is 6415800.0000, 
raw observation next is [24.91666666666666, 92.0, 1.0, 2.0, 0.7844265294543509, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999998, 6.9112, 121.9260426156618, 894080.4617347716, 894080.4617347725, 192737.5988280916], 
processed observation next is [1.0, 0.2608695652173913, 0.47839506172839485, 0.92, 1.0, 1.0, 0.7433649160170843, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -1.7763568394002506e-16, 0.0, 0.8094621288201359, 0.31931445061956126, 0.3193144506195616, 0.37064922851556076], 
reward next is 0.6294, 
noisyNet noise sample is [array([-1.19271], dtype=float32), -0.49710032]. 
=============================================
[2019-04-27 20:03:17,992] A3C_AGENT_WORKER-Thread-16 INFO:Local step 77000, global step 1232582: loss 4.4963
[2019-04-27 20:03:17,996] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 77000, global step 1232584: learning rate 0.0001
[2019-04-27 20:03:18,232] A3C_AGENT_WORKER-Thread-11 INFO:Local step 77000, global step 1232695: loss 30.1063
[2019-04-27 20:03:18,235] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 77000, global step 1232695: learning rate 0.0001
[2019-04-27 20:03:18,260] A3C_AGENT_WORKER-Thread-12 INFO:Local step 77000, global step 1232710: loss 28.2248
[2019-04-27 20:03:18,263] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 77000, global step 1232710: learning rate 0.0001
[2019-04-27 20:03:18,313] A3C_AGENT_WORKER-Thread-18 INFO:Local step 77000, global step 1232733: loss 38.0848
[2019-04-27 20:03:18,316] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 77000, global step 1232734: learning rate 0.0001
[2019-04-27 20:03:18,476] A3C_AGENT_WORKER-Thread-17 INFO:Local step 77000, global step 1232810: loss 37.3593
[2019-04-27 20:03:18,480] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 77000, global step 1232811: learning rate 0.0001
[2019-04-27 20:03:18,588] A3C_AGENT_WORKER-Thread-15 INFO:Local step 77000, global step 1232860: loss 5.8027
[2019-04-27 20:03:18,594] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 77000, global step 1232860: learning rate 0.0001
[2019-04-27 20:03:18,783] A3C_AGENT_WORKER-Thread-9 INFO:Local step 77000, global step 1232951: loss 8.5187
[2019-04-27 20:03:18,785] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 77000, global step 1232952: learning rate 0.0001
[2019-04-27 20:03:18,826] A3C_AGENT_WORKER-Thread-20 INFO:Local step 77000, global step 1232973: loss 6.6258
[2019-04-27 20:03:18,831] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 77000, global step 1232976: learning rate 0.0001
[2019-04-27 20:03:18,845] A3C_AGENT_WORKER-Thread-10 INFO:Local step 77000, global step 1232980: loss 11.1741
[2019-04-27 20:03:18,846] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 77000, global step 1232980: learning rate 0.0001
[2019-04-27 20:03:18,849] A3C_AGENT_WORKER-Thread-2 INFO:Local step 77000, global step 1232981: loss 11.5203
[2019-04-27 20:03:18,850] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 77000, global step 1232981: learning rate 0.0001
[2019-04-27 20:03:18,889] A3C_AGENT_WORKER-Thread-22 INFO:Local step 77000, global step 1232999: loss 8.7351
[2019-04-27 20:03:18,891] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 77000, global step 1232999: learning rate 0.0001
[2019-04-27 20:03:19,006] A3C_AGENT_WORKER-Thread-14 INFO:Local step 77000, global step 1233055: loss 8.6361
[2019-04-27 20:03:19,009] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 77000, global step 1233056: learning rate 0.0001
[2019-04-27 20:03:19,058] A3C_AGENT_WORKER-Thread-13 INFO:Local step 77000, global step 1233075: loss 6.5493
[2019-04-27 20:03:19,061] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 77000, global step 1233076: learning rate 0.0001
[2019-04-27 20:03:19,214] A3C_AGENT_WORKER-Thread-21 INFO:Local step 77000, global step 1233150: loss 4.3825
[2019-04-27 20:03:19,215] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 77000, global step 1233150: learning rate 0.0001
[2019-04-27 20:03:20,594] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.03349606e-23 1.00000000e+00 7.43888702e-23 4.49128441e-25
 6.38627877e-13], sum to 1.0000
[2019-04-27 20:03:20,603] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6052
[2019-04-27 20:03:20,608] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.38333333333333, 77.5, 1.0, 2.0, 0.6666545713875222, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 759778.7864329072, 759778.7864329072, 169926.078441901], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6483000.0000, 
sim time next is 6483600.0000, 
raw observation next is [27.3, 78.0, 1.0, 2.0, 0.6657143714607185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 758706.7207463186, 758706.720746319, 169753.6050056959], 
processed observation next is [1.0, 0.043478260869565216, 0.5666666666666667, 0.78, 1.0, 1.0, 0.6020409184056174, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.27096668598082807, 0.27096668598082824, 0.326449240395569], 
reward next is 0.6736, 
noisyNet noise sample is [array([-0.27853316], dtype=float32), 0.36847416]. 
=============================================
[2019-04-27 20:03:21,899] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1705281e-16 4.8417809e-10 3.5765559e-12 2.2284937e-13 1.0000000e+00], sum to 1.0000
[2019-04-27 20:03:21,910] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8784
[2019-04-27 20:03:21,917] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.23333333333333, 84.50000000000001, 1.0, 2.0, 0.5818634015426485, 1.0, 2.0, 0.5818634015426485, 1.0, 2.0, 0.9263454852239186, 6.911199999999999, 6.9112, 121.94756008, 1990951.55958858, 1990951.559588581, 385582.425512227], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6513000.0000, 
sim time next is 6513600.0000, 
raw observation next is [27.36666666666667, 84.0, 1.0, 2.0, 0.5925439332953971, 1.0, 2.0, 0.5925439332953971, 1.0, 2.0, 0.9433492396149296, 6.9112, 6.9112, 121.94756008, 2027538.423043924, 2027538.423043924, 391328.1116155254], 
processed observation next is [1.0, 0.391304347826087, 0.569135802469136, 0.84, 1.0, 1.0, 0.5149332539230917, 1.0, 1.0, 0.5149332539230917, 1.0, 1.0, 0.9291865495186619, 0.0, 0.0, 0.8096049824067558, 0.7241208653728299, 0.7241208653728299, 0.7525540607990873], 
reward next is 0.2474, 
noisyNet noise sample is [array([-0.76807326], dtype=float32), 0.6160958]. 
=============================================
[2019-04-27 20:03:24,624] A3C_AGENT_WORKER-Thread-3 INFO:Local step 77500, global step 1235753: loss 0.0429
[2019-04-27 20:03:24,625] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 77500, global step 1235754: learning rate 0.0001
[2019-04-27 20:03:29,593] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.8452654e-26 1.0000000e+00 1.3072299e-22 1.3761316e-27 7.5931140e-13], sum to 1.0000
[2019-04-27 20:03:29,600] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8511
[2019-04-27 20:03:29,603] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.93333333333333, 43.0, 1.0, 2.0, 0.2812826536941891, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 362722.2761369405, 362722.2761369405, 113281.314956098], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6655200.0000, 
sim time next is 6655800.0000, 
raw observation next is [23.81666666666667, 43.5, 1.0, 2.0, 0.2805765084173165, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 361841.8191145526, 361841.8191145526, 113196.1611618759], 
processed observation next is [1.0, 0.0, 0.43765432098765444, 0.435, 1.0, 1.0, 0.1435434624015673, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1292292211123402, 0.1292292211123402, 0.21768492531129982], 
reward next is 0.7823, 
noisyNet noise sample is [array([0.19122821], dtype=float32), -0.81932664]. 
=============================================
[2019-04-27 20:03:31,535] A3C_AGENT_WORKER-Thread-19 INFO:Local step 78000, global step 1239390: loss 0.2389
[2019-04-27 20:03:31,537] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 78000, global step 1239390: learning rate 0.0001
[2019-04-27 20:03:33,813] A3C_AGENT_WORKER-Thread-16 INFO:Local step 77500, global step 1240580: loss 0.0445
[2019-04-27 20:03:33,816] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 77500, global step 1240581: learning rate 0.0001
[2019-04-27 20:03:33,951] A3C_AGENT_WORKER-Thread-11 INFO:Local step 77500, global step 1240655: loss 0.0387
[2019-04-27 20:03:33,953] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 77500, global step 1240656: learning rate 0.0001
[2019-04-27 20:03:33,984] A3C_AGENT_WORKER-Thread-12 INFO:Local step 77500, global step 1240671: loss 0.0557
[2019-04-27 20:03:33,986] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 77500, global step 1240672: learning rate 0.0001
[2019-04-27 20:03:34,206] A3C_AGENT_WORKER-Thread-17 INFO:Local step 77500, global step 1240786: loss 0.0762
[2019-04-27 20:03:34,209] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 77500, global step 1240787: learning rate 0.0001
[2019-04-27 20:03:34,277] A3C_AGENT_WORKER-Thread-18 INFO:Local step 77500, global step 1240823: loss 0.0708
[2019-04-27 20:03:34,282] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 77500, global step 1240825: learning rate 0.0001
[2019-04-27 20:03:34,322] A3C_AGENT_WORKER-Thread-15 INFO:Local step 77500, global step 1240847: loss 0.0895
[2019-04-27 20:03:34,328] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 77500, global step 1240849: learning rate 0.0001
[2019-04-27 20:03:34,539] A3C_AGENT_WORKER-Thread-9 INFO:Local step 77500, global step 1240961: loss 0.1055
[2019-04-27 20:03:34,540] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 77500, global step 1240961: learning rate 0.0001
[2019-04-27 20:03:34,599] A3C_AGENT_WORKER-Thread-2 INFO:Local step 77500, global step 1240993: loss 0.0913
[2019-04-27 20:03:34,605] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 77500, global step 1240994: learning rate 0.0001
[2019-04-27 20:03:34,623] A3C_AGENT_WORKER-Thread-10 INFO:Local step 77500, global step 1241004: loss 0.0764
[2019-04-27 20:03:34,625] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 77500, global step 1241004: learning rate 0.0001
[2019-04-27 20:03:34,639] A3C_AGENT_WORKER-Thread-20 INFO:Local step 77500, global step 1241010: loss 0.0639
[2019-04-27 20:03:34,641] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 77500, global step 1241011: learning rate 0.0001
[2019-04-27 20:03:34,646] A3C_AGENT_WORKER-Thread-13 INFO:Local step 77500, global step 1241013: loss 0.0577
[2019-04-27 20:03:34,647] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 77500, global step 1241014: learning rate 0.0001
[2019-04-27 20:03:34,706] A3C_AGENT_WORKER-Thread-14 INFO:Local step 77500, global step 1241045: loss 0.0403
[2019-04-27 20:03:34,709] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 77500, global step 1241045: learning rate 0.0001
[2019-04-27 20:03:34,809] A3C_AGENT_WORKER-Thread-22 INFO:Local step 77500, global step 1241100: loss 0.0239
[2019-04-27 20:03:34,813] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 77500, global step 1241100: learning rate 0.0001
[2019-04-27 20:03:34,974] A3C_AGENT_WORKER-Thread-21 INFO:Local step 77500, global step 1241185: loss 0.0182
[2019-04-27 20:03:34,975] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 77500, global step 1241185: learning rate 0.0001
[2019-04-27 20:03:39,800] A3C_AGENT_WORKER-Thread-3 INFO:Local step 78000, global step 1243720: loss 0.0589
[2019-04-27 20:03:39,803] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 78000, global step 1243720: learning rate 0.0001
[2019-04-27 20:03:40,628] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.3592763e-33], sum to 1.0000
[2019-04-27 20:03:40,634] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2515
[2019-04-27 20:03:40,641] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.16666666666666, 48.83333333333334, 1.0, 2.0, 0.507783598663727, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 603097.0513937526, 603097.0513937526, 144027.8340835352], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6873000.0000, 
sim time next is 6873600.0000, 
raw observation next is [30.33333333333334, 48.66666666666667, 1.0, 2.0, 0.5129985590177474, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 608064.3328104336, 608064.3328104331, 144808.282825858], 
processed observation next is [0.0, 0.5652173913043478, 0.6790123456790126, 0.4866666666666667, 1.0, 1.0, 0.4202363797830326, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.21716583314658341, 0.21716583314658325, 0.27847746697280384], 
reward next is 0.7215, 
noisyNet noise sample is [array([-0.7956047], dtype=float32), 0.3408471]. 
=============================================
[2019-04-27 20:03:42,761] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3942268e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.5506087e-33], sum to 1.0000
[2019-04-27 20:03:42,767] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0837
[2019-04-27 20:03:42,771] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 73.0, 1.0, 2.0, 0.4128613073467292, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 509269.1085372934, 509269.1085372934, 130392.2378168453], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6915600.0000, 
sim time next is 6916200.0000, 
raw observation next is [23.05, 73.5, 1.0, 2.0, 0.4135580176872634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 509972.4726175907, 509972.4726175907, 130488.1084338108], 
processed observation next is [0.0, 0.043478260869565216, 0.40925925925925927, 0.735, 1.0, 1.0, 0.30185478296102786, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1821330259348538, 0.1821330259348538, 0.2509386700650208], 
reward next is 0.7491, 
noisyNet noise sample is [array([-0.7638111], dtype=float32), 0.5521405]. 
=============================================
[2019-04-27 20:03:45,262] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 1.6222143e-36 0.0000000e+00 3.7527196e-27], sum to 1.0000
[2019-04-27 20:03:45,272] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0193
[2019-04-27 20:03:45,278] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 51.0, 1.0, 2.0, 0.5792332855147203, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 671967.4252264837, 671967.4252264837, 155108.8989273303], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6967200.0000, 
sim time next is 6967800.0000, 
raw observation next is [31.0, 50.5, 1.0, 2.0, 0.5736049632336135, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 666804.776432595, 666804.776432595, 154219.4835527587], 
processed observation next is [0.0, 0.6521739130434783, 0.7037037037037037, 0.505, 1.0, 1.0, 0.49238686099239704, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.23814456301164105, 0.23814456301164105, 0.2965759299091513], 
reward next is 0.7034, 
noisyNet noise sample is [array([2.0555522], dtype=float32), -1.3441083]. 
=============================================
[2019-04-27 20:03:45,899] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.01029755e-25 9.99999881e-01 2.00945533e-24 6.90151680e-27
 1.21715502e-07], sum to 1.0000
[2019-04-27 20:03:45,905] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7228
[2019-04-27 20:03:45,909] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 85.33333333333334, 1.0, 2.0, 0.4178314270715514, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 514493.2218141967, 514493.2218141967, 131082.7167120885], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7339200.0000, 
sim time next is 7339800.0000, 
raw observation next is [21.35, 86.16666666666666, 1.0, 2.0, 0.4156421371258129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 512113.5283415362, 512113.5283415357, 130776.1258432647], 
processed observation next is [1.0, 0.9565217391304348, 0.3462962962962963, 0.8616666666666666, 1.0, 1.0, 0.3043358775307296, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.18289768869340578, 0.1828976886934056, 0.251492549698586], 
reward next is 0.7485, 
noisyNet noise sample is [array([0.48722976], dtype=float32), -1.0756259]. 
=============================================
[2019-04-27 20:03:46,700] A3C_AGENT_WORKER-Thread-19 INFO:Local step 78500, global step 1247326: loss 0.0216
[2019-04-27 20:03:46,703] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 78500, global step 1247326: learning rate 0.0001
[2019-04-27 20:03:49,184] A3C_AGENT_WORKER-Thread-11 INFO:Local step 78000, global step 1248628: loss 0.0029
[2019-04-27 20:03:49,194] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 78000, global step 1248628: learning rate 0.0001
[2019-04-27 20:03:49,207] A3C_AGENT_WORKER-Thread-16 INFO:Local step 78000, global step 1248635: loss 0.0061
[2019-04-27 20:03:49,208] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 78000, global step 1248635: learning rate 0.0001
[2019-04-27 20:03:49,280] A3C_AGENT_WORKER-Thread-12 INFO:Local step 78000, global step 1248674: loss 0.0091
[2019-04-27 20:03:49,282] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 78000, global step 1248674: learning rate 0.0001
[2019-04-27 20:03:49,480] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.3477399e-25 9.9992502e-01 1.9081088e-21 8.5980754e-26 7.4992400e-05], sum to 1.0000
[2019-04-27 20:03:49,484] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1156
[2019-04-27 20:03:49,487] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.16666666666667, 76.33333333333333, 1.0, 2.0, 0.3896076922121061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 483439.1701342592, 483439.1701342587, 127180.153102654], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7245600.0000, 
sim time next is 7246200.0000, 
raw observation next is [22.08333333333334, 76.66666666666667, 1.0, 2.0, 0.3880568060589425, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 481745.8387140298, 481745.8387140298, 126969.6802014052], 
processed observation next is [1.0, 0.8695652173913043, 0.373456790123457, 0.7666666666666667, 1.0, 1.0, 0.27149619768921723, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.17205208525501064, 0.17205208525501064, 0.2441724619257792], 
reward next is 0.7558, 
noisyNet noise sample is [array([-1.1346024], dtype=float32), 0.389312]. 
=============================================
[2019-04-27 20:03:49,506] A3C_AGENT_WORKER-Thread-15 INFO:Local step 78000, global step 1248785: loss 0.0714
[2019-04-27 20:03:49,508] A3C_AGENT_WORKER-Thread-17 INFO:Local step 78000, global step 1248786: loss 0.0800
[2019-04-27 20:03:49,510] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 78000, global step 1248786: learning rate 0.0001
[2019-04-27 20:03:49,514] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 78000, global step 1248788: learning rate 0.0001
[2019-04-27 20:03:49,566] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1306504e-34 3.7387990e-21 1.5183889e-27 2.1297494e-30 1.0000000e+00], sum to 1.0000
[2019-04-27 20:03:49,581] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7637
[2019-04-27 20:03:49,585] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.9, 73.33333333333334, 1.0, 2.0, 0.3285775966667731, 1.0, 2.0, 0.3285775966667731, 1.0, 2.0, 0.5240358304490892, 6.9112, 6.9112, 121.94756008, 1142529.470579148, 1142529.470579148, 267246.6743757892], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7057200.0000, 
sim time next is 7057800.0000, 
raw observation next is [24.65, 74.5, 1.0, 2.0, 0.3072493499844727, 1.0, 2.0, 0.3072493499844727, 1.0, 2.0, 0.490482498015339, 6.911199999999998, 6.9112, 121.94756008, 1073444.150222681, 1073444.150222682, 258839.4831546275], 
processed observation next is [1.0, 0.6956521739130435, 0.46851851851851845, 0.745, 1.0, 1.0, 0.17529684521961034, 1.0, 1.0, 0.17529684521961034, 1.0, 1.0, 0.3631031225191737, -1.7763568394002506e-16, 0.0, 0.8096049824067558, 0.38337291079381464, 0.38337291079381497, 0.4977682368358221], 
reward next is 0.5022, 
noisyNet noise sample is [array([0.53627926], dtype=float32), -0.5378135]. 
=============================================
[2019-04-27 20:03:49,734] A3C_AGENT_WORKER-Thread-18 INFO:Local step 78000, global step 1248905: loss 0.1622
[2019-04-27 20:03:49,735] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 78000, global step 1248905: learning rate 0.0001
[2019-04-27 20:03:49,918] A3C_AGENT_WORKER-Thread-9 INFO:Local step 78000, global step 1249000: loss 0.1187
[2019-04-27 20:03:49,923] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 78000, global step 1249002: learning rate 0.0001
[2019-04-27 20:03:49,941] A3C_AGENT_WORKER-Thread-20 INFO:Local step 78000, global step 1249010: loss 0.1614
[2019-04-27 20:03:49,946] A3C_AGENT_WORKER-Thread-2 INFO:Local step 78000, global step 1249013: loss 0.1218
[2019-04-27 20:03:49,947] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 78000, global step 1249013: learning rate 0.0001
[2019-04-27 20:03:49,949] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 78000, global step 1249013: learning rate 0.0001
[2019-04-27 20:03:49,992] A3C_AGENT_WORKER-Thread-13 INFO:Local step 78000, global step 1249037: loss 0.0916
[2019-04-27 20:03:49,994] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 78000, global step 1249037: learning rate 0.0001
[2019-04-27 20:03:49,996] A3C_AGENT_WORKER-Thread-10 INFO:Local step 78000, global step 1249037: loss 0.1065
[2019-04-27 20:03:49,998] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 78000, global step 1249038: learning rate 0.0001
[2019-04-27 20:03:50,083] A3C_AGENT_WORKER-Thread-14 INFO:Local step 78000, global step 1249086: loss 0.0531
[2019-04-27 20:03:50,085] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 78000, global step 1249086: learning rate 0.0001
[2019-04-27 20:03:50,113] A3C_AGENT_WORKER-Thread-22 INFO:Local step 78000, global step 1249100: loss 0.0318
[2019-04-27 20:03:50,115] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 78000, global step 1249100: learning rate 0.0001
[2019-04-27 20:03:50,193] A3C_AGENT_WORKER-Thread-21 INFO:Local step 78000, global step 1249138: loss 0.0099
[2019-04-27 20:03:50,194] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 78000, global step 1249139: learning rate 0.0001
[2019-04-27 20:03:51,827] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-27 20:03:51,830] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 20:03:51,831] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 20:03:51,831] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:03:51,832] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 20:03:51,832] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:03:51,833] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 20:03:51,833] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:03:51,833] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:03:51,833] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 20:03:51,836] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:03:51,854] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run51
[2019-04-27 20:03:51,855] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run51
[2019-04-27 20:03:51,876] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run51
[2019-04-27 20:03:51,921] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run51
[2019-04-27 20:03:51,944] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run51
[2019-04-27 20:04:20,365] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04398725], dtype=float32), -0.022493048]
[2019-04-27 20:04:20,366] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [22.57354633, 104.9752806133333, 1.0, 2.0, 0.6215155927905727, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 729149.0058191153, 729149.0058191153, 162793.1083238183]
[2019-04-27 20:04:20,367] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 20:04:20,373] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.4975614e-32 1.0000000e+00 6.5969730e-32 1.0528293e-34 3.9418060e-23], sampled 0.13472288945016941
[2019-04-27 20:04:36,231] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04398725], dtype=float32), -0.022493048]
[2019-04-27 20:04:36,232] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [27.0, 79.0, 1.0, 2.0, 0.6440705459629654, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 734027.7006492481, 734027.7006492481, 165825.2953508347]
[2019-04-27 20:04:36,233] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 20:04:36,236] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [2.9830161e-35 1.0000000e+00 3.9681992e-35 5.8514026e-38 3.3240133e-28], sampled 0.8348184699986182
[2019-04-27 20:04:43,264] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04398725], dtype=float32), -0.022493048]
[2019-04-27 20:04:43,265] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [32.394766135, 74.41330134666669, 1.0, 2.0, 0.8906301288280913, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 1015210.301818016, 1015210.301818015, 215396.173089211]
[2019-04-27 20:04:43,266] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 20:04:43,268] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.21130245e-30 1.00000000e+00 1.00442820e-28 2.26293195e-31
 1.91070516e-15], sampled 0.7669303984784588
[2019-04-27 20:04:45,809] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04398725], dtype=float32), -0.022493048]
[2019-04-27 20:04:45,812] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [29.46666666666667, 69.33333333333333, 1.0, 2.0, 0.8519726471899604, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 971117.540618647, 971117.5406186465, 206911.1116907617]
[2019-04-27 20:04:45,813] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:04:45,816] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [2.3330558e-31 1.0000000e+00 1.5711457e-30 3.6359036e-33 2.1350792e-21], sampled 0.6017694518945633
[2019-04-27 20:04:55,202] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04398725], dtype=float32), -0.022493048]
[2019-04-27 20:04:55,203] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [27.0, 84.0, 1.0, 2.0, 0.6746167427299107, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 768857.7354572975, 768857.7354572975, 171395.5565868821]
[2019-04-27 20:04:55,206] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 20:04:55,207] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [8.2331211e-31 1.0000000e+00 1.3377198e-29 2.7723364e-32 1.1988886e-18], sampled 0.28285462495757097
[2019-04-27 20:05:15,004] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04398725], dtype=float32), -0.022493048]
[2019-04-27 20:05:15,005] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [28.43333333333333, 72.33333333333334, 1.0, 2.0, 1.019525616064309, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.264969489826826, 6.9112, 121.9242756984441, 1343543.376035606, 1162384.403027962, 245467.3410617628]
[2019-04-27 20:05:15,006] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 20:05:15,010] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.2458378e-27 1.0000000e+00 2.0937456e-26 1.0420045e-28 8.3647799e-17], sampled 0.42647064204194196
[2019-04-27 20:05:15,012] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1343543.376035606 W.
[2019-04-27 20:05:25,378] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04398725], dtype=float32), -0.022493048]
[2019-04-27 20:05:25,380] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [22.75, 94.0, 1.0, 2.0, 0.520354500100039, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 616360.7142136948, 616360.7142136943, 145966.7478670431]
[2019-04-27 20:05:25,382] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:05:25,387] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [7.2244155e-38 1.0000000e+00 3.2839926e-38 0.0000000e+00 2.4052688e-32], sampled 0.2697511117473006
[2019-04-27 20:05:40,917] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8930.8770 2125614154.1269 391.0000
[2019-04-27 20:05:40,939] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8202.1513 2457245291.0286 479.0000
[2019-04-27 20:05:41,069] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8821.2105 2175596924.6810 395.0000
[2019-04-27 20:05:41,123] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8760.8967 2205702114.6336 435.0000
[2019-04-27 20:05:41,138] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8671.3676 2257139303.0718 359.0000
[2019-04-27 20:05:42,154] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1250000, evaluation results [1250000.0, 8202.151308555513, 2457245291.0286036, 479.0, 8821.21052657721, 2175596924.6809864, 395.0, 8930.876956496988, 2125614154.1268713, 391.0, 8671.3675568044, 2257139303.0718083, 359.0, 8760.89669530606, 2205702114.6335945, 435.0]
[2019-04-27 20:05:43,205] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.0472426e-25 1.0000000e+00 1.0392132e-22 1.0279830e-24 1.5100193e-08], sum to 1.0000
[2019-04-27 20:05:43,212] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3717
[2019-04-27 20:05:43,215] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.93333333333334, 71.66666666666667, 1.0, 2.0, 0.7207700686349978, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999998, 6.9112, 121.9260426156618, 892412.3765200947, 892412.3765200956, 182938.0806370097], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7126800.0000, 
sim time next is 7127400.0000, 
raw observation next is [23.01666666666667, 70.83333333333333, 1.0, 2.0, 0.7322584120161031, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 907092.9650568756, 907092.9650568753, 185234.2628550909], 
processed observation next is [1.0, 0.4782608695652174, 0.40802469135802477, 0.7083333333333333, 1.0, 1.0, 0.6812600143048846, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.3239617732345984, 0.3239617732345983, 0.3562197362597902], 
reward next is 0.6438, 
noisyNet noise sample is [array([1.7992815], dtype=float32), 1.5545388]. 
=============================================
[2019-04-27 20:05:45,767] A3C_AGENT_WORKER-Thread-3 INFO:Local step 78500, global step 1251708: loss 1.1386
[2019-04-27 20:05:45,772] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 78500, global step 1251711: learning rate 0.0001
[2019-04-27 20:05:49,200] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.9454819e-20 9.4632095e-01 1.1013719e-14 1.1516251e-20 5.3679060e-02], sum to 1.0000
[2019-04-27 20:05:49,209] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8081
[2019-04-27 20:05:49,218] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.85, 70.0, 1.0, 2.0, 0.2581558408580878, 1.0, 2.0, 0.2581558408580878, 1.0, 2.0, 0.4166517978050921, 6.9112, 6.9112, 121.94756008, 928660.1487304857, 928660.1487304857, 239962.0351511982], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7230600.0000, 
sim time next is 7231200.0000, 
raw observation next is [23.83333333333334, 70.0, 1.0, 2.0, 0.6705524499082788, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 818698.4103810563, 818698.4103810563, 172907.0385539437], 
processed observation next is [1.0, 0.6956521739130435, 0.43827160493827183, 0.7, 1.0, 1.0, 0.6078005356050937, 0.0, 0.5, -0.1904761904761905, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2923922894218058, 0.2923922894218058, 0.332513535680661], 
reward next is 0.6675, 
noisyNet noise sample is [array([2.1957824], dtype=float32), 0.2807837]. 
=============================================
[2019-04-27 20:05:50,552] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.5977190e-26 9.7601920e-01 1.8285771e-23 3.9550694e-26 2.3980744e-02], sum to 1.0000
[2019-04-27 20:05:50,564] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1880
[2019-04-27 20:05:50,570] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.31666666666667, 79.33333333333334, 1.0, 2.0, 0.3740140937133927, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 466707.0884256142, 466707.0884256142, 125086.3875476137], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7251000.0000, 
sim time next is 7251600.0000, 
raw observation next is [21.23333333333333, 79.66666666666667, 1.0, 2.0, 0.3722794679193347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 464771.047527387, 464771.047527387, 124854.271799847], 
processed observation next is [1.0, 0.9565217391304348, 0.34197530864197523, 0.7966666666666667, 1.0, 1.0, 0.25271365228492226, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16598965983120964, 0.16598965983120964, 0.24010436884585962], 
reward next is 0.7599, 
noisyNet noise sample is [array([-2.8375573], dtype=float32), -1.8505274]. 
=============================================
[2019-04-27 20:05:51,642] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.6334151e-36 1.0000000e+00 7.6697252e-38 0.0000000e+00 1.8035828e-30], sum to 1.0000
[2019-04-27 20:05:51,651] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9392
[2019-04-27 20:05:51,657] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.45, 87.0, 1.0, 2.0, 0.377824482356769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 470804.9671626744, 470804.9671626744, 125594.9737258816], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7270200.0000, 
sim time next is 7270800.0000, 
raw observation next is [20.43333333333333, 87.33333333333334, 1.0, 2.0, 0.3778095166899872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 470637.4411611676, 470637.4411611676, 125589.9885384235], 
processed observation next is [1.0, 0.13043478260869565, 0.31234567901234556, 0.8733333333333334, 1.0, 1.0, 0.25929704367855616, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1680848004147027, 0.1680848004147027, 0.2415192087277375], 
reward next is 0.7585, 
noisyNet noise sample is [array([-0.43813866], dtype=float32), -0.54471284]. 
=============================================
[2019-04-27 20:05:53,351] A3C_AGENT_WORKER-Thread-19 INFO:Local step 79000, global step 1255312: loss 1.2311
[2019-04-27 20:05:53,353] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 79000, global step 1255312: learning rate 0.0001
[2019-04-27 20:05:55,697] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.7843085e-32 1.0000000e+00 1.8540951e-31 1.9793418e-33 3.6775242e-20], sum to 1.0000
[2019-04-27 20:05:55,706] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2622
[2019-04-27 20:05:55,711] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.93333333333334, 78.66666666666667, 1.0, 2.0, 0.4420644554898553, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 540003.7077979653, 540003.7077979653, 134504.7884001213], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7333800.0000, 
sim time next is 7334400.0000, 
raw observation next is [22.76666666666667, 79.33333333333334, 1.0, 2.0, 0.4404849289388171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 538653.6978664433, 538653.6978664433, 134287.7652796051], 
processed observation next is [1.0, 0.9130434782608695, 0.3987654320987655, 0.7933333333333334, 1.0, 1.0, 0.33391062968906793, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1923763206665869, 0.1923763206665869, 0.25824570246077905], 
reward next is 0.7418, 
noisyNet noise sample is [array([0.8136955], dtype=float32), -0.08331711]. 
=============================================
[2019-04-27 20:05:56,000] A3C_AGENT_WORKER-Thread-16 INFO:Local step 78500, global step 1256568: loss 0.0227
[2019-04-27 20:05:56,005] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 78500, global step 1256571: learning rate 0.0001
[2019-04-27 20:05:56,106] A3C_AGENT_WORKER-Thread-11 INFO:Local step 78500, global step 1256619: loss 0.0823
[2019-04-27 20:05:56,109] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 78500, global step 1256619: learning rate 0.0001
[2019-04-27 20:05:56,193] A3C_AGENT_WORKER-Thread-12 INFO:Local step 78500, global step 1256664: loss 0.1179
[2019-04-27 20:05:56,197] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 78500, global step 1256665: learning rate 0.0001
[2019-04-27 20:05:56,434] A3C_AGENT_WORKER-Thread-15 INFO:Local step 78500, global step 1256777: loss 0.3646
[2019-04-27 20:05:56,438] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 78500, global step 1256777: learning rate 0.0001
[2019-04-27 20:05:56,448] A3C_AGENT_WORKER-Thread-17 INFO:Local step 78500, global step 1256780: loss 0.3414
[2019-04-27 20:05:56,451] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 78500, global step 1256781: learning rate 0.0001
[2019-04-27 20:05:56,732] A3C_AGENT_WORKER-Thread-18 INFO:Local step 78500, global step 1256920: loss 0.0121
[2019-04-27 20:05:56,733] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 78500, global step 1256920: learning rate 0.0001
[2019-04-27 20:05:56,869] A3C_AGENT_WORKER-Thread-9 INFO:Local step 78500, global step 1256986: loss 0.0023
[2019-04-27 20:05:56,872] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 78500, global step 1256987: learning rate 0.0001
[2019-04-27 20:05:56,968] A3C_AGENT_WORKER-Thread-2 INFO:Local step 78500, global step 1257031: loss 0.0386
[2019-04-27 20:05:56,969] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 78500, global step 1257031: learning rate 0.0001
[2019-04-27 20:05:57,016] A3C_AGENT_WORKER-Thread-10 INFO:Local step 78500, global step 1257055: loss 0.0516
[2019-04-27 20:05:57,019] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 78500, global step 1257055: learning rate 0.0001
[2019-04-27 20:05:57,048] A3C_AGENT_WORKER-Thread-21 INFO:Local step 78500, global step 1257068: loss 0.0377
[2019-04-27 20:05:57,049] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 78500, global step 1257068: learning rate 0.0001
[2019-04-27 20:05:57,056] A3C_AGENT_WORKER-Thread-13 INFO:Local step 78500, global step 1257070: loss 0.0735
[2019-04-27 20:05:57,057] A3C_AGENT_WORKER-Thread-14 INFO:Local step 78500, global step 1257070: loss 0.0675
[2019-04-27 20:05:57,058] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 78500, global step 1257070: learning rate 0.0001
[2019-04-27 20:05:57,060] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 78500, global step 1257070: learning rate 0.0001
[2019-04-27 20:05:57,126] A3C_AGENT_WORKER-Thread-20 INFO:Local step 78500, global step 1257102: loss 0.0520
[2019-04-27 20:05:57,128] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 78500, global step 1257102: learning rate 0.0001
[2019-04-27 20:05:57,177] A3C_AGENT_WORKER-Thread-22 INFO:Local step 78500, global step 1257125: loss 0.0213
[2019-04-27 20:05:57,179] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 78500, global step 1257125: learning rate 0.0001
[2019-04-27 20:06:00,591] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4914384e-32 1.0000000e+00 8.5571174e-29 8.1877075e-31 4.1628379e-15], sum to 1.0000
[2019-04-27 20:06:00,598] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6898
[2019-04-27 20:06:00,605] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.28333333333333, 90.0, 1.0, 2.0, 0.3851170264154949, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 478582.9441831722, 478582.9441831722, 126572.7796376187], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7426200.0000, 
sim time next is 7426800.0000, 
raw observation next is [20.3, 90.0, 1.0, 2.0, 0.3840881073214752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 477198.1306143554, 477198.1306143554, 126428.3204310284], 
processed observation next is [1.0, 1.0, 0.3074074074074074, 0.9, 1.0, 1.0, 0.26677155633508953, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.17042790379084122, 0.17042790379084122, 0.2431313854442854], 
reward next is 0.7569, 
noisyNet noise sample is [array([-0.9071052], dtype=float32), -0.5742121]. 
=============================================
[2019-04-27 20:06:02,041] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:06:02,051] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1901
[2019-04-27 20:06:02,058] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.9, 97.0, 1.0, 2.0, 0.3579519182450738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 448160.4474447812, 448160.4474447812, 122944.7746063279], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7448400.0000, 
sim time next is 7449000.0000, 
raw observation next is [18.98333333333333, 96.66666666666667, 1.0, 2.0, 0.3583237983945239, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 448484.7091140829, 448484.7091140829, 122992.1406458033], 
processed observation next is [0.0, 0.21739130434782608, 0.25864197530864186, 0.9666666666666667, 1.0, 1.0, 0.23609975999348082, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16017311039788676, 0.16017311039788676, 0.23652334739577557], 
reward next is 0.7635, 
noisyNet noise sample is [array([-0.6196301], dtype=float32), 0.9405464]. 
=============================================
[2019-04-27 20:06:02,070] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[69.66237 ]
 [69.666504]
 [69.67373 ]
 [69.68551 ]
 [69.687675]], R is [[69.6940918 ]
 [69.7607193 ]
 [69.82630157]
 [69.89083862]
 [69.95433807]].
[2019-04-27 20:06:02,599] A3C_AGENT_WORKER-Thread-3 INFO:Local step 79000, global step 1259650: loss -23.6238
[2019-04-27 20:06:02,601] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 79000, global step 1259650: learning rate 0.0001
[2019-04-27 20:06:02,673] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.2574274e-37], sum to 1.0000
[2019-04-27 20:06:02,682] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1815
[2019-04-27 20:06:02,686] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.66666666666666, 94.33333333333334, 1.0, 2.0, 0.372261010647395, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 463345.9992650067, 463345.9992650067, 124824.5912623287], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7454400.0000, 
sim time next is 7455000.0000, 
raw observation next is [19.73333333333333, 94.16666666666667, 1.0, 2.0, 0.3740170062083767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 465229.4147697355, 465229.4147697355, 125057.6343968954], 
processed observation next is [0.0, 0.2608695652173913, 0.28641975308641965, 0.9416666666666668, 1.0, 1.0, 0.2547821502480675, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16615336241776268, 0.16615336241776268, 0.2404954507632604], 
reward next is 0.7595, 
noisyNet noise sample is [array([-0.71224064], dtype=float32), 0.76030433]. 
=============================================
[2019-04-27 20:06:02,707] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[69.42095 ]
 [69.447914]
 [69.46862 ]
 [69.4916  ]
 [69.5352  ]], R is [[69.46807098]
 [69.53334808]
 [69.59844208]
 [69.66333771]
 [69.72799683]].
[2019-04-27 20:06:05,394] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.6915140e-35 1.0000000e+00 3.1760827e-35 2.6007301e-36 3.2885900e-25], sum to 1.0000
[2019-04-27 20:06:05,399] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9839834e-33 1.0000000e+00 3.1747050e-33 3.6868630e-36 2.7531337e-25], sum to 1.0000
[2019-04-27 20:06:05,403] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1004
[2019-04-27 20:06:05,408] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 95.0, 1.0, 2.0, 0.4626948189007392, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 558998.2359981152, 558998.2359981152, 137403.8431584238], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7516800.0000, 
sim time next is 7517400.0000, 
raw observation next is [21.45, 95.16666666666667, 1.0, 2.0, 0.4612002873353537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 557530.5346170064, 557530.534617006, 137189.1859290316], 
processed observation next is [0.0, 0.0, 0.35, 0.9516666666666667, 1.0, 1.0, 0.35857177063732576, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1991180480775023, 0.19911804807750214, 0.26382535755583003], 
reward next is 0.7362, 
noisyNet noise sample is [array([0.6515903], dtype=float32), 0.7822105]. 
=============================================
[2019-04-27 20:06:05,409] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1141
[2019-04-27 20:06:05,416] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.95, 94.0, 1.0, 2.0, 0.4774217872444759, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 573627.2464989118, 573627.2464989118, 139542.3522831036], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7511400.0000, 
sim time next is 7512000.0000, 
raw observation next is [21.9, 94.33333333333333, 1.0, 2.0, 0.4768074892950998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 572982.4012826721, 572982.4012826721, 139451.3166179553], 
processed observation next is [0.0, 0.9565217391304348, 0.36666666666666664, 0.9433333333333332, 1.0, 1.0, 0.37715177297035696, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2046365718866686, 0.2046365718866686, 0.26817560888068326], 
reward next is 0.7318, 
noisyNet noise sample is [array([-0.54966193], dtype=float32), -0.9209817]. 
=============================================
[2019-04-27 20:06:05,432] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[69.46955]
 [69.43182]
 [69.40498]
 [69.38313]
 [69.33619]], R is [[69.52985382]
 [69.56620789]
 [69.60198975]
 [69.63717651]
 [69.67177582]].
[2019-04-27 20:06:08,601] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5523035e-36 1.0000000e+00 4.2675651e-35 2.9884901e-38 1.9694447e-26], sum to 1.0000
[2019-04-27 20:06:08,613] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0276
[2019-04-27 20:06:08,619] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.88333333333334, 53.83333333333334, 1.0, 2.0, 0.5018136793915345, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 597278.7137802792, 597278.7137802792, 143134.1007494363], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7927800.0000, 
sim time next is 7928400.0000, 
raw observation next is [28.66666666666667, 54.66666666666667, 1.0, 2.0, 0.5026658954091716, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 598684.9647947743, 598684.9647947743, 143282.9325409733], 
processed observation next is [1.0, 0.782608695652174, 0.6172839506172841, 0.5466666666666667, 1.0, 1.0, 0.40793558977282335, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21381605885527655, 0.21381605885527655, 0.2755441010403333], 
reward next is 0.7245, 
noisyNet noise sample is [array([0.6813506], dtype=float32), 0.5635815]. 
=============================================
[2019-04-27 20:06:10,701] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:06:10,702] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:06:10,756] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res14/Eplus-env-sub_run7
[2019-04-27 20:06:12,000] A3C_AGENT_WORKER-Thread-16 INFO:Local step 79000, global step 1264465: loss 0.5368
[2019-04-27 20:06:12,001] A3C_AGENT_WORKER-Thread-11 INFO:Local step 79000, global step 1264465: loss 0.5837
[2019-04-27 20:06:12,002] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 79000, global step 1264465: learning rate 0.0001
[2019-04-27 20:06:12,002] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 79000, global step 1264465: learning rate 0.0001
[2019-04-27 20:06:12,145] A3C_AGENT_WORKER-Thread-12 INFO:Local step 79000, global step 1264542: loss 0.2003
[2019-04-27 20:06:12,149] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 79000, global step 1264543: learning rate 0.0001
[2019-04-27 20:06:12,417] A3C_AGENT_WORKER-Thread-17 INFO:Local step 79000, global step 1264682: loss -15.9859
[2019-04-27 20:06:12,422] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 79000, global step 1264683: learning rate 0.0001
[2019-04-27 20:06:12,508] A3C_AGENT_WORKER-Thread-15 INFO:Local step 79000, global step 1264730: loss 0.1573
[2019-04-27 20:06:12,511] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 79000, global step 1264731: learning rate 0.0001
[2019-04-27 20:06:12,671] A3C_AGENT_WORKER-Thread-18 INFO:Local step 79000, global step 1264824: loss 0.0645
[2019-04-27 20:06:12,675] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 79000, global step 1264825: learning rate 0.0001
[2019-04-27 20:06:12,721] A3C_AGENT_WORKER-Thread-9 INFO:Local step 79000, global step 1264850: loss 0.8690
[2019-04-27 20:06:12,722] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 79000, global step 1264850: learning rate 0.0001
[2019-04-27 20:06:12,799] A3C_AGENT_WORKER-Thread-10 INFO:Local step 79000, global step 1264890: loss 0.0597
[2019-04-27 20:06:12,805] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 79000, global step 1264890: learning rate 0.0001
[2019-04-27 20:06:12,934] A3C_AGENT_WORKER-Thread-13 INFO:Local step 79000, global step 1264958: loss 0.0286
[2019-04-27 20:06:12,936] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 79000, global step 1264959: learning rate 0.0001
[2019-04-27 20:06:13,000] A3C_AGENT_WORKER-Thread-2 INFO:Local step 79000, global step 1264995: loss 0.0100
[2019-04-27 20:06:13,002] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 79000, global step 1264995: learning rate 0.0001
[2019-04-27 20:06:13,019] A3C_AGENT_WORKER-Thread-22 INFO:Local step 79000, global step 1265003: loss 0.0132
[2019-04-27 20:06:13,020] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 79000, global step 1265003: learning rate 0.0001
[2019-04-27 20:06:13,113] A3C_AGENT_WORKER-Thread-21 INFO:Local step 79000, global step 1265052: loss 0.0341
[2019-04-27 20:06:13,115] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 79000, global step 1265053: learning rate 0.0001
[2019-04-27 20:06:13,162] A3C_AGENT_WORKER-Thread-14 INFO:Local step 79000, global step 1265076: loss 0.0320
[2019-04-27 20:06:13,166] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 79000, global step 1265076: learning rate 0.0001
[2019-04-27 20:06:13,244] A3C_AGENT_WORKER-Thread-20 INFO:Local step 79000, global step 1265118: loss 0.0222
[2019-04-27 20:06:13,245] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 79000, global step 1265118: learning rate 0.0001
[2019-04-27 20:06:13,797] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.2991917e-30 1.0000000e+00 3.9916049e-30 1.8925030e-32 2.4717447e-19], sum to 1.0000
[2019-04-27 20:06:13,802] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2227
[2019-04-27 20:06:13,810] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.66666666666667, 72.0, 1.0, 2.0, 0.2843058283344168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 364766.7722348623, 364766.7722348619, 113657.813188766], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7692000.0000, 
sim time next is 7692600.0000, 
raw observation next is [19.68333333333333, 70.5, 1.0, 2.0, 0.2790097383682963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 358555.4866151759, 358555.4866151759, 113018.1658264872], 
processed observation next is [1.0, 0.0, 0.28456790123456777, 0.705, 1.0, 1.0, 0.1416782599622575, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.12805553093399138, 0.12805553093399138, 0.21734262658939846], 
reward next is 0.7827, 
noisyNet noise sample is [array([-0.26247793], dtype=float32), -0.9580915]. 
=============================================
[2019-04-27 20:06:14,810] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 2.3824890e-36 0.0000000e+00 1.3862456e-30], sum to 1.0000
[2019-04-27 20:06:14,816] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2257
[2019-04-27 20:06:14,823] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.7, 61.5, 1.0, 2.0, 0.2397746894222118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 309286.0602243494, 309286.0602243499, 97263.12307719156], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7696200.0000, 
sim time next is 7696800.0000, 
raw observation next is [19.7, 60.0, 1.0, 2.0, 0.2334398747016029, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 301113.1606111623, 301113.1606111628, 94453.746900641], 
processed observation next is [1.0, 0.08695652173913043, 0.28518518518518515, 0.6, 1.0, 1.0, 0.08742842226381298, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.10754041450398653, 0.1075404145039867, 0.18164182096277115], 
reward next is 0.8184, 
noisyNet noise sample is [array([0.21928518], dtype=float32), -0.2585209]. 
=============================================
[2019-04-27 20:06:18,740] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:06:18,741] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:06:18,798] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res3/Eplus-env-sub_run7
[2019-04-27 20:06:23,436] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.7479863e-28 1.0000000e+00 2.9105144e-28 4.0935161e-31 3.7256571e-14], sum to 1.0000
[2019-04-27 20:06:23,445] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9628
[2019-04-27 20:06:23,451] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.13333333333333, 74.0, 1.0, 2.0, 0.4194132392338866, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 515992.4159968887, 515992.4159968887, 131299.1132037926], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7863600.0000, 
sim time next is 7864200.0000, 
raw observation next is [23.05, 74.5, 1.0, 2.0, 0.4180165838678038, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 514343.2155642529, 514343.2155642529, 131099.7710779202], 
processed observation next is [1.0, 0.0, 0.40925925925925927, 0.745, 1.0, 1.0, 0.30716259984262356, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.18369400555866175, 0.18369400555866175, 0.25211494438061577], 
reward next is 0.7479, 
noisyNet noise sample is [array([0.35536313], dtype=float32), -1.501496]. 
=============================================
[2019-04-27 20:06:27,453] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:06:27,454] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:06:27,470] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res11/Eplus-env-sub_run7
[2019-04-27 20:06:27,576] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:06:27,577] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:06:27,590] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res6/Eplus-env-sub_run7
[2019-04-27 20:06:27,780] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:06:27,781] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:06:27,792] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res7/Eplus-env-sub_run7
[2019-04-27 20:06:27,971] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:06:27,971] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:06:27,977] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res10/Eplus-env-sub_run7
[2019-04-27 20:06:28,113] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:06:28,114] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:06:28,122] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res12/Eplus-env-sub_run7
[2019-04-27 20:06:28,148] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:06:28,148] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:06:28,161] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res13/Eplus-env-sub_run7
[2019-04-27 20:06:28,192] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:06:28,193] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:06:28,197] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res4/Eplus-env-sub_run7
[2019-04-27 20:06:28,229] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:06:28,232] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:06:28,237] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:06:28,239] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:06:28,243] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res5/Eplus-env-sub_run7
[2019-04-27 20:06:28,275] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res8/Eplus-env-sub_run7
[2019-04-27 20:06:28,313] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:06:28,313] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:06:28,317] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res17/Eplus-env-sub_run7
[2019-04-27 20:06:28,363] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:06:28,364] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:06:28,365] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:06:28,366] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:06:28,377] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:06:28,368] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:06:28,380] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:06:28,374] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:06:28,386] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res9/Eplus-env-sub_run7
[2019-04-27 20:06:28,380] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res16/Eplus-env-sub_run7
[2019-04-27 20:06:28,368] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res15/Eplus-env-sub_run7
[2019-04-27 20:06:28,420] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res2/Eplus-env-sub_run7
[2019-04-27 20:06:33,098] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-27 20:06:33,099] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 20:06:33,100] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:06:33,101] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 20:06:33,102] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:06:33,102] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 20:06:33,103] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 20:06:33,105] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 20:06:33,104] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:06:33,107] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:06:33,108] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:06:33,126] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run52
[2019-04-27 20:06:33,146] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run52
[2019-04-27 20:06:33,168] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run52
[2019-04-27 20:06:33,168] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run52
[2019-04-27 20:06:33,213] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run52
[2019-04-27 20:07:22,213] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04857315], dtype=float32), -0.030129459]
[2019-04-27 20:07:22,214] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [32.91666666666667, 68.5, 1.0, 2.0, 0.9168612557374368, 1.0, 2.0, 0.9168612557374368, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 2091589.936698787, 2091589.936698788, 394412.5207956134]
[2019-04-27 20:07:22,215] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 20:07:22,217] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [5.0169286e-26 1.0000000e+00 4.2849088e-26 2.3422175e-28 2.7051066e-20], sampled 0.4462583997375975
[2019-04-27 20:07:22,218] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2091589.936698787 W.
[2019-04-27 20:07:26,433] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04857315], dtype=float32), -0.030129459]
[2019-04-27 20:07:26,436] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [28.8, 78.0, 1.0, 2.0, 0.7452583022545795, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 849412.2213097662, 849412.2213097657, 184893.0526726494]
[2019-04-27 20:07:26,437] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:07:26,442] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.1945538301334283
[2019-04-27 20:07:53,909] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04857315], dtype=float32), -0.030129459]
[2019-04-27 20:07:53,909] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [32.41666666666667, 51.5, 1.0, 2.0, 0.8956026818207089, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1020882.184777056, 1020882.184777056, 216479.9572856499]
[2019-04-27 20:07:53,910] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:07:53,912] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5209400483552642
[2019-04-27 20:08:01,786] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04857315], dtype=float32), -0.030129459]
[2019-04-27 20:08:01,787] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [26.762523215, 92.57596567, 1.0, 2.0, 0.8377981175311692, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 954950.6939560606, 954950.6939560601, 203877.4448580238]
[2019-04-27 20:08:01,789] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 20:08:01,792] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9561419260642836
[2019-04-27 20:08:22,122] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8696.6451 2196123842.7844 571.0000
[2019-04-27 20:08:22,355] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8579.7404 2249030529.1217 553.0000
[2019-04-27 20:08:22,367] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8096.1228 2445685030.0399 746.0000
[2019-04-27 20:08:22,501] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8766.9702 2170957530.8983 493.0000
[2019-04-27 20:08:22,509] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8921.1611 2120848252.9630 431.0000
[2019-04-27 20:08:23,526] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1275000, evaluation results [1275000.0, 8096.1228420842035, 2445685030.039925, 746.0, 8766.970175200424, 2170957530.898276, 493.0, 8921.161124137237, 2120848252.9629614, 431.0, 8579.740429012678, 2249030529.1217084, 553.0, 8696.645113319491, 2196123842.7843685, 571.0]
[2019-04-27 20:08:24,054] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:08:24,060] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8172
[2019-04-27 20:08:24,065] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 68.66666666666667, 1.0, 2.0, 0.4178592707776346, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 513859.7290058789, 513859.7290058789, 131069.6960763919], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 87000.0000, 
sim time next is 87600.0000, 
raw observation next is [23.9, 69.33333333333334, 1.0, 2.0, 0.4187074376357676, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 514863.4061783089, 514863.4061783089, 131190.7346649135], 
processed observation next is [1.0, 0.0, 0.4407407407407407, 0.6933333333333335, 1.0, 1.0, 0.3079850448044853, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1838797879208246, 0.1838797879208246, 0.2522898743556029], 
reward next is 0.7477, 
noisyNet noise sample is [array([0.45975313], dtype=float32), -0.5829324]. 
=============================================
[2019-04-27 20:08:28,516] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.4665390e-35 1.0000000e+00 3.0966810e-34 2.2970358e-37 1.7801602e-20], sum to 1.0000
[2019-04-27 20:08:28,524] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3230
[2019-04-27 20:08:28,530] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.33333333333334, 13.0, 1.0, 2.0, 0.3744037703406473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 482985.013067246, 482985.013067246, 125198.76485877], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 156000.0000, 
sim time next is 156600.0000, 
raw observation next is [33.05, 13.5, 1.0, 2.0, 0.3714191872577372, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 479147.3829352708, 479147.3829352704, 124517.8099618375], 
processed observation next is [1.0, 0.8260869565217391, 0.7796296296296296, 0.135, 1.0, 1.0, 0.2516895086401633, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.17112406533402527, 0.17112406533402513, 0.23945732684968749], 
reward next is 0.7605, 
noisyNet noise sample is [array([1.647357], dtype=float32), 0.8161706]. 
=============================================
[2019-04-27 20:08:34,622] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2674840e-25 4.8734441e-12 8.7923816e-22 1.8709556e-23 1.0000000e+00], sum to 1.0000
[2019-04-27 20:08:34,632] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1148
[2019-04-27 20:08:34,638] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.53333333333333, 32.33333333333334, 1.0, 2.0, 0.4533403881931752, 1.0, 2.0, 0.4533403881931752, 1.0, 2.0, 0.7279050111909993, 6.9112, 6.9112, 121.94756008, 1614100.047842609, 1614100.047842609, 321142.762799213], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 574800.0000, 
sim time next is 575400.0000, 
raw observation next is [31.61666666666667, 32.16666666666666, 1.0, 2.0, 0.456356680569022, 1.0, 2.0, 0.456356680569022, 1.0, 2.0, 0.7325000750697952, 6.9112, 6.9112, 121.94756008, 1623502.058373797, 1623502.058373797, 322571.3156280496], 
processed observation next is [1.0, 0.6521739130434783, 0.7265432098765433, 0.32166666666666655, 1.0, 1.0, 0.35280557210597857, 1.0, 1.0, 0.35280557210597857, 1.0, 1.0, 0.665625093837244, 0.0, 0.0, 0.8096049824067558, 0.5798221637049275, 0.5798221637049275, 0.6203294531308646], 
reward next is 0.3797, 
noisyNet noise sample is [array([0.58951175], dtype=float32), 1.2708521]. 
=============================================
[2019-04-27 20:08:35,509] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:08:35,522] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1211
[2019-04-27 20:08:35,527] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.56666666666666, 50.66666666666667, 1.0, 2.0, 0.257986190803827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 332782.2246281187, 332782.2246281187, 94260.93457939157], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 268800.0000, 
sim time next is 269400.0000, 
raw observation next is [20.48333333333333, 50.83333333333333, 1.0, 2.0, 0.2571165758668353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 331660.2461021968, 331660.2461021968, 93852.90949045497], 
processed observation next is [0.0, 0.08695652173913043, 0.31419753086419744, 0.5083333333333333, 1.0, 1.0, 0.115614971270042, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.11845008789364171, 0.11845008789364171, 0.1804863644047211], 
reward next is 0.8195, 
noisyNet noise sample is [array([-0.7423357], dtype=float32), 1.202528]. 
=============================================
[2019-04-27 20:08:35,688] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:08:35,695] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9770
[2019-04-27 20:08:35,705] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.2, 34.0, 1.0, 2.0, 0.2886717545868255, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 372373.8369667386, 372373.8369667386, 100416.5954597608], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 289800.0000, 
sim time next is 290400.0000, 
raw observation next is [24.33333333333333, 33.66666666666667, 1.0, 2.0, 0.2902953084443825, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 374468.6611776749, 374468.6611776749, 100940.6008992357], 
processed observation next is [0.0, 0.34782608695652173, 0.45679012345678993, 0.3366666666666667, 1.0, 1.0, 0.1551134624337887, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.13373880756345533, 0.13373880756345533, 0.19411654019083788], 
reward next is 0.8059, 
noisyNet noise sample is [array([2.1686277], dtype=float32), -0.40866834]. 
=============================================
[2019-04-27 20:08:37,107] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.222584e-33 1.000000e+00 1.239028e-33 8.031130e-36 1.805817e-26], sum to 1.0000
[2019-04-27 20:08:37,117] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9263
[2019-04-27 20:08:37,120] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.76666666666667, 63.33333333333333, 1.0, 2.0, 0.3762194495029246, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 479118.1205071408, 479118.1205071408, 125507.7593300479], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 618000.0000, 
sim time next is 618600.0000, 
raw observation next is [21.58333333333333, 64.16666666666667, 1.0, 2.0, 0.3677192594262543, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 468535.1334868283, 468535.1334868283, 124349.5147853625], 
processed observation next is [1.0, 0.13043478260869565, 0.35493827160493807, 0.6416666666666667, 1.0, 1.0, 0.2472848326503027, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16733397624529583, 0.16733397624529583, 0.23913368227954326], 
reward next is 0.7609, 
noisyNet noise sample is [array([0.78268254], dtype=float32), -0.2518612]. 
=============================================
[2019-04-27 20:08:38,772] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:08:38,781] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7474
[2019-04-27 20:08:38,785] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 52.33333333333334, 1.0, 2.0, 0.3840932160572779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 477416.9237210405, 477416.9237210405, 126433.5905156698], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 811200.0000, 
sim time next is 811800.0000, 
raw observation next is [26.4, 51.5, 1.0, 2.0, 0.3878690794766997, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 481358.9713252078, 481358.9713252078, 126940.4451537402], 
processed observation next is [0.0, 0.391304347826087, 0.5333333333333333, 0.515, 1.0, 1.0, 0.27127271366273775, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.17191391833043135, 0.17191391833043135, 0.24411624068026963], 
reward next is 0.7559, 
noisyNet noise sample is [array([0.07368966], dtype=float32), 0.24236122]. 
=============================================
[2019-04-27 20:08:38,859] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:08:38,868] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1764
[2019-04-27 20:08:38,878] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.13333333333334, 43.0, 1.0, 2.0, 0.2932592385387378, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 377608.8539673844, 377608.853967384, 114738.8258633974], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 339600.0000, 
sim time next is 340200.0000, 
raw observation next is [24.0, 43.5, 1.0, 2.0, 0.2919038243552602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 375938.5143793615, 375938.5143793615, 114572.5366859188], 
processed observation next is [0.0, 0.9565217391304348, 0.4444444444444444, 0.435, 1.0, 1.0, 0.1570283623276907, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.13426375513548627, 0.13426375513548627, 0.22033180131907462], 
reward next is 0.7797, 
noisyNet noise sample is [array([0.01774079], dtype=float32), 0.0042336923]. 
=============================================
[2019-04-27 20:08:44,199] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2015294e-27 1.6260746e-13 1.6809201e-21 3.5232851e-26 1.0000000e+00], sum to 1.0000
[2019-04-27 20:08:44,215] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8455
[2019-04-27 20:08:44,227] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.15000000000001, 23.0, 1.0, 2.0, 0.3355842356398185, 1.0, 2.0, 0.3355842356398185, 1.0, 2.0, 0.5515384637038773, 6.911200000000001, 6.9112, 121.94756008, 1237044.803569571, 1237044.803569571, 268451.0229767364], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 743400.0000, 
sim time next is 744000.0000, 
raw observation next is [32.13333333333333, 23.0, 1.0, 2.0, 0.3452435311056064, 1.0, 2.0, 0.3452435311056064, 1.0, 2.0, 0.5673650223544987, 6.9112, 6.9112, 121.94756008, 1272569.295032759, 1272569.295032759, 272375.5240626521], 
processed observation next is [1.0, 0.6086956521739131, 0.745679012345679, 0.23, 1.0, 1.0, 0.22052801322096002, 1.0, 1.0, 0.22052801322096002, 1.0, 1.0, 0.45920627794312335, 0.0, 0.0, 0.8096049824067558, 0.45448903394027107, 0.45448903394027107, 0.5237990847358693], 
reward next is 0.4762, 
noisyNet noise sample is [array([-0.3201363], dtype=float32), 0.7174677]. 
=============================================
[2019-04-27 20:08:44,234] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[74.12406]
 [73.40008]
 [73.05079]
 [72.10787]
 [71.76735]], R is [[74.27014923]
 [74.01119995]
 [73.75242615]
 [73.49020386]
 [73.23101044]].
[2019-04-27 20:08:50,005] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1236076e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.8195022e-37], sum to 1.0000
[2019-04-27 20:08:50,014] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4314
[2019-04-27 20:08:50,019] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.86666666666667, 75.0, 1.0, 2.0, 0.3293455430465352, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 420313.636904403, 420313.636904403, 119278.4122688979], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 531600.0000, 
sim time next is 532200.0000, 
raw observation next is [19.78333333333333, 75.5, 1.0, 2.0, 0.3246235054713112, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 414361.1856674658, 414361.1856674658, 118672.409002684], 
processed observation next is [1.0, 0.13043478260869565, 0.28827160493827153, 0.755, 1.0, 1.0, 0.19598036365632285, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.14798613773838065, 0.14798613773838065, 0.2282161711590077], 
reward next is 0.7718, 
noisyNet noise sample is [array([-0.6362997], dtype=float32), 1.0568736]. 
=============================================
[2019-04-27 20:08:54,099] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2580565e-34 1.0000000e+00 2.9688131e-36 3.9212594e-38 4.0349168e-29], sum to 1.0000
[2019-04-27 20:08:54,107] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3846
[2019-04-27 20:08:54,113] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.3, 71.0, 1.0, 2.0, 0.3005439443253635, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 384020.1455242097, 384020.1455242097, 115643.7780385059], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 622800.0000, 
sim time next is 623400.0000, 
raw observation next is [20.58333333333334, 70.0, 1.0, 2.0, 0.3420175202503838, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 436627.382288394, 436627.3822883936, 120925.3866494345], 
processed observation next is [1.0, 0.21739130434782608, 0.3179012345679015, 0.7, 1.0, 1.0, 0.21668752410759975, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.15593835081728355, 0.15593835081728344, 0.23254882047968173], 
reward next is 0.7675, 
noisyNet noise sample is [array([-0.54320645], dtype=float32), -1.5687461]. 
=============================================
[2019-04-27 20:08:59,773] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4009721e-25 9.9995220e-01 1.4716538e-26 7.0560144e-26 4.7788577e-05], sum to 1.0000
[2019-04-27 20:08:59,781] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6008
[2019-04-27 20:08:59,784] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.05, 28.0, 1.0, 2.0, 0.3481066062000315, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 441603.5201271847, 441603.5201271847, 121711.5438070185], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 761400.0000, 
sim time next is 762000.0000, 
raw observation next is [29.93333333333333, 28.33333333333334, 1.0, 2.0, 0.3464880502972588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 439587.1093489051, 439587.1093489051, 121498.2615944353], 
processed observation next is [1.0, 0.8260869565217391, 0.6641975308641974, 0.2833333333333334, 1.0, 1.0, 0.22200958368721288, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.15699539619603753, 0.15699539619603753, 0.23365050306622173], 
reward next is 0.7663, 
noisyNet noise sample is [array([0.5178161], dtype=float32), -0.03922568]. 
=============================================
[2019-04-27 20:08:59,800] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[77.092064]
 [77.31883 ]
 [77.52741 ]
 [77.91219 ]
 [78.17719 ]], R is [[76.94419098]
 [76.94068909]
 [76.93676758]
 [76.93202209]
 [76.92652893]].
[2019-04-27 20:09:02,880] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:09:02,887] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0334
[2019-04-27 20:09:02,891] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 52.33333333333334, 1.0, 2.0, 0.3840932160572779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 477416.9237210405, 477416.9237210405, 126433.5905156698], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 811200.0000, 
sim time next is 811800.0000, 
raw observation next is [26.4, 51.5, 1.0, 2.0, 0.3878690794766997, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 481358.9713252078, 481358.9713252078, 126940.4451537402], 
processed observation next is [0.0, 0.391304347826087, 0.5333333333333333, 0.515, 1.0, 1.0, 0.27127271366273775, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.17191391833043135, 0.17191391833043135, 0.24411624068026963], 
reward next is 0.7559, 
noisyNet noise sample is [array([-0.6263067], dtype=float32), -1.6553946]. 
=============================================
[2019-04-27 20:09:03,741] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 8.982721e-34], sum to 1.0000
[2019-04-27 20:09:03,749] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7259
[2019-04-27 20:09:03,760] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.5, 54.0, 1.0, 2.0, 0.375405871865336, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 468079.2147791556, 468079.2147791556, 125269.6319547523], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 810000.0000, 
sim time next is 810600.0000, 
raw observation next is [25.8, 53.16666666666666, 1.0, 2.0, 0.3799260576840032, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 472983.279061183, 472983.2790611825, 125874.8014675363], 
processed observation next is [0.0, 0.391304347826087, 0.5111111111111112, 0.5316666666666666, 1.0, 1.0, 0.26181673533809907, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1689225996647082, 0.16892259966470805, 0.24206692589910825], 
reward next is 0.7579, 
noisyNet noise sample is [array([0.18946871], dtype=float32), -0.5323601]. 
=============================================
[2019-04-27 20:09:06,401] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4156295e-26 1.0000000e+00 1.0867946e-23 5.6452119e-31 3.8532262e-16], sum to 1.0000
[2019-04-27 20:09:06,406] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2070
[2019-04-27 20:09:06,410] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.93333333333333, 65.0, 1.0, 2.0, 0.6306202207452155, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 799659.5752499886, 799659.5752499886, 166146.834477036], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1171200.0000, 
sim time next is 1171800.0000, 
raw observation next is [22.0, 65.0, 1.0, 2.0, 0.6814941947018979, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 863278.7872275835, 863278.7872275835, 175652.5017574293], 
processed observation next is [1.0, 0.5652173913043478, 0.37037037037037035, 0.65, 1.0, 1.0, 0.6208264222641641, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3083138525812798, 0.3083138525812798, 0.33779327261044095], 
reward next is 0.6622, 
noisyNet noise sample is [array([-1.3304319], dtype=float32), 0.33630407]. 
=============================================
[2019-04-27 20:09:12,704] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.4699656e-22 3.7022298e-11 2.5092353e-17 5.5717239e-23 1.0000000e+00], sum to 1.0000
[2019-04-27 20:09:12,712] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6597
[2019-04-27 20:09:12,719] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.1, 55.5, 1.0, 2.0, 0.2879519246441555, 1.0, 2.0, 0.2879519246441555, 1.0, 2.0, 0.4691825543507079, 6.9112, 6.9112, 121.94756008, 1050928.115933574, 1050928.115933574, 250393.1471838288], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 991800.0000, 
sim time next is 992400.0000, 
raw observation next is [25.13333333333333, 55.33333333333333, 1.0, 2.0, 0.2973198883865363, 1.0, 2.0, 0.2973198883865363, 1.0, 2.0, 0.4841080842805774, 6.9112, 6.9112, 121.94756008, 1084161.038888376, 1084161.038888376, 253984.8206494954], 
processed observation next is [1.0, 0.4782608695652174, 0.4864197530864196, 0.5533333333333332, 1.0, 1.0, 0.1634760576030194, 1.0, 1.0, 0.1634760576030194, 1.0, 1.0, 0.3551351053507217, 0.0, 0.0, 0.8096049824067558, 0.38720037103156285, 0.38720037103156285, 0.4884323474028758], 
reward next is 0.5116, 
noisyNet noise sample is [array([0.7171047], dtype=float32), -0.58083934]. 
=============================================
[2019-04-27 20:09:13,598] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-27 20:09:13,601] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 20:09:13,601] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 20:09:13,602] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:09:13,604] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 20:09:13,607] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 20:09:13,604] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:09:13,607] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 20:09:13,609] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:09:13,610] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:09:13,616] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:09:13,630] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run53
[2019-04-27 20:09:13,653] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run53
[2019-04-27 20:09:13,678] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run53
[2019-04-27 20:09:13,680] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run53
[2019-04-27 20:09:13,680] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run53
[2019-04-27 20:09:20,165] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04884176], dtype=float32), -0.036758177]
[2019-04-27 20:09:20,165] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [16.55, 84.0, 1.0, 2.0, 0.2307724815117061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 297671.8331317827, 297671.8331317827, 94125.23732797989]
[2019-04-27 20:09:20,166] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:09:20,168] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [2.7993114e-37 1.0000000e+00 3.1604504e-38 0.0000000e+00 1.1888387e-31], sampled 0.9842133942881984
[2019-04-27 20:09:33,719] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04884176], dtype=float32), -0.036758177]
[2019-04-27 20:09:33,720] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [18.0, 89.5, 1.0, 2.0, 0.3182815786813011, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 406560.8769969786, 406560.8769969786, 117865.1850955496]
[2019-04-27 20:09:33,722] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 20:09:33,725] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 6.710545e-35], sampled 0.9740170315268074
[2019-04-27 20:10:15,784] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04884176], dtype=float32), -0.036758177]
[2019-04-27 20:10:15,785] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [27.5, 72.0, 1.0, 2.0, 0.5895380556070177, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 676016.3958298899, 676016.3958298899, 156492.7031929638]
[2019-04-27 20:10:15,785] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:10:15,792] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [2.1822215e-35 1.0000000e+00 1.5183074e-35 1.3566716e-38 2.6248408e-28], sampled 0.6089558834652818
[2019-04-27 20:10:18,667] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04884176], dtype=float32), -0.036758177]
[2019-04-27 20:10:18,669] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [23.337354655, 100.45852754, 1.0, 2.0, 0.5784693772407693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 667452.0151096894, 667452.015109689, 154812.5721326687]
[2019-04-27 20:10:18,670] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 20:10:18,672] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [3.3454837e-37 1.0000000e+00 1.2694924e-37 0.0000000e+00 5.2845261e-31], sampled 0.12499353904023514
[2019-04-27 20:10:31,679] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04884176], dtype=float32), -0.036758177]
[2019-04-27 20:10:31,682] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [28.08503917, 68.6977645, 1.0, 2.0, 0.592040819090975, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 680599.5691566722, 680599.5691566722, 157003.2229812867]
[2019-04-27 20:10:31,682] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 20:10:31,685] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [4.9618816e-35 1.0000000e+00 4.6972774e-35 2.5314462e-38 9.8188800e-27], sampled 0.6771119830735367
[2019-04-27 20:10:55,592] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04884176], dtype=float32), -0.036758177]
[2019-04-27 20:10:55,593] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [22.5, 65.83333333333333, 1.0, 2.0, 0.3466593503090751, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 436586.9412020699, 436586.9412020695, 121486.0970543358]
[2019-04-27 20:10:55,593] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:10:55,596] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.5586170e-36 1.0000000e+00 5.1184070e-37 0.0000000e+00 2.3704314e-29], sampled 0.9291294465935431
[2019-04-27 20:11:01,538] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8190.7863 2453093859.3422 505.0000
[2019-04-27 20:11:01,726] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8803.3267 2173330110.5678 432.0000
[2019-04-27 20:11:01,804] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8642.4788 2252636865.9658 416.0000
[2019-04-27 20:11:01,871] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8738.0478 2200817488.4177 479.0000
[2019-04-27 20:11:01,966] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8925.5400 2123105243.2274 404.0000
[2019-04-27 20:11:02,982] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1300000, evaluation results [1300000.0, 8190.7863154642355, 2453093859.3422256, 505.0, 8803.326694964508, 2173330110.5677547, 432.0, 8925.539977028955, 2123105243.22744, 404.0, 8642.47883090743, 2252636865.9658036, 416.0, 8738.047781614408, 2200817488.417701, 479.0]
[2019-04-27 20:11:06,646] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3712537e-37 1.0000000e+00 2.0406134e-35 0.0000000e+00 2.1845408e-24], sum to 1.0000
[2019-04-27 20:11:06,656] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8085
[2019-04-27 20:11:06,661] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.63333333333333, 76.33333333333334, 1.0, 2.0, 0.4727414613196616, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 574141.7462042167, 574141.7462042163, 139026.0902882549], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1543200.0000, 
sim time next is 1543800.0000, 
raw observation next is [23.61666666666667, 75.66666666666666, 1.0, 2.0, 0.4678311596701277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 569183.6733609801, 569183.6733609801, 138306.9990268826], 
processed observation next is [0.0, 0.8695652173913043, 0.4302469135802471, 0.7566666666666666, 1.0, 1.0, 0.3664656662739616, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.20327988334320718, 0.20327988334320718, 0.2659749981286204], 
reward next is 0.7340, 
noisyNet noise sample is [array([0.09250896], dtype=float32), 0.93133664]. 
=============================================
[2019-04-27 20:11:09,620] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.2813822e-36 1.0000000e+00 4.9858213e-37 0.0000000e+00 8.1567362e-27], sum to 1.0000
[2019-04-27 20:11:09,626] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0451
[2019-04-27 20:11:09,630] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.2, 74.0, 1.0, 2.0, 0.2771509314595217, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 356301.6846647697, 356301.6846647692, 112795.1375354087], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1126800.0000, 
sim time next is 1127400.0000, 
raw observation next is [19.18333333333333, 74.0, 1.0, 2.0, 0.2761866390931678, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 355117.9015901437, 355117.9015901437, 112679.6667058073], 
processed observation next is [1.0, 0.043478260869565216, 0.2660493827160493, 0.74, 1.0, 1.0, 0.13831742749186646, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.12682782199647988, 0.12682782199647988, 0.21669166674193713], 
reward next is 0.7833, 
noisyNet noise sample is [array([2.1244092], dtype=float32), 0.84954524]. 
=============================================
[2019-04-27 20:11:10,673] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.4742094e-34 1.0000000e+00 1.3825778e-34 0.0000000e+00 1.1933472e-29], sum to 1.0000
[2019-04-27 20:11:10,682] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0134
[2019-04-27 20:11:10,689] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.03333333333333, 75.0, 1.0, 2.0, 0.2695027247311505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 346616.0097843836, 346616.0097843836, 111885.6191014021], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1143600.0000, 
sim time next is 1144200.0000, 
raw observation next is [19.11666666666667, 74.5, 1.0, 2.0, 0.2688318322007884, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 345703.9545943809, 345703.9545943809, 111806.7370009589], 
processed observation next is [1.0, 0.21739130434782608, 0.2635802469135804, 0.745, 1.0, 1.0, 0.12956170500093855, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.12346569806942176, 0.12346569806942176, 0.21501295577107482], 
reward next is 0.7850, 
noisyNet noise sample is [array([-0.27985385], dtype=float32), 0.7350712]. 
=============================================
[2019-04-27 20:11:11,559] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2008275e-36 1.0000000e+00 1.2272374e-37 0.0000000e+00 9.2336116e-31], sum to 1.0000
[2019-04-27 20:11:11,567] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1988
[2019-04-27 20:11:11,572] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.03333333333333, 75.0, 1.0, 2.0, 0.2695027247311505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 346616.0097843836, 346616.0097843836, 111885.6191014021], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1143600.0000, 
sim time next is 1144200.0000, 
raw observation next is [19.11666666666667, 74.5, 1.0, 2.0, 0.2688318322007884, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 345703.9545943809, 345703.9545943809, 111806.7370009589], 
processed observation next is [1.0, 0.21739130434782608, 0.2635802469135804, 0.745, 1.0, 1.0, 0.12956170500093855, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.12346569806942176, 0.12346569806942176, 0.21501295577107482], 
reward next is 0.7850, 
noisyNet noise sample is [array([1.1863023], dtype=float32), 1.1429428]. 
=============================================
[2019-04-27 20:11:15,094] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.0910281e-37 1.0000000e+00 8.3685571e-37 0.0000000e+00 4.5348743e-33], sum to 1.0000
[2019-04-27 20:11:15,102] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6597
[2019-04-27 20:11:15,106] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.88333333333333, 93.0, 1.0, 2.0, 0.3135464285808139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 399373.5204233818, 399373.5204233818, 117262.9377464251], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1224600.0000, 
sim time next is 1225200.0000, 
raw observation next is [17.86666666666667, 93.0, 1.0, 2.0, 0.3064748978975573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 390432.0411450056, 390432.0411450056, 116376.8982450002], 
processed observation next is [1.0, 0.17391304347826086, 0.2172839506172841, 0.93, 1.0, 1.0, 0.17437487844947294, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.13944001469464487, 0.13944001469464487, 0.22380172739423115], 
reward next is 0.7762, 
noisyNet noise sample is [array([-1.0241997], dtype=float32), 0.9410209]. 
=============================================
[2019-04-27 20:11:15,139] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.1070198e-36 1.0000000e+00 4.3659353e-35 0.0000000e+00 6.5081735e-29], sum to 1.0000
[2019-04-27 20:11:15,146] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2933
[2019-04-27 20:11:15,152] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.95, 93.0, 1.0, 2.0, 0.3099180747284179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 394472.2494697886, 394472.2494697886, 116805.671852361], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1222200.0000, 
sim time next is 1222800.0000, 
raw observation next is [17.93333333333333, 93.0, 1.0, 2.0, 0.3078846671314152, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 391956.305094546, 391956.305094546, 116551.5936283999], 
processed observation next is [1.0, 0.13043478260869565, 0.21975308641975297, 0.93, 1.0, 1.0, 0.1760531751564467, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.13998439467662357, 0.13998439467662357, 0.2241376800546152], 
reward next is 0.7759, 
noisyNet noise sample is [array([-0.7508207], dtype=float32), 0.72986907]. 
=============================================
[2019-04-27 20:11:17,951] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3970023e-28 4.8987873e-18 7.4310344e-22 8.2354629e-27 1.0000000e+00], sum to 1.0000
[2019-04-27 20:11:17,959] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1415
[2019-04-27 20:11:17,964] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.1, 57.0, 1.0, 2.0, 0.3051924362573332, 1.0, 2.0, 0.3051924362573332, 1.0, 2.0, 0.4914510378212282, 6.9112, 6.9112, 121.94756008, 1093223.63743999, 1093223.63743999, 257634.3791807188], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1261800.0000, 
sim time next is 1262400.0000, 
raw observation next is [26.13333333333333, 56.66666666666666, 1.0, 2.0, 0.331809044295815, 1.0, 2.0, 0.331809044295815, 1.0, 2.0, 0.5341553111966407, 6.911200000000001, 6.9112, 121.94756008, 1187928.567690651, 1187928.56769065, 268130.2041298677], 
processed observation next is [1.0, 0.6086956521739131, 0.5234567901234567, 0.5666666666666665, 1.0, 1.0, 0.20453457654263688, 1.0, 1.0, 0.20453457654263688, 1.0, 1.0, 0.4176941389958009, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.42426020274666104, 0.4242602027466607, 0.5156350079420533], 
reward next is 0.4844, 
noisyNet noise sample is [array([0.17675342], dtype=float32), -1.9350389]. 
=============================================
[2019-04-27 20:11:37,693] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:11:37,703] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2547
[2019-04-27 20:11:37,713] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.4, 56.0, 1.0, 2.0, 0.3252774380151826, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 412749.6168986285, 412749.616898628, 118743.0803801802], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1636800.0000, 
sim time next is 1637400.0000, 
raw observation next is [23.25, 56.5, 1.0, 2.0, 0.3242960005906851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 411743.4396051399, 411743.4396051399, 118619.3423924398], 
processed observation next is [1.0, 0.9565217391304348, 0.4166666666666667, 0.565, 1.0, 1.0, 0.19559047689367273, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1470512284304071, 0.1470512284304071, 0.22811411998546113], 
reward next is 0.7719, 
noisyNet noise sample is [array([0.6608301], dtype=float32), 1.6274056]. 
=============================================
[2019-04-27 20:11:38,994] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:11:39,002] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2330
[2019-04-27 20:11:39,007] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.56666666666667, 81.66666666666667, 1.0, 2.0, 0.7132590494095371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 904382.9550504226, 904382.9550504221, 181835.3865695551], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1678200.0000, 
sim time next is 1678800.0000, 
raw observation next is [19.63333333333333, 81.33333333333334, 1.0, 2.0, 0.7035146826874729, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 891606.9993186609, 891606.9993186609, 179917.9091897372], 
processed observation next is [1.0, 0.43478260869565216, 0.2827160493827159, 0.8133333333333335, 1.0, 1.0, 0.6470412889136582, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.31843107118523606, 0.31843107118523606, 0.3459959792110331], 
reward next is 0.6540, 
noisyNet noise sample is [array([0.43841147], dtype=float32), 1.262221]. 
=============================================
[2019-04-27 20:11:43,150] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.6036455e-33 1.0000000e+00 1.6842933e-32 7.8641447e-36 3.8152619e-28], sum to 1.0000
[2019-04-27 20:11:43,157] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2711
[2019-04-27 20:11:43,161] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.4, 72.0, 1.0, 2.0, 0.75216536025585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 925277.8708283833, 925277.8708283833, 189077.4284736365], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1760400.0000, 
sim time next is 1761000.0000, 
raw observation next is [23.5, 71.66666666666667, 1.0, 2.0, 0.9051495465248885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.996429629283855, 6.9112, 121.9257398520821, 1156200.529293845, 1112555.449741284, 222206.1806991973], 
processed observation next is [1.0, 0.391304347826087, 0.42592592592592593, 0.7166666666666667, 1.0, 1.0, 0.8870827934820101, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.008522962928385525, 0.0, 0.8094601187848187, 0.4129287604620875, 0.3973412320504585, 0.42731957826768713], 
reward next is 0.1465, 
noisyNet noise sample is [array([-1.4361781], dtype=float32), -0.57102257]. 
=============================================
[2019-04-27 20:11:43,174] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[59.336666]
 [59.40053 ]
 [59.60427 ]
 [60.007645]
 [60.841747]], R is [[58.26183319]
 [58.31560516]
 [58.36752701]
 [58.42238617]
 [58.47500229]].
[2019-04-27 20:11:44,687] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.2762704e-26 2.5544579e-14 1.5355830e-21 2.9768376e-24 1.0000000e+00], sum to 1.0000
[2019-04-27 20:11:44,692] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9404
[2019-04-27 20:11:44,699] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.16666666666666, 62.16666666666666, 1.0, 2.0, 0.4123000798311809, 1.0, 2.0, 0.4123000798311809, 1.0, 2.0, 0.6589411325456587, 6.911200000000001, 6.9112, 121.94756008, 1447710.455346944, 1447710.455346944, 302556.2223127773], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1779000.0000, 
sim time next is 1779600.0000, 
raw observation next is [26.53333333333333, 61.33333333333334, 1.0, 2.0, 0.4332597588152626, 1.0, 2.0, 0.4332597588152626, 1.0, 2.0, 0.6913502520941496, 6.9112, 6.9112, 121.94756008, 1511084.161490363, 1511084.161490363, 312054.2542503829], 
processed observation next is [1.0, 0.6086956521739131, 0.5382716049382715, 0.6133333333333334, 1.0, 1.0, 0.32530923668483636, 1.0, 1.0, 0.32530923668483636, 1.0, 1.0, 0.6141878151176869, 0.0, 0.0, 0.8096049824067558, 0.5396729148179868, 0.5396729148179868, 0.6001043350968902], 
reward next is 0.3999, 
noisyNet noise sample is [array([1.6945165], dtype=float32), -1.0009447]. 
=============================================
[2019-04-27 20:11:46,722] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.8548102e-25 1.0000000e+00 3.1802678e-26 7.2204919e-26 5.3876841e-09], sum to 1.0000
[2019-04-27 20:11:46,732] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6004
[2019-04-27 20:11:46,737] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.33333333333334, 91.33333333333334, 1.0, 2.0, 0.3179580101273181, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 403860.1874726859, 403860.1874726859, 117813.7629458997], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1819200.0000, 
sim time next is 1819800.0000, 
raw observation next is [18.3, 91.5, 1.0, 2.0, 0.3166508484279428, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 402263.3656473195, 402263.3656473195, 117648.6971493905], 
processed observation next is [1.0, 0.043478260869565216, 0.23333333333333336, 0.915, 1.0, 1.0, 0.18648910527136045, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.14366548773118554, 0.14366548773118554, 0.22624749451805864], 
reward next is 0.7738, 
noisyNet noise sample is [array([0.3460908], dtype=float32), -1.7987149]. 
=============================================
[2019-04-27 20:11:50,402] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2768885e-31 1.0000000e+00 3.0740424e-30 4.8905484e-35 2.7119936e-16], sum to 1.0000
[2019-04-27 20:11:50,411] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5431
[2019-04-27 20:11:50,421] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.1, 90.5, 1.0, 2.0, 0.4232319930000827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 519274.4318275136, 519274.4318275136, 131813.7474378971], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1884600.0000, 
sim time next is 1885200.0000, 
raw observation next is [21.06666666666667, 90.66666666666667, 1.0, 2.0, 0.4232797501619387, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 519433.8963052109, 519433.8963052104, 131823.342911245], 
processed observation next is [1.0, 0.8260869565217391, 0.3358024691358026, 0.9066666666666667, 1.0, 1.0, 0.313428274002308, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1855121058232896, 0.18551210582328945, 0.2535064286754712], 
reward next is 0.7465, 
noisyNet noise sample is [array([-1.8888454], dtype=float32), -1.0976664]. 
=============================================
[2019-04-27 20:11:50,570] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3390676e-29 1.0000000e+00 2.4213406e-26 1.4857388e-30 5.1250638e-13], sum to 1.0000
[2019-04-27 20:11:50,573] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9064
[2019-04-27 20:11:50,580] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.48333333333333, 92.0, 1.0, 2.0, 0.4054417108240293, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 500514.634425865, 500514.634425865, 129345.4220924473], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1900200.0000, 
sim time next is 1900800.0000, 
raw observation next is [20.4, 92.0, 1.0, 2.0, 0.4028843700229011, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 497956.1227372261, 497956.1227372256, 128997.8092753684], 
processed observation next is [1.0, 0.0, 0.31111111111111106, 0.92, 1.0, 1.0, 0.28914805955107276, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.17784147240615217, 0.177841472406152, 0.24807271014493923], 
reward next is 0.7519, 
noisyNet noise sample is [array([-1.6169866], dtype=float32), 2.527868]. 
=============================================
[2019-04-27 20:11:53,341] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-27 20:11:53,342] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 20:11:53,343] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 20:11:53,344] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:11:53,344] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 20:11:53,345] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:11:53,346] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:11:53,346] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 20:11:53,348] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 20:11:53,349] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:11:53,351] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:11:53,366] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run54
[2019-04-27 20:11:53,389] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run54
[2019-04-27 20:11:53,390] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run54
[2019-04-27 20:11:53,429] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run54
[2019-04-27 20:11:53,454] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run54
[2019-04-27 20:11:55,458] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05000161], dtype=float32), -0.03496562]
[2019-04-27 20:11:55,459] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [19.66666666666666, 48.0, 1.0, 2.0, 0.2910005445738659, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 375378.6085784049, 375378.6085784049, 94041.03081855836]
[2019-04-27 20:11:55,460] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:11:55,462] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8846558587028668
[2019-04-27 20:12:15,111] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05000161], dtype=float32), -0.03496562]
[2019-04-27 20:12:15,112] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [33.66666666666667, 28.66666666666667, 1.0, 2.0, 0.4241811962089002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 518932.2165107781, 518932.2165107781, 131910.0632163515]
[2019-04-27 20:12:15,113] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:12:15,116] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.0126295e-36], sampled 0.499411888710664
[2019-04-27 20:12:20,910] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05000161], dtype=float32), -0.03496562]
[2019-04-27 20:12:20,911] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [31.87338591666667, 46.49971899, 1.0, 2.0, 0.5318515828027844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 621671.2727838403, 621671.2727838403, 147480.7658161146]
[2019-04-27 20:12:20,911] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 20:12:20,914] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 7.525717e-37], sampled 0.83859817246509
[2019-04-27 20:12:31,038] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05000161], dtype=float32), -0.03496562]
[2019-04-27 20:12:31,040] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [23.86666666666667, 87.33333333333334, 1.0, 2.0, 0.6052898369516326, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9652733771845255, 6.9112, 6.9112, 121.9260426156314, 1402708.256910729, 1402708.256910729, 295564.8451945436]
[2019-04-27 20:12:31,040] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 20:12:31,042] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.3087171e-29 1.0000000e+00 1.1814437e-29 2.6226334e-32 1.7493936e-22], sampled 0.9787190270197771
[2019-04-27 20:12:31,046] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1402708.256910729 W.
[2019-04-27 20:12:51,576] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05000161], dtype=float32), -0.03496562]
[2019-04-27 20:12:51,578] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [21.86666666666667, 87.0, 1.0, 2.0, 0.4305360365704811, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 525918.1185334316, 525918.1185334316, 132811.7540055849]
[2019-04-27 20:12:51,578] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 20:12:51,581] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.5645563e-38], sampled 0.6209435726868054
[2019-04-27 20:12:54,694] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05000161], dtype=float32), -0.03496562]
[2019-04-27 20:12:54,694] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [26.16666666666667, 84.0, 1.0, 2.0, 0.7841631316494033, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 893780.068842706, 893780.068842706, 192684.5278145905]
[2019-04-27 20:12:54,695] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 20:12:54,696] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.0072634e-35], sampled 0.09371976345403521
[2019-04-27 20:13:19,156] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05000161], dtype=float32), -0.03496562]
[2019-04-27 20:13:19,157] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [24.9, 83.0, 1.0, 2.0, 0.5724344258834972, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 668607.05726827, 668607.05726827, 154162.1612150317]
[2019-04-27 20:13:19,159] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 20:13:19,161] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5705177e-36], sampled 0.6465751111922199
[2019-04-27 20:13:19,307] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05000161], dtype=float32), -0.03496562]
[2019-04-27 20:13:19,308] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [24.33333333333333, 87.66666666666667, 1.0, 2.0, 0.5754261203286125, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 671178.2816124414, 671178.2816124414, 154627.2458202479]
[2019-04-27 20:13:19,311] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 20:13:19,314] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7049608546390336
[2019-04-27 20:13:41,592] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8770.2140 2171299732.5194 484.0000
[2019-04-27 20:13:41,811] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8920.2914 2121058612.9305 425.0000
[2019-04-27 20:13:41,868] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8581.4447 2249771050.6041 531.0000
[2019-04-27 20:13:41,995] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8103.9552 2446661461.6099 703.0000
[2019-04-27 20:13:42,029] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8698.4862 2196768202.2129 560.0000
[2019-04-27 20:13:43,045] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1325000, evaluation results [1325000.0, 8103.955178752533, 2446661461.609882, 703.0, 8770.213987489069, 2171299732.5194006, 484.0, 8920.291352453662, 2121058612.9304936, 425.0, 8581.44471687146, 2249771050.6040816, 531.0, 8698.486223952745, 2196768202.2128577, 560.0]
[2019-04-27 20:13:43,293] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.5913115e-25 1.0000000e+00 2.2201034e-23 2.3719172e-27 2.5736017e-12], sum to 1.0000
[2019-04-27 20:13:43,301] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0211
[2019-04-27 20:13:43,309] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1468665.234481252 W.
[2019-04-27 20:13:43,319] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.3, 60.0, 1.0, 2.0, 0.4293530613393029, 1.0, 2.0, 0.4293530613393029, 1.0, 1.0, 0.6835440567051703, 6.911200000000001, 6.9112, 121.94756008, 1468665.234481252, 1468665.234481251, 310179.0178865554], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1958400.0000, 
sim time next is 1959000.0000, 
raw observation next is [28.4, 59.66666666666666, 1.0, 2.0, 0.6111397157673368, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9738772754395841, 6.911199999999999, 6.9112, 121.9260426156618, 1409269.198590219, 1409269.19859022, 297988.0606094419], 
processed observation next is [1.0, 0.6956521739130435, 0.6074074074074074, 0.5966666666666666, 1.0, 1.0, 0.5370710901992105, 0.0, 0.5, -0.1904761904761905, 1.0, 1.0, 0.9673465942994802, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.5033104280679354, 0.5033104280679357, 0.5730539627104652], 
reward next is 0.4269, 
noisyNet noise sample is [array([1.5321636], dtype=float32), 0.73531413]. 
=============================================
[2019-04-27 20:13:43,340] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[67.79161 ]
 [67.9748  ]
 [68.144485]
 [68.47334 ]
 [68.4837  ]], R is [[67.98326874]
 [67.30343628]
 [67.06342316]
 [66.39279175]
 [66.11798859]].
[2019-04-27 20:13:43,491] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:13:43,499] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8942
[2019-04-27 20:13:43,504] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 63.0, 1.0, 2.0, 0.3181550113512155, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 404564.438648834, 404564.438648834, 117841.6251642741], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2422800.0000, 
sim time next is 2423400.0000, 
raw observation next is [21.85, 62.83333333333333, 1.0, 2.0, 0.3127259019829188, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 398282.6655142977, 398282.6655142977, 117159.28415824], 
processed observation next is [1.0, 0.043478260869565216, 0.36481481481481487, 0.6283333333333333, 1.0, 1.0, 0.1818165499796652, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.14224380911224918, 0.14224380911224918, 0.22530631568892306], 
reward next is 0.7747, 
noisyNet noise sample is [array([1.0647515], dtype=float32), 1.1031305]. 
=============================================
[2019-04-27 20:13:49,147] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.9742074e-38], sum to 1.0000
[2019-04-27 20:13:49,160] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2307
[2019-04-27 20:13:49,166] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 65.0, 1.0, 2.0, 0.5173563695269268, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 611302.7931325737, 611302.7931325737, 145427.3277859599], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2034000.0000, 
sim time next is 2034600.0000, 
raw observation next is [27.35, 64.66666666666667, 1.0, 2.0, 0.5211466575179315, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 614769.9499536863, 614769.9499536863, 145993.2910345082], 
processed observation next is [0.0, 0.5652173913043478, 0.5685185185185185, 0.6466666666666667, 1.0, 1.0, 0.4299364970451566, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21956069641203083, 0.21956069641203083, 0.2807563289125158], 
reward next is 0.7192, 
noisyNet noise sample is [array([1.085993], dtype=float32), -0.19210084]. 
=============================================
[2019-04-27 20:13:49,195] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:13:49,204] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7156
[2019-04-27 20:13:49,208] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.91666666666667, 86.5, 1.0, 2.0, 0.4352894704547075, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 531438.348879268, 531438.348879268, 133498.6413928096], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2083800.0000, 
sim time next is 2084400.0000, 
raw observation next is [21.8, 87.0, 1.0, 2.0, 0.4329682321402208, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 529030.5022538377, 529030.5022538372, 133170.7910171292], 
processed observation next is [0.0, 0.13043478260869565, 0.362962962962963, 0.87, 1.0, 1.0, 0.3249621811193105, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.18893946509065632, 0.18893946509065615, 0.2560976750329408], 
reward next is 0.7439, 
noisyNet noise sample is [array([-2.0344892], dtype=float32), 1.581532]. 
=============================================
[2019-04-27 20:13:49,702] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8682526e-38], sum to 1.0000
[2019-04-27 20:13:49,710] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1310
[2019-04-27 20:13:49,715] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.61666666666667, 94.5, 1.0, 2.0, 0.4207809699237333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 516439.5341104118, 516439.5341104113, 131464.2659939436], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2092200.0000, 
sim time next is 2092800.0000, 
raw observation next is [20.73333333333333, 94.0, 1.0, 2.0, 0.4223334168373572, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 517898.7626031915, 517898.7626031915, 131676.5697465199], 
processed observation next is [0.0, 0.21739130434782608, 0.3234567901234567, 0.94, 1.0, 1.0, 0.31230168671113956, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1849638437868541, 0.1849638437868541, 0.25322417258946134], 
reward next is 0.7468, 
noisyNet noise sample is [array([-0.46612424], dtype=float32), -0.096115775]. 
=============================================
[2019-04-27 20:13:54,487] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:13:54,496] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4404
[2019-04-27 20:13:54,504] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.6, 97.0, 1.0, 2.0, 0.4444046744616402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 542857.3420342486, 542857.3420342486, 134851.279304727], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2610000.0000, 
sim time next is 2610600.0000, 
raw observation next is [20.55, 97.5, 1.0, 2.0, 0.443762536750348, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 542098.0774568812, 542098.0774568812, 134756.8455089256], 
processed observation next is [0.0, 0.21739130434782608, 0.3166666666666667, 0.975, 1.0, 1.0, 0.3378125437504143, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.19360645623460043, 0.19360645623460043, 0.25914777982485687], 
reward next is 0.7409, 
noisyNet noise sample is [array([-0.8309592], dtype=float32), -0.8422202]. 
=============================================
[2019-04-27 20:14:01,298] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.8319146e-27 1.0000000e+00 1.2367475e-25 3.5993136e-29 4.8110471e-16], sum to 1.0000
[2019-04-27 20:14:01,307] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3541
[2019-04-27 20:14:01,315] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1730995.579551304 W.
[2019-04-27 20:14:01,325] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.46666666666667, 32.33333333333334, 1.0, 2.0, 0.4997503956312262, 1.0, 2.0, 0.4997503956312262, 1.0, 2.0, 0.7964999093154206, 6.9112, 6.9112, 121.94756008, 1730995.579551304, 1730995.579551304, 343576.4378471938], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2562000.0000, 
sim time next is 2562600.0000, 
raw observation next is [33.43333333333333, 32.66666666666666, 1.0, 2.0, 0.4994469381157791, 1.0, 2.0, 0.4994469381157791, 1.0, 2.0, 0.7959326758792586, 6.911200000000001, 6.9112, 121.94756008, 1728606.088707082, 1728606.088707082, 343423.9250473371], 
processed observation next is [1.0, 0.6521739130434783, 0.7938271604938271, 0.32666666666666655, 1.0, 1.0, 0.40410349775687987, 1.0, 1.0, 0.40410349775687987, 1.0, 1.0, 0.7449158448490734, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.6173593173953864, 0.6173593173953864, 0.6604306250910329], 
reward next is 0.3396, 
noisyNet noise sample is [array([2.7538514], dtype=float32), 0.23402975]. 
=============================================
[2019-04-27 20:14:05,827] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.7870275e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.2035335e-37], sum to 1.0000
[2019-04-27 20:14:05,837] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7074
[2019-04-27 20:14:05,840] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 93.66666666666667, 1.0, 2.0, 0.4812818089183559, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 578371.9605671011, 578371.9605671011, 140139.8353476838], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2337600.0000, 
sim time next is 2338200.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.4829733628801072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 579985.4650784361, 579985.4650784356, 140386.3762359109], 
processed observation next is [1.0, 0.043478260869565216, 0.37037037037037035, 0.94, 1.0, 1.0, 0.38449209866679435, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.20713766609944145, 0.2071376660994413, 0.2699738004536748], 
reward next is 0.7300, 
noisyNet noise sample is [array([0.5812465], dtype=float32), 1.487028]. 
=============================================
[2019-04-27 20:14:07,694] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5042016e-23 6.7220324e-05 3.5486055e-20 5.4189601e-24 9.9993277e-01], sum to 1.0000
[2019-04-27 20:14:07,701] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4275
[2019-04-27 20:14:07,708] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.1, 37.0, 1.0, 2.0, 0.3870168816974746, 1.0, 2.0, 0.3870168816974746, 1.0, 2.0, 0.6203744888714483, 6.911200000000001, 6.9112, 121.94756008, 1371825.416133635, 1371825.416133635, 291343.9127209848], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2383200.0000, 
sim time next is 2383800.0000, 
raw observation next is [31.08333333333334, 37.0, 1.0, 2.0, 0.3928410764631339, 1.0, 2.0, 0.3928410764631339, 1.0, 2.0, 0.6295050790378344, 6.9112, 6.9112, 121.94756008, 1391222.285116647, 1391222.285116647, 293883.5734116058], 
processed observation next is [1.0, 0.6086956521739131, 0.7067901234567904, 0.37, 1.0, 1.0, 0.277191757694207, 1.0, 1.0, 0.277191757694207, 1.0, 1.0, 0.5368813487972929, 0.0, 0.0, 0.8096049824067558, 0.49686510182737387, 0.49686510182737387, 0.5651607180992418], 
reward next is 0.4348, 
noisyNet noise sample is [array([-0.792051], dtype=float32), 0.392004]. 
=============================================
[2019-04-27 20:14:13,263] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.9156431e-27 2.8732459e-16 3.5599316e-22 5.9424576e-24 1.0000000e+00], sum to 1.0000
[2019-04-27 20:14:13,271] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0406
[2019-04-27 20:14:13,274] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.88333333333333, 23.0, 1.0, 2.0, 0.4503445611961842, 1.0, 2.0, 0.4503445611961842, 1.0, 2.0, 0.7233170098397002, 6.911200000000001, 6.9112, 121.94756008, 1604604.699917599, 1604604.699917598, 319726.0169885136], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2469000.0000, 
sim time next is 2469600.0000, 
raw observation next is [35.0, 23.0, 1.0, 2.0, 0.4612339262342223, 1.0, 2.0, 0.4612339262342223, 1.0, 2.0, 0.740232167495298, 6.911199999999999, 6.9112, 121.94756008, 1640336.735818885, 1640336.735818885, 324866.191114464], 
processed observation next is [1.0, 0.6086956521739131, 0.8518518518518519, 0.23, 1.0, 1.0, 0.35861181694550276, 1.0, 1.0, 0.35861181694550276, 1.0, 1.0, 0.6752902093691223, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.5858345485067447, 0.5858345485067447, 0.6247426752201232], 
reward next is 0.3753, 
noisyNet noise sample is [array([-0.8934126], dtype=float32), -1.0192789]. 
=============================================
[2019-04-27 20:14:17,198] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.5199431e-33 1.0000000e+00 1.6052436e-35 0.0000000e+00 8.6685256e-37], sum to 1.0000
[2019-04-27 20:14:17,205] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8336
[2019-04-27 20:14:17,209] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.83333333333334, 35.66666666666667, 1.0, 2.0, 0.4653943583719419, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 555622.0081435917, 555622.0081435922, 137581.5119083676], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2568000.0000, 
sim time next is 2568600.0000, 
raw observation next is [32.65000000000001, 36.5, 1.0, 2.0, 0.4578284709304449, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 549731.7093723896, 549731.7093723896, 136556.9274801212], 
processed observation next is [1.0, 0.7391304347826086, 0.7648148148148153, 0.365, 1.0, 1.0, 0.3545577034886248, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.196332753347282, 0.196332753347282, 0.26260947592331], 
reward next is 0.7374, 
noisyNet noise sample is [array([-1.0121411], dtype=float32), 0.008722181]. 
=============================================
[2019-04-27 20:14:23,581] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:14:23,591] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6741
[2019-04-27 20:14:23,595] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 66.0, 1.0, 2.0, 0.5359136368463536, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 633120.1164270171, 633120.1164270171, 148416.4549151537], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2710800.0000, 
sim time next is 2711400.0000, 
raw observation next is [27.16666666666666, 65.33333333333333, 1.0, 2.0, 0.5481179665186613, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 647153.9873307901, 647153.9873307906, 150402.6789678746], 
processed observation next is [0.0, 0.391304347826087, 0.5617283950617282, 0.6533333333333333, 1.0, 1.0, 0.46204519823650153, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.23112642404671077, 0.23112642404671094, 0.2892359210920666], 
reward next is 0.7108, 
noisyNet noise sample is [array([-0.02454758], dtype=float32), -1.0406592]. 
=============================================
[2019-04-27 20:14:23,625] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:14:23,632] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0676
[2019-04-27 20:14:23,636] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.8, 90.0, 1.0, 2.0, 0.5575438111664973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 653162.2089117036, 653162.2089117036, 151753.4052119445], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2674800.0000, 
sim time next is 2675400.0000, 
raw observation next is [23.66666666666667, 91.66666666666666, 1.0, 2.0, 0.5607624033764662, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 656164.1401354364, 656164.1401354364, 152256.6596261633], 
processed observation next is [0.0, 1.0, 0.43209876543209896, 0.9166666666666665, 1.0, 1.0, 0.47709809925769786, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.23434433576265587, 0.23434433576265587, 0.2928012685118525], 
reward next is 0.7072, 
noisyNet noise sample is [array([0.08480825], dtype=float32), 1.136603]. 
=============================================
[2019-04-27 20:14:33,783] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-27 20:14:33,784] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 20:14:33,785] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:14:33,785] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 20:14:33,785] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 20:14:33,786] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 20:14:33,787] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:14:33,787] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 20:14:33,787] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:14:33,788] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:14:33,789] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:14:33,810] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run55
[2019-04-27 20:14:33,832] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run55
[2019-04-27 20:14:33,833] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run55
[2019-04-27 20:14:33,877] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run55
[2019-04-27 20:14:33,904] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run55
[2019-04-27 20:14:36,499] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05083584], dtype=float32), -0.038079966]
[2019-04-27 20:14:36,501] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [37.4, 12.0, 1.0, 2.0, 0.8392199413350254, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9563629096235545, 6.911200000000001, 6.9112, 121.9260426156618, 1755157.780786652, 1755157.780786652, 329990.755038472]
[2019-04-27 20:14:36,502] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 20:14:36,505] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.3906251e-31 1.0000000e+00 3.0724888e-34 1.1367471e-36 7.9435171e-32], sampled 0.8667588426225713
[2019-04-27 20:14:36,506] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1755157.780786652 W.
[2019-04-27 20:14:47,018] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05083584], dtype=float32), -0.038079966]
[2019-04-27 20:14:47,020] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [25.8, 25.0, 1.0, 2.0, 0.4666300135655314, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 602022.2607595033, 602022.2607595033, 126437.3127050568]
[2019-04-27 20:14:47,021] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 20:14:47,024] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.22269796859688884
[2019-04-27 20:14:57,982] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05083584], dtype=float32), -0.038079966]
[2019-04-27 20:14:57,982] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [20.5334635, 78.194706935, 1.0, 2.0, 0.3730771993876199, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 471109.7863009957, 471109.7863009957, 125044.5512194526]
[2019-04-27 20:14:57,984] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 20:14:57,987] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7274054582454824
[2019-04-27 20:14:58,909] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05083584], dtype=float32), -0.038079966]
[2019-04-27 20:14:58,910] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [19.13333333333333, 95.33333333333334, 1.0, 2.0, 0.3662948814625788, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 458549.8180645819, 458549.8180645819, 124065.190644827]
[2019-04-27 20:14:58,911] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:14:58,913] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.22606946695403007
[2019-04-27 20:15:07,409] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05083584], dtype=float32), -0.038079966]
[2019-04-27 20:15:07,411] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [34.0, 50.0, 1.0, 2.0, 0.6929138863266268, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 789721.6520362148, 789721.6520362153, 174809.8367955195]
[2019-04-27 20:15:07,415] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:15:07,417] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8037204539078989
[2019-04-27 20:15:25,536] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05083584], dtype=float32), -0.038079966]
[2019-04-27 20:15:25,537] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [33.35, 57.0, 1.0, 2.0, 0.7479797582073309, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 852515.7406982417, 852515.7406982417, 185432.9965023214]
[2019-04-27 20:15:25,539] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:15:25,542] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8246882634807712
[2019-04-27 20:15:37,054] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05083584], dtype=float32), -0.038079966]
[2019-04-27 20:15:37,056] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [26.23754507666667, 78.09315442833334, 1.0, 2.0, 0.5965057561141581, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 690268.9933965869, 690268.9933965873, 157982.937762678]
[2019-04-27 20:15:37,057] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 20:15:37,059] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.21312313243711833
[2019-04-27 20:15:45,335] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05083584], dtype=float32), -0.038079966]
[2019-04-27 20:15:45,337] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [19.79088236, 87.87589358, 1.0, 2.0, 0.4318105582055695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 539941.024845847, 539941.024845847, 133288.9543286842]
[2019-04-27 20:15:45,337] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 20:15:45,342] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.009671366151059013
[2019-04-27 20:15:57,539] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05083584], dtype=float32), -0.038079966]
[2019-04-27 20:15:57,543] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [30.66666666666666, 53.0, 1.0, 2.0, 0.6879131535252675, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.926042528566, 1499005.841062638, 1499005.841062638, 315046.2905905363]
[2019-04-27 20:15:57,544] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 20:15:57,549] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [6.7223148e-32 1.0000000e+00 3.2353558e-34 1.2243062e-36 1.7678677e-31], sampled 0.5466463885292124
[2019-04-27 20:15:57,552] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1499005.841062638 W.
[2019-04-27 20:16:21,890] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8766.9240 2170981565.8343 493.0000
[2019-04-27 20:16:22,065] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8578.9294 2249090996.1329 553.0000
[2019-04-27 20:16:22,337] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8921.0960 2120856567.2892 431.0000
[2019-04-27 20:16:22,414] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8695.1756 2196179891.6480 571.0000
[2019-04-27 20:16:22,429] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8095.3398 2445740619.5097 746.0000
[2019-04-27 20:16:23,446] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 1350000, evaluation results [1350000.0, 8095.339826117275, 2445740619.509737, 746.0, 8766.923954169622, 2170981565.8342915, 493.0, 8921.096020705756, 2120856567.2892263, 431.0, 8578.929448185872, 2249090996.132882, 553.0, 8695.175590657107, 2196179891.648001, 571.0]
[2019-04-27 20:16:29,854] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.8527942e-31 1.0000000e+00 1.4068549e-33 6.6778006e-37 1.2041338e-30], sum to 1.0000
[2019-04-27 20:16:29,863] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7567
[2019-04-27 20:16:29,871] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.28333333333333, 97.66666666666667, 1.0, 2.0, 0.6701776278556356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 763795.9750989944, 763795.975098994, 170577.4033886538], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3001800.0000, 
sim time next is 3002400.0000, 
raw observation next is [25.0, 100.0, 1.0, 2.0, 0.6803593641937274, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 775405.8863025384, 775405.8863025384, 172461.0625455337], 
processed observation next is [1.0, 0.782608695652174, 0.48148148148148145, 1.0, 1.0, 1.0, 0.6194754335639612, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.276930673679478, 0.276930673679478, 0.33165588951064173], 
reward next is 0.6683, 
noisyNet noise sample is [array([-0.78790253], dtype=float32), 0.12243496]. 
=============================================
[2019-04-27 20:16:31,112] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.5541442e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0129436e-34], sum to 1.0000
[2019-04-27 20:16:31,122] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3993
[2019-04-27 20:16:31,133] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.66666666666667, 96.0, 1.0, 2.0, 0.6696604098340553, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 763206.2124771667, 763206.2124771667, 170478.0468787134], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3014400.0000, 
sim time next is 3015000.0000, 
raw observation next is [24.5, 97.0, 1.0, 2.0, 0.6675610301110462, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 760812.3800345018, 760812.3800345018, 170091.8973812267], 
processed observation next is [1.0, 0.9130434782608695, 0.46296296296296297, 0.97, 1.0, 1.0, 0.6042393215607692, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2717187071551792, 0.2717187071551792, 0.3270998026562052], 
reward next is 0.6729, 
noisyNet noise sample is [array([-0.7130344], dtype=float32), 1.4472122]. 
=============================================
[2019-04-27 20:16:31,154] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[71.40108 ]
 [71.260155]
 [71.177185]
 [70.98482 ]
 [70.962006]], R is [[71.35734558]
 [71.31593323]
 [71.27489471]
 [71.23416901]
 [71.19372559]].
[2019-04-27 20:16:37,506] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.6455201e-26 1.0000000e+00 3.3659535e-29 2.7739988e-31 2.8640369e-20], sum to 1.0000
[2019-04-27 20:16:37,514] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0423
[2019-04-27 20:16:37,517] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.86666666666667, 54.66666666666666, 1.0, 2.0, 0.7312190424790237, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 869284.599869317, 869284.599869317, 183812.4506216194], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3138000.0000, 
sim time next is 3138600.0000, 
raw observation next is [29.33333333333334, 53.83333333333334, 1.0, 2.0, 0.7481737970442174, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 885268.9045452969, 885268.9045452964, 187013.6533374556], 
processed observation next is [1.0, 0.30434782608695654, 0.6419753086419755, 0.5383333333333334, 1.0, 1.0, 0.7002069012431159, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.3161674659090346, 0.31616746590903444, 0.35964164103356844], 
reward next is 0.6404, 
noisyNet noise sample is [array([-2.0696507], dtype=float32), -0.9693853]. 
=============================================
[2019-04-27 20:16:37,863] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4535947e-17 1.3337133e-05 9.3478196e-16 3.8978157e-19 9.9998665e-01], sum to 1.0000
[2019-04-27 20:16:37,871] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0224
[2019-04-27 20:16:37,876] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.6, 32.0, 1.0, 2.0, 0.5902481232500856, 1.0, 2.0, 0.5902481232500856, 1.0, 2.0, 0.9396942352534803, 6.911199999999999, 6.9112, 121.94756008, 2019673.84839805, 2019673.84839805, 390087.8738328847], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3164400.0000, 
sim time next is 3165000.0000, 
raw observation next is [34.66666666666667, 32.0, 1.0, 2.0, 0.5560276354388543, 1.0, 2.0, 0.5560276354388543, 1.0, 2.0, 0.8852141041745181, 6.9112, 6.9112, 121.94756008, 1902455.641451224, 1902455.641451224, 371938.2533461791], 
processed observation next is [1.0, 0.6521739130434783, 0.8395061728395063, 0.32, 1.0, 1.0, 0.47146147076054074, 1.0, 1.0, 0.47146147076054074, 1.0, 1.0, 0.8565176302181476, 0.0, 0.0, 0.8096049824067558, 0.6794484433754371, 0.6794484433754371, 0.7152658718195751], 
reward next is 0.2847, 
noisyNet noise sample is [array([-1.0767269], dtype=float32), -0.14924037]. 
=============================================
[2019-04-27 20:16:37,892] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[49.14733 ]
 [49.146965]
 [49.230354]
 [49.626514]
 [49.85676 ]], R is [[49.03679276]
 [48.79625702]
 [48.61426544]
 [48.42897034]
 [48.22756195]].
[2019-04-27 20:16:50,062] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.9507187e-33 1.0000000e+00 1.6302613e-35 2.1633618e-38 1.8611649e-26], sum to 1.0000
[2019-04-27 20:16:50,069] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3139
[2019-04-27 20:16:50,073] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.06666666666666, 90.66666666666667, 1.0, 2.0, 0.6445027952951419, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 734520.5581464361, 734520.5581464361, 165901.7671642661], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3368400.0000, 
sim time next is 3369000.0000, 
raw observation next is [25.03333333333333, 92.33333333333334, 1.0, 2.0, 0.6555784458812256, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 747149.2974928991, 747149.2974928991, 167903.2377241526], 
processed observation next is [0.0, 1.0, 0.482716049382716, 0.9233333333333335, 1.0, 1.0, 0.5899743403347923, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.26683903481889254, 0.26683903481889254, 0.32289084177721655], 
reward next is 0.6771, 
noisyNet noise sample is [array([-1.26593], dtype=float32), -0.96126765]. 
=============================================
[2019-04-27 20:16:50,088] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[68.075645]
 [68.10074 ]
 [68.107796]
 [68.08658 ]
 [68.04282 ]], R is [[68.04045105]
 [68.041008  ]
 [68.04660034]
 [68.05609131]
 [68.06796265]].
[2019-04-27 20:16:50,282] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.9916714e-33 1.0000000e+00 1.5452273e-34 1.6633071e-38 5.9272833e-26], sum to 1.0000
[2019-04-27 20:16:50,292] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7479
[2019-04-27 20:16:50,299] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 99.33333333333334, 1.0, 2.0, 0.5972612417000993, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 708399.9173478782, 708399.9173478782, 158855.1147576839], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3652800.0000, 
sim time next is 3653400.0000, 
raw observation next is [22.0, 99.16666666666666, 1.0, 2.0, 0.6009817423245589, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 713060.6185037747, 713060.6185037743, 159513.225833063], 
processed observation next is [1.0, 0.2608695652173913, 0.37037037037037035, 0.9916666666666666, 1.0, 1.0, 0.524978264672094, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.254664506608491, 0.2546645066084908, 0.3067562035251212], 
reward next is 0.6932, 
noisyNet noise sample is [array([-0.992462], dtype=float32), -0.91504276]. 
=============================================
[2019-04-27 20:16:50,561] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.7084918e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.9429949e-29], sum to 1.0000
[2019-04-27 20:16:50,567] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7046
[2019-04-27 20:16:50,573] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.76666666666667, 93.66666666666667, 1.0, 2.0, 0.6582931482492995, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 750244.7021354008, 750244.7021354008, 168396.395099201], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3370800.0000, 
sim time next is 3371400.0000, 
raw observation next is [24.65, 93.5, 1.0, 2.0, 0.6529771218320314, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 744183.1827555321, 744183.1827555316, 167430.3544525626], 
processed observation next is [1.0, 0.0, 0.46851851851851845, 0.935, 1.0, 1.0, 0.5868775259905136, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.26577970812697577, 0.2657797081269756, 0.3219814508703127], 
reward next is 0.6780, 
noisyNet noise sample is [array([-0.5083746], dtype=float32), 0.65614194]. 
=============================================
[2019-04-27 20:17:04,976] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.6743548e-20 1.0000000e+00 9.6825095e-22 2.5048967e-22 1.3578352e-10], sum to 1.0000
[2019-04-27 20:17:04,982] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2873
[2019-04-27 20:17:04,986] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.06666666666667, 98.33333333333333, 1.0, 2.0, 0.634188705885162, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 738312.1080492209, 738312.1080492209, 164804.2335012389], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3638400.0000, 
sim time next is 3639000.0000, 
raw observation next is [23.03333333333333, 99.16666666666667, 1.0, 2.0, 0.6303125583935705, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 732785.4333435268, 732785.4333435268, 164064.5705338401], 
processed observation next is [1.0, 0.08695652173913043, 0.4086419753086419, 0.9916666666666667, 1.0, 1.0, 0.5598959028494886, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.26170908333697385, 0.26170908333697385, 0.31550878948815403], 
reward next is 0.6845, 
noisyNet noise sample is [array([-0.04843401], dtype=float32), -0.066378996]. 
=============================================
[2019-04-27 20:17:05,003] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[55.98112 ]
 [56.179035]
 [56.23619 ]
 [56.03799 ]
 [56.889286]], R is [[55.6478653 ]
 [55.77445602]
 [55.89060593]
 [55.97762299]
 [55.9894104 ]].
[2019-04-27 20:17:06,239] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.7204968e-26 3.5663313e-12 2.2630856e-25 2.5203794e-27 1.0000000e+00], sum to 1.0000
[2019-04-27 20:17:06,250] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9096
[2019-04-27 20:17:06,255] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.93333333333333, 85.66666666666667, 1.0, 2.0, 0.2205568109749073, 1.0, 2.0, 0.2205568109749073, 1.0, 2.0, 0.3511336261059121, 6.9112, 6.9112, 121.94756008, 754095.5597744619, 754095.5597744619, 227427.3886154045], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3691200.0000, 
sim time next is 3691800.0000, 
raw observation next is [26.95, 86.5, 1.0, 2.0, 0.2220324561017189, 1.0, 2.0, 0.2220324561017189, 1.0, 2.0, 0.3534829012062029, 6.9112, 6.9112, 121.94756008, 759143.3674948912, 759143.3674948912, 227926.9620936222], 
processed observation next is [1.0, 0.7391304347826086, 0.5537037037037037, 0.865, 1.0, 1.0, 0.07384816202585583, 1.0, 1.0, 0.07384816202585583, 1.0, 1.0, 0.19185362650775362, 0.0, 0.0, 0.8096049824067558, 0.2711226312481754, 0.2711226312481754, 0.43832108094927347], 
reward next is 0.5617, 
noisyNet noise sample is [array([-1.1240437], dtype=float32), 1.5413518]. 
=============================================
[2019-04-27 20:17:06,937] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.8290424e-33 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-27 20:17:06,943] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9689
[2019-04-27 20:17:06,946] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.5359463944252911, 1.0, 2.0, 0.5359463944252911, 1.0, 2.0, 0.8532441144805639, 6.9112, 6.9112, 121.94756008, 1833676.902025449, 1833676.902025449, 361582.105257318], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3684600.0000, 
sim time next is 3685200.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5697603932136073, 1.0, 2.0, 0.5697603932136073, 1.0, 2.0, 0.9070771017966218, 6.911199999999999, 6.9112, 121.94756008, 1949493.720404522, 1949493.720404522, 379145.8395363434], 
processed observation next is [1.0, 0.6521739130434783, 0.5555555555555556, 0.89, 1.0, 1.0, 0.48780999192096103, 1.0, 1.0, 0.48780999192096103, 1.0, 1.0, 0.8838463772457772, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.6962477572873293, 0.6962477572873293, 0.7291266144929681], 
reward next is 0.2709, 
noisyNet noise sample is [array([0.08771589], dtype=float32), -1.5931917]. 
=============================================
[2019-04-27 20:17:07,076] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.8197904e-31 3.9675173e-28 8.2728966e-28 9.5708344e-30 1.0000000e+00], sum to 1.0000
[2019-04-27 20:17:07,082] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3051
[2019-04-27 20:17:07,087] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.3552852479567959, 1.0, 2.0, 0.3552852479567959, 1.0, 2.0, 0.5657901941567914, 6.9112, 6.9112, 121.94756008, 1220996.861708065, 1220996.861708065, 278082.1508490741], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3664200.0000, 
sim time next is 3664800.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.3837686907643021, 1.0, 2.0, 0.3837686907643021, 1.0, 2.0, 0.611110354375117, 6.911199999999999, 6.9112, 121.94756008, 1317833.095096036, 1317833.095096036, 290077.6161382088], 
processed observation next is [1.0, 0.43478260869565216, 0.37037037037037035, 1.0, 1.0, 1.0, 0.26639129852893106, 1.0, 1.0, 0.26639129852893106, 1.0, 1.0, 0.5138879429688962, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.4706546768200129, 0.4706546768200129, 0.5578415694965554], 
reward next is 0.4422, 
noisyNet noise sample is [array([0.46860254], dtype=float32), 0.8001311]. 
=============================================
[2019-04-27 20:17:08,694] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.9430156e-21 4.8396482e-06 1.8135887e-18 4.9286108e-23 9.9999511e-01], sum to 1.0000
[2019-04-27 20:17:08,707] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3968
[2019-04-27 20:17:08,712] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.03333333333333, 94.0, 1.0, 2.0, 0.2240728406377454, 1.0, 2.0, 0.2240728406377454, 1.0, 2.0, 0.3567312598382422, 6.9112, 6.9112, 121.94756008, 766123.0602525186, 766123.0602525186, 228619.7319977943], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3709200.0000, 
sim time next is 3709800.0000, 
raw observation next is [25.05, 94.0, 1.0, 2.0, 0.2201924015777285, 1.0, 2.0, 0.2201924015777285, 1.0, 2.0, 0.350553474477618, 6.9112, 6.9112, 121.94756008, 752849.0127666801, 752849.0127666801, 227304.2068697598], 
processed observation next is [1.0, 0.9565217391304348, 0.48333333333333334, 0.94, 1.0, 1.0, 0.07165762092586728, 1.0, 1.0, 0.07165762092586728, 1.0, 1.0, 0.18819184309702244, 0.0, 0.0, 0.8096049824067558, 0.26887464741667144, 0.26887464741667144, 0.43712347474953805], 
reward next is 0.5629, 
noisyNet noise sample is [array([-1.7627122], dtype=float32), -0.016463485]. 
=============================================
[2019-04-27 20:17:14,314] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-27 20:17:14,315] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 20:17:14,316] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 20:17:14,316] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 20:17:14,317] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:17:14,317] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:17:14,318] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:17:14,318] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 20:17:14,317] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 20:17:14,321] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:17:14,323] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:17:14,343] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run56
[2019-04-27 20:17:14,365] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run56
[2019-04-27 20:17:14,389] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run56
[2019-04-27 20:17:14,411] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run56
[2019-04-27 20:17:14,412] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run56
[2019-04-27 20:17:21,628] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05594495], dtype=float32), -0.035834122]
[2019-04-27 20:17:21,630] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [17.0, 83.0, 1.0, 2.0, 0.2428178613974992, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 313212.2581220088, 313212.2581220088, 99091.06018854245]
[2019-04-27 20:17:21,631] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:17:21,633] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [3.8184224e-35 1.0000000e+00 2.3932900e-38 0.0000000e+00 6.0243274e-32], sampled 0.5996380598916101
[2019-04-27 20:17:23,361] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05594495], dtype=float32), -0.035834122]
[2019-04-27 20:17:23,363] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [31.66666666666667, 26.0, 1.0, 2.0, 0.5244116379159863, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 660243.098029369, 660243.098029369, 147717.2964305366]
[2019-04-27 20:17:23,363] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 20:17:23,365] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [8.8652471e-33 1.0000000e+00 8.5422842e-35 5.4096632e-37 1.6854425e-26], sampled 0.7654425416923379
[2019-04-27 20:17:29,066] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05594495], dtype=float32), -0.035834122]
[2019-04-27 20:17:29,066] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [21.33333333333333, 68.16666666666666, 1.0, 2.0, 0.3162314899107179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 401414.1982518281, 401414.1982518281, 117593.1862039446]
[2019-04-27 20:17:29,068] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 20:17:29,070] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [2.2570994e-35 1.0000000e+00 3.1287822e-38 0.0000000e+00 1.4690121e-30], sampled 0.7986282167719088
[2019-04-27 20:17:32,969] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05594495], dtype=float32), -0.035834122]
[2019-04-27 20:17:32,970] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [21.384620515, 44.46597586666667, 1.0, 2.0, 0.2495401894421546, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 321885.2501557446, 321885.2501557446, 91433.02336771485]
[2019-04-27 20:17:32,975] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 20:17:32,977] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1904286e-36], sampled 0.15155900848721338
[2019-04-27 20:17:37,916] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05594495], dtype=float32), -0.035834122]
[2019-04-27 20:17:37,917] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [18.43333333333333, 90.66666666666667, 1.0, 2.0, 0.3180068731282135, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 403843.0735344013, 403843.0735344008, 117819.4055123867]
[2019-04-27 20:17:37,918] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 20:17:37,923] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [3.7960981e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.4460657e-34], sampled 0.41341368657066746
[2019-04-27 20:17:44,023] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05594495], dtype=float32), -0.035834122]
[2019-04-27 20:17:44,024] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [29.0, 55.0, 1.0, 2.0, 0.5240267213695795, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 619587.6387660096, 619587.6387660096, 146512.1000069114]
[2019-04-27 20:17:44,024] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 20:17:44,027] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [8.582721e-35 1.000000e+00 8.470304e-37 0.000000e+00 9.519290e-27], sampled 0.6426072676291632
[2019-04-27 20:17:46,308] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05594495], dtype=float32), -0.035834122]
[2019-04-27 20:17:46,308] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [26.48804302, 37.55773957, 1.0, 2.0, 0.3092908199284803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 395038.236442878, 395038.236442878, 116731.9384799081]
[2019-04-27 20:17:46,310] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 20:17:46,311] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [3.5793120e-35 1.0000000e+00 3.3793826e-38 0.0000000e+00 7.7932976e-31], sampled 0.8151264683753093
[2019-04-27 20:17:46,506] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05594495], dtype=float32), -0.035834122]
[2019-04-27 20:17:46,507] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [26.4, 51.5, 1.0, 2.0, 0.4223403011895947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 524059.2419611163, 524059.2419611163, 131827.8745422124]
[2019-04-27 20:17:46,508] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 20:17:46,511] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [6.3202928e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.6301113e-34], sampled 0.1197533811992566
[2019-04-27 20:17:50,800] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05594495], dtype=float32), -0.035834122]
[2019-04-27 20:17:50,802] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [23.1288973, 93.45419352, 1.0, 2.0, 0.5587471352049356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 656329.2308735035, 656329.2308735035, 152027.4545022633]
[2019-04-27 20:17:50,804] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 20:17:50,806] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [4.8229101e-34 1.0000000e+00 5.1941956e-36 0.0000000e+00 1.8045222e-26], sampled 0.516934059264641
[2019-04-27 20:17:57,910] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05594495], dtype=float32), -0.035834122]
[2019-04-27 20:17:57,910] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [23.01647182333333, 74.21070904, 1.0, 2.0, 0.4202223640712089, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 517073.1690927836, 517073.1690927831, 131417.7152711251]
[2019-04-27 20:17:57,912] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 20:17:57,916] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [2.8596233e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1865268e-33], sampled 0.3019706116204044
[2019-04-27 20:18:04,323] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05594495], dtype=float32), -0.035834122]
[2019-04-27 20:18:04,324] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [24.8, 85.0, 1.0, 2.0, 0.5715696960827459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 665591.37240706, 665591.3724070595, 153928.2786389283]
[2019-04-27 20:18:04,324] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:18:04,327] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [2.3828605e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.9274518e-33], sampled 0.3152366637842223
[2019-04-27 20:18:05,728] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05594495], dtype=float32), -0.035834122]
[2019-04-27 20:18:05,729] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [25.66666666666667, 91.33333333333334, 1.0, 2.0, 0.6673588243566447, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 760581.8139005923, 760581.8139005923, 170056.8450023025]
[2019-04-27 20:18:05,731] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:18:05,736] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [2.1828685e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2684688e-33], sampled 0.17386355138648235
[2019-04-27 20:18:21,186] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05594495], dtype=float32), -0.035834122]
[2019-04-27 20:18:21,188] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [25.4, 91.0, 1.0, 2.0, 0.7939920168965332, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.9260426156291, 1620083.197034977, 1620083.197034977, 335041.1778837271]
[2019-04-27 20:18:21,189] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 20:18:21,192] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [2.1164897e-24 1.0000000e+00 4.4893660e-25 6.5078575e-27 3.7692577e-14], sampled 0.4304267654173418
[2019-04-27 20:18:21,193] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1620083.197034977 W.
[2019-04-27 20:19:02,588] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8595.4928 2249780846.9456 499.0000
[2019-04-27 20:19:02,833] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8922.2841 2121296666.5365 422.0000
[2019-04-27 20:19:02,970] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8771.0109 2171241226.7257 480.0000
[2019-04-27 20:19:03,032] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8130.8148 2446881891.5362 617.0000
[2019-04-27 20:19:03,108] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8708.3133 2196998725.4228 541.0000
[2019-04-27 20:19:04,127] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1375000, evaluation results [1375000.0, 8130.814807385275, 2446881891.5361924, 617.0, 8771.010852492127, 2171241226.725686, 480.0, 8922.28408782935, 2121296666.5365345, 422.0, 8595.492834094495, 2249780846.9455724, 499.0, 8708.31334635952, 2196998725.4227514, 541.0]
[2019-04-27 20:19:05,858] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.5326069e-34 1.0000000e+00 1.8908972e-36 3.4214767e-37 1.1216471e-25], sum to 1.0000
[2019-04-27 20:19:05,867] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6585
[2019-04-27 20:19:05,871] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.01666666666667, 52.66666666666667, 1.0, 2.0, 0.6996203881469315, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 797369.1023711546, 797369.1023711546, 176073.312633954], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3855000.0000, 
sim time next is 3855600.0000, 
raw observation next is [33.0, 53.0, 1.0, 2.0, 0.7032069573763973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 801458.9126472514, 801458.912647251, 176753.3682251929], 
processed observation next is [0.0, 0.6521739130434783, 0.7777777777777778, 0.53, 1.0, 1.0, 0.6466749492576158, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.28623532594544693, 0.2862353259454468, 0.33991032350998635], 
reward next is 0.6601, 
noisyNet noise sample is [array([-0.56925994], dtype=float32), 0.3577812]. 
=============================================
[2019-04-27 20:19:08,068] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.5520103e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.4411493e-34], sum to 1.0000
[2019-04-27 20:19:08,078] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0766
[2019-04-27 20:19:08,083] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.4, 95.33333333333334, 1.0, 2.0, 0.7137214916315797, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 813448.8948747708, 813448.8948747708, 178757.7593073488], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3904800.0000, 
sim time next is 3905400.0000, 
raw observation next is [25.25, 95.66666666666666, 1.0, 2.0, 0.7087103723454543, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 807734.5688099293, 807734.5688099293, 177798.9833638772], 
processed observation next is [0.0, 0.17391304347826086, 0.49074074074074076, 0.9566666666666666, 1.0, 1.0, 0.6532266337445884, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2884766317178319, 0.2884766317178319, 0.34192112185361], 
reward next is 0.6581, 
noisyNet noise sample is [array([-0.47879887], dtype=float32), 0.7900773]. 
=============================================
[2019-04-27 20:19:08,766] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.16009989e-30 1.00000000e+00 4.99887965e-32 1.51486095e-33
 6.68457016e-25], sum to 1.0000
[2019-04-27 20:19:08,778] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5253
[2019-04-27 20:19:08,782] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.65, 90.5, 1.0, 2.0, 0.7283041801474996, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 830078.204004865, 830078.204004865, 181574.3829633163], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3915000.0000, 
sim time next is 3915600.0000, 
raw observation next is [26.86666666666667, 89.33333333333334, 1.0, 2.0, 0.7465004605607573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 850828.7638557577, 850828.7638557572, 185136.8198241538], 
processed observation next is [0.0, 0.30434782608695654, 0.5506172839506175, 0.8933333333333334, 1.0, 1.0, 0.6982148340009015, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.3038674156627706, 0.3038674156627704, 0.3560323458156804], 
reward next is 0.6440, 
noisyNet noise sample is [array([0.39766333], dtype=float32), 0.3419018]. 
=============================================
[2019-04-27 20:19:10,877] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4682326e-31 1.0000000e+00 1.4852677e-31 1.2458339e-33 2.9199633e-23], sum to 1.0000
[2019-04-27 20:19:10,886] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5627
[2019-04-27 20:19:10,891] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.06666666666667, 69.83333333333334, 1.0, 2.0, 0.7511916441545003, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 856178.5573983099, 856178.5573983104, 186064.8920458225], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3952200.0000, 
sim time next is 3952800.0000, 
raw observation next is [29.9, 71.0, 1.0, 2.0, 0.7538626833660054, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000002, 6.9112, 121.9260426156618, 859224.6088495184, 859224.6088495174, 186595.0128430666], 
processed observation next is [0.0, 0.782608695652174, 0.6629629629629629, 0.71, 1.0, 1.0, 0.7069793849595302, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 1.7763568394002506e-16, 0.0, 0.8094621288201359, 0.30686593173197085, 0.3068659317319705, 0.35883656315974344], 
reward next is 0.6412, 
noisyNet noise sample is [array([1.9290712], dtype=float32), -1.1388432]. 
=============================================
[2019-04-27 20:19:19,425] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0597554e-35 1.0000000e+00 1.8295351e-36 0.0000000e+00 6.4261429e-30], sum to 1.0000
[2019-04-27 20:19:19,431] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5629
[2019-04-27 20:19:19,436] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 94.0, 1.0, 2.0, 0.5809474561304158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 675712.2408315028, 675712.2408315028, 155478.8704360882], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4563000.0000, 
sim time next is 4563600.0000, 
raw observation next is [23.46666666666667, 94.0, 1.0, 2.0, 0.5734449492247621, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 668836.6896821251, 668836.6896821251, 154290.9621920363], 
processed observation next is [0.0, 0.8260869565217391, 0.42469135802469143, 0.94, 1.0, 1.0, 0.49219636812471673, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2388702463150447, 0.2388702463150447, 0.29671338883083903], 
reward next is 0.7033, 
noisyNet noise sample is [array([-0.3809961], dtype=float32), 0.13963895]. 
=============================================
[2019-04-27 20:19:21,961] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8762088e-23 1.4265206e-18 3.7383875e-20 8.5876574e-22 1.0000000e+00], sum to 1.0000
[2019-04-27 20:19:21,973] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1003
[2019-04-27 20:19:21,981] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.0, 71.33333333333333, 1.0, 2.0, 0.5888956637330274, 1.0, 2.0, 0.5888956637330274, 1.0, 2.0, 0.9375410756557923, 6.911200000000001, 6.9112, 121.94756008, 2015040.867357677, 2015040.867357676, 389358.580133492], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4117200.0000, 
sim time next is 4117800.0000, 
raw observation next is [29.0, 70.66666666666667, 1.0, 2.0, 0.589022164882255, 1.0, 2.0, 0.589022164882255, 1.0, 2.0, 0.9377424696052173, 6.9112, 6.9112, 121.94756008, 2015474.20827525, 2015474.20827525, 389426.7522168418], 
processed observation next is [1.0, 0.6521739130434783, 0.6296296296296297, 0.7066666666666667, 1.0, 1.0, 0.510740672478875, 1.0, 1.0, 0.510740672478875, 1.0, 1.0, 0.9221780870065216, 0.0, 0.0, 0.8096049824067558, 0.7198122172411607, 0.7198122172411607, 0.7488976004170035], 
reward next is 0.2511, 
noisyNet noise sample is [array([-0.07359789], dtype=float32), -0.20334214]. 
=============================================
[2019-04-27 20:19:25,685] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.2843492e-27 1.0000000e+00 5.2556852e-28 4.5272388e-31 2.0313902e-24], sum to 1.0000
[2019-04-27 20:19:25,694] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2314
[2019-04-27 20:19:25,700] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1601311.930611084 W.
[2019-04-27 20:19:25,706] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.21666666666667, 34.5, 1.0, 2.0, 1.02, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.666787267766345, 6.9112, 121.9230546481252, 1601311.930611084, 1214393.198106009, 248270.5213441518], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4194600.0000, 
sim time next is 4195200.0000, 
raw observation next is [33.43333333333334, 33.0, 1.0, 2.0, 0.4816273116774497, 1.0, 1.0, 0.4816273116774497, 1.0, 1.0, 0.7684009877464922, 6.9112, 6.9112, 121.94756008, 1678508.982084101, 1678508.982084101, 334774.2666552402], 
processed observation next is [1.0, 0.5652173913043478, 0.7938271604938273, 0.33, 1.0, 1.0, 0.38288965675886877, 1.0, 0.5, 0.38288965675886877, 1.0, 0.5, 0.7105012346831152, 0.0, 0.0, 0.8096049824067558, 0.5994674936014647, 0.5994674936014647, 0.6437966666446927], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.96044755], dtype=float32), -0.2035687]. 
=============================================
[2019-04-27 20:19:29,192] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.2703550e-33 1.0000000e+00 4.5025469e-36 1.0479558e-36 1.4508692e-28], sum to 1.0000
[2019-04-27 20:19:29,201] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5219
[2019-04-27 20:19:29,213] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 83.0, 1.0, 2.0, 0.3788884547103323, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 471721.7122842773, 471721.7122842768, 125732.6981106499], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4255200.0000, 
sim time next is 4255800.0000, 
raw observation next is [21.5, 81.5, 1.0, 2.0, 0.4970268220583736, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 617142.3220942385, 617142.3220942385, 143154.5279237729], 
processed observation next is [1.0, 0.2608695652173913, 0.35185185185185186, 0.815, 1.0, 1.0, 0.4012224072123496, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.22040797217651376, 0.22040797217651376, 0.2752971690841787], 
reward next is 0.7247, 
noisyNet noise sample is [array([0.0865525], dtype=float32), 0.43372825]. 
=============================================
[2019-04-27 20:19:33,281] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.6237249e-24 5.9528036e-24 1.4371921e-20 4.1746566e-21 1.0000000e+00], sum to 1.0000
[2019-04-27 20:19:33,290] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1170
[2019-04-27 20:19:33,294] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.5363192343566817, 1.0, 2.0, 0.5363192343566817, 1.0, 2.0, 0.8538376877938867, 6.9112, 6.9112, 121.94756008, 1834953.83962398, 1834953.83962398, 361772.3970392392], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4352400.0000, 
sim time next is 4353000.0000, 
raw observation next is [27.16666666666666, 79.00000000000001, 1.0, 2.0, 0.5140448079669885, 1.0, 2.0, 0.5140448079669885, 1.0, 2.0, 0.8183760755540721, 6.911200000000001, 6.9112, 121.94756008, 1758669.471637687, 1758669.471637686, 350535.8888499512], 
processed observation next is [1.0, 0.391304347826087, 0.5617283950617282, 0.7900000000000001, 1.0, 1.0, 0.42148191424641485, 1.0, 1.0, 0.42148191424641485, 1.0, 1.0, 0.7729700944425901, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.6280962398706025, 0.6280962398706021, 0.6741074785575984], 
reward next is 0.3259, 
noisyNet noise sample is [array([-0.11467881], dtype=float32), -2.9217222]. 
=============================================
[2019-04-27 20:19:33,312] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[48.77791 ]
 [49.09652 ]
 [48.96298 ]
 [48.359035]
 [45.095753]], R is [[48.15073776]
 [47.97351456]
 [47.83713531]
 [47.70068741]
 [47.47949982]].
[2019-04-27 20:19:34,404] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0232611e-17 2.6022368e-05 1.7149683e-16 1.2708091e-18 9.9997401e-01], sum to 1.0000
[2019-04-27 20:19:34,413] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9548
[2019-04-27 20:19:34,419] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.6, 64.0, 1.0, 2.0, 0.5992977620684141, 1.0, 2.0, 0.5992977620684141, 1.0, 2.0, 0.9541015549784198, 6.911200000000002, 6.9112, 121.94756008, 2050674.880899232, 2050674.880899231, 394993.1118589526], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4361400.0000, 
sim time next is 4362000.0000, 
raw observation next is [29.73333333333333, 63.33333333333333, 1.0, 2.0, 0.6041131015337805, 1.0, 2.0, 0.6041131015337805, 1.0, 2.0, 0.9617677322319408, 6.911199999999999, 6.9112, 121.94756008, 2067171.06021642, 2067171.060216421, 397621.1793709811], 
processed observation next is [1.0, 0.4782608695652174, 0.65679012345679, 0.6333333333333333, 1.0, 1.0, 0.5287060732545006, 1.0, 1.0, 0.5287060732545006, 1.0, 1.0, 0.9522096652899259, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.7382753786487215, 0.7382753786487218, 0.7646561141749637], 
reward next is 0.2353, 
noisyNet noise sample is [array([0.9138072], dtype=float32), -0.778438]. 
=============================================
[2019-04-27 20:19:34,433] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[54.53456 ]
 [54.29442 ]
 [54.090424]
 [54.054707]
 [53.84516 ]], R is [[54.4613533 ]
 [54.15713882]
 [53.8872261 ]
 [53.60659027]
 [53.38801575]].
[2019-04-27 20:19:39,532] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:19:39,539] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9891
[2019-04-27 20:19:39,542] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666667, 83.16666666666667, 1.0, 2.0, 0.6300437814582697, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 718034.3348025001, 718034.3348025001, 163322.2981182449], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4439400.0000, 
sim time next is 4440000.0000, 
raw observation next is [26.33333333333334, 82.33333333333334, 1.0, 2.0, 0.6381782678915837, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 727309.2639811118, 727309.2639811118, 164769.3508700177], 
processed observation next is [0.0, 0.391304347826087, 0.5308641975308644, 0.8233333333333335, 1.0, 1.0, 0.5692598427280758, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2597533085646828, 0.2597533085646828, 0.3168641362884956], 
reward next is 0.6831, 
noisyNet noise sample is [array([-0.08662281], dtype=float32), 1.9552644]. 
=============================================
[2019-04-27 20:19:39,555] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[65.11429 ]
 [65.15588 ]
 [65.21456 ]
 [65.279366]
 [65.32658 ]], R is [[65.12566376]
 [65.16033173]
 [65.19604492]
 [65.23765564]
 [65.28421021]].
[2019-04-27 20:19:40,345] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.73437e-38 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00], sum to 1.0000
[2019-04-27 20:19:40,355] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1228
[2019-04-27 20:19:40,361] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.728133080311069, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 829883.0889152735, 829883.0889152735, 181541.7028957919], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4462200.0000, 
sim time next is 4462800.0000, 
raw observation next is [30.0, 70.0, 1.0, 2.0, 0.7333141580214746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 835791.3950273665, 835791.3950273665, 182549.9663078595], 
processed observation next is [0.0, 0.6521739130434783, 0.6666666666666666, 0.7, 1.0, 1.0, 0.6825168547874697, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.298496926795488, 0.298496926795488, 0.3510576275151144], 
reward next is 0.6489, 
noisyNet noise sample is [array([0.6852403], dtype=float32), 1.2049837]. 
=============================================
[2019-04-27 20:19:40,445] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2122087e-37], sum to 1.0000
[2019-04-27 20:19:40,453] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8972
[2019-04-27 20:19:40,457] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.7077329746065462, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 806620.0185117348, 806620.0185117348, 177613.2096852566], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4482000.0000, 
sim time next is 4482600.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.6955472635266586, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 792724.4932899722, 792724.4932899722, 175303.2181584823], 
processed observation next is [0.0, 0.9130434782608695, 0.5555555555555556, 0.84, 1.0, 1.0, 0.6375562661031651, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.28311589046070434, 0.28311589046070434, 0.33712157338169674], 
reward next is 0.6629, 
noisyNet noise sample is [array([-0.48874602], dtype=float32), -0.34715545]. 
=============================================
[2019-04-27 20:19:49,900] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1737429e-31 1.0000000e+00 1.6359695e-33 4.9531231e-36 3.0437072e-28], sum to 1.0000
[2019-04-27 20:19:49,908] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1156
[2019-04-27 20:19:49,912] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.6126828871171806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 721221.0181866242, 721221.0181866242, 161334.4457623793], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4687200.0000, 
sim time next is 4687800.0000, 
raw observation next is [23.0, 95.0, 1.0, 2.0, 0.6068843315299293, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 713655.9125132433, 713655.9125132428, 160284.9571075685], 
processed observation next is [1.0, 0.2608695652173913, 0.4074074074074074, 0.95, 1.0, 1.0, 0.5320051565832491, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2548771116118726, 0.25487711161187243, 0.3082403021299394], 
reward next is 0.6918, 
noisyNet noise sample is [array([-1.4891199], dtype=float32), -1.6898407]. 
=============================================
[2019-04-27 20:19:54,184] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.16692405e-20 9.94871616e-01 1.50966911e-19 6.56850927e-22
 5.12841763e-03], sum to 1.0000
[2019-04-27 20:19:54,190] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7162
[2019-04-27 20:19:54,199] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.33333333333333, 92.33333333333334, 1.0, 2.0, 0.6754530220922546, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 769811.3179742, 769811.3179742, 171547.8625288249], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4743600.0000, 
sim time next is 4744200.0000, 
raw observation next is [25.16666666666667, 93.16666666666667, 1.0, 2.0, 0.6759829253545243, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 770415.5502711849, 770415.5502711849, 171645.6613856204], 
processed observation next is [1.0, 0.9130434782608695, 0.4876543209876545, 0.9316666666666668, 1.0, 1.0, 0.6142653873268147, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.27514841081113744, 0.27514841081113744, 0.3300878103569623], 
reward next is 0.6699, 
noisyNet noise sample is [array([1.3353957], dtype=float32), 0.49178693]. 
=============================================
[2019-04-27 20:19:55,056] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-04-27 20:19:55,057] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 20:19:55,058] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 20:19:55,058] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:19:55,058] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:19:55,061] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 20:19:55,062] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:19:55,063] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 20:19:55,063] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:19:55,064] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 20:19:55,064] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:19:55,091] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run57
[2019-04-27 20:19:55,092] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run57
[2019-04-27 20:19:55,092] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run57
[2019-04-27 20:19:55,129] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run57
[2019-04-27 20:19:55,169] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run57
[2019-04-27 20:20:03,496] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05055905], dtype=float32), -0.042553447]
[2019-04-27 20:20:03,497] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [31.85, 31.0, 1.0, 2.0, 0.379574807043376, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 469316.1777747129, 469316.1777747124, 125755.8607446883]
[2019-04-27 20:20:03,498] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 20:20:03,501] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [6.8401983e-31 1.0000000e+00 7.6896109e-32 8.3407110e-35 1.0209111e-21], sampled 0.0965335890091511
[2019-04-27 20:20:36,757] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05055905], dtype=float32), -0.042553447]
[2019-04-27 20:20:36,760] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [26.49612114, 47.52222055666667, 1.0, 2.0, 0.3801047207835058, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 474472.3688322842, 474472.3688322847, 125923.3210721862]
[2019-04-27 20:20:36,763] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 20:20:36,766] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [8.9898420e-36 1.0000000e+00 1.5126981e-37 0.0000000e+00 2.5885131e-31], sampled 0.46229032103504863
[2019-04-27 20:20:37,892] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05055905], dtype=float32), -0.042553447]
[2019-04-27 20:20:37,893] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [24.93346432166667, 75.01188012833333, 1.0, 2.0, 0.5041630906318366, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 601395.2090524747, 601395.2090524742, 143553.3134645761]
[2019-04-27 20:20:37,894] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 20:20:37,901] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.3469077e-34 1.0000000e+00 5.3668922e-36 0.0000000e+00 8.3058464e-28], sampled 0.67410329789342
[2019-04-27 20:20:39,005] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05055905], dtype=float32), -0.042553447]
[2019-04-27 20:20:39,006] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [33.0, 44.0, 1.0, 2.0, 0.4540077785198022, 1.0, 2.0, 0.4540077785198022, 1.0, 2.0, 0.722795169404607, 6.911199999999999, 6.9112, 121.94756008, 1553085.823917466, 1553085.823917466, 321547.9501388404]
[2019-04-27 20:20:39,006] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 20:20:39,010] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [3.7665721e-19 1.2282020e-04 1.1432287e-16 9.5237171e-19 9.9987721e-01], sampled 0.895035358970192
[2019-04-27 20:21:15,066] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05055905], dtype=float32), -0.042553447]
[2019-04-27 20:21:15,068] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [23.52386653166667, 78.76504081333333, 1.0, 2.0, 0.4519343645483009, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 547043.0460157959, 547043.0460157959, 135821.7276611365]
[2019-04-27 20:21:15,071] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 20:21:15,074] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [3.0323406e-34 1.0000000e+00 1.0573486e-35 3.2238670e-38 1.6113838e-29], sampled 0.04200285821065808
[2019-04-27 20:21:19,263] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05055905], dtype=float32), -0.042553447]
[2019-04-27 20:21:19,265] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [22.5, 87.0, 1.0, 2.0, 0.4715221460329455, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 569737.3137958134, 569737.3137958129, 138747.1275013859]
[2019-04-27 20:21:19,265] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 20:21:19,272] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [6.0523915e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.4944961e-33], sampled 0.5794527875109174
[2019-04-27 20:21:42,446] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8926.3385 2122680612.2946 405.0000
[2019-04-27 20:21:42,639] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8627.5177 2251396865.9098 444.0000
[2019-04-27 20:21:42,706] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8786.6603 2172308897.5313 455.0000
[2019-04-27 20:21:42,896] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8178.4597 2451443966.3647 529.0000
[2019-04-27 20:21:42,922] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8732.9611 2199900919.1414 489.0000
[2019-04-27 20:21:43,935] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 1400000, evaluation results [1400000.0, 8178.459677918225, 2451443966.3647423, 529.0, 8786.660284792246, 2172308897.531282, 455.0, 8926.338511595226, 2122680612.2946002, 405.0, 8627.517719083486, 2251396865.9097877, 444.0, 8732.961114851272, 2199900919.1414332, 489.0]
[2019-04-27 20:21:44,340] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.1324481e-24 1.0000000e+00 6.3131296e-24 4.2998417e-26 6.2243427e-16], sum to 1.0000
[2019-04-27 20:21:44,348] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3533
[2019-04-27 20:21:44,353] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 94.0, 1.0, 2.0, 0.6214697833503099, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 717234.2221847743, 717234.2221847743, 162248.0625577098], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4765800.0000, 
sim time next is 4766400.0000, 
raw observation next is [24.0, 94.0, 1.0, 2.0, 0.6204263413175363, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 716035.3029573772, 716035.3029573772, 162064.2459091393], 
processed observation next is [1.0, 0.17391304347826086, 0.4444444444444444, 0.94, 1.0, 1.0, 0.5481265968065908, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.25572689391334896, 0.25572689391334896, 0.31166201136372945], 
reward next is 0.6883, 
noisyNet noise sample is [array([0.18218064], dtype=float32), 0.83911127]. 
=============================================
[2019-04-27 20:21:49,173] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3921617e-20 6.7519457e-10 2.3244276e-18 3.8985516e-19 1.0000000e+00], sum to 1.0000
[2019-04-27 20:21:49,184] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0963
[2019-04-27 20:21:49,189] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.6, 66.0, 1.0, 2.0, 0.5474563534084134, 1.0, 1.0, 0.5474563534084134, 1.0, 1.0, 0.8715683440348889, 6.911199999999999, 6.9112, 121.94756008, 1873098.144036904, 1873098.144036905, 367491.2728129727], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5319000.0000, 
sim time next is 5319600.0000, 
raw observation next is [27.8, 65.0, 1.0, 2.0, 0.4569053759672685, 1.0, 2.0, 0.4569053759672685, 1.0, 2.0, 0.7274082389091341, 6.911200000000001, 6.9112, 121.94756008, 1563008.142205772, 1563008.142205771, 322906.1701352796], 
processed observation next is [1.0, 0.5652173913043478, 0.5851851851851853, 0.65, 1.0, 1.0, 0.3534587809134149, 1.0, 1.0, 0.3534587809134149, 1.0, 1.0, 0.6592602986364177, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.5582171936449186, 0.5582171936449183, 0.6209734041063069], 
reward next is 0.3790, 
noisyNet noise sample is [array([-0.84341687], dtype=float32), -0.77176523]. 
=============================================
[2019-04-27 20:21:50,971] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.99712576e-13 8.90193999e-01 2.88356873e-11 8.88135560e-15
 1.09805964e-01], sum to 1.0000
[2019-04-27 20:21:50,979] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5504
[2019-04-27 20:21:50,986] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2069429.860593304 W.
[2019-04-27 20:21:50,996] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.3, 86.5, 1.0, 2.0, 0.604772452780318, 1.0, 2.0, 0.604772452780318, 1.0, 2.0, 0.9628174408899995, 6.911200000000001, 6.9112, 121.94756008, 2069429.860593304, 2069429.860593303, 397982.0050110238], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4879800.0000, 
sim time next is 4880400.0000, 
raw observation next is [28.2, 87.33333333333333, 1.0, 2.0, 0.9466884985705163, 1.0, 2.0, 0.9466884985705163, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 2159715.631218646, 2159715.631218647, 408098.2693833789], 
processed observation next is [1.0, 0.4782608695652174, 0.6, 0.8733333333333333, 1.0, 1.0, 0.9365339268696623, 1.0, 1.0, 0.9365339268696623, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.7713270111495165, 0.7713270111495169, 0.7848043641988056], 
reward next is 0.2152, 
noisyNet noise sample is [array([-0.2760061], dtype=float32), -0.81543064]. 
=============================================
[2019-04-27 20:21:52,805] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.3903282e-29 1.0000000e+00 4.7176196e-29 2.6902816e-32 1.1940157e-23], sum to 1.0000
[2019-04-27 20:21:52,813] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8097
[2019-04-27 20:21:52,828] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.73333333333333, 88.66666666666667, 1.0, 2.0, 0.8754226478196306, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000002, 6.9112, 121.9260426156618, 997864.3314455446, 997864.3314455437, 212028.0521445826], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4911600.0000, 
sim time next is 4912200.0000, 
raw observation next is [28.55, 90.0, 1.0, 2.0, 0.8764468459415348, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 999032.5408769954, 999032.5408769954, 212253.2062892888], 
processed observation next is [1.0, 0.8695652173913043, 0.612962962962963, 0.9, 1.0, 1.0, 0.8529129118351604, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.35679733602749836, 0.35679733602749836, 0.4081792428640169], 
reward next is 0.5918, 
noisyNet noise sample is [array([1.8165383], dtype=float32), -0.30660453]. 
=============================================
[2019-04-27 20:21:53,567] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3887113e-16 1.0000000e+00 1.0860354e-17 3.5509871e-18 1.4405930e-08], sum to 1.0000
[2019-04-27 20:21:53,576] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3495
[2019-04-27 20:21:53,584] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1836522.741071712 W.
[2019-04-27 20:21:53,590] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.66666666666666, 77.66666666666667, 1.0, 2.0, 0.8051658357307679, 1.0, 2.0, 0.8051658357307679, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1836522.741071712, 1836522.741071713, 345893.5658070559], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5395200.0000, 
sim time next is 5395800.0000, 
raw observation next is [27.83333333333334, 76.83333333333333, 1.0, 2.0, 0.8236338908553404, 1.0, 2.0, 0.8236338908553404, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1878691.288474284, 1878691.288474284, 353617.0508091753], 
processed observation next is [1.0, 0.43478260869565216, 0.58641975308642, 0.7683333333333333, 1.0, 1.0, 0.7900403462563577, 1.0, 1.0, 0.7900403462563577, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.6709611744551015, 0.6709611744551015, 0.6800327900176448], 
reward next is 0.3200, 
noisyNet noise sample is [array([-0.76071364], dtype=float32), -0.4830407]. 
=============================================
[2019-04-27 20:21:55,406] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5558240e-22 1.0000000e+00 1.4482353e-23 1.0676489e-25 2.9879443e-22], sum to 1.0000
[2019-04-27 20:21:55,418] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6422
[2019-04-27 20:21:55,424] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1516746.04242478 W.
[2019-04-27 20:21:55,428] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.25, 83.16666666666667, 1.0, 2.0, 0.7034560872306188, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.9260426156618, 1516746.04242478, 1516746.042424781, 317853.6291067592], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4965000.0000, 
sim time next is 4965600.0000, 
raw observation next is [25.40000000000001, 83.33333333333334, 1.0, 2.0, 0.8313413948467417, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.9260426156618, 1662713.236979338, 1662713.236979338, 342541.0707508727], 
processed observation next is [1.0, 0.4782608695652174, 0.4962962962962966, 0.8333333333333335, 1.0, 1.0, 0.7992159462461211, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.9972168686025908, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.5938261560640493, 0.5938261560640493, 0.6587328283670629], 
reward next is 0.3413, 
noisyNet noise sample is [array([-1.1448047], dtype=float32), -0.9171908]. 
=============================================
[2019-04-27 20:21:59,071] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:21:59,084] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6125
[2019-04-27 20:21:59,089] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.8, 96.0, 1.0, 2.0, 0.5434321163730481, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 639641.646869959, 639641.646869959, 149550.4923696228], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5020200.0000, 
sim time next is 5020800.0000, 
raw observation next is [22.86666666666667, 95.33333333333334, 1.0, 2.0, 0.542801456213829, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 639095.5090791234, 639095.5090791234, 149455.1289007891], 
processed observation next is [0.0, 0.08695652173913043, 0.4024691358024693, 0.9533333333333335, 1.0, 1.0, 0.45571601930217737, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.22824839609968692, 0.22824839609968692, 0.28741370942459443], 
reward next is 0.7126, 
noisyNet noise sample is [array([0.4204631], dtype=float32), -1.4788624]. 
=============================================
[2019-04-27 20:22:14,587] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.5377212e-18 2.8306840e-10 1.4687992e-15 6.4147504e-16 1.0000000e+00], sum to 1.0000
[2019-04-27 20:22:14,593] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4190
[2019-04-27 20:22:14,600] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.9, 75.66666666666667, 1.0, 2.0, 0.808974472300734, 1.0, 2.0, 0.7178518981268017, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.94756008, 2456901.086343428, 2456901.086343428, 459807.0436547381], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5581200.0000, 
sim time next is 5581800.0000, 
raw observation next is [29.6, 76.5, 1.0, 2.0, 0.8004257920582091, 1.0, 2.0, 0.7135775580055392, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.94756008, 2442251.831328751, 2442251.831328751, 457316.3669124919], 
processed observation next is [1.0, 0.6086956521739131, 0.6518518518518519, 0.765, 1.0, 1.0, 0.7624116572121536, 1.0, 1.0, 0.6590209023875467, 1.0, 1.0, 0.9972168686025908, 0.0, 0.0, 0.8096049824067558, 0.8722327969031254, 0.8722327969031254, 0.8794545517547921], 
reward next is 0.1205, 
noisyNet noise sample is [array([0.16533256], dtype=float32), 0.16308895]. 
=============================================
[2019-04-27 20:22:16,347] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.3142994e-18 7.4665854e-03 5.0983194e-16 1.2013440e-18 9.9253345e-01], sum to 1.0000
[2019-04-27 20:22:16,358] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7117
[2019-04-27 20:22:16,362] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.43333333333333, 66.33333333333334, 1.0, 2.0, 0.5324332711162145, 1.0, 2.0, 0.5324332711162145, 1.0, 2.0, 0.8476511077580756, 6.9112, 6.9112, 121.94756008, 1821644.913367005, 1821644.913367005, 359792.7537282027], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5329200.0000, 
sim time next is 5329800.0000, 
raw observation next is [28.45, 66.5, 1.0, 2.0, 0.5388849869037098, 1.0, 2.0, 0.5388849869037098, 1.0, 2.0, 0.8579224494094821, 6.9112, 6.9112, 121.94756008, 1843741.32206723, 1843741.32206723, 363083.9568895742], 
processed observation next is [1.0, 0.6956521739130435, 0.6092592592592593, 0.665, 1.0, 1.0, 0.4510535558377497, 1.0, 1.0, 0.4510535558377497, 1.0, 1.0, 0.8224030617618527, 0.0, 0.0, 0.8096049824067558, 0.6584790435954393, 0.6584790435954393, 0.6982383786337965], 
reward next is 0.3018, 
noisyNet noise sample is [array([0.19791569], dtype=float32), 0.5209489]. 
=============================================
[2019-04-27 20:22:23,980] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.1808297e-26 4.0056333e-25 1.3293115e-21 8.1867410e-23 1.0000000e+00], sum to 1.0000
[2019-04-27 20:22:23,987] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7026
[2019-04-27 20:22:23,993] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.73333333333333, 77.33333333333334, 1.0, 2.0, 0.8455895082108859, 1.0, 2.0, 0.7361594160818775, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.94756008, 2519648.345021874, 2519648.345021874, 470653.6690839092], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5485200.0000, 
sim time next is 5485800.0000, 
raw observation next is [30.91666666666667, 76.16666666666666, 1.0, 2.0, 0.8569935852931256, 1.0, 2.0, 0.7418614546229976, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.94756008, 2539192.435045659, 2539192.435045659, 474091.1411556911], 
processed observation next is [1.0, 0.4782608695652174, 0.7006172839506175, 0.7616666666666666, 1.0, 1.0, 0.829754268206102, 1.0, 1.0, 0.6926922078845209, 1.0, 1.0, 0.9972168686025908, 0.0, 0.0, 0.8096049824067558, 0.9068544410877354, 0.9068544410877354, 0.9117137329917137], 
reward next is 0.0883, 
noisyNet noise sample is [array([-0.34960434], dtype=float32), -0.3986708]. 
=============================================
[2019-04-27 20:22:33,893] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.2231723e-37], sum to 1.0000
[2019-04-27 20:22:33,900] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5540
[2019-04-27 20:22:33,906] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.7, 75.0, 1.0, 2.0, 0.708057424930339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 806989.9968891094, 806989.9968891094, 177676.5278089105], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5662800.0000, 
sim time next is 5663400.0000, 
raw observation next is [28.81666666666667, 74.5, 1.0, 2.0, 0.708427301738597, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 807411.7763788893, 807411.7763788893, 177747.242330613], 
processed observation next is [0.0, 0.5652173913043478, 0.6228395061728397, 0.745, 1.0, 1.0, 0.6528896449269012, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2883613487067462, 0.2883613487067462, 0.34182161986656345], 
reward next is 0.6582, 
noisyNet noise sample is [array([0.31057996], dtype=float32), -0.6529942]. 
=============================================
[2019-04-27 20:22:35,374] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-04-27 20:22:35,378] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 20:22:35,379] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 20:22:35,379] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:22:35,380] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 20:22:35,380] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:22:35,380] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 20:22:35,380] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:22:35,381] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 20:22:35,382] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:22:35,382] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:22:35,405] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run58
[2019-04-27 20:22:35,427] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run58
[2019-04-27 20:22:35,427] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run58
[2019-04-27 20:22:35,470] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run58
[2019-04-27 20:22:35,471] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run58
[2019-04-27 20:22:39,807] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04972616], dtype=float32), -0.041278545]
[2019-04-27 20:22:39,808] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [22.508526465, 31.149672055, 1.0, 2.0, 0.2735330901008821, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 352841.1349392137, 352841.1349392137, 90062.8999411354]
[2019-04-27 20:22:39,809] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 20:22:39,811] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2046987e-38], sampled 0.3294505122656497
[2019-04-27 20:22:45,250] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04972616], dtype=float32), -0.041278545]
[2019-04-27 20:22:45,252] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [33.2, 26.5, 1.0, 2.0, 0.5157002624399846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 639155.2135114685, 639155.2135114685, 146113.2363730393]
[2019-04-27 20:22:45,254] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:22:45,257] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [9.2821382e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.3677749e-36], sampled 0.19893407477549552
[2019-04-27 20:23:13,050] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04972616], dtype=float32), -0.041278545]
[2019-04-27 20:23:13,052] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [30.75, 65.16666666666667, 1.0, 2.0, 0.8587598516543111, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.911200000000001, 6.9112, 121.9260426156618, 1694009.561989092, 1694009.561989092, 348208.5539933135]
[2019-04-27 20:23:13,055] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 20:23:13,059] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.7701270e-25 1.0000000e+00 8.9748405e-27 2.0298456e-27 7.8458269e-16], sampled 0.7196002470877856
[2019-04-27 20:23:13,061] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1694009.561989092 W.
[2019-04-27 20:23:35,168] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04972616], dtype=float32), -0.041278545]
[2019-04-27 20:23:35,172] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [24.38484707166667, 99.597855075, 1.0, 2.0, 0.6558789812894873, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 747491.9785317227, 747491.9785317227, 167959.2217318614]
[2019-04-27 20:23:35,173] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 20:23:35,179] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [2.8315833e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.2597723e-36], sampled 0.4332857178061924
[2019-04-27 20:24:13,838] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04972616], dtype=float32), -0.041278545]
[2019-04-27 20:24:13,839] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [23.13333333333333, 94.0, 1.0, 2.0, 0.5436654231711456, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8667735341090319, 6.911199999999999, 6.9112, 121.9260426156618, 1257789.078537526, 1257789.078537527, 272290.7646375967]
[2019-04-27 20:24:13,840] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:24:13,842] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.6188457e-28 1.0000000e+00 4.3308118e-31 1.8484456e-31 8.6717821e-26], sampled 0.6274122131385609
[2019-04-27 20:24:21,619] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04972616], dtype=float32), -0.041278545]
[2019-04-27 20:24:21,620] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [25.5, 55.5, 1.0, 2.0, 0.384033897547128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 477039.5688614508, 477039.5688614503, 126418.9457547789]
[2019-04-27 20:24:21,621] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:24:21,626] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [4.7783997e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2855549e-37], sampled 0.9810623623541372
[2019-04-27 20:24:22,832] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8919.9399 2121042754.8019 431.0000
[2019-04-27 20:24:22,866] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8587.6049 2249943771.1782 525.0000
[2019-04-27 20:24:22,970] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8697.2378 2196610025.8092 565.0000
[2019-04-27 20:24:22,985] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8105.6388 2446278643.0298 695.0000
[2019-04-27 20:24:23,046] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8766.6020 2171250662.6012 489.0000
[2019-04-27 20:24:24,066] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 1425000, evaluation results [1425000.0, 8105.63877191688, 2446278643.029815, 695.0, 8766.601959354855, 2171250662.6012473, 489.0, 8919.939924331327, 2121042754.8018548, 431.0, 8587.6049454263, 2249943771.1782293, 525.0, 8697.237827428113, 2196610025.809153, 565.0]
[2019-04-27 20:24:26,346] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:24:26,356] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1045
[2019-04-27 20:24:26,365] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.25, 96.83333333333334, 1.0, 2.0, 0.4757820635055587, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 575038.6463976405, 575038.6463976405, 139404.0630660539], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5717400.0000, 
sim time next is 5718000.0000, 
raw observation next is [21.2, 96.66666666666667, 1.0, 2.0, 0.4723759474896193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 571600.7001982458, 571600.7001982458, 138904.340761957], 
processed observation next is [0.0, 0.17391304347826086, 0.34074074074074073, 0.9666666666666667, 1.0, 1.0, 0.37187612796383246, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.20414310721365922, 0.20414310721365922, 0.2671237322345327], 
reward next is 0.7329, 
noisyNet noise sample is [array([-0.30313084], dtype=float32), 0.62603426]. 
=============================================
[2019-04-27 20:24:26,374] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[74.038925]
 [74.11719 ]
 [74.1434  ]
 [74.22629 ]
 [74.29224 ]], R is [[74.02633667]
 [74.01799011]
 [74.00868988]
 [73.99832916]
 [73.98691559]].
[2019-04-27 20:24:27,904] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.17480590e-38 1.00000000e+00 0.00000000e+00 0.00000000e+00
 1.40608235e-33], sum to 1.0000
[2019-04-27 20:24:27,915] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4945
[2019-04-27 20:24:27,921] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.55, 73.33333333333334, 1.0, 2.0, 0.5726167971785852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 666428.8495832073, 666428.8495832073, 154087.5604409867], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5770200.0000, 
sim time next is 5770800.0000, 
raw observation next is [26.4, 74.0, 1.0, 2.0, 0.5708042198235168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 664864.2054856966, 664864.2054856966, 153806.6022367868], 
processed observation next is [0.0, 0.8260869565217391, 0.5333333333333333, 0.74, 1.0, 1.0, 0.4890526426470438, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.23745150195917733, 0.23745150195917733, 0.29578192737843617], 
reward next is 0.7042, 
noisyNet noise sample is [array([0.30002406], dtype=float32), 0.31749994]. 
=============================================
[2019-04-27 20:24:53,094] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:24:53,101] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8100
[2019-04-27 20:24:53,110] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.9, 74.0, 1.0, 2.0, 0.4972502351633676, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 595248.4770954245, 595248.4770954245, 142542.7010816298], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6220800.0000, 
sim time next is 6221400.0000, 
raw observation next is [24.81666666666667, 74.33333333333334, 1.0, 2.0, 0.4955321210412312, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 593465.4522269061, 593465.4522269061, 142283.304694051], 
processed observation next is [0.0, 0.0, 0.4746913580246915, 0.7433333333333334, 1.0, 1.0, 0.39944300123956095, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21195194722389504, 0.21195194722389504, 0.27362173979625193], 
reward next is 0.7264, 
noisyNet noise sample is [array([-0.20227116], dtype=float32), -0.26339978]. 
=============================================
[2019-04-27 20:25:06,546] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.0492396e-21 1.8243694e-15 9.9403438e-20 9.8907981e-19 1.0000000e+00], sum to 1.0000
[2019-04-27 20:25:06,554] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9227
[2019-04-27 20:25:06,562] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.13333333333333, 68.33333333333333, 1.0, 2.0, 0.7764558131918613, 1.0, 2.0, 0.7015925685723653, 1.0, 2.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.94756008, 2401177.494176813, 2401177.494176814, 450416.9183702305], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6436200.0000, 
sim time next is 6436800.0000, 
raw observation next is [30.3, 67.0, 1.0, 2.0, 0.7457638835951187, 1.0, 2.0, 0.686246603773994, 1.0, 2.0, 0.9977734948820727, 6.911200000000001, 6.9112, 121.94756008, 2348587.354491862, 2348587.354491862, 441763.9928792792], 
processed observation next is [1.0, 0.5217391304347826, 0.6777777777777778, 0.67, 1.0, 1.0, 0.6973379566608556, 1.0, 1.0, 0.6264840521118976, 1.0, 1.0, 0.9972168686025908, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.8387811980328078, 0.8387811980328078, 0.84954614015246], 
reward next is 0.1505, 
noisyNet noise sample is [array([-2.0612285], dtype=float32), -0.54169333]. 
=============================================
[2019-04-27 20:25:07,035] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1256359e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.9425360e-34], sum to 1.0000
[2019-04-27 20:25:07,041] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5945
[2019-04-27 20:25:07,049] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.1, 79.5, 1.0, 2.0, 0.6671117182751779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 760300.0500358714, 760300.0500358714, 170010.1264338021], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6485400.0000, 
sim time next is 6486000.0000, 
raw observation next is [27.03333333333333, 80.0, 1.0, 2.0, 0.6672034570204474, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 760404.6555569951, 760404.6555569951, 170027.0185829971], 
processed observation next is [1.0, 0.043478260869565216, 0.55679012345679, 0.8, 1.0, 1.0, 0.6038136393100564, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2715730912703554, 0.2715730912703554, 0.32697503573653286], 
reward next is 0.6730, 
noisyNet noise sample is [array([-0.06703295], dtype=float32), 1.8488581]. 
=============================================
[2019-04-27 20:25:07,059] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[61.031574]
 [61.226074]
 [61.327595]
 [61.434994]
 [61.347633]], R is [[60.93734741]
 [61.00103378]
 [61.06429291]
 [61.12722015]
 [61.1894989 ]].
[2019-04-27 20:25:10,499] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.6806873e-32 1.0000000e+00 4.1332241e-34 4.0828445e-33 1.8726283e-21], sum to 1.0000
[2019-04-27 20:25:10,508] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1220
[2019-04-27 20:25:10,516] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.06666666666667, 86.0, 1.0, 2.0, 0.7126435609050775, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 812219.6953053667, 812219.6953053667, 178553.2935821977], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6549600.0000, 
sim time next is 6550200.0000, 
raw observation next is [27.0, 86.5, 1.0, 2.0, 0.7128839433466093, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 812493.8110739061, 812493.8110739061, 178599.3604114442], 
processed observation next is [1.0, 0.8260869565217391, 0.5555555555555556, 0.865, 1.0, 1.0, 0.6581951706507254, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.29017636109782363, 0.29017636109782363, 0.3434603084835466], 
reward next is 0.6565, 
noisyNet noise sample is [array([0.02328798], dtype=float32), 0.22156905]. 
=============================================
[2019-04-27 20:25:15,537] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-04-27 20:25:15,538] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 20:25:15,538] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 20:25:15,539] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 20:25:15,540] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 20:25:15,541] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:25:15,540] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:25:15,542] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 20:25:15,542] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:25:15,547] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:25:15,549] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:25:15,564] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run59
[2019-04-27 20:25:15,585] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run59
[2019-04-27 20:25:15,608] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run59
[2019-04-27 20:25:15,628] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run59
[2019-04-27 20:25:15,630] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run59
[2019-04-27 20:25:23,978] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.053302], dtype=float32), -0.031667806]
[2019-04-27 20:25:23,979] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [26.120297025, 32.68493584, 1.0, 2.0, 0.3075403010238291, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 396285.5356504354, 396285.5356504354, 115032.4252477847]
[2019-04-27 20:25:23,979] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 20:25:23,981] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [4.6927033e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.4176928e-30], sampled 0.3069958137102624
[2019-04-27 20:26:09,686] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.053302], dtype=float32), -0.031667806]
[2019-04-27 20:26:09,687] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [29.8, 53.0, 1.0, 2.0, 0.6354821426813558, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9855437607170686, 6.9112, 6.9112, 121.9260426156618, 1449113.146535606, 1449113.146535606, 303656.6564694942]
[2019-04-27 20:26:09,687] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 20:26:09,689] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [5.1391429e-23 1.0000000e+00 1.7250391e-23 1.4477713e-23 2.7891185e-09], sampled 0.9040818607330002
[2019-04-27 20:26:09,690] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1449113.146535606 W.
[2019-04-27 20:26:40,359] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.053302], dtype=float32), -0.031667806]
[2019-04-27 20:26:40,360] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [32.57711205666666, 61.707293055, 1.0, 2.0, 0.7430643694187528, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 846910.2928155975, 846910.2928155975, 184465.0901576923]
[2019-04-27 20:26:40,360] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 20:26:40,366] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [5.4734305e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.2080208e-30], sampled 0.8082180041868833
[2019-04-27 20:26:54,688] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.053302], dtype=float32), -0.031667806]
[2019-04-27 20:26:54,689] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [21.31163961, 84.93155463666666, 1.0, 2.0, 0.3891799728488668, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 482208.0557115662, 482208.0557115662, 127105.8278363211]
[2019-04-27 20:26:54,690] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 20:26:54,692] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [3.0558841e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5997260e-32], sampled 0.2583564484215872
[2019-04-27 20:26:58,161] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.053302], dtype=float32), -0.031667806]
[2019-04-27 20:26:58,162] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [20.7920795, 94.52842382, 1.0, 2.0, 0.4150677678354996, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 507843.3224652185, 507843.3224652185, 130599.7232406212]
[2019-04-27 20:26:58,164] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 20:26:58,168] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.7103205e-38], sampled 0.5961399667317838
[2019-04-27 20:27:02,613] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8773.3026 2171285057.2237 475.0000
[2019-04-27 20:27:02,786] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8131.3577 2447672248.6314 616.0000
[2019-04-27 20:27:02,830] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8711.8200 2197885428.4135 531.0000
[2019-04-27 20:27:02,928] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8922.3342 2121726735.7163 420.0000
[2019-04-27 20:27:03,021] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8597.9387 2250032148.3023 495.0000
[2019-04-27 20:27:04,037] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 1450000, evaluation results [1450000.0, 8131.357743000744, 2447672248.631405, 616.0, 8773.302648704419, 2171285057.223684, 475.0, 8922.334191568541, 2121726735.716273, 420.0, 8597.938749377028, 2250032148.3023014, 495.0, 8711.820046944746, 2197885428.4134703, 531.0]
[2019-04-27 20:27:04,217] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5316666e-32 1.0000000e+00 6.9100054e-33 3.9110581e-37 1.1853223e-20], sum to 1.0000
[2019-04-27 20:27:04,224] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9599
[2019-04-27 20:27:04,234] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.58333333333334, 47.0, 1.0, 2.0, 0.3578257849784743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 447561.500033996, 447561.500033996, 122920.2542690139], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6634200.0000, 
sim time next is 6634800.0000, 
raw observation next is [26.3, 48.0, 1.0, 2.0, 0.3564826877478128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 446203.4047953143, 446203.4047953143, 122746.5646018186], 
processed observation next is [1.0, 0.8260869565217391, 0.5296296296296297, 0.48, 1.0, 1.0, 0.23390796160453903, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1593583588554694, 0.1593583588554694, 0.23605108577272807], 
reward next is 0.7639, 
noisyNet noise sample is [array([0.9933027], dtype=float32), 0.4078322]. 
=============================================
[2019-04-27 20:27:05,600] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.9269181e-30 1.0000000e+00 1.3677472e-35 2.3929035e-33 2.8896395e-20], sum to 1.0000
[2019-04-27 20:27:05,610] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8826
[2019-04-27 20:27:05,614] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.45, 54.5, 1.0, 2.0, 0.3710558548280448, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 462399.5867539573, 462399.5867539573, 124671.8390542295], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6636600.0000, 
sim time next is 6637200.0000, 
raw observation next is [25.16666666666666, 56.66666666666666, 1.0, 2.0, 0.3772884279659198, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 469456.1042201903, 469456.1042201903, 125507.9751864744], 
processed observation next is [1.0, 0.8260869565217391, 0.4876543209876541, 0.5666666666666665, 1.0, 1.0, 0.2586766999594284, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1676628943643537, 0.1676628943643537, 0.24136149074322], 
reward next is 0.7586, 
noisyNet noise sample is [array([-0.40036675], dtype=float32), 0.7747514]. 
=============================================
[2019-04-27 20:27:13,613] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.7672356e-28 1.0000000e+00 2.1930133e-26 3.8976059e-31 3.1057648e-09], sum to 1.0000
[2019-04-27 20:27:13,625] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3553
[2019-04-27 20:27:13,631] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.51666666666667, 79.5, 1.0, 2.0, 0.4704832069002184, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 568352.763475807, 568352.7634758066, 138584.2949343426], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7084200.0000, 
sim time next is 7084800.0000, 
raw observation next is [23.5, 79.0, 1.0, 2.0, 0.4664300784977178, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 564259.8559146657, 564259.8559146657, 137993.7009940438], 
processed observation next is [1.0, 0.0, 0.42592592592592593, 0.79, 1.0, 1.0, 0.36479771249728304, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2015213771123806, 0.2015213771123806, 0.26537250191162265], 
reward next is 0.7346, 
noisyNet noise sample is [array([-0.3931762], dtype=float32), 1.8175428]. 
=============================================
[2019-04-27 20:27:19,574] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0735243e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 9.3644978e-25], sum to 1.0000
[2019-04-27 20:27:19,583] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6400
[2019-04-27 20:27:19,588] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 59.33333333333334, 1.0, 2.0, 0.4383704275454699, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 535919.550871799, 535919.550871799, 133971.8442861775], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6898800.0000, 
sim time next is 6899400.0000, 
raw observation next is [25.85, 60.0, 1.0, 2.0, 0.436948972285028, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 534386.5107737497, 534386.5107737492, 133768.4299294778], 
processed observation next is [0.0, 0.8695652173913043, 0.5129629629629631, 0.6, 1.0, 1.0, 0.3297011574821762, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.19085232527633916, 0.190852325276339, 0.2572469806336112], 
reward next is 0.7428, 
noisyNet noise sample is [array([-0.21515946], dtype=float32), -0.6006199]. 
=============================================
[2019-04-27 20:27:24,066] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.7801181e-29 1.0000000e+00 2.4293095e-31 1.2337375e-31 7.8288549e-21], sum to 1.0000
[2019-04-27 20:27:24,076] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7258
[2019-04-27 20:27:24,081] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.9, 75.0, 1.0, 2.0, 0.4203265166172646, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 517697.9021527995, 517697.902152799, 131445.5825471556], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7000800.0000, 
sim time next is 7001400.0000, 
raw observation next is [22.8, 75.5, 1.0, 2.0, 0.4191629596725802, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 516460.9060900605, 516460.9060900605, 131282.7020667031], 
processed observation next is [1.0, 0.0, 0.4, 0.755, 1.0, 1.0, 0.30852733294354784, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.18445032360359304, 0.18445032360359304, 0.25246673474365977], 
reward next is 0.7475, 
noisyNet noise sample is [array([0.617277], dtype=float32), -0.14642571]. 
=============================================
[2019-04-27 20:27:26,476] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [8.5197160e-31 2.1943076e-21 3.9732898e-27 2.0026199e-26 1.0000000e+00], sum to 1.0000
[2019-04-27 20:27:26,482] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5508
[2019-04-27 20:27:26,489] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.53333333333333, 78.66666666666667, 1.0, 2.0, 0.3256078631603195, 1.0, 2.0, 0.3256078631603195, 1.0, 2.0, 0.520832258905464, 6.911200000000001, 6.9112, 121.94756008, 1146569.1865997, 1146569.1865997, 265967.2023394789], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7032000.0000, 
sim time next is 7032600.0000, 
raw observation next is [23.7, 78.0, 1.0, 2.0, 0.3228666942934743, 1.0, 2.0, 0.3228666942934743, 1.0, 2.0, 0.5164112477219954, 6.9112, 6.9112, 121.94756008, 1136637.156822554, 1136637.156822554, 264882.4992414151], 
processed observation next is [1.0, 0.391304347826087, 0.4333333333333333, 0.78, 1.0, 1.0, 0.19388892177794562, 1.0, 1.0, 0.19388892177794562, 1.0, 1.0, 0.39551405965249425, 0.0, 0.0, 0.8096049824067558, 0.4059418417223407, 0.4059418417223407, 0.509389421618106], 
reward next is 0.4906, 
noisyNet noise sample is [array([-0.427424], dtype=float32), 0.05813898]. 
=============================================
[2019-04-27 20:27:28,246] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3680095e-33 1.0000000e+00 1.6948488e-34 1.9737669e-36 3.6177962e-24], sum to 1.0000
[2019-04-27 20:27:28,256] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3102
[2019-04-27 20:27:28,263] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.15, 79.0, 1.0, 2.0, 0.4112266081418879, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 507777.8279089141, 507777.8279089141, 130171.3598498505], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7097400.0000, 
sim time next is 7098000.0000, 
raw observation next is [22.03333333333333, 79.66666666666667, 1.0, 2.0, 0.4080152142950841, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 503993.7797441803, 503993.7797441799, 129718.1045314433], 
processed observation next is [1.0, 0.13043478260869565, 0.37160493827160485, 0.7966666666666667, 1.0, 1.0, 0.2952562074941477, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1799977784800644, 0.17999777848006426, 0.24945789332969864], 
reward next is 0.7505, 
noisyNet noise sample is [array([0.910423], dtype=float32), 0.06525775]. 
=============================================
[2019-04-27 20:27:28,282] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[69.77872 ]
 [69.91894 ]
 [70.10047 ]
 [70.279076]
 [70.40482 ]], R is [[69.76174164]
 [69.813797  ]
 [69.86478424]
 [69.90332031]
 [69.95269012]].
[2019-04-27 20:27:29,808] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.1218606e-31 1.0000000e+00 1.8634671e-33 5.5422966e-32 2.4416691e-21], sum to 1.0000
[2019-04-27 20:27:29,814] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4288
[2019-04-27 20:27:29,820] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.03333333333333, 75.0, 1.0, 2.0, 0.4882373178355869, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 599917.1255116487, 599917.1255116487, 141618.7240266437], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7093200.0000, 
sim time next is 7093800.0000, 
raw observation next is [22.9, 75.5, 1.0, 2.0, 0.442889532564898, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 544758.5145935109, 544758.5145935104, 134728.3488385898], 
processed observation next is [1.0, 0.08695652173913043, 0.4037037037037037, 0.755, 1.0, 1.0, 0.33677325305345, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.19455661235482533, 0.19455661235482516, 0.2590929785357496], 
reward next is 0.7409, 
noisyNet noise sample is [array([-1.6816902], dtype=float32), 0.6520707]. 
=============================================
[2019-04-27 20:27:33,300] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.4765324e-32 1.0000000e+00 3.3870705e-33 3.5569351e-35 3.4259242e-18], sum to 1.0000
[2019-04-27 20:27:33,310] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7384
[2019-04-27 20:27:33,315] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.81666666666667, 74.16666666666667, 1.0, 2.0, 0.3652759817639959, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 457063.5208351433, 457063.5208351429, 123923.8526025683], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7150200.0000, 
sim time next is 7150800.0000, 
raw observation next is [21.63333333333333, 75.33333333333334, 1.0, 2.0, 0.3657249767673954, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 457716.8219946089, 457716.8219946089, 123986.0397792484], 
processed observation next is [1.0, 0.782608695652174, 0.35679012345678995, 0.7533333333333334, 1.0, 1.0, 0.24491068662785168, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16347029356950318, 0.16347029356950318, 0.23843469188317], 
reward next is 0.7616, 
noisyNet noise sample is [array([-0.08876234], dtype=float32), -0.75326306]. 
=============================================
[2019-04-27 20:27:44,738] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.586040e-38 1.000000e+00 0.000000e+00 0.000000e+00 9.585608e-35], sum to 1.0000
[2019-04-27 20:27:44,749] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8002
[2019-04-27 20:27:44,753] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.2, 88.0, 1.0, 2.0, 0.3853233413976993, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 480945.9890144164, 480945.9890144159, 126643.2463385121], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7354800.0000, 
sim time next is 7355400.0000, 
raw observation next is [20.15, 88.66666666666667, 1.0, 2.0, 0.4213737330327731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 525698.4176843334, 525698.4176843329, 131745.7611064575], 
processed observation next is [1.0, 0.13043478260869565, 0.3018518518518518, 0.8866666666666667, 1.0, 1.0, 0.31115920599139657, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.18774943488726192, 0.18774943488726176, 0.25335723289703366], 
reward next is 0.7466, 
noisyNet noise sample is [array([0.9316008], dtype=float32), 0.29222384]. 
=============================================
[2019-04-27 20:27:52,477] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:27:52,477] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:27:52,547] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res14/Eplus-env-sub_run8
[2019-04-27 20:27:53,983] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:27:53,991] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9177
[2019-04-27 20:27:53,995] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 95.0, 1.0, 2.0, 0.4357045955477148, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 530869.5181663992, 530869.5181663992, 133528.3550181051], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7545600.0000, 
sim time next is 7546200.0000, 
raw observation next is [21.18333333333333, 94.33333333333334, 1.0, 2.0, 0.4393901362459653, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 534754.4986671219, 534754.4986671219, 134052.3347524344], 
processed observation next is [0.0, 0.34782608695652173, 0.34012345679012335, 0.9433333333333335, 1.0, 1.0, 0.33260730505472064, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1909837495239721, 0.1909837495239721, 0.2577929514469892], 
reward next is 0.7422, 
noisyNet noise sample is [array([-0.5207706], dtype=float32), -0.6925533]. 
=============================================
[2019-04-27 20:27:54,326] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:27:54,334] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8710
[2019-04-27 20:27:54,339] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666666, 66.0, 1.0, 2.0, 0.5349089928771333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 629577.981908564, 629577.981908564, 148157.1008815281], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7566600.0000, 
sim time next is 7567200.0000, 
raw observation next is [27.0, 66.0, 1.0, 2.0, 0.5265408018357839, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 621568.7504674372, 621568.7504674372, 146877.6023841393], 
processed observation next is [0.0, 0.6086956521739131, 0.5555555555555556, 0.66, 1.0, 1.0, 0.43635809742355225, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.22198883945265616, 0.22198883945265616, 0.2824569276618063], 
reward next is 0.7175, 
noisyNet noise sample is [array([0.47439113], dtype=float32), -0.6366393]. 
=============================================
[2019-04-27 20:27:55,562] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-27 20:27:55,564] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 20:27:55,565] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 20:27:55,566] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:27:55,567] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:27:55,567] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 20:27:55,569] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 20:27:55,571] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:27:55,571] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 20:27:55,574] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:27:55,574] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:27:55,590] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run60
[2019-04-27 20:27:55,614] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run60
[2019-04-27 20:27:55,637] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run60
[2019-04-27 20:27:55,637] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run60
[2019-04-27 20:27:55,639] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run60
[2019-04-27 20:28:00,994] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.0542655], dtype=float32), -0.032970455]
[2019-04-27 20:28:00,995] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [24.56666666666666, 41.66666666666667, 1.0, 2.0, 0.3226607747043918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 414999.2670004602, 414999.2670004602, 118416.5082991579]
[2019-04-27 20:28:00,995] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 20:28:00,999] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.32733476825958396
[2019-04-27 20:28:01,351] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0542655], dtype=float32), -0.032970455]
[2019-04-27 20:28:01,352] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [26.76230766166667, 28.82767674666667, 1.0, 2.0, 0.5195907948890658, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 670379.4354700348, 670379.4354700348, 144554.7562616993]
[2019-04-27 20:28:01,354] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 20:28:01,357] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.0275957e-37], sampled 0.05000426599961061
[2019-04-27 20:28:28,593] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0542655], dtype=float32), -0.032970455]
[2019-04-27 20:28:28,596] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [22.04500831166666, 98.109721025, 1.0, 2.0, 0.5174434230987561, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 614964.6737213132, 614964.6737213132, 145579.8620115738]
[2019-04-27 20:28:28,597] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 20:28:28,599] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.23829413744995653
[2019-04-27 20:29:16,708] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0542655], dtype=float32), -0.032970455]
[2019-04-27 20:29:16,710] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [29.43287542666667, 44.42275871666667, 1.0, 2.0, 0.7897325132606992, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260425973425, 957073.8047879667, 957073.8047879667, 196374.5906483707]
[2019-04-27 20:29:16,711] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 20:29:16,714] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [2.3475775e-34 1.0000000e+00 2.7027845e-36 8.2814156e-37 7.8038188e-26], sampled 0.15012593317114953
[2019-04-27 20:29:17,489] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0542655], dtype=float32), -0.032970455]
[2019-04-27 20:29:17,490] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [23.57125277666666, 84.68251614666666, 1.0, 2.0, 0.4911695374463579, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 586573.6176223696, 586573.6176223696, 141542.4499807755]
[2019-04-27 20:29:17,491] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 20:29:17,493] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.4085397e-35], sampled 0.08137279923079677
[2019-04-27 20:29:20,797] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0542655], dtype=float32), -0.032970455]
[2019-04-27 20:29:20,798] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [25.55, 88.0, 1.0, 2.0, 0.6321272297226812, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.9260421237655, 1435336.977900677, 1435336.977900677, 305316.6625747939]
[2019-04-27 20:29:20,799] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 20:29:20,802] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [3.9683455e-30 1.0000000e+00 3.4896889e-31 1.2079563e-31 1.5666157e-22], sampled 0.9397916046398873
[2019-04-27 20:29:20,803] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1435336.977900677 W.
[2019-04-27 20:29:21,739] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0542655], dtype=float32), -0.032970455]
[2019-04-27 20:29:21,740] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [26.66666666666666, 72.16666666666667, 1.0, 2.0, 0.5634329458317705, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 656966.0817344475, 656966.0817344475, 152601.5667105506]
[2019-04-27 20:29:21,741] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:29:21,745] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 5.047716e-35], sampled 0.3721878328755157
[2019-04-27 20:29:36,879] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0542655], dtype=float32), -0.032970455]
[2019-04-27 20:29:36,880] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [20.96666666666667, 96.0, 1.0, 2.0, 0.4383477626756594, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 533445.6106477823, 533445.6106477823, 133897.5954242102]
[2019-04-27 20:29:36,882] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 20:29:36,885] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9605828968710924
[2019-04-27 20:29:41,572] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8697.3760 2197031692.6988 561.0000
[2019-04-27 20:29:41,959] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8587.2416 2250129074.3510 523.0000
[2019-04-27 20:29:42,255] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8918.6739 2121236088.5844 427.0000
[2019-04-27 20:29:42,283] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8107.6276 2446837696.6177 684.0000
[2019-04-27 20:29:42,310] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8766.9118 2171400003.4898 485.0000
[2019-04-27 20:29:43,326] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1475000, evaluation results [1475000.0, 8107.627616431718, 2446837696.6177487, 684.0, 8766.91179276683, 2171400003.4898076, 485.0, 8918.673918112483, 2121236088.584359, 427.0, 8587.241558371703, 2250129074.3509645, 523.0, 8697.376004338132, 2197031692.698769, 561.0]
[2019-04-27 20:29:49,396] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7995715e-35], sum to 1.0000
[2019-04-27 20:29:49,405] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.0102200e-26 1.0000000e+00 2.6404471e-28 8.4103271e-28 2.0401895e-16], sum to 1.0000
[2019-04-27 20:29:49,409] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7047
[2019-04-27 20:29:49,412] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2154
[2019-04-27 20:29:49,415] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.8, 80.0, 1.0, 2.0, 0.322042622797934, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 408737.690563814, 408737.6905638136, 118330.6812394425], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7680600.0000, 
sim time next is 7681200.0000, 
raw observation next is [19.76666666666667, 80.66666666666667, 1.0, 2.0, 0.3230369751178911, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 409770.8072932655, 409770.8072932655, 118455.6877524351], 
processed observation next is [1.0, 0.9130434782608695, 0.2876543209876544, 0.8066666666666668, 1.0, 1.0, 0.19409163704510846, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.14634671689045195, 0.14634671689045195, 0.22779939952391365], 
reward next is 0.7722, 
noisyNet noise sample is [array([-1.2238045], dtype=float32), 0.12797327]. 
=============================================
[2019-04-27 20:29:49,422] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1428962.750925799 W.
[2019-04-27 20:29:49,429] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.3, 66.0, 1.0, 2.0, 0.9822144893503479, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.387413175428748, 6.9112, 121.9240629655979, 1428962.750925799, 1185103.001144507, 239677.7728058318], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7660800.0000, 
sim time next is 7661400.0000, 
raw observation next is [25.15, 66.0, 1.0, 2.0, 0.3641178767860743, 1.0, 1.0, 0.3641178767860743, 1.0, 1.0, 0.5836619937172054, 6.9112, 6.9112, 121.94756008, 1290551.234863942, 1290551.234863942, 281599.7465405963], 
processed observation next is [1.0, 0.6956521739130435, 0.487037037037037, 0.66, 1.0, 1.0, 0.2429974723643742, 1.0, 0.5, 0.2429974723643742, 1.0, 0.5, 0.4795774921465067, 0.0, 0.0, 0.8096049824067558, 0.4609111553085507, 0.4609111553085507, 0.5415379741165314], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.65193737], dtype=float32), 0.059900727]. 
=============================================
[2019-04-27 20:29:49,617] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:29:49,618] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:29:49,675] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res3/Eplus-env-sub_run8
[2019-04-27 20:29:52,979] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.3514969e-24 4.0057839e-11 2.1565992e-21 2.6791004e-23 1.0000000e+00], sum to 1.0000
[2019-04-27 20:29:52,985] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7816
[2019-04-27 20:29:52,989] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.26666666666667, 40.33333333333334, 1.0, 2.0, 0.3062738909546968, 1.0, 1.0, 0.3062738909546968, 1.0, 1.0, 0.5063296006125599, 6.911199999999999, 6.9112, 121.94756008, 1135280.268579455, 1135280.268579455, 256535.7776647368], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 40800.0000, 
sim time next is 41400.0000, 
raw observation next is [27.45, 40.0, 1.0, 2.0, 0.3122086759378303, 1.0, 2.0, 0.3122086759378303, 1.0, 2.0, 0.5128861232115624, 6.9112, 6.9112, 121.94756008, 1150272.789385713, 1150272.789385713, 259210.2999552235], 
processed observation next is [1.0, 0.4782608695652174, 0.5722222222222222, 0.4, 1.0, 1.0, 0.18120080468789324, 1.0, 1.0, 0.18120080468789324, 1.0, 1.0, 0.3911076540144529, 0.0, 0.0, 0.8096049824067558, 0.41081171049489745, 0.41081171049489745, 0.49848134606773753], 
reward next is 0.5015, 
noisyNet noise sample is [array([-0.02496809], dtype=float32), 1.8548878]. 
=============================================
[2019-04-27 20:29:56,212] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.5084793e-33 1.0000000e+00 1.6337904e-36 4.0596338e-37 3.0159946e-26], sum to 1.0000
[2019-04-27 20:29:56,225] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9333
[2019-04-27 20:29:56,231] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.4, 75.66666666666667, 1.0, 2.0, 0.4943296064675693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 611947.9465854232, 611947.9465854235, 142686.8147302137], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 96000.0000, 
sim time next is 96600.0000, 
raw observation next is [22.3, 75.83333333333333, 1.0, 2.0, 0.4756617071860541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 589501.9120654766, 589501.9120654766, 139790.4315709532], 
processed observation next is [1.0, 0.08695652173913043, 0.38148148148148153, 0.7583333333333333, 1.0, 1.0, 0.3757877466500645, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21053639716624165, 0.21053639716624165, 0.2688277530210638], 
reward next is 0.7312, 
noisyNet noise sample is [array([0.10489777], dtype=float32), 0.22824922]. 
=============================================
[2019-04-27 20:30:02,984] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:30:02,984] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:30:03,044] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res11/Eplus-env-sub_run8
[2019-04-27 20:30:03,910] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:30:03,911] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:30:03,926] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res12/Eplus-env-sub_run8
[2019-04-27 20:30:04,023] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:30:04,024] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:30:04,036] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res7/Eplus-env-sub_run8
[2019-04-27 20:30:04,126] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.70221787e-24 1.24297875e-14 5.92792847e-22 1.28366134e-25
 1.00000000e+00], sum to 1.0000
[2019-04-27 20:30:04,126] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2942
[2019-04-27 20:30:04,128] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 28.0, 1.0, 2.0, 0.4418025734883673, 1.0, 2.0, 0.4418025734883673, 1.0, 2.0, 0.7130310386019919, 6.911200000000001, 6.9112, 121.94756008, 1589900.29290329, 1589900.29290329, 315506.5331899864], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 482400.0000, 
sim time next is 483000.0000, 
raw observation next is [32.03333333333333, 27.66666666666667, 1.0, 2.0, 0.3855525951652035, 1.0, 2.0, 0.3855525951652035, 1.0, 2.0, 0.6232693433929646, 6.911199999999999, 6.9112, 121.94756008, 1391280.824853298, 1391280.824853299, 290271.2245570926], 
processed observation next is [1.0, 0.6086956521739131, 0.7419753086419753, 0.2766666666666667, 1.0, 1.0, 0.26851499424428993, 1.0, 1.0, 0.26851499424428993, 1.0, 1.0, 0.5290866792412057, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.49688600887617784, 0.4968860088761782, 0.5582138933790242], 
reward next is 0.4418, 
noisyNet noise sample is [array([-0.09288986], dtype=float32), -0.48127088]. 
=============================================
[2019-04-27 20:30:04,132] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[67.72072 ]
 [67.25093 ]
 [66.85286 ]
 [66.593445]
 [66.20723 ]], R is [[67.96818542]
 [67.6817627 ]
 [67.4009552 ]
 [67.12286377]
 [66.84602356]].
[2019-04-27 20:30:04,259] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:30:04,259] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:30:04,275] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:30:04,275] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:30:04,283] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res10/Eplus-env-sub_run8
[2019-04-27 20:30:04,299] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:30:04,301] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:30:04,306] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:30:04,307] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:30:04,316] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res4/Eplus-env-sub_run8
[2019-04-27 20:30:04,349] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res13/Eplus-env-sub_run8
[2019-04-27 20:30:04,389] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res16/Eplus-env-sub_run8
[2019-04-27 20:30:04,421] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:30:04,423] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:30:04,426] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res6/Eplus-env-sub_run8
[2019-04-27 20:30:04,549] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:30:04,549] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:30:04,559] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res17/Eplus-env-sub_run8
[2019-04-27 20:30:04,588] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:30:04,589] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:30:04,593] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res5/Eplus-env-sub_run8
[2019-04-27 20:30:04,711] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:30:04,712] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:30:04,719] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res9/Eplus-env-sub_run8
[2019-04-27 20:30:04,747] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:30:04,748] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:30:04,756] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res2/Eplus-env-sub_run8
[2019-04-27 20:30:04,822] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:30:04,824] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:30:04,827] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res8/Eplus-env-sub_run8
[2019-04-27 20:30:04,864] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:30:04,864] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:30:04,872] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res15/Eplus-env-sub_run8
[2019-04-27 20:30:10,035] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.1416353e-27 1.0000000e+00 5.3222905e-26 6.9713392e-29 2.7875824e-12], sum to 1.0000
[2019-04-27 20:30:10,043] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7248
[2019-04-27 20:30:10,049] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.8, 53.0, 1.0, 2.0, 0.4154943668464474, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 511019.7248349458, 511019.7248349458, 130731.8090467296], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 75600.0000, 
sim time next is 76200.0000, 
raw observation next is [26.63333333333334, 54.0, 1.0, 2.0, 0.4182460221308443, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 514231.149009096, 514231.149009096, 131122.6626470859], 
processed observation next is [1.0, 0.9130434782608695, 0.5419753086419755, 0.54, 1.0, 1.0, 0.30743574063195744, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.18365398178896286, 0.18365398178896286, 0.2521589666290113], 
reward next is 0.7478, 
noisyNet noise sample is [array([2.7137249], dtype=float32), -0.647258]. 
=============================================
[2019-04-27 20:30:16,970] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 4.240784e-37], sum to 1.0000
[2019-04-27 20:30:16,977] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8092
[2019-04-27 20:30:16,983] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.43333333333333, 58.0, 1.0, 2.0, 0.2490274391758767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 321223.7081762793, 321223.7081762788, 100365.7973080992], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 193200.0000, 
sim time next is 193800.0000, 
raw observation next is [20.61666666666667, 59.0, 1.0, 2.0, 0.2506257788732593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 323285.8617084854, 323285.8617084854, 104303.3450150214], 
processed observation next is [0.0, 0.21739130434782608, 0.319135802469136, 0.59, 1.0, 1.0, 0.10788783199197538, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.11545923632445908, 0.11545923632445908, 0.2005833557981181], 
reward next is 0.7994, 
noisyNet noise sample is [array([-0.6243278], dtype=float32), -1.3328576]. 
=============================================
[2019-04-27 20:30:23,309] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1373604e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5508606e-27], sum to 1.0000
[2019-04-27 20:30:23,317] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3673
[2019-04-27 20:30:23,325] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.93333333333333, 31.0, 1.0, 2.0, 0.3325545561061453, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 425608.33375291, 425608.33375291, 119694.2766720755], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 315600.0000, 
sim time next is 316200.0000, 
raw observation next is [27.96666666666667, 31.0, 1.0, 2.0, 0.3330331224214904, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 426119.9960390076, 426119.9960390081, 119756.2165729614], 
processed observation next is [0.0, 0.6521739130434783, 0.5913580246913581, 0.31, 1.0, 1.0, 0.20599181240653622, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.15218571287107416, 0.15218571287107432, 0.23030041648646424], 
reward next is 0.7697, 
noisyNet noise sample is [array([-0.36704668], dtype=float32), -0.33204642]. 
=============================================
[2019-04-27 20:30:27,170] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.9003970e-26 4.1874516e-12 3.4885051e-23 6.0207422e-25 1.0000000e+00], sum to 1.0000
[2019-04-27 20:30:27,175] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7385
[2019-04-27 20:30:27,181] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.86666666666667, 26.16666666666667, 1.0, 2.0, 0.2662741589378029, 1.0, 2.0, 0.2662741589378029, 1.0, 2.0, 0.4458501033268978, 6.911199999999999, 6.9112, 121.94756008, 996991.6145086075, 996991.614508608, 240810.3312358508], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 391800.0000, 
sim time next is 392400.0000, 
raw observation next is [30.0, 26.0, 1.0, 2.0, 0.2439615742073713, 1.0, 2.0, 0.2439615742073713, 1.0, 2.0, 0.4085536910664296, 6.9112, 6.9112, 121.94756008, 913499.480945295, 913499.480945295, 232836.4491868544], 
processed observation next is [1.0, 0.5652173913043478, 0.6666666666666666, 0.26, 1.0, 1.0, 0.09995425500877536, 1.0, 1.0, 0.09995425500877536, 1.0, 1.0, 0.2606921138330369, 0.0, 0.0, 0.8096049824067558, 0.32624981462331964, 0.32624981462331964, 0.4477624022824123], 
reward next is 0.5522, 
noisyNet noise sample is [array([1.1962113], dtype=float32), 0.9021377]. 
=============================================
[2019-04-27 20:30:35,022] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.5689646e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.8171204e-33], sum to 1.0000
[2019-04-27 20:30:35,028] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6565
[2019-04-27 20:30:35,032] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.5, 77.0, 1.0, 2.0, 0.2951550912612306, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 377177.9315004048, 377177.9315004048, 114980.12256691], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 537000.0000, 
sim time next is 537600.0000, 
raw observation next is [19.7, 76.0, 1.0, 2.0, 0.2937149957632992, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 375107.6419252217, 375107.6419252217, 114803.2294244522], 
processed observation next is [1.0, 0.21739130434782608, 0.28518518518518515, 0.76, 1.0, 1.0, 0.15918451876583237, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.13396701497329347, 0.13396701497329347, 0.22077544120086962], 
reward next is 0.7792, 
noisyNet noise sample is [array([-1.2270067], dtype=float32), -0.6484341]. 
=============================================
[2019-04-27 20:30:35,195] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.1144345e-37 1.0000000e+00 2.6817688e-38 0.0000000e+00 2.2372507e-31], sum to 1.0000
[2019-04-27 20:30:35,207] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9270
[2019-04-27 20:30:35,210] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.11666666666667, 73.5, 1.0, 2.0, 0.3703968605895649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 472470.4378310322, 472470.4378310322, 124715.391196053], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 529800.0000, 
sim time next is 530400.0000, 
raw observation next is [20.03333333333333, 74.0, 1.0, 2.0, 0.3421959109767498, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 436549.7898222028, 436549.7898222023, 120947.7619206089], 
processed observation next is [1.0, 0.13043478260869565, 0.2975308641975308, 0.74, 1.0, 1.0, 0.21689989401994023, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1559106392222153, 0.1559106392222151, 0.2325918498473248], 
reward next is 0.7674, 
noisyNet noise sample is [array([0.4823995], dtype=float32), 0.46171525]. 
=============================================
[2019-04-27 20:30:35,301] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-27 20:30:35,303] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 20:30:35,303] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 20:30:35,304] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:30:35,304] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 20:30:35,305] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 20:30:35,306] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:30:35,307] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:30:35,307] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 20:30:35,308] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:30:35,309] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:30:35,538] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run61
[2019-04-27 20:30:35,544] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run61
[2019-04-27 20:30:35,574] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run61
[2019-04-27 20:30:35,574] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run61
[2019-04-27 20:30:35,661] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run61
[2019-04-27 20:31:31,529] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04660839], dtype=float32), -0.03334331]
[2019-04-27 20:31:31,531] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [22.0, 94.00000000000001, 1.0, 2.0, 0.627080098413594, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 752577.7301421787, 752577.7301421787, 164455.3233472275]
[2019-04-27 20:31:31,532] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 20:31:31,535] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [2.1479282e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.4692203e-33], sampled 0.15770885014412173
[2019-04-27 20:31:34,438] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04660839], dtype=float32), -0.03334331]
[2019-04-27 20:31:34,439] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [24.23333333333333, 93.00000000000001, 1.0, 2.0, 0.5852225825512963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 674351.2441102957, 674351.2441102957, 155915.6219049625]
[2019-04-27 20:31:34,440] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:31:34,442] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [3.6178393e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8687696e-34], sampled 0.571717381636679
[2019-04-27 20:31:49,983] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04660839], dtype=float32), -0.03334331]
[2019-04-27 20:31:49,983] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [24.32820590666667, 94.11599684333333, 1.0, 2.0, 0.6216428498584153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 710351.3888187599, 710351.3888187599, 161934.0586193235]
[2019-04-27 20:31:49,984] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 20:31:49,988] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [8.3177254e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 9.8600209e-34], sampled 0.5279627759896228
[2019-04-27 20:32:07,225] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04660839], dtype=float32), -0.03334331]
[2019-04-27 20:32:07,225] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [27.0, 74.0, 1.0, 2.0, 0.5830127461409147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 671544.6953761237, 671544.6953761237, 155527.8808406872]
[2019-04-27 20:32:07,226] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 20:32:07,229] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [9.9881079e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.1305181e-33], sampled 0.5357439273360027
[2019-04-27 20:32:21,877] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8775.4179 2171190867.2552 474.0000
[2019-04-27 20:32:22,065] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8144.3583 2447547612.5717 588.0000
[2019-04-27 20:32:22,233] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8924.0670 2121553212.9685 419.0000
[2019-04-27 20:32:22,245] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8712.0258 2197521482.0443 532.0000
[2019-04-27 20:32:22,272] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8604.6939 2249514166.5047 482.0000
[2019-04-27 20:32:23,288] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1500000, evaluation results [1500000.0, 8144.358336388837, 2447547612.571684, 588.0, 8775.417904430518, 2171190867.255234, 474.0, 8924.066989256533, 2121553212.968518, 419.0, 8604.693858827324, 2249514166.5047436, 482.0, 8712.025757958023, 2197521482.044251, 532.0]
[2019-04-27 20:32:33,514] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.1774640e-27 1.0000000e+00 6.8084452e-29 1.3325858e-29 3.3958514e-20], sum to 1.0000
[2019-04-27 20:32:33,524] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0045
[2019-04-27 20:32:33,528] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.1, 54.00000000000001, 1.0, 2.0, 0.4100490740951208, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 518679.3911358484, 518679.3911358484, 130222.5151801996], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 717600.0000, 
sim time next is 718200.0000, 
raw observation next is [24.2, 54.0, 1.0, 2.0, 0.40998944360115, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 518134.2455291689, 518134.2455291689, 130208.9639223938], 
processed observation next is [1.0, 0.30434782608695654, 0.45185185185185184, 0.54, 1.0, 1.0, 0.29760648047755955, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.18504794483184606, 0.18504794483184606, 0.25040185369691115], 
reward next is 0.7496, 
noisyNet noise sample is [array([-0.23665564], dtype=float32), 0.17975254]. 
=============================================
[2019-04-27 20:32:38,852] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.2823271e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.4137918e-30], sum to 1.0000
[2019-04-27 20:32:38,858] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0343
[2019-04-27 20:32:38,862] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.73333333333333, 36.0, 1.0, 2.0, 0.4399085733618613, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 535531.9123863152, 535531.9123863152, 134132.9833957125], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 830400.0000, 
sim time next is 831000.0000, 
raw observation next is [31.76666666666667, 36.0, 1.0, 2.0, 0.4412219809081736, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 536909.3928697594, 536909.3928697594, 134320.1077467447], 
processed observation next is [0.0, 0.6086956521739131, 0.7320987654320988, 0.36, 1.0, 1.0, 0.3347880725097305, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.19175335459634263, 0.19175335459634263, 0.25830789951297056], 
reward next is 0.7417, 
noisyNet noise sample is [array([0.6185411], dtype=float32), 0.19480488]. 
=============================================
[2019-04-27 20:32:38,879] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[79.174835]
 [79.14813 ]
 [79.13162 ]
 [79.10524 ]
 [79.083984]], R is [[79.17450714]
 [79.12480927]
 [79.0758667 ]
 [79.02760315]
 [78.97969055]].
[2019-04-27 20:32:40,042] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.8062798e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.6427193e-28], sum to 1.0000
[2019-04-27 20:32:40,054] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2790
[2019-04-27 20:32:40,064] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.13333333333333, 38.0, 1.0, 2.0, 0.4119977418478685, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 507559.7391736138, 507559.7391736142, 130252.7372543693], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 844800.0000, 
sim time next is 845400.0000, 
raw observation next is [29.91666666666666, 38.5, 1.0, 2.0, 0.4096416569494263, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 505122.4370312027, 505122.4370312031, 129928.2414962906], 
processed observation next is [0.0, 0.782608695652174, 0.66358024691358, 0.385, 1.0, 1.0, 0.297192448749317, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1804008703682867, 0.18040087036828684, 0.24986200287748192], 
reward next is 0.7501, 
noisyNet noise sample is [array([-0.7256331], dtype=float32), -0.015808703]. 
=============================================
[2019-04-27 20:32:44,439] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.2707387e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.8931007e-29], sum to 1.0000
[2019-04-27 20:32:44,450] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0039
[2019-04-27 20:32:44,454] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.9, 52.33333333333333, 1.0, 2.0, 0.3543339989158085, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 445818.808018348, 445818.808018348, 122496.3886468374], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 938400.0000, 
sim time next is 939000.0000, 
raw observation next is [24.8, 52.16666666666667, 1.0, 2.0, 0.3501506132553511, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 441155.3507295502, 441155.3507295502, 121949.3028163029], 
processed observation next is [0.0, 0.8695652173913043, 0.4740740740740741, 0.5216666666666667, 1.0, 1.0, 0.22636977768494176, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.15755548240341077, 0.15755548240341077, 0.2345178900313517], 
reward next is 0.7655, 
noisyNet noise sample is [array([0.02057131], dtype=float32), -0.26724437]. 
=============================================
[2019-04-27 20:32:44,479] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[75.35136 ]
 [75.36683 ]
 [75.389114]
 [75.41292 ]
 [75.436   ]], R is [[75.34508514]
 [75.35606384]
 [75.36579132]
 [75.3744812 ]
 [75.38237   ]].
[2019-04-27 20:32:50,792] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.3099935e-29 1.0000000e+00 2.8807196e-31 1.2996412e-31 1.4962097e-15], sum to 1.0000
[2019-04-27 20:32:50,801] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6978
[2019-04-27 20:32:50,809] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.4, 68.0, 1.0, 2.0, 0.3220427310771429, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 408761.2545186008, 408761.2545186008, 118330.7499411868], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1110600.0000, 
sim time next is 1111200.0000, 
raw observation next is [21.26666666666667, 68.33333333333334, 1.0, 2.0, 0.320238882137346, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 406811.75589362, 406811.75589362, 118103.6145758228], 
processed observation next is [1.0, 0.8695652173913043, 0.34320987654320995, 0.6833333333333335, 1.0, 1.0, 0.19076057397303098, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.14528991281915002, 0.14528991281915002, 0.22712233572273613], 
reward next is 0.7729, 
noisyNet noise sample is [array([-1.2410623], dtype=float32), 0.31231758]. 
=============================================
[2019-04-27 20:32:52,695] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2180926e-27 9.9489327e-17 2.6495826e-25 2.3221859e-25 1.0000000e+00], sum to 1.0000
[2019-04-27 20:32:52,701] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1269
[2019-04-27 20:32:52,704] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.3, 51.0, 1.0, 2.0, 0.2634123868484958, 1.0, 2.0, 0.2634123868484958, 1.0, 2.0, 0.4382274372779671, 6.9112, 6.9112, 121.94756008, 981516.1334076115, 981516.1334076115, 240135.6978896918], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1076400.0000, 
sim time next is 1077000.0000, 
raw observation next is [24.45, 50.33333333333334, 1.0, 2.0, 0.2311115969682974, 1.0, 2.0, 0.2311115969682974, 1.0, 2.0, 0.3847793052930208, 6.9112, 6.9112, 121.94756008, 861601.6200885267, 861601.6200885267, 228698.8465877795], 
processed observation next is [1.0, 0.4782608695652174, 0.4611111111111111, 0.5033333333333334, 1.0, 1.0, 0.08465666305749689, 1.0, 1.0, 0.08465666305749689, 1.0, 1.0, 0.23097413161627597, 0.0, 0.0, 0.8096049824067558, 0.30771486431733097, 0.30771486431733097, 0.4398054742072683], 
reward next is 0.5602, 
noisyNet noise sample is [array([0.80749625], dtype=float32), 0.29451606]. 
=============================================
[2019-04-27 20:32:52,718] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[68.800514]
 [68.85024 ]
 [68.71549 ]
 [68.33799 ]
 [68.73113 ]], R is [[68.70523071]
 [68.55638123]
 [68.41888428]
 [68.27876282]
 [68.13165283]].
[2019-04-27 20:32:53,999] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.6992453e-27 9.9525124e-11 2.5905190e-23 1.9460313e-27 1.0000000e+00], sum to 1.0000
[2019-04-27 20:32:54,010] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2771
[2019-04-27 20:32:54,013] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.6, 43.0, 1.0, 2.0, 0.3149483755818846, 1.0, 2.0, 0.3149483755818846, 1.0, 2.0, 0.5172582926447592, 6.911199999999999, 6.9112, 121.94756008, 1160075.910800423, 1160075.910800424, 260295.2229053774], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1092600.0000, 
sim time next is 1093200.0000, 
raw observation next is [26.56666666666666, 43.33333333333334, 1.0, 2.0, 0.3267472351437339, 1.0, 2.0, 0.3267472351437339, 1.0, 2.0, 0.5363276966280126, 6.911199999999999, 6.9112, 121.94756008, 1202846.228337202, 1202846.228337203, 264987.0252256548], 
processed observation next is [1.0, 0.6521739130434783, 0.5395061728395059, 0.4333333333333334, 1.0, 1.0, 0.19850861326634986, 1.0, 1.0, 0.19850861326634986, 1.0, 1.0, 0.4204096207850157, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.42958793869185785, 0.4295879386918582, 0.5095904331262593], 
reward next is 0.4904, 
noisyNet noise sample is [array([-0.98983705], dtype=float32), -0.43770537]. 
=============================================
[2019-04-27 20:33:01,980] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.6183469e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3917488e-33], sum to 1.0000
[2019-04-27 20:33:01,987] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9516
[2019-04-27 20:33:01,993] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.96666666666667, 88.33333333333334, 1.0, 2.0, 0.3367154523185307, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 426407.3404886673, 426407.3404886673, 120211.8262000424], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1237200.0000, 
sim time next is 1237800.0000, 
raw observation next is [19.03333333333333, 88.16666666666667, 1.0, 2.0, 0.3339865596203327, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 422739.6716642681, 422739.6716642686, 119855.5469762064], 
processed observation next is [1.0, 0.30434782608695654, 0.26049382716049374, 0.8816666666666667, 1.0, 1.0, 0.20712685669087225, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.15097845416581004, 0.1509784541658102, 0.23049143649270462], 
reward next is 0.7695, 
noisyNet noise sample is [array([0.22212344], dtype=float32), 1.3423463]. 
=============================================
[2019-04-27 20:33:08,557] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.8775744e-32 1.0000000e+00 9.8201142e-35 6.4072326e-34 1.9867988e-20], sum to 1.0000
[2019-04-27 20:33:08,571] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4778
[2019-04-27 20:33:08,576] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.03333333333333, 59.0, 1.0, 2.0, 0.3617843355449866, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 453667.6622166314, 453667.662216631, 123470.02657682], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1374600.0000, 
sim time next is 1375200.0000, 
raw observation next is [23.8, 60.0, 1.0, 2.0, 0.3591863863820412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 450700.817990955, 450700.817990955, 123126.1839754896], 
processed observation next is [1.0, 0.9565217391304348, 0.43703703703703706, 0.6, 1.0, 1.0, 0.23712665045481096, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16096457785391252, 0.16096457785391252, 0.2367811230297877], 
reward next is 0.7632, 
noisyNet noise sample is [array([-1.6663013], dtype=float32), -0.6471853]. 
=============================================
[2019-04-27 20:33:12,218] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:33:12,228] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0482
[2019-04-27 20:33:12,235] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.83333333333333, 31.66666666666667, 1.0, 2.0, 0.347349274558854, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 437305.0886046739, 437305.0886046734, 121574.9671783055], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1417800.0000, 
sim time next is 1418400.0000, 
raw observation next is [30.0, 31.0, 1.0, 2.0, 0.3494005611689667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 440013.3892923219, 440013.3892923219, 121847.5335296563], 
processed observation next is [0.0, 0.43478260869565216, 0.6666666666666666, 0.31, 1.0, 1.0, 0.22547685853448413, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1571476390329721, 0.1571476390329721, 0.23432217986472365], 
reward next is 0.7657, 
noisyNet noise sample is [array([0.2702367], dtype=float32), -1.2596256]. 
=============================================
[2019-04-27 20:33:14,374] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:33:14,377] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5664
[2019-04-27 20:33:14,383] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.48333333333333, 62.16666666666666, 1.0, 2.0, 0.3311116828444315, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 419803.8563129078, 419803.8563129078, 119490.3100633562], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1479000.0000, 
sim time next is 1479600.0000, 
raw observation next is [22.4, 63.0, 1.0, 2.0, 0.3326222642079921, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 421497.0749684218, 421497.0749684214, 119683.5262104917], 
processed observation next is [0.0, 0.13043478260869565, 0.38518518518518513, 0.63, 1.0, 1.0, 0.2055026954857049, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.15053466963157922, 0.15053466963157908, 0.23016062732786866], 
reward next is 0.7698, 
noisyNet noise sample is [array([-0.42296082], dtype=float32), 1.1659733]. 
=============================================
[2019-04-27 20:33:15,003] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-27 20:33:15,003] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 20:33:15,004] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 20:33:15,004] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:33:15,005] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:33:15,007] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 20:33:15,008] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 20:33:15,006] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 20:33:15,010] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:33:15,011] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:33:15,012] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:33:15,027] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run62
[2019-04-27 20:33:15,050] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run62
[2019-04-27 20:33:15,074] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run62
[2019-04-27 20:33:15,094] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run62
[2019-04-27 20:33:15,117] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run62
[2019-04-27 20:33:31,267] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04836715], dtype=float32), -0.037591115]
[2019-04-27 20:33:31,268] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [38.0, 15.66666666666667, 1.0, 2.0, 0.2873506137649276, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4706033172223126, 6.911199999999999, 6.9112, 121.9260426156618, 703271.0757521755, 703271.075752176, 189513.0462630582]
[2019-04-27 20:33:31,270] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 20:33:31,274] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [9.1946782e-36 1.0000000e+00 1.9167283e-37 1.9439356e-38 3.9797842e-26], sampled 0.8724072327737203
[2019-04-27 20:33:42,994] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04836715], dtype=float32), -0.037591115]
[2019-04-27 20:33:42,996] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [22.7, 97.33333333333333, 1.0, 2.0, 0.544921666062068, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 640818.9780656846, 640818.9780656846, 149771.3417012416]
[2019-04-27 20:33:42,997] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 20:33:43,001] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.44349082375809834
[2019-04-27 20:34:10,843] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04836715], dtype=float32), -0.037591115]
[2019-04-27 20:34:10,845] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [25.06666666666667, 78.33333333333334, 1.0, 2.0, 0.5553034366926217, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 654210.4289146812, 654210.4289146812, 151534.5712762493]
[2019-04-27 20:34:10,846] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 20:34:10,851] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.08879897867400144
[2019-04-27 20:34:37,757] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04836715], dtype=float32), -0.037591115]
[2019-04-27 20:34:37,759] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [27.0, 77.0, 1.0, 2.0, 0.6066703486339735, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 692115.3257542483, 692115.3257542483, 159265.0796237434]
[2019-04-27 20:34:37,761] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:34:37,763] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9904962343444643
[2019-04-27 20:34:38,254] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04836715], dtype=float32), -0.037591115]
[2019-04-27 20:34:38,256] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [23.1, 92.5, 1.0, 2.0, 0.5213249663963282, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 615437.9094633323, 615437.9094633323, 146040.2138842786]
[2019-04-27 20:34:38,258] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:34:38,260] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6899070671813566
[2019-04-27 20:34:42,551] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04836715], dtype=float32), -0.037591115]
[2019-04-27 20:34:42,552] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [30.599553325, 35.42952155166667, 1.0, 2.0, 0.8438114856700436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 1039559.501984576, 1039559.501984575, 208486.1504948535]
[2019-04-27 20:34:42,553] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 20:34:42,557] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.2827179e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.1333682e-36], sampled 0.29343598427083006
[2019-04-27 20:34:45,037] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04836715], dtype=float32), -0.037591115]
[2019-04-27 20:34:45,038] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [24.78333333333333, 84.33333333333334, 1.0, 2.0, 0.5575943502114764, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 650383.64544912, 650383.6454491195, 151639.2070160757]
[2019-04-27 20:34:45,040] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:34:45,042] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.524334895818059
[2019-04-27 20:35:00,161] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8698.7717 2196523229.9552 560.0000
[2019-04-27 20:35:00,428] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8769.0859 2171249438.9273 485.0000
[2019-04-27 20:35:00,598] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8108.0190 2446269701.7249 693.0000
[2019-04-27 20:35:00,782] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8920.3643 2121040542.3671 430.0000
[2019-04-27 20:35:01,194] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8587.8748 2249874313.5943 527.0000
[2019-04-27 20:35:02,212] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1525000, evaluation results [1525000.0, 8108.019035671175, 2446269701.7248893, 693.0, 8769.085945532634, 2171249438.9273496, 485.0, 8920.364337450028, 2121040542.3670866, 430.0, 8587.87477560312, 2249874313.594255, 527.0, 8698.771722404023, 2196523229.95517, 560.0]
[2019-04-27 20:35:06,045] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.1249033e-35 1.0000000e+00 2.1972789e-38 0.0000000e+00 1.5662762e-28], sum to 1.0000
[2019-04-27 20:35:06,054] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4680
[2019-04-27 20:35:06,061] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 47.5, 1.0, 2.0, 0.3550584062810239, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 445926.6518210773, 445926.6518210773, 122581.2039178194], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1625400.0000, 
sim time next is 1626000.0000, 
raw observation next is [25.96666666666667, 48.0, 1.0, 2.0, 0.3543099084362493, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 445111.2934274951, 445111.2934274951, 122483.4374607933], 
processed observation next is [1.0, 0.8260869565217391, 0.517283950617284, 0.48, 1.0, 1.0, 0.23132131956696347, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.15896831908124825, 0.15896831908124825, 0.23554507203998712], 
reward next is 0.7645, 
noisyNet noise sample is [array([-1.4270614], dtype=float32), 1.6553538]. 
=============================================
[2019-04-27 20:35:06,080] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[71.31663 ]
 [71.18615 ]
 [71.03681 ]
 [70.95513 ]
 [70.816986]], R is [[71.50576782]
 [71.55497742]
 [71.60367584]
 [71.65209198]
 [71.70022583]].
[2019-04-27 20:35:09,032] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3996893e-30 1.0000000e+00 4.2813315e-33 4.4055379e-34 8.6786286e-27], sum to 1.0000
[2019-04-27 20:35:09,040] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0205
[2019-04-27 20:35:09,044] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.7, 81.0, 1.0, 2.0, 0.6943178688492355, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 879710.0721332314, 879710.0721332314, 178125.2616745973], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1679400.0000, 
sim time next is 1680000.0000, 
raw observation next is [19.76666666666667, 80.66666666666666, 1.0, 2.0, 0.6327327726035314, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 801544.2244172244, 801544.224417224, 166524.9951579104], 
processed observation next is [1.0, 0.43478260869565216, 0.2876543209876544, 0.8066666666666665, 1.0, 1.0, 0.5627771102422994, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.286265794434723, 0.28626579443472283, 0.32024037530367383], 
reward next is 0.6798, 
noisyNet noise sample is [array([-0.46734437], dtype=float32), 1.1735336]. 
=============================================
[2019-04-27 20:35:09,076] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[58.500134]
 [58.618877]
 [58.697865]
 [59.262928]
 [59.398754]], R is [[58.60042191]
 [58.67187119]
 [58.73915482]
 [58.80208206]
 [58.91498947]].
[2019-04-27 20:35:09,416] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.533513e-23 1.000000e+00 5.689313e-24 9.040677e-27 1.015984e-15], sum to 1.0000
[2019-04-27 20:35:09,426] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4059
[2019-04-27 20:35:09,438] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1315278.805954539 W.
[2019-04-27 20:35:09,444] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.6, 42.66666666666667, 1.0, 2.0, 0.9250036058617738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.224454933105038, 6.9112, 121.9246737379867, 1315278.805954539, 1154866.082788824, 227288.5293132474], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1610400.0000, 
sim time next is 1611000.0000, 
raw observation next is [27.65, 42.5, 1.0, 2.0, 0.5037009554000872, 0.0, 2.0, 0.0, 1.0, 1.0, 0.8318305994419412, 6.911199999999999, 6.9112, 121.925853956854, 1243617.467125646, 1243617.467125646, 254537.1263412042], 
processed observation next is [1.0, 0.6521739130434783, 0.5796296296296296, 0.425, 1.0, 1.0, 0.40916780404772285, 0.0, 1.0, -0.1904761904761905, 1.0, 0.5, 0.7897882493024263, -8.881784197001253e-17, 0.0, 0.8094608763218435, 0.44414909540201647, 0.44414909540201647, 0.48949447373308497], 
reward next is 0.5105, 
noisyNet noise sample is [array([0.6190164], dtype=float32), -0.38935167]. 
=============================================
[2019-04-27 20:35:09,460] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[51.16987 ]
 [50.557713]
 [49.92471 ]
 [49.704945]
 [50.22227 ]], R is [[51.43513107]
 [50.92078018]
 [50.4115715 ]
 [50.37716293]
 [49.8733902 ]].
[2019-04-27 20:35:13,836] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0516126e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.3969687e-36], sum to 1.0000
[2019-04-27 20:35:13,844] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7021
[2019-04-27 20:35:13,849] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 73.0, 1.0, 2.0, 0.5790951217988284, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 724269.5517084831, 724269.5517084831, 156841.5490449847], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1688400.0000, 
sim time next is 1689000.0000, 
raw observation next is [22.18333333333334, 72.33333333333334, 1.0, 2.0, 0.5879524353584642, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 734554.0014198081, 734554.0014198077, 158366.0820737993], 
processed observation next is [1.0, 0.5652173913043478, 0.37716049382716077, 0.7233333333333334, 1.0, 1.0, 0.5094671849505527, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.26234071479278864, 0.2623407147927885, 0.30455015783422945], 
reward next is 0.6954, 
noisyNet noise sample is [array([-0.7912486], dtype=float32), 0.30363345]. 
=============================================
[2019-04-27 20:35:13,866] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[57.90033 ]
 [57.564346]
 [57.18316 ]
 [56.714584]
 [55.998734]], R is [[58.28541946]
 [58.40094757]
 [58.51830292]
 [58.63601303]
 [58.75562286]].
[2019-04-27 20:35:14,185] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0979335e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.4926994e-38], sum to 1.0000
[2019-04-27 20:35:14,192] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7608
[2019-04-27 20:35:14,199] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.75, 70.0, 1.0, 2.0, 0.4007846838954787, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 493183.4229695576, 493183.4229695576, 128648.0167200488], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1704600.0000, 
sim time next is 1705200.0000, 
raw observation next is [23.7, 70.66666666666667, 1.0, 2.0, 0.4055112561074348, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 498661.612519639, 498661.612519639, 129306.9179825865], 
processed observation next is [1.0, 0.7391304347826086, 0.4333333333333333, 0.7066666666666667, 1.0, 1.0, 0.29227530488980336, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.17809343304272823, 0.17809343304272823, 0.24866714996651249], 
reward next is 0.7513, 
noisyNet noise sample is [array([0.75670767], dtype=float32), -0.2955713]. 
=============================================
[2019-04-27 20:35:19,825] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.4836096e-24 2.3615706e-12 1.8243032e-19 2.0299581e-23 1.0000000e+00], sum to 1.0000
[2019-04-27 20:35:19,835] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5633
[2019-04-27 20:35:19,843] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.63333333333333, 58.83333333333334, 1.0, 2.0, 0.4644149445900548, 1.0, 2.0, 0.4644149445900548, 1.0, 2.0, 0.7396527647685952, 6.9112, 6.9112, 121.94756008, 1598349.637994189, 1598349.637994189, 326514.3198331781], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1781400.0000, 
sim time next is 1782000.0000, 
raw observation next is [28.0, 58.0, 1.0, 2.0, 0.4634240737218908, 1.0, 2.0, 0.4634240737218908, 1.0, 2.0, 0.7378709198012642, 6.9112, 6.9112, 121.94756008, 1588942.195140454, 1588942.195140454, 326006.5780560717], 
processed observation next is [1.0, 0.6521739130434783, 0.5925925925925926, 0.58, 1.0, 1.0, 0.3612191353832034, 1.0, 1.0, 0.3612191353832034, 1.0, 1.0, 0.6723386497515801, 0.0, 0.0, 0.8096049824067558, 0.567479355407305, 0.567479355407305, 0.6269357270309072], 
reward next is 0.3731, 
noisyNet noise sample is [array([1.8045927], dtype=float32), 0.19214411]. 
=============================================
[2019-04-27 20:35:19,864] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[64.88783 ]
 [64.09977 ]
 [62.923393]
 [61.81105 ]
 [61.88401 ]], R is [[64.7908783 ]
 [64.5150528 ]
 [64.25124359]
 [63.60873032]
 [62.97264481]].
[2019-04-27 20:35:32,983] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.3510436e-36 1.0000000e+00 3.6975073e-38 0.0000000e+00 4.7255507e-34], sum to 1.0000
[2019-04-27 20:35:32,991] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4716
[2019-04-27 20:35:32,997] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.35, 64.66666666666667, 1.0, 2.0, 0.5211466575179315, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 614769.9499536863, 614769.9499536863, 145993.2910345082], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2034600.0000, 
sim time next is 2035200.0000, 
raw observation next is [27.5, 64.33333333333334, 1.0, 2.0, 0.5250595564329001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 618403.9938132049, 618403.9938132049, 146581.8996114313], 
processed observation next is [0.0, 0.5652173913043478, 0.5740740740740741, 0.6433333333333334, 1.0, 1.0, 0.43459471003916683, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.22085856921900177, 0.22085856921900177, 0.2818882684835217], 
reward next is 0.7181, 
noisyNet noise sample is [array([0.79340374], dtype=float32), 0.53123665]. 
=============================================
[2019-04-27 20:35:36,795] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 3.366765e-35], sum to 1.0000
[2019-04-27 20:35:36,801] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1002
[2019-04-27 20:35:36,809] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.23333333333333, 89.33333333333334, 1.0, 2.0, 0.461499729708419, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 557604.3524556154, 557604.3524556154, 137225.1633563873], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2100000.0000, 
sim time next is 2100600.0000, 
raw observation next is [22.35, 89.0, 1.0, 2.0, 0.464609476300296, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 560661.0847080282, 560661.0847080282, 137672.3522424626], 
processed observation next is [0.0, 0.30434782608695654, 0.38333333333333336, 0.89, 1.0, 1.0, 0.36263032892892383, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.20023610168143866, 0.20023610168143866, 0.2647545235431973], 
reward next is 0.7352, 
noisyNet noise sample is [array([0.6292224], dtype=float32), -0.85778016]. 
=============================================
[2019-04-27 20:35:48,826] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4836371e-24 4.0784141e-07 1.6846216e-23 2.6142827e-24 9.9999964e-01], sum to 1.0000
[2019-04-27 20:35:48,834] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2368
[2019-04-27 20:35:48,842] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.18333333333333, 67.5, 1.0, 2.0, 0.1979710534561312, 1.0, 2.0, 0.1979710534561312, 1.0, 2.0, 0.3159954451222834, 6.9112, 6.9112, 121.94756008, 691090.3773455175, 691090.3773455175, 219879.9275131366], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2308200.0000, 
sim time next is 2308800.0000, 
raw observation next is [26.06666666666667, 68.0, 1.0, 2.0, 0.1664343066342006, 1.0, 2.0, 0.1664343066342006, 1.0, 2.0, 0.2660898473381141, 6.9112, 6.9112, 121.94756008, 584829.0271657141, 584829.0271657141, 209808.0621062736], 
processed observation next is [1.0, 0.7391304347826086, 0.5209876543209878, 0.68, 1.0, 1.0, 0.007659888850238821, 1.0, 1.0, 0.007659888850238821, 1.0, 1.0, 0.08261230917264259, 0.0, 0.0, 0.8096049824067558, 0.20886750970204074, 0.20886750970204074, 0.4034770425120646], 
reward next is 0.5965, 
noisyNet noise sample is [array([-1.8828897], dtype=float32), -3.1050186]. 
=============================================
[2019-04-27 20:35:52,972] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.04608436e-21 6.86146279e-11 1.07595195e-19 2.00307100e-18
 1.00000000e+00], sum to 1.0000
[2019-04-27 20:35:52,981] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9272
[2019-04-27 20:35:52,985] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 37.83333333333334, 1.0, 2.0, 0.3805494541210032, 1.0, 2.0, 0.3805494541210032, 1.0, 2.0, 0.6133281583115406, 6.911199999999999, 6.9112, 121.94756008, 1365738.832041565, 1365738.832041565, 288287.4221129173], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2376600.0000, 
sim time next is 2377200.0000, 
raw observation next is [30.1, 37.66666666666667, 1.0, 2.0, 0.2996798437208544, 1.0, 2.0, 0.2996798437208544, 1.0, 2.0, 0.483277670365474, 6.9112, 6.9112, 121.94756008, 1076524.565926471, 1076524.565926471, 255434.0231716627], 
processed observation next is [1.0, 0.5217391304347826, 0.6703703703703704, 0.3766666666666667, 1.0, 1.0, 0.16628552823911236, 1.0, 1.0, 0.16628552823911236, 1.0, 1.0, 0.3540970879568424, 0.0, 0.0, 0.8096049824067558, 0.38447305925945396, 0.38447305925945396, 0.4912192753301206], 
reward next is 0.5088, 
noisyNet noise sample is [array([-0.331993], dtype=float32), -1.6223322]. 
=============================================
[2019-04-27 20:35:54,207] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-27 20:35:54,208] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 20:35:54,208] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:35:54,209] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 20:35:54,211] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 20:35:54,213] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 20:35:54,214] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:35:54,214] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:35:54,212] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 20:35:54,215] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:35:54,221] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:35:54,240] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run63
[2019-04-27 20:35:54,261] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run63
[2019-04-27 20:35:54,287] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run63
[2019-04-27 20:35:54,288] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run63
[2019-04-27 20:35:54,334] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run63
[2019-04-27 20:35:57,999] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04607286], dtype=float32), -0.045357663]
[2019-04-27 20:35:58,000] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [27.5, 25.5, 1.0, 2.0, 0.3083790882275963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 397802.0218660602, 397802.0218660602, 109291.6867348353]
[2019-04-27 20:35:58,002] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:35:58,008] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [3.1768201e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8159878e-31], sampled 0.5259962784375088
[2019-04-27 20:36:06,913] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04607286], dtype=float32), -0.045357663]
[2019-04-27 20:36:06,914] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [25.94158839166667, 54.46733565833333, 1.0, 2.0, 0.3915426796978366, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 483397.4591866956, 483397.4591866956, 127392.8416725102]
[2019-04-27 20:36:06,914] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 20:36:06,919] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [3.4424238e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5045178e-33], sampled 0.9366241063850127
[2019-04-27 20:36:41,695] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04607286], dtype=float32), -0.045357663]
[2019-04-27 20:36:41,698] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [31.495008, 68.29395806833334, 1.0, 2.0, 0.8637387858691059, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 984537.7412643795, 984537.741264379, 209469.6328088805]
[2019-04-27 20:36:41,698] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 20:36:41,702] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [5.2289802e-31 1.0000000e+00 1.7351353e-32 2.8506546e-33 4.5671906e-22], sampled 0.015050760593038448
[2019-04-27 20:36:43,441] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04607286], dtype=float32), -0.045357663]
[2019-04-27 20:36:43,442] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [25.33333333333334, 91.66666666666667, 1.0, 2.0, 0.6581797872861219, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 750115.443497423, 750115.443497423, 168377.2319341265]
[2019-04-27 20:36:43,443] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 20:36:43,445] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 5.080169e-35], sampled 0.06921668317832408
[2019-04-27 20:37:05,846] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04607286], dtype=float32), -0.045357663]
[2019-04-27 20:37:05,849] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [28.19196148833333, 83.10829095, 1.0, 2.0, 0.4609288462312719, 1.0, 2.0, 0.4609288462312719, 1.0, 2.0, 0.7338137346047059, 6.9112, 6.9112, 121.94756008, 1576786.029242265, 1576786.029242265, 324799.8223960204]
[2019-04-27 20:37:05,850] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 20:37:05,853] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [4.2964975e-22 5.7688826e-06 1.3801979e-20 8.5732717e-22 9.9999428e-01], sampled 0.8552450269708464
[2019-04-27 20:37:21,707] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04607286], dtype=float32), -0.045357663]
[2019-04-27 20:37:21,708] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [24.86876331, 46.11956302, 1.0, 2.0, 0.5697849713660738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 723794.3573424177, 723794.3573424177, 155385.2085754554]
[2019-04-27 20:37:21,709] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 20:37:21,711] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [9.3221536e-33 1.0000000e+00 1.1578693e-34 4.9283789e-35 6.9791726e-26], sampled 0.33775712487847365
[2019-04-27 20:37:31,208] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04607286], dtype=float32), -0.045357663]
[2019-04-27 20:37:31,209] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [27.59467949, 70.01093783333334, 1.0, 2.0, 0.9762380468607774, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.097502011438942, 6.9112, 121.9253168517867, 1226712.182455711, 1131309.469001474, 236059.2732954502]
[2019-04-27 20:37:31,210] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 20:37:31,213] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.8393797e-30 1.0000000e+00 3.6176018e-32 1.0614765e-32 1.2228493e-22], sampled 0.32451125492154187
[2019-04-27 20:37:31,952] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04607286], dtype=float32), -0.045357663]
[2019-04-27 20:37:31,954] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [23.1, 78.0, 1.0, 2.0, 0.4437119589189752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 541524.4306412261, 541524.4306412261, 134734.6542720724]
[2019-04-27 20:37:31,956] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 20:37:31,959] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [6.1536621e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.6095173e-32], sampled 0.8337303929324075
[2019-04-27 20:37:40,135] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8715.9925 2197856686.6028 521.0000
[2019-04-27 20:37:40,206] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8775.7651 2171272683.1716 472.0000
[2019-04-27 20:37:40,381] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8609.0732 2249561883.6938 472.0000
[2019-04-27 20:37:40,440] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8147.0277 2447960494.9863 569.0000
[2019-04-27 20:37:40,531] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8925.3650 2121774806.2447 414.0000
[2019-04-27 20:37:41,546] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1550000, evaluation results [1550000.0, 8147.027678139327, 2447960494.9862905, 569.0, 8775.76512033294, 2171272683.171634, 472.0, 8925.364963979682, 2121774806.2447097, 414.0, 8609.073248970004, 2249561883.693784, 472.0, 8715.992485403824, 2197856686.602793, 521.0]
[2019-04-27 20:37:48,000] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.0253845e-20 2.9030596e-06 3.1353644e-18 3.9945884e-18 9.9999714e-01], sum to 1.0000
[2019-04-27 20:37:48,007] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2098
[2019-04-27 20:37:48,010] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.4, 36.5, 1.0, 2.0, 0.394857500396622, 1.0, 1.0, 0.394857500396622, 1.0, 2.0, 0.6360575676343881, 6.9112, 6.9112, 121.94756008, 1415687.793625922, 1415687.793625922, 294503.2244680334], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2539800.0000, 
sim time next is 2540400.0000, 
raw observation next is [30.56666666666666, 35.66666666666666, 1.0, 2.0, 0.402290729402934, 1.0, 2.0, 0.402290729402934, 1.0, 2.0, 0.6471427239012981, 6.911199999999999, 6.9112, 121.94756008, 1438272.95460165, 1438272.95460165, 297834.9449917247], 
processed observation next is [1.0, 0.391304347826087, 0.687654320987654, 0.3566666666666666, 1.0, 1.0, 0.28844134452730236, 1.0, 1.0, 0.28844134452730236, 1.0, 1.0, 0.5589284048766227, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.5136689123577322, 0.5136689123577322, 0.5727595095994705], 
reward next is 0.4272, 
noisyNet noise sample is [array([1.4671936], dtype=float32), 0.39636537]. 
=============================================
[2019-04-27 20:37:48,884] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [8.5999316e-19 1.0000000e+00 2.5313743e-19 4.1640683e-19 8.4692690e-11], sum to 1.0000
[2019-04-27 20:37:48,891] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1378
[2019-04-27 20:37:48,901] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1477249.959293368 W.
[2019-04-27 20:37:48,907] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.0, 30.0, 1.0, 2.0, 0.6120162503089767, 1.0, 2.0, 0.6120162503089767, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1477249.959293368, 1477249.959293369, 275709.1989901457], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2545200.0000, 
sim time next is 2545800.0000, 
raw observation next is [32.1, 30.0, 1.0, 2.0, 0.6484402538485087, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9571949886467789, 6.911199999999999, 6.9112, 121.9260426156618, 1502459.768673417, 1502459.768673418, 296954.5028324999], 
processed observation next is [1.0, 0.4782608695652174, 0.7444444444444445, 0.3, 1.0, 1.0, 0.581476492676796, 0.0, 0.5, -0.1904761904761905, 1.0, 0.5, 0.9464937358084735, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.5365927745262203, 0.5365927745262208, 0.5710663516009614], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.02251956], dtype=float32), -0.23298205]. 
=============================================
[2019-04-27 20:37:55,767] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:37:55,772] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7058
[2019-04-27 20:37:55,781] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.93333333333333, 89.33333333333334, 1.0, 2.0, 0.5644887751340467, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 660690.7425127276, 660690.7425127272, 152886.3021130285], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2672400.0000, 
sim time next is 2673000.0000, 
raw observation next is [23.9, 89.5, 1.0, 2.0, 0.5638182115227753, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 660055.7744158955, 660055.7744158951, 152780.5027385705], 
processed observation next is [0.0, 0.9565217391304348, 0.4407407407407407, 0.895, 1.0, 1.0, 0.480735966098542, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2357342051485341, 0.23573420514853397, 0.2938086591126356], 
reward next is 0.7062, 
noisyNet noise sample is [array([0.43711066], dtype=float32), -1.1204268]. 
=============================================
[2019-04-27 20:37:55,803] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[72.875984]
 [72.86547 ]
 [72.85355 ]
 [72.81176 ]
 [72.80878 ]], R is [[72.86164856]
 [72.83901978]
 [72.81632996]
 [72.79353333]
 [72.77070618]].
[2019-04-27 20:37:57,021] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:37:57,031] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0316
[2019-04-27 20:37:57,038] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 47.33333333333334, 1.0, 2.0, 0.5327528280550194, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 628005.4418996944, 628005.4418996944, 147846.7277089412], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3246000.0000, 
sim time next is 3246600.0000, 
raw observation next is [31.0, 47.66666666666666, 1.0, 2.0, 0.5360056780739846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 631011.2924244389, 631011.2924244389, 148341.6264568634], 
processed observation next is [0.0, 0.5652173913043478, 0.7037037037037037, 0.47666666666666657, 1.0, 1.0, 0.447625807230934, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.22536117586587104, 0.22536117586587104, 0.2852723585708911], 
reward next is 0.7147, 
noisyNet noise sample is [array([-1.7823085], dtype=float32), -0.40730384]. 
=============================================
[2019-04-27 20:38:02,215] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.7823207e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 20:38:02,224] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1019
[2019-04-27 20:38:02,229] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.95, 84.5, 1.0, 2.0, 0.8691133478080072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 990667.927357428, 990667.927357428, 210627.3159220024], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2791800.0000, 
sim time next is 2792400.0000, 
raw observation next is [26.26666666666667, 83.0, 1.0, 2.0, 0.8018132263408223, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 913909.4377317763, 913909.4377317763, 196309.5388405391], 
processed observation next is [1.0, 0.30434782608695654, 0.5283950617283951, 0.83, 1.0, 1.0, 0.7640633646914551, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3263962277613487, 0.3263962277613487, 0.37751834392411365], 
reward next is 0.6225, 
noisyNet noise sample is [array([0.5802098], dtype=float32), -1.211485]. 
=============================================
[2019-04-27 20:38:02,378] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:38:02,386] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3109
[2019-04-27 20:38:02,391] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 91.5, 1.0, 2.0, 0.5945283801563622, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 698151.6222518372, 698151.6222518372, 158093.3383084411], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2784600.0000, 
sim time next is 2785200.0000, 
raw observation next is [23.66666666666667, 90.66666666666666, 1.0, 2.0, 0.5897181276686847, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 691700.2843428771, 691700.2843428768, 157229.6721606435], 
processed observation next is [1.0, 0.21739130434782608, 0.43209876543209896, 0.9066666666666666, 1.0, 1.0, 0.511569199605577, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.24703581583674183, 0.2470358158367417, 0.3023647541550837], 
reward next is 0.6976, 
noisyNet noise sample is [array([-0.04482227], dtype=float32), -0.6585901]. 
=============================================
[2019-04-27 20:38:03,285] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.37662115e-14 1.00000000e+00 1.15949442e-14 9.66814523e-16
 1.40195473e-08], sum to 1.0000
[2019-04-27 20:38:03,291] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1899
[2019-04-27 20:38:03,297] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2050198.804214486 W.
[2019-04-27 20:38:03,301] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.83333333333334, 74.83333333333334, 1.0, 2.0, 0.8987380045766474, 1.0, 2.0, 0.8987380045766474, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426155714, 2050198.804214486, 2050198.804214486, 386244.2300949014], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2800200.0000, 
sim time next is 2800800.0000, 
raw observation next is [29.0, 74.0, 1.0, 2.0, 0.9356330231123074, 1.0, 2.0, 0.9356330231123074, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 2134464.218250042, 2134464.218250042, 402988.0686050838], 
processed observation next is [1.0, 0.43478260869565216, 0.6296296296296297, 0.74, 1.0, 1.0, 0.9233726465622708, 1.0, 1.0, 0.9233726465622708, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.762308649375015, 0.762308649375015, 0.7749770550097765], 
reward next is 0.2250, 
noisyNet noise sample is [array([-0.13749836], dtype=float32), -0.46447492]. 
=============================================
[2019-04-27 20:38:12,160] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.6362175e-31 1.0000000e+00 8.6215078e-35 1.7311856e-35 7.6571936e-28], sum to 1.0000
[2019-04-27 20:38:12,167] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2958
[2019-04-27 20:38:12,175] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1454904.980067597 W.
[2019-04-27 20:38:12,181] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.41666666666666, 87.33333333333334, 1.0, 2.0, 1.02, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.473891590748492, 6.9112, 122.9175454268369, 1454904.980067597, 1164413.369565664, 245810.5580564204], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2945400.0000, 
sim time next is 2946000.0000, 
raw observation next is [25.33333333333334, 88.66666666666667, 1.0, 2.0, 0.3412133683757362, 1.0, 1.0, 0.3412133683757362, 1.0, 1.0, 0.543222794997773, 6.911199999999999, 6.9112, 121.94756008, 1166940.840790101, 1166940.840790101, 272288.9723157343], 
processed observation next is [1.0, 0.08695652173913043, 0.49382716049382736, 0.8866666666666667, 1.0, 1.0, 0.21573020044730498, 1.0, 0.5, 0.21573020044730498, 1.0, 0.5, 0.42902849374721624, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.41676458599646465, 0.41676458599646465, 0.5236326390687198], 
reward next is 0.4764, 
noisyNet noise sample is [array([-0.02967255], dtype=float32), 1.3175387]. 
=============================================
[2019-04-27 20:38:12,213] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[58.88224 ]
 [60.91651 ]
 [60.674404]
 [60.80793 ]
 [61.150284]], R is [[57.94427872]
 [57.36483765]
 [57.4760437 ]
 [57.58420563]
 [57.68951416]].
[2019-04-27 20:38:13,317] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.5879469e-13 2.0953758e-04 2.2435719e-12 9.9036111e-13 9.9979049e-01], sum to 1.0000
[2019-04-27 20:38:13,324] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9800
[2019-04-27 20:38:13,335] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.0, 86.5, 1.0, 2.0, 0.496158421607227, 1.0, 2.0, 0.496158421607227, 1.0, 2.0, 0.7899003659503959, 6.9112, 6.9112, 121.94756008, 1697417.763722115, 1697417.763722115, 341707.540251466], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2971800.0000, 
sim time next is 2972400.0000, 
raw observation next is [27.33333333333333, 85.66666666666667, 1.0, 2.0, 0.4972776555899487, 1.0, 2.0, 0.4972776555899487, 1.0, 2.0, 0.7916822229017949, 6.9112, 6.9112, 121.94756008, 1701250.44343775, 1701250.44343775, 342254.8827049591], 
processed observation next is [1.0, 0.391304347826087, 0.5679012345679011, 0.8566666666666667, 1.0, 1.0, 0.40152101855946276, 1.0, 1.0, 0.40152101855946276, 1.0, 1.0, 0.7396027786272436, 0.0, 0.0, 0.8096049824067558, 0.6075894440849107, 0.6075894440849107, 0.6581824667403059], 
reward next is 0.3418, 
noisyNet noise sample is [array([-0.6878378], dtype=float32), 0.5880243]. 
=============================================
[2019-04-27 20:38:14,082] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.5939069e-22 1.0000000e+00 3.4770113e-24 2.9078918e-23 1.3151222e-15], sum to 1.0000
[2019-04-27 20:38:14,090] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2839
[2019-04-27 20:38:14,098] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.56666666666667, 84.33333333333333, 1.0, 2.0, 0.787847378070799, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 904310.8366846641, 904310.8366846641, 193769.0503875258], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3397800.0000, 
sim time next is 3398400.0000, 
raw observation next is [25.8, 83.0, 1.0, 2.0, 0.7414505728593513, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 850319.5889179686, 850319.5889179686, 184406.0381754394], 
processed observation next is [1.0, 0.34782608695652173, 0.5111111111111112, 0.83, 1.0, 1.0, 0.6922030629277992, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3036855674707031, 0.3036855674707031, 0.35462699649122964], 
reward next is 0.6454, 
noisyNet noise sample is [array([-0.48530352], dtype=float32), -0.12535143]. 
=============================================
[2019-04-27 20:38:17,544] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.9419219e-14 8.0182910e-01 7.9636623e-13 1.7206013e-13 1.9817083e-01], sum to 1.0000
[2019-04-27 20:38:17,555] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8096
[2019-04-27 20:38:17,561] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.6, 89.33333333333333, 1.0, 2.0, 0.5079131476532127, 1.0, 1.0, 0.5079131476532127, 1.0, 2.0, 0.8086142726403063, 6.911200000000001, 6.9112, 121.94756008, 1737671.204033102, 1737671.204033101, 347489.8916202337], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3055800.0000, 
sim time next is 3056400.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.4934183271116215, 1.0, 2.0, 0.4934183271116215, 1.0, 2.0, 0.7855380462747444, 6.9112, 6.9112, 121.94756008, 1688034.715059355, 1688034.715059355, 340370.4120202927], 
processed observation next is [1.0, 0.391304347826087, 0.5555555555555556, 0.89, 1.0, 1.0, 0.39692657989478747, 1.0, 1.0, 0.39692657989478747, 1.0, 1.0, 0.7319225578434304, 0.0, 0.0, 0.8096049824067558, 0.6028695410926268, 0.6028695410926268, 0.654558484654409], 
reward next is 0.3454, 
noisyNet noise sample is [array([-1.5935946], dtype=float32), -0.53332275]. 
=============================================
[2019-04-27 20:38:24,891] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.4900150e-29 1.0000000e+00 3.0840262e-30 1.2559843e-32 1.4907101e-25], sum to 1.0000
[2019-04-27 20:38:24,903] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6157
[2019-04-27 20:38:24,911] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 87.0, 1.0, 2.0, 0.8618484764860362, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1013835.535284581, 1013835.535284581, 210644.162247648], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3588000.0000, 
sim time next is 3588600.0000, 
raw observation next is [24.0, 88.0, 1.0, 2.0, 0.8801878240295244, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1032693.184277457, 1032693.184277457, 214576.6673291448], 
processed observation next is [1.0, 0.5217391304347826, 0.4444444444444444, 0.88, 1.0, 1.0, 0.8573664571780053, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.36881899438480603, 0.36881899438480603, 0.4126474371714323], 
reward next is 0.5874, 
noisyNet noise sample is [array([1.303371], dtype=float32), 1.8012947]. 
=============================================
[2019-04-27 20:38:25,505] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:38:25,516] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5359
[2019-04-27 20:38:25,526] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 68.0, 1.0, 2.0, 0.512037171124954, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 611036.1802359482, 611036.1802359482, 144810.7120844044], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3199200.0000, 
sim time next is 3199800.0000, 
raw observation next is [26.0, 66.5, 1.0, 2.0, 0.4999643946697936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 599139.4748597388, 599139.4748597384, 142991.5188850824], 
processed observation next is [0.0, 0.0, 0.5185185185185185, 0.665, 1.0, 1.0, 0.40471951746403995, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.21397838387847815, 0.21397838387847798, 0.27498369016362], 
reward next is 0.7250, 
noisyNet noise sample is [array([-0.62285453], dtype=float32), -1.9436462]. 
=============================================
[2019-04-27 20:38:34,058] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-27 20:38:34,060] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 20:38:34,062] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:38:34,062] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 20:38:34,064] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 20:38:34,065] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 20:38:34,065] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:38:34,067] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:38:34,067] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:38:34,066] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 20:38:34,071] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:38:34,090] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run64
[2019-04-27 20:38:34,091] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run64
[2019-04-27 20:38:34,134] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run64
[2019-04-27 20:38:34,156] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run64
[2019-04-27 20:38:34,179] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run64
[2019-04-27 20:39:07,587] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0470415], dtype=float32), -0.042859443]
[2019-04-27 20:39:07,588] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [34.75, 44.5, 1.0, 2.0, 0.6629533556910089, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 755558.4731939647, 755558.4731939647, 169250.2827180517]
[2019-04-27 20:39:07,589] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:39:07,592] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [9.2525536e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.8409777e-34], sampled 0.5614240191482751
[2019-04-27 20:39:08,768] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.0470415], dtype=float32), -0.042859443]
[2019-04-27 20:39:08,771] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [26.5, 73.5, 1.0, 2.0, 0.568525060836971, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 662764.1788263653, 662764.1788263648, 153448.3228764781]
[2019-04-27 20:39:08,774] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 20:39:08,775] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [3.1844608e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.9398574e-38], sampled 0.6491583182669703
[2019-04-27 20:39:13,232] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0470415], dtype=float32), -0.042859443]
[2019-04-27 20:39:13,235] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [27.3262439, 80.21191043, 1.0, 2.0, 0.5985584801768498, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9529245941303189, 6.911199999999999, 6.9112, 121.9260426156618, 1364879.320343068, 1364879.320343069, 293301.8749117923]
[2019-04-27 20:39:13,235] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 20:39:13,237] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [6.6448512e-27 1.0000000e+00 1.1012770e-29 2.2530631e-29 7.6474085e-22], sampled 0.8117687914794051
[2019-04-27 20:39:13,239] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1364879.320343068 W.
[2019-04-27 20:39:17,046] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.0470415], dtype=float32), -0.042859443]
[2019-04-27 20:39:17,047] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [27.53333333333333, 67.0, 1.0, 2.0, 0.6193105395965095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 717913.9771333841, 717913.9771333836, 162014.3120749824]
[2019-04-27 20:39:17,047] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 20:39:17,050] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [9.957184e-37 1.000000e+00 0.000000e+00 0.000000e+00 6.401125e-34], sampled 0.566017088491593
[2019-04-27 20:39:18,487] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0470415], dtype=float32), -0.042859443]
[2019-04-27 20:39:18,489] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [26.84174866333333, 84.08202224666667, 1.0, 2.0, 0.7480225241966173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 852564.5106773151, 852564.5106773155, 185433.2015464183]
[2019-04-27 20:39:18,490] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 20:39:18,492] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [5.8693379e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.9038804e-34], sampled 0.6899507918102433
[2019-04-27 20:39:49,837] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0470415], dtype=float32), -0.042859443]
[2019-04-27 20:39:49,840] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [30.46666666666667, 42.33333333333334, 1.0, 2.0, 0.4190461792117309, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 507738.7351727727, 507738.7351727727, 131023.7248307879]
[2019-04-27 20:39:49,842] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:39:49,843] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [7.2288575e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2047406e-37], sampled 0.10663631874532409
[2019-04-27 20:39:57,268] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0470415], dtype=float32), -0.042859443]
[2019-04-27 20:39:57,271] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [26.33333333333334, 79.0, 1.0, 2.0, 0.5899877457992305, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 678195.3033135097, 678195.3033135097, 156649.8414061237]
[2019-04-27 20:39:57,271] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:39:57,274] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [2.4876323e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.7564836e-36], sampled 0.9728987226351313
[2019-04-27 20:40:19,597] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8102.0539 2446167673.1299 704.0000
[2019-04-27 20:40:19,753] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8769.3998 2171237346.3856 486.0000
[2019-04-27 20:40:20,165] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8920.6282 2121022292.9674 426.0000
[2019-04-27 20:40:20,168] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8698.0353 2196543245.2623 563.0000
[2019-04-27 20:40:20,268] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8582.6320 2249746811.9179 534.0000
[2019-04-27 20:40:21,287] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 1575000, evaluation results [1575000.0, 8102.053933853703, 2446167673.129931, 704.0, 8769.399823679427, 2171237346.3855934, 486.0, 8920.6282097943, 2121022292.967436, 426.0, 8582.632047334302, 2249746811.917863, 534.0, 8698.035278926925, 2196543245.2622848, 563.0]
[2019-04-27 20:40:24,279] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [9.1433554e-16 2.5375215e-09 8.6250163e-15 2.7788364e-14 1.0000000e+00], sum to 1.0000
[2019-04-27 20:40:24,292] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2513
[2019-04-27 20:40:24,297] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.75, 65.66666666666667, 1.0, 2.0, 0.618277557121504, 1.0, 2.0, 0.618277557121504, 1.0, 2.0, 0.9843180068317099, 6.9112, 6.9112, 121.94756008, 2115696.799015277, 2115696.799015277, 405424.0464694251], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3409800.0000, 
sim time next is 3410400.0000, 
raw observation next is [30.0, 64.33333333333334, 1.0, 2.0, 0.5967222935793017, 1.0, 2.0, 0.5967222935793017, 1.0, 2.0, 0.9500013252665999, 6.911200000000001, 6.9112, 121.94756008, 2041852.078657981, 2041852.07865798, 393592.6227608947], 
processed observation next is [1.0, 0.4782608695652174, 0.6666666666666666, 0.6433333333333334, 1.0, 1.0, 0.5199074923563116, 1.0, 1.0, 0.5199074923563116, 1.0, 1.0, 0.9375016565832497, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.7292328852349932, 0.7292328852349929, 0.7569088899247975], 
reward next is 0.2431, 
noisyNet noise sample is [array([-0.37890768], dtype=float32), -0.21533595]. 
=============================================
[2019-04-27 20:40:28,823] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.1136840e-22 1.0000000e+00 1.1194981e-23 1.7983266e-23 3.2942066e-15], sum to 1.0000
[2019-04-27 20:40:28,832] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6828
[2019-04-27 20:40:28,837] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.33333333333334, 88.66666666666666, 1.0, 2.0, 0.7526443272227301, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 857835.1947618364, 857835.1947618369, 186344.2423820634], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3483600.0000, 
sim time next is 3484200.0000, 
raw observation next is [25.41666666666666, 87.33333333333333, 1.0, 2.0, 0.7312421307144903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 833605.4440593066, 833605.4440593066, 182146.4842579416], 
processed observation next is [1.0, 0.30434782608695654, 0.49691358024691334, 0.8733333333333333, 1.0, 1.0, 0.6800501556124884, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2977162300211809, 0.2977162300211809, 0.3502817004960415], 
reward next is 0.6497, 
noisyNet noise sample is [array([0.6289866], dtype=float32), 0.81957936]. 
=============================================
[2019-04-27 20:40:30,445] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.2583354e-30 1.0000000e+00 4.0873639e-33 3.1743902e-33 2.1684607e-22], sum to 1.0000
[2019-04-27 20:40:30,451] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4962
[2019-04-27 20:40:30,458] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.61666666666667, 75.66666666666667, 1.0, 2.0, 0.593182275104243, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 685025.0217701205, 685025.0217701205, 157345.6869280104], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3531000.0000, 
sim time next is 3531600.0000, 
raw observation next is [27.0, 74.0, 1.0, 2.0, 0.599064073900767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 690151.516148759, 690151.516148759, 158279.5169470511], 
processed observation next is [1.0, 0.9130434782608695, 0.5555555555555556, 0.74, 1.0, 1.0, 0.5226953260723416, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.24648268433884252, 0.24648268433884252, 0.3043836864366367], 
reward next is 0.6956, 
noisyNet noise sample is [array([1.3348876], dtype=float32), -0.3172098]. 
=============================================
[2019-04-27 20:40:39,796] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.4877452e-30 1.0863594e-23 2.6755842e-25 4.5605482e-26 1.0000000e+00], sum to 1.0000
[2019-04-27 20:40:39,804] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6037
[2019-04-27 20:40:39,808] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.91666666666667, 84.83333333333333, 1.0, 2.0, 0.5054010633272479, 1.0, 2.0, 0.5054010633272479, 1.0, 2.0, 0.8046149525804952, 6.9112, 6.9112, 121.94756008, 1729068.55117845, 1729068.55117845, 346247.8593101032], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3689400.0000, 
sim time next is 3690000.0000, 
raw observation next is [26.9, 84.0, 1.0, 2.0, 0.4466559651824425, 1.0, 2.0, 0.4466559651824425, 1.0, 2.0, 0.7110908431396856, 6.911199999999999, 6.9112, 121.94756008, 1527911.395162055, 1527911.395162056, 318122.6769072711], 
processed observation next is [1.0, 0.7391304347826086, 0.5518518518518518, 0.84, 1.0, 1.0, 0.3412571014076697, 1.0, 1.0, 0.3412571014076697, 1.0, 1.0, 0.638863553924607, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.5456826411293054, 0.5456826411293058, 0.611774378667829], 
reward next is 0.3882, 
noisyNet noise sample is [array([-0.22156762], dtype=float32), 0.1534952]. 
=============================================
[2019-04-27 20:40:39,832] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[65.40662 ]
 [65.37575 ]
 [65.24789 ]
 [65.185715]
 [65.24244 ]], R is [[65.54589844]
 [65.22457886]
 [64.9018631 ]
 [64.57878876]
 [64.26818085]].
[2019-04-27 20:40:42,848] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.3870330e-21 1.0000000e+00 4.7710778e-23 5.5452188e-23 9.2423974e-10], sum to 1.0000
[2019-04-27 20:40:42,858] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0150
[2019-04-27 20:40:42,862] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 100.0, 1.0, 2.0, 0.7014627358198725, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 799469.9520095333, 799469.9520095333, 176417.2662065039], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3739800.0000, 
sim time next is 3740400.0000, 
raw observation next is [24.0, 100.0, 1.0, 2.0, 0.677500949363094, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 772146.5087235766, 772146.5087235766, 171925.4094873653], 
processed observation next is [1.0, 0.30434782608695654, 0.4444444444444444, 1.0, 1.0, 1.0, 0.6160725587655881, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2757666102584202, 0.2757666102584202, 0.33062578747570254], 
reward next is 0.6694, 
noisyNet noise sample is [array([-0.5918611], dtype=float32), -1.0448127]. 
=============================================
[2019-04-27 20:40:49,080] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.9126357e-32 1.0000000e+00 2.5516257e-35 3.4432826e-34 1.9214610e-26], sum to 1.0000
[2019-04-27 20:40:49,089] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8506
[2019-04-27 20:40:49,095] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.01666666666667, 55.16666666666667, 1.0, 2.0, 0.7216768438271586, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 822520.7034468419, 822520.7034468414, 180291.7771446201], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3849000.0000, 
sim time next is 3849600.0000, 
raw observation next is [33.03333333333334, 54.33333333333334, 1.0, 2.0, 0.7205202628059695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 821201.8008476354, 821201.8008476349, 180067.7562480613], 
processed observation next is [0.0, 0.5652173913043478, 0.7790123456790126, 0.5433333333333334, 1.0, 1.0, 0.6672860271499637, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.29328635744558407, 0.2932863574455839, 0.34628414663088714], 
reward next is 0.6537, 
noisyNet noise sample is [array([2.3210442], dtype=float32), -0.39785358]. 
=============================================
[2019-04-27 20:40:54,273] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6676823e-35 1.0000000e+00 7.2470493e-38 0.0000000e+00 1.4849504e-31], sum to 1.0000
[2019-04-27 20:40:54,280] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9570
[2019-04-27 20:40:54,285] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.95, 67.0, 1.0, 2.0, 0.7560797090045543, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 861752.9126997008, 861752.9126997008, 187037.423597437], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3947400.0000, 
sim time next is 3948000.0000, 
raw observation next is [30.93333333333333, 66.0, 1.0, 2.0, 0.7491721406588624, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 853875.5249079737, 853875.5249079737, 185665.8712532452], 
processed observation next is [0.0, 0.6956521739130435, 0.7012345679012344, 0.66, 1.0, 1.0, 0.7013954055462648, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.30495554460999064, 0.30495554460999064, 0.3570497524100869], 
reward next is 0.6430, 
noisyNet noise sample is [array([0.08977245], dtype=float32), 1.2426103]. 
=============================================
[2019-04-27 20:40:54,308] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[66.13777]
 [66.11699]
 [66.06456]
 [66.05264]
 [66.04564]], R is [[66.16653442]
 [66.14517975]
 [66.12405396]
 [66.09037781]
 [66.04676056]].
[2019-04-27 20:40:59,244] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.0054097e-14 2.1981795e-03 1.6565980e-13 7.5709914e-13 9.9780184e-01], sum to 1.0000
[2019-04-27 20:40:59,252] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4109
[2019-04-27 20:40:59,258] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.06666666666667, 87.33333333333334, 1.0, 2.0, 0.5463678927251391, 1.0, 2.0, 0.5463678927251391, 1.0, 2.0, 0.8698354791784265, 6.9112, 6.9112, 121.94756008, 1869370.126473741, 1869370.126473741, 366929.3949721779], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4027800.0000, 
sim time next is 4028400.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.485299484039739, 1.0, 2.0, 0.485299484039739, 1.0, 2.0, 0.7726125836920481, 6.911199999999999, 6.9112, 121.94756008, 1660233.515278834, 1660233.515278834, 336432.4514139523], 
processed observation next is [1.0, 0.6521739130434783, 0.5185185185185185, 0.89, 1.0, 1.0, 0.3872612905234988, 1.0, 1.0, 0.3872612905234988, 1.0, 1.0, 0.7157657296150601, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.5929405411710121, 0.5929405411710121, 0.6469854834883698], 
reward next is 0.3530, 
noisyNet noise sample is [array([-0.02630136], dtype=float32), 0.50084484]. 
=============================================
[2019-04-27 20:40:59,614] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3540183e-32 1.0000000e+00 7.1644838e-36 2.0759591e-37 1.9490074e-30], sum to 1.0000
[2019-04-27 20:40:59,624] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8624
[2019-04-27 20:40:59,630] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.16666666666666, 96.0, 1.0, 2.0, 0.6432201923596365, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 734674.6909072524, 734674.6909072524, 165752.6029487009], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4054800.0000, 
sim time next is 4055400.0000, 
raw observation next is [24.25, 94.0, 1.0, 2.0, 0.6277624505740885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 719807.7341837866, 719807.7341837866, 163135.641455112], 
processed observation next is [1.0, 0.9565217391304348, 0.4537037037037037, 0.94, 1.0, 1.0, 0.5568600602072482, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2570741907799238, 0.2570741907799238, 0.3137223874136769], 
reward next is 0.6863, 
noisyNet noise sample is [array([1.1794722], dtype=float32), 0.51428944]. 
=============================================
[2019-04-27 20:41:13,110] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.1303897e-24 1.0000000e+00 9.7982264e-27 6.6841772e-28 2.3927811e-21], sum to 1.0000
[2019-04-27 20:41:13,119] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0640
[2019-04-27 20:41:13,122] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 38.0, 1.0, 2.0, 0.365100770123046, 1.0, 2.0, 0.365100770123046, 1.0, 2.0, 0.5827356196578152, 6.9112, 6.9112, 121.94756008, 1274722.293960181, 1274722.293960181, 282174.073244758], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4279200.0000, 
sim time next is 4279800.0000, 
raw observation next is [32.0, 38.0, 1.0, 2.0, 1.006666446243207, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.478178945288726, 6.9112, 121.92368003886, 1492283.069128882, 1201944.79916944, 245130.1225492653], 
processed observation next is [1.0, 0.5217391304347826, 0.7407407407407407, 0.38, 1.0, 1.0, 1.0079362455276273, 0.0, 0.5, -0.1904761904761905, 0.0, 0.5, -0.25, 0.05669789452887262, 0.0, 0.8094464437670786, 0.5329582389746007, 0.4292659997033714, 0.4714040818255102], 
reward next is 0.0000, 
noisyNet noise sample is [array([-3.0497766], dtype=float32), 0.78873134]. 
=============================================
[2019-04-27 20:41:14,044] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-27 20:41:14,045] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 20:41:14,045] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 20:41:14,046] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 20:41:14,046] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:41:14,047] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:41:14,048] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 20:41:14,049] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 20:41:14,051] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:41:14,048] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:41:14,052] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:41:14,070] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run65
[2019-04-27 20:41:14,091] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run65
[2019-04-27 20:41:14,115] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run65
[2019-04-27 20:41:14,115] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run65
[2019-04-27 20:41:14,158] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run65
[2019-04-27 20:41:29,242] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04825871], dtype=float32), -0.043941207]
[2019-04-27 20:41:29,243] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [18.40799662, 80.011857425, 1.0, 2.0, 0.2535572250726608, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 326378.4385905857, 326378.4385905857, 110023.1517184091]
[2019-04-27 20:41:29,244] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 20:41:29,247] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.35846976494931426
[2019-04-27 20:42:12,135] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04825871], dtype=float32), -0.043941207]
[2019-04-27 20:42:12,136] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [25.25, 87.0, 1.0, 2.0, 0.595365480071792, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 682898.9458019709, 682898.9458019709, 157499.3156364547]
[2019-04-27 20:42:12,139] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:42:12,142] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [7.9002186e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.0558822e-37], sampled 0.3162330599985467
[2019-04-27 20:42:27,204] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04825871], dtype=float32), -0.043941207]
[2019-04-27 20:42:27,205] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [23.892066615, 95.43080988, 1.0, 2.0, 0.5882874593916121, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 678627.0826233296, 678627.0826233291, 156472.9139486339]
[2019-04-27 20:42:27,206] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 20:42:27,210] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4742765300520363
[2019-04-27 20:42:29,759] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04825871], dtype=float32), -0.043941207]
[2019-04-27 20:42:29,761] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [23.12436589, 88.94936001666667, 1.0, 2.0, 0.4900206206642809, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 583497.7733742793, 583497.7733742793, 141300.6961000309]
[2019-04-27 20:42:29,763] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 20:42:29,765] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.41685659231598327
[2019-04-27 20:42:52,828] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04825871], dtype=float32), -0.043941207]
[2019-04-27 20:42:52,830] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [19.23333333333333, 95.66666666666666, 1.0, 2.0, 0.3623565402418747, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 452721.8091513683, 452721.8091513683, 123518.4480314583]
[2019-04-27 20:42:52,832] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 20:42:52,835] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6148438145509771
[2019-04-27 20:42:58,912] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8770.0423 2171297437.0512 485.0000
[2019-04-27 20:42:59,359] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8580.1502 2249591736.2629 537.0000
[2019-04-27 20:42:59,586] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8103.7199 2446224992.3045 716.0000
[2019-04-27 20:42:59,670] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8920.7520 2121007452.2777 427.0000
[2019-04-27 20:42:59,813] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8696.6255 2196456160.6021 565.0000
[2019-04-27 20:43:00,832] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 1600000, evaluation results [1600000.0, 8103.719886139569, 2446224992.3044567, 716.0, 8770.04229756892, 2171297437.0512166, 485.0, 8920.7519740266, 2121007452.2776587, 427.0, 8580.150183623986, 2249591736.2628865, 537.0, 8696.625454273677, 2196456160.6021066, 565.0]
[2019-04-27 20:43:03,766] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.6751387e-16 2.1300246e-03 2.1995753e-17 1.2934301e-15 9.9786997e-01], sum to 1.0000
[2019-04-27 20:43:03,774] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8369
[2019-04-27 20:43:03,779] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 59.0, 1.0, 2.0, 0.5651677152206807, 1.0, 2.0, 0.5651677152206807, 1.0, 1.0, 0.8997654088588027, 6.911199999999999, 6.9112, 121.94756008, 1933762.391027852, 1933762.391027852, 376724.0591424633], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4366800.0000, 
sim time next is 4367400.0000, 
raw observation next is [31.98333333333333, 58.0, 1.0, 2.0, 0.5500867620831745, 1.0, 2.0, 0.5500867620831745, 1.0, 2.0, 0.8757560403115384, 6.911199999999999, 6.9112, 121.94756008, 1882107.453973398, 1882107.453973398, 368851.768116735], 
processed observation next is [1.0, 0.5652173913043478, 0.7401234567901234, 0.58, 1.0, 1.0, 0.46438900247996967, 1.0, 1.0, 0.46438900247996967, 1.0, 1.0, 0.8446950503894229, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.6721812335619279, 0.6721812335619279, 0.7093303233014134], 
reward next is 0.2907, 
noisyNet noise sample is [array([0.1543522], dtype=float32), -1.597471]. 
=============================================
[2019-04-27 20:43:10,616] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.9020914e-14 1.6525757e-01 2.3068661e-14 5.1895700e-13 8.3474243e-01], sum to 1.0000
[2019-04-27 20:43:10,625] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2696
[2019-04-27 20:43:10,633] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.86666666666667, 90.66666666666666, 1.0, 2.0, 0.5938888368692447, 1.0, 2.0, 0.5938888368692447, 1.0, 2.0, 0.9454903699049463, 6.9112, 6.9112, 121.94756008, 2032145.584867805, 2032145.584867805, 392055.9715342468], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4873200.0000, 
sim time next is 4873800.0000, 
raw observation next is [27.08333333333333, 89.83333333333333, 1.0, 2.0, 0.5991844511918474, 1.0, 2.0, 0.5991844511918474, 1.0, 2.0, 0.9539211603726478, 6.911200000000001, 6.9112, 121.94756008, 2050286.709123665, 2050286.709123664, 394931.42052028], 
processed observation next is [1.0, 0.391304347826087, 0.5586419753086418, 0.8983333333333333, 1.0, 1.0, 0.5228386323712468, 1.0, 1.0, 0.5228386323712468, 1.0, 1.0, 0.9424014504658098, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.7322452532584518, 0.7322452532584515, 0.7594835010005385], 
reward next is 0.2405, 
noisyNet noise sample is [array([-0.5315036], dtype=float32), 0.4560951]. 
=============================================
[2019-04-27 20:43:11,610] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 4.747714e-37], sum to 1.0000
[2019-04-27 20:43:11,621] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4565
[2019-04-27 20:43:11,627] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.1, 83.0, 1.0, 2.0, 0.7063758447674884, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 805072.4531963737, 805072.4531963737, 177354.4338556909], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4480200.0000, 
sim time next is 4480800.0000, 
raw observation next is [27.06666666666667, 83.33333333333334, 1.0, 2.0, 0.706060880115109, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 804713.2924504514, 804713.2924504514, 177294.5574506612], 
processed observation next is [0.0, 0.8695652173913043, 0.5580246913580248, 0.8333333333333335, 1.0, 1.0, 0.6500724763275108, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2873976044465898, 0.2873976044465898, 0.3409510720205023], 
reward next is 0.6590, 
noisyNet noise sample is [array([-0.0580405], dtype=float32), -1.2659041]. 
=============================================
[2019-04-27 20:43:12,199] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:43:12,210] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0645
[2019-04-27 20:43:12,214] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.98333333333333, 93.33333333333334, 1.0, 2.0, 0.6099344678199877, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 705056.2968787298, 705056.2968787298, 160276.8824071318], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4497000.0000, 
sim time next is 4497600.0000, 
raw observation next is [23.96666666666667, 92.66666666666667, 1.0, 2.0, 0.6022207172572422, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 697681.1244212678, 697681.1244212673, 159006.8056688128], 
processed observation next is [0.0, 0.043478260869565216, 0.4432098765432099, 0.9266666666666667, 1.0, 1.0, 0.5264532348300502, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.24917183015045277, 0.2491718301504526, 0.3057823185938708], 
reward next is 0.6942, 
noisyNet noise sample is [array([1.0257767], dtype=float32), 0.5616177]. 
=============================================
[2019-04-27 20:43:15,043] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:43:15,051] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6515
[2019-04-27 20:43:15,060] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 97.0, 1.0, 2.0, 0.5656707968679139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 660526.4577357398, 660526.4577357398, 153017.4885326418], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4566600.0000, 
sim time next is 4567200.0000, 
raw observation next is [23.06666666666667, 98.0, 1.0, 2.0, 0.569772348042185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 664080.0430902912, 664080.0430902912, 153651.7259342367], 
processed observation next is [0.0, 0.8695652173913043, 0.40987654320987665, 0.98, 1.0, 1.0, 0.48782422385974405, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2371714439608183, 0.2371714439608183, 0.29548408833507056], 
reward next is 0.7045, 
noisyNet noise sample is [array([-0.39362767], dtype=float32), 0.7198036]. 
=============================================
[2019-04-27 20:43:15,812] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:43:15,826] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1293
[2019-04-27 20:43:15,830] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.9, 95.0, 1.0, 2.0, 0.6054446582943405, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 698554.4250720291, 698554.4250720287, 159433.355846398], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4554000.0000, 
sim time next is 4554600.0000, 
raw observation next is [23.91666666666667, 94.83333333333334, 1.0, 2.0, 0.6047597434497026, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 697631.6595992418, 697631.6595992418, 159308.1157656058], 
processed observation next is [0.0, 0.7391304347826086, 0.4413580246913582, 0.9483333333333335, 1.0, 1.0, 0.5294758850591698, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.24915416414258637, 0.24915416414258637, 0.30636176108770347], 
reward next is 0.6936, 
noisyNet noise sample is [array([-0.07845046], dtype=float32), 0.12912954]. 
=============================================
[2019-04-27 20:43:24,701] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.6708638e-36], sum to 1.0000
[2019-04-27 20:43:24,710] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5201
[2019-04-27 20:43:24,718] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.7324855340571844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 834846.4603711811, 834846.4603711811, 182388.6578220013], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4734000.0000, 
sim time next is 4734600.0000, 
raw observation next is [26.95, 87.83333333333334, 1.0, 2.0, 0.7242483268195306, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 825453.092052763, 825453.092052763, 180787.7995839164], 
processed observation next is [1.0, 0.8260869565217391, 0.5537037037037037, 0.8783333333333334, 1.0, 1.0, 0.6717241985946792, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.29480467573312963, 0.29480467573312963, 0.3476688453536854], 
reward next is 0.6523, 
noisyNet noise sample is [array([-0.60059], dtype=float32), -2.1041808]. 
=============================================
[2019-04-27 20:43:31,372] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.3921612e-28 1.0000000e+00 2.9966889e-29 1.0752772e-30 3.5883839e-12], sum to 1.0000
[2019-04-27 20:43:31,380] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4838
[2019-04-27 20:43:31,384] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.36666666666667, 91.33333333333334, 1.0, 2.0, 0.8772549979567837, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 999954.3274664787, 999954.3274664787, 212430.993091008], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4912800.0000, 
sim time next is 4913400.0000, 
raw observation next is [28.18333333333333, 92.66666666666667, 1.0, 2.0, 0.8788151756388153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 1001733.885485655, 1001733.885485654, 212774.4716153327], 
processed observation next is [1.0, 0.8695652173913043, 0.5993827160493826, 0.9266666666666667, 1.0, 1.0, 0.8557323519509705, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.3577621019591625, 0.35776210195916214, 0.4091816761833321], 
reward next is 0.5908, 
noisyNet noise sample is [array([0.98870945], dtype=float32), 0.9703916]. 
=============================================
[2019-04-27 20:43:31,959] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.7433445e-21 9.9999809e-01 8.9379037e-24 5.0123165e-23 1.9291997e-06], sum to 1.0000
[2019-04-27 20:43:31,966] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2257
[2019-04-27 20:43:31,971] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.91666666666667, 93.33333333333334, 1.0, 2.0, 0.7102163936568184, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 809451.9245349576, 809451.9245349572, 178088.3769715219], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4835400.0000, 
sim time next is 4836000.0000, 
raw observation next is [25.73333333333333, 94.66666666666667, 1.0, 2.0, 0.7100553239145743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 809268.2522863428, 809268.2522863428, 178057.5754630123], 
processed observation next is [1.0, 1.0, 0.5086419753086419, 0.9466666666666668, 1.0, 1.0, 0.6548277665649693, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.289024375816551, 0.289024375816551, 0.3424184143519467], 
reward next is 0.6576, 
noisyNet noise sample is [array([-1.9948887], dtype=float32), -0.322856]. 
=============================================
[2019-04-27 20:43:31,992] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[71.855644]
 [71.72293 ]
 [71.52732 ]
 [71.44829 ]
 [71.35671 ]], R is [[71.92589569]
 [71.86415863]
 [71.80312347]
 [71.74284363]
 [71.68291473]].
[2019-04-27 20:43:34,206] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.6015330e-31 1.0000000e+00 1.5824937e-33 2.9337933e-33 1.1012877e-25], sum to 1.0000
[2019-04-27 20:43:34,216] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8325
[2019-04-27 20:43:34,223] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.43333333333333, 85.16666666666667, 1.0, 2.0, 0.776154960661893, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 884647.1801359899, 884647.1801359894, 191075.9996043859], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4902600.0000, 
sim time next is 4903200.0000, 
raw observation next is [29.0, 89.0, 1.0, 2.0, 0.8032810717761752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 915583.4923511527, 915583.4923511527, 196632.9749180514], 
processed observation next is [1.0, 0.782608695652174, 0.6296296296296297, 0.89, 1.0, 1.0, 0.7658107997335419, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.326994104411126, 0.326994104411126, 0.3781403363808681], 
reward next is 0.6219, 
noisyNet noise sample is [array([0.9100659], dtype=float32), 0.09577949]. 
=============================================
[2019-04-27 20:43:39,137] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2112720e-22 1.0000000e+00 1.6019125e-24 5.7490909e-24 7.8637031e-19], sum to 1.0000
[2019-04-27 20:43:39,145] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5846
[2019-04-27 20:43:39,154] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1523550.403769572 W.
[2019-04-27 20:43:39,162] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.5, 94.0, 1.0, 2.0, 0.4453823786711342, 1.0, 1.0, 0.4453823786711342, 1.0, 2.0, 0.7090632519358657, 6.9112, 6.9112, 121.94756008, 1523550.403769572, 1523550.403769572, 317532.3373564723], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4974600.0000, 
sim time next is 4975200.0000, 
raw observation next is [24.2, 95.0, 1.0, 2.0, 0.4195622537389691, 1.0, 2.0, 0.4195622537389691, 1.0, 2.0, 0.6679567721410965, 6.911199999999999, 6.9112, 121.94756008, 1435142.963392581, 1435142.963392582, 305757.4875742423], 
processed observation next is [1.0, 0.6086956521739131, 0.45185185185185184, 0.95, 1.0, 1.0, 0.3090026830225823, 1.0, 1.0, 0.3090026830225823, 1.0, 1.0, 0.5849459651763705, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.5125510583544932, 0.5125510583544935, 0.5879951684120044], 
reward next is 0.4120, 
noisyNet noise sample is [array([-0.04269965], dtype=float32), 0.38942617]. 
=============================================
[2019-04-27 20:43:46,125] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:43:46,133] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5046
[2019-04-27 20:43:46,138] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.86666666666667, 98.66666666666667, 1.0, 2.0, 0.6972788844665742, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 794699.0678694922, 794699.0678694927, 175629.2856186639], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5116800.0000, 
sim time next is 5117400.0000, 
raw observation next is [24.9, 99.0, 1.0, 2.0, 0.6997310125369762, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 797495.2484367747, 797495.2484367747, 176093.1684611981], 
processed observation next is [0.0, 0.21739130434782608, 0.47777777777777775, 0.99, 1.0, 1.0, 0.6425369196868764, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2848197315845624, 0.2848197315845624, 0.3386407085792271], 
reward next is 0.6614, 
noisyNet noise sample is [array([1.1635456], dtype=float32), -0.28553098]. 
=============================================
[2019-04-27 20:43:48,071] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:43:48,077] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9672
[2019-04-27 20:43:48,081] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 71.0, 1.0, 2.0, 0.8766951898356354, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 999315.8043861691, 999315.8043861686, 212309.5964318768], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5151600.0000, 
sim time next is 5152200.0000, 
raw observation next is [31.83333333333334, 70.83333333333334, 1.0, 2.0, 0.8689497291473964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 990481.3043844161, 990481.3043844161, 210610.541553415], 
processed observation next is [0.0, 0.6521739130434783, 0.7345679012345682, 0.7083333333333335, 1.0, 1.0, 0.8439877727945195, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.35374332299443434, 0.35374332299443434, 0.40502027221810577], 
reward next is 0.5950, 
noisyNet noise sample is [array([-0.3012484], dtype=float32), -1.1410617]. 
=============================================
[2019-04-27 20:43:50,664] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:43:50,675] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6566
[2019-04-27 20:43:50,680] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.83333333333334, 89.00000000000001, 1.0, 2.0, 0.6197568302694257, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 713286.4575619393, 713286.4575619393, 161852.2401668335], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5181000.0000, 
sim time next is 5181600.0000, 
raw observation next is [24.86666666666667, 90.0, 1.0, 2.0, 0.6277761451416136, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 719901.7097885847, 719901.7097885847, 163142.3549948393], 
processed observation next is [0.0, 1.0, 0.47654320987654336, 0.9, 1.0, 1.0, 0.5568763632638257, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.25710775349592313, 0.25710775349592313, 0.3137352980669986], 
reward next is 0.6863, 
noisyNet noise sample is [array([-1.2772431], dtype=float32), -1.0751464]. 
=============================================
[2019-04-27 20:43:53,839] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-27 20:43:53,841] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 20:43:53,841] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:43:53,842] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 20:43:53,842] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:43:53,843] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 20:43:53,844] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 20:43:53,844] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:43:53,844] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:43:53,846] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 20:43:53,847] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:43:53,866] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run66
[2019-04-27 20:43:53,889] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run66
[2019-04-27 20:43:53,890] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run66
[2019-04-27 20:43:53,937] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run66
[2019-04-27 20:43:53,968] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run66
[2019-04-27 20:44:04,786] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0519979], dtype=float32), -0.04417799]
[2019-04-27 20:44:04,787] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [28.7, 17.0, 1.0, 2.0, 0.3263016333499121, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 420928.0432254958, 420928.0432254958, 103839.7712893891]
[2019-04-27 20:44:04,788] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:44:04,791] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.34556308493338406
[2019-04-27 20:44:22,037] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0519979], dtype=float32), -0.04417799]
[2019-04-27 20:44:22,038] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [24.15899843, 85.36891754999999, 1.0, 2.0, 0.5780284405963303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 681852.780826499, 681852.780826499, 155392.9924507417]
[2019-04-27 20:44:22,040] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 20:44:22,042] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.08796219186703358
[2019-04-27 20:44:43,141] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0519979], dtype=float32), -0.04417799]
[2019-04-27 20:44:43,142] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [31.18528079, 69.96094328, 1.0, 2.0, 0.8025730847918023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 914776.0438806115, 914776.0438806111, 196480.9871435409]
[2019-04-27 20:44:43,145] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 20:44:43,149] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.17123736559427882
[2019-04-27 20:44:48,130] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0519979], dtype=float32), -0.04417799]
[2019-04-27 20:44:48,131] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [25.66666666666667, 83.66666666666667, 1.0, 2.0, 0.6210189390997047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 712043.2314131403, 712043.2314131403, 161943.3405617178]
[2019-04-27 20:44:48,133] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 20:44:48,137] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9783472906388263
[2019-04-27 20:45:07,038] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.0519979], dtype=float32), -0.04417799]
[2019-04-27 20:45:07,039] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [32.5, 51.0, 1.0, 2.0, 0.6328164686820231, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 721195.7359920498, 721195.7359920498, 163815.7164009809]
[2019-04-27 20:45:07,040] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 20:45:07,044] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3525196146927886
[2019-04-27 20:45:23,556] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0519979], dtype=float32), -0.04417799]
[2019-04-27 20:45:23,558] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [24.66718818333334, 73.05971543, 1.0, 2.0, 0.4585772793709563, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 553437.1980939178, 553437.1980939178, 136764.4672304334]
[2019-04-27 20:45:23,558] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 20:45:23,563] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7556786801613016
[2019-04-27 20:45:32,256] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0519979], dtype=float32), -0.04417799]
[2019-04-27 20:45:32,258] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [19.73333333333333, 94.16666666666667, 1.0, 2.0, 0.3740170062083767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 465229.4147697355, 465229.4147697355, 125057.6343968954]
[2019-04-27 20:45:32,260] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 20:45:32,264] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9309843363567648
[2019-04-27 20:45:38,714] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8767.7850 2170896759.9961 493.0000
[2019-04-27 20:45:38,847] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8697.2652 2195924588.4321 571.0000
[2019-04-27 20:45:39,013] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8580.4445 2248939950.6587 554.0000
[2019-04-27 20:45:39,022] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8096.9038 2445628494.3108 746.0000
[2019-04-27 20:45:39,037] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8922.0495 2120823351.2149 431.0000
[2019-04-27 20:45:40,054] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1625000, evaluation results [1625000.0, 8096.903836826734, 2445628494.3108454, 746.0, 8767.784964774493, 2170896759.9960513, 493.0, 8922.049506177987, 2120823351.2149136, 431.0, 8580.444490253156, 2248939950.6587467, 554.0, 8697.265245538018, 2195924588.432124, 571.0]
[2019-04-27 20:45:40,739] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:45:40,747] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9050
[2019-04-27 20:45:40,755] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.43333333333334, 88.66666666666667, 1.0, 2.0, 0.7052407743460064, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 803778.1095693308, 803778.1095693308, 177139.2011854372], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5260800.0000, 
sim time next is 5261400.0000, 
raw observation next is [26.35, 89.0, 1.0, 2.0, 0.7082674244214303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 807229.4643704302, 807229.4643704302, 177715.4562499278], 
processed observation next is [1.0, 0.9130434782608695, 0.5314814814814816, 0.89, 1.0, 1.0, 0.652699314787417, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.28829623727515363, 0.28829623727515363, 0.3417604927883227], 
reward next is 0.6582, 
noisyNet noise sample is [array([-1.1026131], dtype=float32), 0.6747377]. 
=============================================
[2019-04-27 20:45:57,580] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:45:57,591] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9602
[2019-04-27 20:45:57,598] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.65, 98.0, 1.0, 2.0, 0.6090040143692838, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 700454.9358553117, 700454.9358553117, 159947.533409252], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5614200.0000, 
sim time next is 5614800.0000, 
raw observation next is [23.63333333333333, 98.0, 1.0, 2.0, 0.6067728058296021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 698148.8618809205, 698148.8618809205, 159571.9738876311], 
processed observation next is [1.0, 1.0, 0.430864197530864, 0.98, 1.0, 1.0, 0.5318723878923834, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.24933887924318587, 0.24933887924318587, 0.3068691805531367], 
reward next is 0.6931, 
noisyNet noise sample is [array([0.81431705], dtype=float32), -0.4632327]. 
=============================================
[2019-04-27 20:45:59,209] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.5260978e-32 1.0000000e+00 2.5544000e-36 1.1206785e-36 8.9605736e-35], sum to 1.0000
[2019-04-27 20:45:59,216] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7994
[2019-04-27 20:45:59,226] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1604155.433842835 W.
[2019-04-27 20:45:59,231] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.81666666666666, 83.33333333333334, 1.0, 2.0, 0.7033830566054253, 1.0, 1.0, 0.7033830566054253, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9252086717147, 1604155.433842835, 1604155.433842836, 305447.1008682056], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5561400.0000, 
sim time next is 5562000.0000, 
raw observation next is [25.9, 83.0, 1.0, 2.0, 0.6535443106150904, 1.0, 2.0, 0.6535443106150904, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260423613597, 1490384.262482131, 1490384.262482131, 286952.0499767325], 
processed observation next is [1.0, 0.391304347826087, 0.5148148148148147, 0.83, 1.0, 1.0, 0.5875527507322506, 1.0, 1.0, 0.5875527507322506, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621271318345, 0.5322800937436182, 0.5322800937436182, 0.5518308653398702], 
reward next is 0.4482, 
noisyNet noise sample is [array([-1.4494771], dtype=float32), -0.5423098]. 
=============================================
[2019-04-27 20:45:59,244] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[52.097637]
 [55.435867]
 [59.233315]
 [60.401764]
 [61.80527 ]], R is [[51.99571228]
 [51.88835526]
 [51.3694725 ]
 [50.88147354]
 [50.95568466]].
[2019-04-27 20:45:59,551] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.9252915e-33 1.0000000e+00 3.6671259e-37 2.3162301e-34 0.0000000e+00], sum to 1.0000
[2019-04-27 20:45:59,558] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1329
[2019-04-27 20:45:59,567] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1534036.521579559 W.
[2019-04-27 20:45:59,571] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.8, 79.0, 1.0, 2.0, 0.6726669889604918, 1.0, 1.0, 0.6726669889604918, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1534036.521579559, 1534036.521579559, 293946.9403789088], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5569200.0000, 
sim time next is 5569800.0000, 
raw observation next is [27.0, 78.83333333333334, 1.0, 2.0, 0.4720298782984435, 1.0, 2.0, 0.4720298782984435, 1.0, 1.0, 0.7514869391910174, 6.911200000000001, 6.9112, 121.94756008, 1614796.519101522, 1614796.519101521, 330070.8962273963], 
processed observation next is [1.0, 0.4782608695652174, 0.5555555555555556, 0.7883333333333334, 1.0, 1.0, 0.3714641408314804, 1.0, 1.0, 0.3714641408314804, 1.0, 0.5, 0.6893586739887716, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.5767130425362579, 0.5767130425362575, 0.6347517235142237], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.32988232], dtype=float32), -0.85981154]. 
=============================================
[2019-04-27 20:46:03,746] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:46:03,755] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7479
[2019-04-27 20:46:03,759] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.51666666666667, 97.0, 1.0, 2.0, 0.5890773063504808, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 681377.8228941287, 681377.8228941287, 156693.3711400261], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5622600.0000, 
sim time next is 5623200.0000, 
raw observation next is [23.5, 97.0, 1.0, 2.0, 0.587657454142707, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 679981.3399780886, 679981.3399780886, 156462.1216159851], 
processed observation next is [0.0, 0.08695652173913043, 0.42592592592592593, 0.97, 1.0, 1.0, 0.5091160168365559, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2428504785636031, 0.2428504785636031, 0.30088869541535596], 
reward next is 0.6991, 
noisyNet noise sample is [array([0.06461445], dtype=float32), -0.15396976]. 
=============================================
[2019-04-27 20:46:07,283] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:46:07,291] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9947
[2019-04-27 20:46:07,295] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.2, 79.0, 1.0, 2.0, 0.450934390038609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 548479.5344253818, 548479.5344253823, 135754.6073402938], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5736000.0000, 
sim time next is 5736600.0000, 
raw observation next is [23.4, 78.0, 1.0, 2.0, 0.4538632959578224, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 551548.2291505411, 551548.2291505411, 136177.7100275356], 
processed observation next is [0.0, 0.391304347826087, 0.42222222222222217, 0.78, 1.0, 1.0, 0.3498372570926457, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.19698151041090756, 0.19698151041090756, 0.2618802115914146], 
reward next is 0.7381, 
noisyNet noise sample is [array([1.0469879], dtype=float32), 0.7860347]. 
=============================================
[2019-04-27 20:46:16,709] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:46:16,716] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0716
[2019-04-27 20:46:16,720] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.68333333333333, 64.33333333333334, 1.0, 2.0, 0.610273460766347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 702326.1805748454, 702326.1805748454, 160188.3656797025], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6289800.0000, 
sim time next is 6290400.0000, 
raw observation next is [28.56666666666667, 64.66666666666667, 1.0, 2.0, 0.6072403608543481, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 699594.3739285101, 699594.3739285101, 159696.5042580312], 
processed observation next is [0.0, 0.8260869565217391, 0.6135802469135804, 0.6466666666666667, 1.0, 1.0, 0.532429001017081, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.24985513354589647, 0.24985513354589647, 0.3071086620346754], 
reward next is 0.6929, 
noisyNet noise sample is [array([0.00297391], dtype=float32), 1.1480864]. 
=============================================
[2019-04-27 20:46:18,949] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.0881874e-23 1.0000000e+00 1.7942280e-24 1.2718718e-23 2.6276219e-16], sum to 1.0000
[2019-04-27 20:46:18,956] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4708
[2019-04-27 20:46:18,964] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1450315.448019081 W.
[2019-04-27 20:46:18,971] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.11666666666667, 70.16666666666667, 1.0, 2.0, 0.4228737291254515, 1.0, 2.0, 0.4228737291254515, 1.0, 1.0, 0.6733211408755159, 6.911199999999999, 6.9112, 121.94756008, 1450315.448019081, 1450315.448019082, 307274.2441335498], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6000600.0000, 
sim time next is 6001200.0000, 
raw observation next is [26.3, 70.0, 1.0, 2.0, 0.5589850849673393, 0.0, 1.0, 0.0, 1.0, 2.0, 0.8914925072689015, 6.911199999999999, 6.9112, 121.9260426156186, 1295836.486330765, 1295836.486330766, 277892.1625622737], 
processed observation next is [1.0, 0.4782608695652174, 0.5296296296296297, 0.7, 1.0, 1.0, 0.4749822440087373, 0.0, 0.5, -0.1904761904761905, 1.0, 1.0, 0.8643656340861269, -8.881784197001253e-17, 0.0, 0.8094621288198491, 0.4627987451181304, 0.46279874511813074, 0.5344080049274493], 
reward next is 0.4656, 
noisyNet noise sample is [array([0.25787565], dtype=float32), 1.9194024]. 
=============================================
[2019-04-27 20:46:22,916] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:46:22,924] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9839
[2019-04-27 20:46:22,931] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 83.0, 1.0, 2.0, 0.5126235108985875, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 609231.5549521815, 609231.5549521815, 144810.5617840939], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6071400.0000, 
sim time next is 6072000.0000, 
raw observation next is [24.06666666666667, 82.66666666666667, 1.0, 2.0, 0.5718375886006896, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 679337.4781324554, 679337.4781324558, 154532.7647036214], 
processed observation next is [1.0, 0.2608695652173913, 0.4469135802469137, 0.8266666666666667, 1.0, 1.0, 0.49028284357224944, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.24262052790444835, 0.24262052790444852, 0.2971783936608104], 
reward next is 0.7028, 
noisyNet noise sample is [array([0.7889526], dtype=float32), -1.1053991]. 
=============================================
[2019-04-27 20:46:22,944] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[65.027466]
 [64.81185 ]
 [64.711426]
 [64.51758 ]
 [64.616394]], R is [[64.82038116]
 [64.89369965]
 [64.96385193]
 [65.01927948]
 [65.07781219]].
[2019-04-27 20:46:26,261] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 4.216403e-33], sum to 1.0000
[2019-04-27 20:46:26,271] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4816
[2019-04-27 20:46:26,280] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.9, 73.0, 1.0, 2.0, 0.5362971773128377, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 632606.9720262836, 632606.9720262836, 148440.0520252969], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6130800.0000, 
sim time next is 6131400.0000, 
raw observation next is [25.76666666666667, 73.83333333333334, 1.0, 2.0, 0.5357137840287526, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 631937.0368069203, 631937.0368069198, 148345.773692929], 
processed observation next is [1.0, 1.0, 0.5098765432098766, 0.7383333333333334, 1.0, 1.0, 0.4472783143199435, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2256917988596144, 0.22569179885961424, 0.28528033402486347], 
reward next is 0.7147, 
noisyNet noise sample is [array([-0.20813575], dtype=float32), -0.21183337]. 
=============================================
[2019-04-27 20:46:30,996] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 7.18325e-38], sum to 1.0000
[2019-04-27 20:46:31,006] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6839
[2019-04-27 20:46:31,009] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.26666666666667, 84.0, 1.0, 2.0, 0.5376291155636106, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 634117.8633293684, 634117.863329368, 148654.7841930868], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6138600.0000, 
sim time next is 6139200.0000, 
raw observation next is [24.13333333333334, 85.0, 1.0, 2.0, 0.5387028505462615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 635313.8490144719, 635313.8490144714, 148827.2084909918], 
processed observation next is [1.0, 0.043478260869565216, 0.44938271604938296, 0.85, 1.0, 1.0, 0.4508367268407875, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.22689780321945424, 0.22689780321945408, 0.28620617017498423], 
reward next is 0.7138, 
noisyNet noise sample is [array([0.14106981], dtype=float32), 0.53478575]. 
=============================================
[2019-04-27 20:46:31,691] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8693219e-23 1.0000000e+00 1.5955561e-26 5.2905072e-27 8.1513115e-15], sum to 1.0000
[2019-04-27 20:46:31,696] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3950
[2019-04-27 20:46:31,702] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.35, 32.5, 1.0, 2.0, 0.5660779299431439, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 719135.3391958678, 719135.3391958678, 154751.0508599161], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6697800.0000, 
sim time next is 6698400.0000, 
raw observation next is [28.5, 32.0, 1.0, 2.0, 0.6131932540924223, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 779009.0361897503, 779009.0361897503, 163005.7944737961], 
processed observation next is [1.0, 0.5217391304347826, 0.6111111111111112, 0.32, 1.0, 1.0, 0.5395157786814552, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2782175129249108, 0.2782175129249108, 0.3134726816803771], 
reward next is 0.6865, 
noisyNet noise sample is [array([1.7857819], dtype=float32), 0.85463446]. 
=============================================
[2019-04-27 20:46:32,889] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-27 20:46:32,890] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 20:46:32,890] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 20:46:32,891] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:46:32,891] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:46:32,892] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 20:46:32,893] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 20:46:32,892] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 20:46:32,895] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:46:32,895] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:46:32,897] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:46:32,915] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run67
[2019-04-27 20:46:32,938] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run67
[2019-04-27 20:46:32,959] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run67
[2019-04-27 20:46:32,991] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run67
[2019-04-27 20:46:33,011] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run67
[2019-04-27 20:46:35,745] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04831338], dtype=float32), -0.047133226]
[2019-04-27 20:46:35,746] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [21.0, 38.66666666666667, 1.0, 2.0, 0.2783311816121659, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 359031.83139506, 359031.83139506, 90919.71678772684]
[2019-04-27 20:46:35,747] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 20:46:35,752] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.560239698735841
[2019-04-27 20:46:53,273] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04831338], dtype=float32), -0.047133226]
[2019-04-27 20:46:53,275] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [20.35035848833333, 78.27020853833334, 1.0, 2.0, 0.3855956382013985, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 487563.8831466326, 487563.8831466322, 126774.2933319898]
[2019-04-27 20:46:53,277] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 20:46:53,280] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.40064549618116807
[2019-04-27 20:47:06,117] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04831338], dtype=float32), -0.047133226]
[2019-04-27 20:47:06,118] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [20.33444976, 97.33588439, 1.0, 2.0, 0.4149504341507346, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 510117.0733827384, 510117.0733827384, 130648.5360251967]
[2019-04-27 20:47:06,119] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 20:47:06,122] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.660669887141371
[2019-04-27 20:47:19,395] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04831338], dtype=float32), -0.047133226]
[2019-04-27 20:47:19,396] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [23.0, 100.0, 1.0, 2.0, 1.00319875583438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.272342415225621, 6.9112, 121.9243544011225, 1348686.716661652, 1163752.082204019, 242614.2176332821]
[2019-04-27 20:47:19,396] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 20:47:19,400] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [7.6186016e-32 1.0000000e+00 7.7543161e-35 1.2871909e-34 1.0683067e-29], sampled 0.7008102559450328
[2019-04-27 20:47:19,401] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1348686.716661652 W.
[2019-04-27 20:47:24,324] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04831338], dtype=float32), -0.047133226]
[2019-04-27 20:47:24,326] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [23.10529645, 72.14709831833333, 1.0, 2.0, 0.564179073665347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 697314.923283006, 697314.923283006, 154103.9764501948]
[2019-04-27 20:47:24,328] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 20:47:24,332] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9652651238390313
[2019-04-27 20:47:27,506] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04831338], dtype=float32), -0.047133226]
[2019-04-27 20:47:27,506] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [23.0, 94.0, 1.0, 2.0, 0.6008826461045582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 709072.504328231, 709072.504328231, 159340.3110858413]
[2019-04-27 20:47:27,508] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 20:47:27,509] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7723900956534197
[2019-04-27 20:47:42,153] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04831338], dtype=float32), -0.047133226]
[2019-04-27 20:47:42,155] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [25.0, 94.0, 1.0, 2.0, 0.8314955841031664, 1.0, 2.0, 0.8314955841031664, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1896642.682913725, 1896642.682913725, 356940.4348435383]
[2019-04-27 20:47:42,156] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:47:42,158] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.1258885e-22 1.0000000e+00 1.6473935e-24 7.4970272e-24 1.5406034e-16], sampled 0.30579675654501515
[2019-04-27 20:47:42,160] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1896642.682913725 W.
[2019-04-27 20:47:46,994] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04831338], dtype=float32), -0.047133226]
[2019-04-27 20:47:46,998] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [26.47991682333333, 71.98681768666667, 1.0, 2.0, 0.5182912767382799, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 610026.0711193011, 610026.0711193008, 145481.6451922945]
[2019-04-27 20:47:46,999] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 20:47:47,001] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6322945087328135
[2019-04-27 20:48:05,493] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04831338], dtype=float32), -0.047133226]
[2019-04-27 20:48:05,494] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [27.0, 84.0, 1.0, 2.0, 0.6827023536515666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 778077.547801896, 778077.547801896, 172896.121764633]
[2019-04-27 20:48:05,495] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 20:48:05,496] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9827744059709094
[2019-04-27 20:48:16,644] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04831338], dtype=float32), -0.047133226]
[2019-04-27 20:48:16,645] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [21.27647867666667, 76.26868976666667, 1.0, 2.0, 0.3689844357214817, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 462612.422769735, 462612.422769735, 124440.5258657939]
[2019-04-27 20:48:16,646] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 20:48:16,651] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7235493458543812
[2019-04-27 20:48:17,654] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8580.5531 2248970883.1630 553.0000
[2019-04-27 20:48:17,817] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8921.1500 2120854052.2403 431.0000
[2019-04-27 20:48:17,918] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8696.7858 2196111390.5544 569.0000
[2019-04-27 20:48:18,014] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8096.9038 2445628494.3108 746.0000
[2019-04-27 20:48:18,110] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8767.9110 2170979945.7617 493.0000
[2019-04-27 20:48:19,128] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 1650000, evaluation results [1650000.0, 8096.903836826734, 2445628494.3108454, 746.0, 8767.910969467755, 2170979945.7616673, 493.0, 8921.14997168085, 2120854052.2402842, 431.0, 8580.553055233811, 2248970883.163011, 553.0, 8696.785776662737, 2196111390.554392, 569.0]
[2019-04-27 20:48:21,956] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:48:21,966] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1291
[2019-04-27 20:48:21,974] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.86666666666667, 83.16666666666667, 1.0, 2.0, 0.5713169825878418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 667535.1512033342, 667535.1512033342, 153983.9570384516], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6304200.0000, 
sim time next is 6304800.0000, 
raw observation next is [24.83333333333334, 83.33333333333334, 1.0, 2.0, 0.5704158159465161, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 666613.2154517521, 666613.2154517521, 153837.8578928058], 
processed observation next is [0.0, 1.0, 0.47530864197530887, 0.8333333333333335, 1.0, 1.0, 0.48859025707918585, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.23807614837562574, 0.23807614837562574, 0.2958420344092419], 
reward next is 0.7042, 
noisyNet noise sample is [array([-0.03229439], dtype=float32), -0.20198634]. 
=============================================
[2019-04-27 20:48:35,435] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.6657468e-22 1.3969311e-12 3.5968164e-21 3.0045206e-20 1.0000000e+00], sum to 1.0000
[2019-04-27 20:48:35,445] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2077
[2019-04-27 20:48:35,459] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.3, 54.00000000000001, 1.0, 2.0, 0.6754212414475116, 1.0, 2.0, 0.6510752827001905, 1.0, 2.0, 0.9977734948820727, 6.911200000000001, 6.9112, 121.94756008, 2228068.025046632, 2228068.025046631, 422700.6464154075], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6445200.0000, 
sim time next is 6445800.0000, 
raw observation next is [32.25, 54.0, 1.0, 2.0, 0.6880813189901145, 1.0, 2.0, 0.6574053214714921, 1.0, 2.0, 0.9977734948820727, 6.911200000000001, 6.9112, 121.94756008, 2249757.566041225, 2249757.566041225, 426052.6771685601], 
processed observation next is [1.0, 0.6086956521739131, 0.75, 0.54, 1.0, 1.0, 0.6286682368929934, 1.0, 1.0, 0.5921491922279668, 1.0, 1.0, 0.9972168686025908, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.8034848450147233, 0.8034848450147233, 0.8193320714780001], 
reward next is 0.1807, 
noisyNet noise sample is [array([-1.243965], dtype=float32), -1.9985336]. 
=============================================
[2019-04-27 20:48:37,796] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.5026625e-22 3.7642602e-20 4.7196212e-20 4.3822875e-20 1.0000000e+00], sum to 1.0000
[2019-04-27 20:48:37,803] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8536
[2019-04-27 20:48:37,807] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.56666666666667, 79.5, 1.0, 2.0, 0.6506258387500184, 1.0, 2.0, 0.638677581351444, 1.0, 2.0, 0.9977734948820727, 6.911200000000001, 6.9112, 121.94756008, 2185589.487400413, 2185589.487400412, 416235.9155117797], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6519000.0000, 
sim time next is 6519600.0000, 
raw observation next is [28.7, 79.0, 1.0, 2.0, 0.6657998225519672, 1.0, 2.0, 0.6462645732524184, 1.0, 2.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.94756008, 2211584.737452077, 2211584.737452077, 420176.3401016486], 
processed observation next is [1.0, 0.4782608695652174, 0.6185185185185185, 0.79, 1.0, 1.0, 0.6021426458951991, 1.0, 1.0, 0.5788863967290695, 1.0, 1.0, 0.9972168686025908, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.7898516919471704, 0.7898516919471704, 0.8080314232724012], 
reward next is 0.1920, 
noisyNet noise sample is [array([0.6929785], dtype=float32), -0.56629777]. 
=============================================
[2019-04-27 20:48:38,367] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [6.1599535e-23 1.0000000e+00 3.6119491e-24 1.3226030e-23 3.0718035e-15], sum to 1.0000
[2019-04-27 20:48:38,373] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6491
[2019-04-27 20:48:38,377] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.3, 88.83333333333334, 1.0, 2.0, 0.8275059340343895, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 943212.0947301729, 943212.0947301724, 201687.2666632207], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6508200.0000, 
sim time next is 6508800.0000, 
raw observation next is [26.3, 89.0, 1.0, 2.0, 0.9038374308001571, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1030275.144814337, 1030275.144814337, 218326.5796852445], 
processed observation next is [1.0, 0.34782608695652173, 0.5296296296296297, 0.89, 1.0, 1.0, 0.885520750952568, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.36795540886226324, 0.36795540886226324, 0.4198588070870087], 
reward next is 0.5801, 
noisyNet noise sample is [array([0.69068015], dtype=float32), -0.7643361]. 
=============================================
[2019-04-27 20:48:43,418] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0762798e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.2008236e-34], sum to 1.0000
[2019-04-27 20:48:43,425] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5307
[2019-04-27 20:48:43,432] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.96666666666667, 48.66666666666667, 1.0, 2.0, 0.6545037440689447, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 828267.5880410032, 828267.5880410027, 170538.9713310663], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6597600.0000, 
sim time next is 6598200.0000, 
raw observation next is [25.13333333333333, 47.83333333333334, 1.0, 2.0, 0.6587226164502114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 833614.3898365131, 833614.3898365131, 171328.3435705092], 
processed observation next is [1.0, 0.34782608695652173, 0.4864197530864196, 0.47833333333333344, 1.0, 1.0, 0.5937174005359659, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2977194249416118, 0.2977194249416118, 0.32947758378944075], 
reward next is 0.6705, 
noisyNet noise sample is [array([0.23151347], dtype=float32), -0.50221527]. 
=============================================
[2019-04-27 20:48:44,825] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.7304332e-18 2.9785065e-02 7.5749618e-19 4.9939309e-17 9.7021490e-01], sum to 1.0000
[2019-04-27 20:48:44,832] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3939
[2019-04-27 20:48:44,837] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.16666666666667, 39.16666666666667, 1.0, 2.0, 0.3561060851642276, 1.0, 2.0, 0.3561060851642276, 1.0, 2.0, 0.5800795399119189, 6.9112, 6.9112, 121.94756008, 1299441.274464269, 1299441.274464269, 277370.9290748365], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6610200.0000, 
sim time next is 6610800.0000, 
raw observation next is [28.33333333333334, 38.33333333333334, 1.0, 2.0, 0.2869545632947066, 1.0, 2.0, 0.2869545632947066, 1.0, 2.0, 0.4683786408503807, 6.911200000000001, 6.9112, 121.94756008, 1049582.783112518, 1049582.783112517, 249916.542752382], 
processed observation next is [1.0, 0.5217391304347826, 0.6049382716049385, 0.3833333333333334, 1.0, 1.0, 0.1511363848746507, 1.0, 1.0, 0.1511363848746507, 1.0, 1.0, 0.33547330106297585, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.37485099396875643, 0.3748509939687561, 0.48060873606227306], 
reward next is 0.5194, 
noisyNet noise sample is [array([0.8725613], dtype=float32), -0.11570171]. 
=============================================
[2019-04-27 20:48:47,907] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.3426572e-20 9.9999678e-01 4.5308410e-21 4.9900751e-19 3.2574847e-06], sum to 1.0000
[2019-04-27 20:48:47,917] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8956
[2019-04-27 20:48:47,920] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.76666666666667, 45.33333333333334, 1.0, 2.0, 0.5486380751232935, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 699835.9488719269, 699835.9488719269, 151812.0617159306], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6686400.0000, 
sim time next is 6687000.0000, 
raw observation next is [25.0, 44.5, 1.0, 2.0, 0.5337376494175294, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 680605.9432795127, 680605.9432795127, 149328.8778857109], 
processed observation next is [1.0, 0.391304347826087, 0.48148148148148145, 0.445, 1.0, 1.0, 0.4449257731161064, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.24307355117125454, 0.24307355117125454, 0.2871709190109825], 
reward next is 0.7128, 
noisyNet noise sample is [array([-1.135189], dtype=float32), 1.1607788]. 
=============================================
[2019-04-27 20:48:47,933] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[74.88328 ]
 [74.547104]
 [74.827095]
 [74.68995 ]
 [74.87873 ]], R is [[74.96921539]
 [74.92757416]
 [74.85529327]
 [74.80751801]
 [74.74816895]].
[2019-04-27 20:49:01,861] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.7279994e-34 1.0000000e+00 2.0713324e-36 8.6009725e-36 5.9581923e-30], sum to 1.0000
[2019-04-27 20:49:01,870] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4931
[2019-04-27 20:49:01,873] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 58.0, 1.0, 2.0, 0.5047216354657339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 601211.1909503365, 601211.1909503365, 143610.1114123375], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6951600.0000, 
sim time next is 6952200.0000, 
raw observation next is [28.16666666666667, 57.5, 1.0, 2.0, 0.5069737371345245, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 603306.2796937936, 603306.2796937936, 143944.1046991046], 
processed observation next is [0.0, 0.4782608695652174, 0.5987654320987656, 0.575, 1.0, 1.0, 0.4130639727791959, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21546652846206915, 0.21546652846206915, 0.27681558595981653], 
reward next is 0.7232, 
noisyNet noise sample is [array([-0.16480377], dtype=float32), -1.2487829]. 
=============================================
[2019-04-27 20:49:08,874] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.3243522e-18 1.9525085e-02 5.7253373e-19 1.5509266e-18 9.8047489e-01], sum to 1.0000
[2019-04-27 20:49:08,883] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7296
[2019-04-27 20:49:08,887] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.58333333333333, 61.33333333333334, 1.0, 2.0, 0.3491101628528502, 1.0, 2.0, 0.3491101628528502, 1.0, 2.0, 0.5560454202521002, 6.9112, 6.9112, 121.94756008, 1202000.131387176, 1202000.131387176, 275548.255780654], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7045800.0000, 
sim time next is 7046400.0000, 
raw observation next is [27.76666666666667, 60.66666666666667, 1.0, 2.0, 0.3880577429116825, 1.0, 2.0, 0.3880577429116825, 1.0, 2.0, 0.6179593221890347, 6.911200000000001, 6.9112, 121.94756008, 1333135.152814216, 1333135.152814216, 291926.5920723601], 
processed observation next is [1.0, 0.5652173913043478, 0.5839506172839507, 0.6066666666666667, 1.0, 1.0, 0.2714973129900982, 1.0, 1.0, 0.2714973129900982, 1.0, 1.0, 0.5224491527362933, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.47611969743364857, 0.47611969743364857, 0.5613972924468463], 
reward next is 0.4386, 
noisyNet noise sample is [array([-1.4642645], dtype=float32), -0.9998684]. 
=============================================
[2019-04-27 20:49:10,611] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.95325684e-23 1.00000000e+00 1.04559825e-24 5.46849719e-25
 1.26397106e-08], sum to 1.0000
[2019-04-27 20:49:10,615] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3460
[2019-04-27 20:49:10,621] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 81.33333333333334, 1.0, 2.0, 0.4831756500701068, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 581656.64006911, 581656.64006911, 140466.5470305738], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7075200.0000, 
sim time next is 7075800.0000, 
raw observation next is [23.5, 81.5, 1.0, 2.0, 0.48349003085223, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 581808.7571377095, 581808.7571377091, 140507.4706880373], 
processed observation next is [1.0, 0.9130434782608695, 0.42592592592592593, 0.815, 1.0, 1.0, 0.38510717958598817, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.20778884183489627, 0.2077888418348961, 0.27020667440007173], 
reward next is 0.7298, 
noisyNet noise sample is [array([-0.9538458], dtype=float32), -0.57780725]. 
=============================================
[2019-04-27 20:49:11,952] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-27 20:49:11,952] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 20:49:11,953] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 20:49:11,954] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 20:49:11,955] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:49:11,955] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:49:11,956] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:49:11,955] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 20:49:11,956] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 20:49:11,960] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:49:11,962] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:49:11,984] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run68
[2019-04-27 20:49:12,007] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run68
[2019-04-27 20:49:12,033] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run68
[2019-04-27 20:49:12,033] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run68
[2019-04-27 20:49:12,034] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run68
[2019-04-27 20:49:13,899] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04824641], dtype=float32), -0.04968208]
[2019-04-27 20:49:13,903] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [30.46666666666667, 24.66666666666667, 1.0, 2.0, 0.3540518198745934, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 451273.5642299781, 451273.5642299785, 122512.9223593425]
[2019-04-27 20:49:13,904] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:49:13,906] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [4.8440083e-32 1.0000000e+00 9.5472176e-35 5.6765802e-34 1.0528639e-22], sampled 0.7208004466305699
[2019-04-27 20:49:29,011] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04824641], dtype=float32), -0.04968208]
[2019-04-27 20:49:29,012] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [37.85, 18.0, 1.0, 2.0, 0.751335025682819, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 921141.6109146491, 921141.6109146496, 188818.5908505933]
[2019-04-27 20:49:29,013] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:49:29,016] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [2.7446225e-30 1.0000000e+00 1.4027775e-32 1.2729005e-31 1.6880286e-19], sampled 0.025960472664663836
[2019-04-27 20:49:29,314] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04824641], dtype=float32), -0.04968208]
[2019-04-27 20:49:29,315] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [31.16666666666667, 28.83333333333334, 1.0, 2.0, 0.3581575562110373, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 448960.8788191205, 448960.8788191205, 122981.278377541]
[2019-04-27 20:49:29,318] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:49:29,320] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [4.6812140e-35 1.0000000e+00 3.7677597e-38 1.6080735e-37 3.3767120e-25], sampled 0.07525931396723184
[2019-04-27 20:49:33,125] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04824641], dtype=float32), -0.04968208]
[2019-04-27 20:49:33,127] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [25.88454317333333, 45.92170218166667, 1.0, 2.0, 0.4389203783367787, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 554572.2189768914, 554572.2189768914, 134422.0830601635]
[2019-04-27 20:49:33,128] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 20:49:33,130] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [6.7076117e-32 1.0000000e+00 1.3234183e-34 1.0204804e-33 2.5870402e-25], sampled 0.26393122334241026
[2019-04-27 20:49:40,858] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04824641], dtype=float32), -0.04968208]
[2019-04-27 20:49:40,859] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [24.66370490333333, 85.34551919, 1.0, 2.0, 0.5525836871668226, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 645235.5950207942, 645235.5950207942, 150840.1857860906]
[2019-04-27 20:49:40,859] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 20:49:40,863] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [2.8252880e-34 1.0000000e+00 2.4838394e-37 1.5452375e-36 4.8420041e-28], sampled 0.7020675438332351
[2019-04-27 20:50:04,479] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04824641], dtype=float32), -0.04968208]
[2019-04-27 20:50:04,480] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [29.0, 72.0, 1.0, 2.0, 0.5883023651932293, 1.0, 2.0, 0.5883023651932293, 1.0, 2.0, 0.9365965250580492, 6.911199999999999, 6.9112, 121.94756008, 2013008.473341351, 2013008.473341351, 389038.9637300496]
[2019-04-27 20:50:04,481] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 20:50:04,485] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [4.9938752e-20 6.5168007e-07 1.6605516e-19 6.1461841e-19 9.9999940e-01], sampled 0.2157427224312508
[2019-04-27 20:50:50,435] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04824641], dtype=float32), -0.04968208]
[2019-04-27 20:50:50,437] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [23.94285315333333, 78.86980034333334, 1.0, 2.0, 0.4731708186837363, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 568503.911143446, 568503.9111434455, 138890.8926392061]
[2019-04-27 20:50:50,438] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 20:50:50,441] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [9.3384761e-36 1.0000000e+00 0.0000000e+00 5.4890890e-38 1.0490103e-31], sampled 0.4370598745635793
[2019-04-27 20:50:52,795] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04824641], dtype=float32), -0.04968208]
[2019-04-27 20:50:52,798] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [21.1577439, 89.32405354333335, 1.0, 2.0, 0.389695180966079, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 480151.6638994091, 480151.6638994091, 127113.4441030687]
[2019-04-27 20:50:52,800] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 20:50:52,804] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [4.1439373e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1134186e-34], sampled 0.6570775977878468
[2019-04-27 20:50:57,092] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8616.3908 2249742229.1180 467.0000
[2019-04-27 20:50:57,177] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8925.0326 2121480955.7167 416.0000
[2019-04-27 20:50:57,178] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8781.6493 2171248529.3121 466.0000
[2019-04-27 20:50:57,221] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8715.5171 2197734079.4404 525.0000
[2019-04-27 20:50:57,341] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8149.4197 2448188913.9931 581.0000
[2019-04-27 20:50:58,358] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 1675000, evaluation results [1675000.0, 8149.419729229962, 2448188913.9931407, 581.0, 8781.649330887989, 2171248529.3120975, 466.0, 8925.03257593905, 2121480955.716715, 416.0, 8616.39078737136, 2249742229.1180134, 467.0, 8715.517069115584, 2197734079.440367, 525.0]
[2019-04-27 20:50:58,791] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6150357e-33 1.0000000e+00 1.0106923e-37 1.8096497e-35 4.9601228e-27], sum to 1.0000
[2019-04-27 20:50:58,805] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4353
[2019-04-27 20:50:58,809] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.53333333333333, 82.66666666666667, 1.0, 2.0, 0.4048839931050919, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 500795.9928313701, 500795.9928313701, 129289.455103953], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7100400.0000, 
sim time next is 7101000.0000, 
raw observation next is [21.4, 83.5, 1.0, 2.0, 0.4034444051291486, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 499184.0462514674, 499184.0462514674, 129089.557869442], 
processed observation next is [1.0, 0.17391304347826086, 0.3481481481481481, 0.835, 1.0, 1.0, 0.2898147680108912, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1782800165183812, 0.1782800165183812, 0.24824914974892692], 
reward next is 0.7518, 
noisyNet noise sample is [array([0.20836331], dtype=float32), -0.4286777]. 
=============================================
[2019-04-27 20:50:58,832] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[67.43105]
 [67.48118]
 [67.35332]
 [67.41863]
 [67.51012]], R is [[67.43733978]
 [67.51433563]
 [67.58782196]
 [67.66402435]
 [67.73915863]].
[2019-04-27 20:51:05,064] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.02103006e-32 1.00000000e+00 1.24187261e-32 8.79090982e-36
 1.65255973e-18], sum to 1.0000
[2019-04-27 20:51:05,071] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2406
[2019-04-27 20:51:05,074] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.4, 79.0, 1.0, 2.0, 0.3761198511190025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 469078.1328911004, 469078.1328911004, 125369.1907431551], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7250400.0000, 
sim time next is 7251000.0000, 
raw observation next is [21.31666666666667, 79.33333333333334, 1.0, 2.0, 0.3740140937133927, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 466707.0884256142, 466707.0884256142, 125086.3875476137], 
processed observation next is [1.0, 0.9565217391304348, 0.34506172839506183, 0.7933333333333334, 1.0, 1.0, 0.2547786829921342, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16668110300914793, 0.16668110300914793, 0.24055074528387252], 
reward next is 0.7594, 
noisyNet noise sample is [array([-0.02208332], dtype=float32), -0.7817028]. 
=============================================
[2019-04-27 20:51:05,086] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[85.466896]
 [85.2724  ]
 [85.2154  ]
 [85.14279 ]
 [85.01776 ]], R is [[85.47351837]
 [85.37768555]
 [85.2820816 ]
 [85.18679047]
 [85.09220123]].
[2019-04-27 20:51:06,364] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1267666e-33 1.0000000e+00 1.5896453e-33 3.3920726e-34 8.3750848e-19], sum to 1.0000
[2019-04-27 20:51:06,372] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6944
[2019-04-27 20:51:06,376] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 77.33333333333334, 1.0, 2.0, 0.3852765476410502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 478828.5192231728, 478828.5192231728, 126595.7753023294], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7247400.0000, 
sim time next is 7248000.0000, 
raw observation next is [21.8, 77.66666666666667, 1.0, 2.0, 0.3834151892394148, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 476846.2590035744, 476846.2590035744, 126345.459053923], 
processed observation next is [1.0, 0.9130434782608695, 0.362962962962963, 0.7766666666666667, 1.0, 1.0, 0.2659704633802557, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.17030223535841943, 0.17030223535841943, 0.2429720366421596], 
reward next is 0.7570, 
noisyNet noise sample is [array([0.44513115], dtype=float32), 0.108337946]. 
=============================================
[2019-04-27 20:51:06,398] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[82.59471]
 [82.43324]
 [82.29598]
 [82.06955]
 [81.94965]], R is [[82.44268799]
 [82.37480927]
 [82.30719757]
 [82.23995209]
 [82.17298126]].
[2019-04-27 20:51:07,247] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9885792e-29 1.0000000e+00 3.2030237e-33 1.7165509e-31 8.7076007e-22], sum to 1.0000
[2019-04-27 20:51:07,257] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2707
[2019-04-27 20:51:07,265] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.46666666666667, 87.66666666666666, 1.0, 2.0, 0.676840733800484, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 829833.2342077352, 829833.2342077352, 174199.3164861039], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7288800.0000, 
sim time next is 7289400.0000, 
raw observation next is [21.58333333333333, 86.83333333333333, 1.0, 2.0, 0.682821191814302, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 836973.9345719052, 836973.9345719047, 175331.1587691557], 
processed observation next is [1.0, 0.34782608695652173, 0.35493827160493807, 0.8683333333333333, 1.0, 1.0, 0.6224061807313119, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.29891926234710897, 0.2989192623471088, 0.33717530532529943], 
reward next is 0.6628, 
noisyNet noise sample is [array([-0.8394291], dtype=float32), -0.875565]. 
=============================================
[2019-04-27 20:51:07,424] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.6347416e-36 1.0000000e+00 0.0000000e+00 6.5847340e-38 1.1765222e-31], sum to 1.0000
[2019-04-27 20:51:07,433] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1326
[2019-04-27 20:51:07,443] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.71666666666667, 83.66666666666666, 1.0, 2.0, 0.3665111857694582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 457640.6342994956, 457640.6342994956, 124073.3128011986], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7260600.0000, 
sim time next is 7261200.0000, 
raw observation next is [20.7, 84.0, 1.0, 2.0, 0.3672164827965325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 458368.1614068938, 458368.1614068934, 124165.8005548205], 
processed observation next is [1.0, 0.043478260869565216, 0.3222222222222222, 0.84, 1.0, 1.0, 0.2466862890434911, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.16370291478817636, 0.16370291478817622, 0.2387803856823471], 
reward next is 0.7612, 
noisyNet noise sample is [array([0.23911493], dtype=float32), 1.1642059]. 
=============================================
[2019-04-27 20:51:07,741] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1905575e-19 1.0520008e-01 5.5735442e-21 5.5043700e-20 8.9479995e-01], sum to 1.0000
[2019-04-27 20:51:07,749] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5100
[2019-04-27 20:51:07,758] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 35.0, 1.0, 2.0, 0.3621444748717801, 1.0, 2.0, 0.3621444748717801, 1.0, 2.0, 0.5822594019677956, 6.9112, 6.9112, 121.94756008, 1293154.104538273, 1293154.104538273, 280624.1541568101], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7831200.0000, 
sim time next is 7831800.0000, 
raw observation next is [31.0, 34.5, 1.0, 2.0, 0.3576441357082377, 1.0, 2.0, 0.3576441357082377, 1.0, 2.0, 0.575658145051205, 6.911199999999999, 6.9112, 121.94756008, 1280097.886610474, 1280097.886610474, 278690.0486950123], 
processed observation next is [1.0, 0.6521739130434783, 0.7037037037037037, 0.345, 1.0, 1.0, 0.23529063774790204, 1.0, 1.0, 0.23529063774790204, 1.0, 1.0, 0.46957268131400626, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.45717781664659785, 0.45717781664659785, 0.5359424013365621], 
reward next is 0.4641, 
noisyNet noise sample is [array([-0.68403], dtype=float32), 0.31980503]. 
=============================================
[2019-04-27 20:51:08,562] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.4707642e-27 1.0000000e+00 1.5915105e-30 1.2685779e-29 6.4318079e-15], sum to 1.0000
[2019-04-27 20:51:08,570] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3849
[2019-04-27 20:51:08,578] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.1, 84.0, 1.0, 2.0, 0.7358855324494312, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 900084.9612374539, 900084.9612374535, 185642.3930773789], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7291800.0000, 
sim time next is 7292400.0000, 
raw observation next is [22.23333333333333, 83.33333333333334, 1.0, 2.0, 0.7339004772297776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000002, 6.9112, 121.9260426156618, 897019.9074671158, 897019.9074671149, 185226.2091944439], 
processed observation next is [1.0, 0.391304347826087, 0.37901234567901226, 0.8333333333333335, 1.0, 1.0, 0.6832148538449733, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 1.7763568394002506e-16, 0.0, 0.8094621288201359, 0.32036425266682705, 0.3203642526668267, 0.35620424845085363], 
reward next is 0.6438, 
noisyNet noise sample is [array([1.6146241], dtype=float32), 0.2919235]. 
=============================================
[2019-04-27 20:51:13,446] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:51:13,454] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0115
[2019-04-27 20:51:13,463] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.36666666666667, 96.0, 1.0, 2.0, 0.3814648601614822, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 475338.084300661, 475338.084300661, 126095.0766824331], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7364400.0000, 
sim time next is 7365000.0000, 
raw observation next is [19.33333333333334, 96.0, 1.0, 2.0, 0.3828855032102159, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 477318.6169445714, 477318.6169445714, 126295.0275450065], 
processed observation next is [1.0, 0.21739130434782608, 0.27160493827160515, 0.96, 1.0, 1.0, 0.26533988477406656, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1704709346230612, 0.1704709346230612, 0.24287505297116632], 
reward next is 0.7571, 
noisyNet noise sample is [array([-0.4660516], dtype=float32), -0.65875834]. 
=============================================
[2019-04-27 20:51:13,477] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[70.92847]
 [70.97946]
 [71.03468]
 [71.01164]
 [71.037  ]], R is [[70.9156723 ]
 [70.96401978]
 [71.01255798]
 [71.06030273]
 [71.10231018]].
[2019-04-27 20:51:14,142] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4822407e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.7219894e-38], sum to 1.0000
[2019-04-27 20:51:14,152] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0574
[2019-04-27 20:51:14,159] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.03333333333333, 95.0, 1.0, 2.0, 0.3895501670830586, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 488358.8678266539, 488358.8678266539, 127267.4781300681], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7373400.0000, 
sim time next is 7374000.0000, 
raw observation next is [19.06666666666667, 95.0, 1.0, 2.0, 0.5807459129423262, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 727543.1348178821, 727543.1348178821, 157148.8893411237], 
processed observation next is [1.0, 0.34782608695652173, 0.2617283950617285, 0.95, 1.0, 1.0, 0.5008879915980075, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2598368338635293, 0.2598368338635293, 0.302209402579084], 
reward next is 0.6978, 
noisyNet noise sample is [array([-0.12637062], dtype=float32), -0.29043165]. 
=============================================
[2019-04-27 20:51:14,171] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[66.69315]
 [66.76545]
 [66.73742]
 [66.77755]
 [66.80345]], R is [[65.43113708]
 [65.5320816 ]
 [65.63703918]
 [65.73866272]
 [65.83907318]].
[2019-04-27 20:51:14,766] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.6312950e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.9294661e-27], sum to 1.0000
[2019-04-27 20:51:14,776] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9646
[2019-04-27 20:51:14,787] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.1, 91.0, 1.0, 2.0, 0.3870097970520082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 481221.1279958175, 481221.1279958175, 126840.8055763572], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7416000.0000, 
sim time next is 7416600.0000, 
raw observation next is [20.11666666666667, 90.83333333333334, 1.0, 2.0, 0.3854415079280883, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 479372.6391445666, 479372.6391445666, 126625.6953118571], 
processed observation next is [1.0, 0.8695652173913043, 0.30061728395061743, 0.9083333333333334, 1.0, 1.0, 0.26838274753343844, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.17120451398020237, 0.17120451398020237, 0.2435109525228021], 
reward next is 0.7565, 
noisyNet noise sample is [array([0.56185746], dtype=float32), -0.93900853]. 
=============================================
[2019-04-27 20:51:15,424] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:51:15,424] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:51:15,489] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res14/Eplus-env-sub_run9
[2019-04-27 20:51:19,898] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3647379e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.4986121e-35], sum to 1.0000
[2019-04-27 20:51:19,909] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3469
[2019-04-27 20:51:19,914] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.95, 94.0, 1.0, 2.0, 0.4774217872444759, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 573627.2464989118, 573627.2464989118, 139542.3522831036], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7511400.0000, 
sim time next is 7512000.0000, 
raw observation next is [21.9, 94.33333333333333, 1.0, 2.0, 0.4768074892950998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 572982.4012826721, 572982.4012826721, 139451.3166179553], 
processed observation next is [0.0, 0.9565217391304348, 0.36666666666666664, 0.9433333333333332, 1.0, 1.0, 0.37715177297035696, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2046365718866686, 0.2046365718866686, 0.26817560888068326], 
reward next is 0.7318, 
noisyNet noise sample is [array([0.33737123], dtype=float32), 0.16004921]. 
=============================================
[2019-04-27 20:51:19,926] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[73.21542]
 [73.15198]
 [73.08494]
 [73.01349]
 [72.89161]], R is [[73.2687912 ]
 [73.2677536 ]
 [73.26652527]
 [73.26506805]
 [73.26338959]].
[2019-04-27 20:51:22,491] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.1804456e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.1786543e-34], sum to 1.0000
[2019-04-27 20:51:22,497] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2001
[2019-04-27 20:51:22,502] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666666, 66.0, 1.0, 2.0, 0.5349089928771333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 629577.981908564, 629577.981908564, 148157.1008815281], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7566600.0000, 
sim time next is 7567200.0000, 
raw observation next is [27.0, 66.0, 1.0, 2.0, 0.5265408018357839, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 621568.7504674372, 621568.7504674372, 146877.6023841393], 
processed observation next is [0.0, 0.6086956521739131, 0.5555555555555556, 0.66, 1.0, 1.0, 0.43635809742355225, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.22198883945265616, 0.22198883945265616, 0.2824569276618063], 
reward next is 0.7175, 
noisyNet noise sample is [array([0.9705943], dtype=float32), 0.18167649]. 
=============================================
[2019-04-27 20:51:23,216] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:51:23,216] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:51:23,276] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res3/Eplus-env-sub_run9
[2019-04-27 20:51:23,498] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.563428e-37 1.000000e+00 0.000000e+00 0.000000e+00 8.739447e-37], sum to 1.0000
[2019-04-27 20:51:23,502] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7918
[2019-04-27 20:51:23,507] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.18333333333333, 94.33333333333334, 1.0, 2.0, 0.4393901362459653, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 534754.4986671219, 534754.4986671219, 134052.3347524344], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7546200.0000, 
sim time next is 7546800.0000, 
raw observation next is [21.36666666666667, 93.66666666666667, 1.0, 2.0, 0.4441508016693149, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 539650.2029755571, 539650.2029755571, 134728.4108116245], 
processed observation next is [0.0, 0.34782608695652173, 0.3469135802469137, 0.9366666666666668, 1.0, 1.0, 0.33827476389204164, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.19273221534841323, 0.19273221534841323, 0.2590930977146625], 
reward next is 0.7409, 
noisyNet noise sample is [array([1.0366172], dtype=float32), 0.30913958]. 
=============================================
[2019-04-27 20:51:31,971] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:51:31,979] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7190
[2019-04-27 20:51:31,984] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.1, 73.0, 1.0, 2.0, 0.2654754295834408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 342147.3830642386, 342147.3830642386, 111405.6482483123], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7701000.0000, 
sim time next is 7701600.0000, 
raw observation next is [19.0, 75.0, 1.0, 2.0, 0.2605002956742637, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 335304.0101204352, 335304.0101204347, 110827.6876964249], 
processed observation next is [1.0, 0.13043478260869565, 0.25925925925925924, 0.75, 1.0, 1.0, 0.1196432091360282, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.11975143218586971, 0.11975143218586953, 0.21313016864697096], 
reward next is 0.7869, 
noisyNet noise sample is [array([0.55520463], dtype=float32), 1.271015]. 
=============================================
[2019-04-27 20:51:40,091] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.7452609e-31 1.0000000e+00 8.7473650e-34 1.0516387e-31 1.2340275e-19], sum to 1.0000
[2019-04-27 20:51:40,102] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3143
[2019-04-27 20:51:40,105] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.45, 58.0, 1.0, 2.0, 0.4377184735213325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 533705.5357258646, 533705.5357258646, 133835.321385375], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7846200.0000, 
sim time next is 7846800.0000, 
raw observation next is [26.13333333333333, 59.66666666666667, 1.0, 2.0, 0.4388831615421846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 535148.4164599017, 535148.4164599017, 134007.3706459967], 
processed observation next is [1.0, 0.8260869565217391, 0.5234567901234567, 0.5966666666666667, 1.0, 1.0, 0.332003763740696, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1911244344499649, 0.1911244344499649, 0.2577064820115321], 
reward next is 0.7423, 
noisyNet noise sample is [array([-0.9337324], dtype=float32), -1.1718167]. 
=============================================
[2019-04-27 20:51:41,506] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:51:41,506] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:51:41,577] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res11/Eplus-env-sub_run9
[2019-04-27 20:51:43,404] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.2428694e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3630609e-33], sum to 1.0000
[2019-04-27 20:51:43,415] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4371
[2019-04-27 20:51:43,421] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.15, 46.5, 1.0, 2.0, 0.2833036039235055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 365426.5926981753, 365426.5926981753, 113217.1536443969], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 343800.0000, 
sim time next is 344400.0000, 
raw observation next is [23.0, 47.0, 1.0, 2.0, 0.2814484225111748, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 363053.3474563881, 363053.3474563877, 112265.0787972713], 
processed observation next is [0.0, 1.0, 0.4074074074074074, 0.47, 1.0, 1.0, 0.1445814553704462, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1296619098058529, 0.12966190980585277, 0.21589438230244481], 
reward next is 0.7841, 
noisyNet noise sample is [array([-0.3930915], dtype=float32), 2.3760538]. 
=============================================
[2019-04-27 20:51:44,459] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:51:44,459] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:51:44,526] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res12/Eplus-env-sub_run9
[2019-04-27 20:51:44,599] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:51:44,599] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:51:44,648] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res16/Eplus-env-sub_run9
[2019-04-27 20:51:44,976] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:51:44,976] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:51:45,001] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res7/Eplus-env-sub_run9
[2019-04-27 20:51:45,393] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:51:45,394] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:51:45,411] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res13/Eplus-env-sub_run9
[2019-04-27 20:51:45,440] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:51:45,441] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:51:45,461] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res10/Eplus-env-sub_run9
[2019-04-27 20:51:45,515] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:51:45,516] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:51:45,526] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res2/Eplus-env-sub_run9
[2019-04-27 20:51:45,650] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:51:45,650] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:51:45,663] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res17/Eplus-env-sub_run9
[2019-04-27 20:51:45,769] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:51:45,770] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:51:45,774] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:51:45,775] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:51:45,780] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res8/Eplus-env-sub_run9
[2019-04-27 20:51:45,803] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:51:45,803] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:51:45,811] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res4/Eplus-env-sub_run9
[2019-04-27 20:51:45,840] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res5/Eplus-env-sub_run9
[2019-04-27 20:51:45,868] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:51:45,869] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:51:45,882] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res6/Eplus-env-sub_run9
[2019-04-27 20:51:45,923] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:51:45,923] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:51:45,931] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res9/Eplus-env-sub_run9
[2019-04-27 20:51:45,990] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 20:51:45,990] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:51:45,994] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res15/Eplus-env-sub_run9
[2019-04-27 20:51:49,676] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3133231e-21 9.9999976e-01 9.6982497e-23 4.8422211e-22 2.1061358e-07], sum to 1.0000
[2019-04-27 20:51:49,687] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6993
[2019-04-27 20:51:49,696] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.1, 38.0, 1.0, 2.0, 0.7963382295811717, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 986752.4592473916, 986752.4592473916, 198416.2277198593], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 46800.0000, 
sim time next is 47400.0000, 
raw observation next is [29.28333333333334, 37.66666666666667, 1.0, 2.0, 0.8553665610120931, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1063872.809446262, 1063872.809446262, 211300.0332433337], 
processed observation next is [1.0, 0.5652173913043478, 0.6401234567901236, 0.3766666666666667, 1.0, 1.0, 0.827817334538206, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.37995457480223643, 0.37995457480223643, 0.4063462177756417], 
reward next is 0.5937, 
noisyNet noise sample is [array([0.9574062], dtype=float32), -1.2293797]. 
=============================================
[2019-04-27 20:51:51,080] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-27 20:51:51,083] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 20:51:51,084] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:51:51,084] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 20:51:51,085] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 20:51:51,087] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:51:51,088] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 20:51:51,090] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 20:51:51,088] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:51:51,093] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:51:51,092] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:51:51,109] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run69
[2019-04-27 20:51:51,134] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run69
[2019-04-27 20:51:51,153] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run69
[2019-04-27 20:51:51,155] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run69
[2019-04-27 20:51:51,198] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run69
[2019-04-27 20:52:39,139] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04882867], dtype=float32), -0.04608323]
[2019-04-27 20:52:39,141] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [31.53333333333333, 57.33333333333334, 1.0, 2.0, 0.5510047615719251, 1.0, 2.0, 0.5510047615719251, 1.0, 2.0, 0.8772175253947854, 6.9112, 6.9112, 121.94756008, 1885251.679528997, 1885251.679528997, 369327.4543495005]
[2019-04-27 20:52:39,141] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 20:52:39,147] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.1594941e-22 1.4123877e-11 3.4496949e-21 2.8635561e-20 1.0000000e+00], sampled 0.29412843672754563
[2019-04-27 20:53:20,565] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04882867], dtype=float32), -0.04608323]
[2019-04-27 20:53:20,566] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [26.36666666666667, 50.33333333333333, 1.0, 2.0, 0.8030602954092736, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.926030586348, 997827.864427295, 997827.864427295, 199909.4023971784]
[2019-04-27 20:53:20,567] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 20:53:20,569] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.2996905e-24 1.0000000e+00 4.7042028e-26 3.8783962e-25 6.5515623e-15], sampled 0.9288633007245012
[2019-04-27 20:53:21,273] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04882867], dtype=float32), -0.04608323]
[2019-04-27 20:53:21,274] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [22.4, 76.0, 1.0, 2.0, 0.4027239474904973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 498262.6167798675, 498262.6167798675, 128986.9146093066]
[2019-04-27 20:53:21,275] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 20:53:21,278] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [3.2801289e-35 1.0000000e+00 1.6740776e-38 8.6148864e-38 1.7620908e-31], sampled 0.7722709423360679
[2019-04-27 20:53:26,915] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04882867], dtype=float32), -0.04608323]
[2019-04-27 20:53:26,915] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [30.66666666666667, 21.66666666666666, 1.0, 2.0, 0.3552388106868805, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 455384.9655241916, 455384.9655241916, 122674.2275417386]
[2019-04-27 20:53:26,916] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:53:26,918] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [3.4404239e-32 1.0000000e+00 3.9008703e-35 1.1984033e-34 2.8676605e-24], sampled 0.3615565083951743
[2019-04-27 20:53:35,605] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8788.3960 2171934595.8609 458.0000
[2019-04-27 20:53:35,858] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8929.3187 2121907954.5679 405.0000
[2019-04-27 20:53:36,081] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8642.5139 2250519039.6279 432.0000
[2019-04-27 20:53:36,084] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8728.6776 2198868135.6460 497.0000
[2019-04-27 20:53:36,278] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8184.3544 2451575231.3310 522.0000
[2019-04-27 20:53:37,296] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 1700000, evaluation results [1700000.0, 8184.354369757636, 2451575231.331045, 522.0, 8788.396039980678, 2171934595.8608665, 458.0, 8929.318717353188, 2121907954.567869, 405.0, 8642.513895897597, 2250519039.6278644, 432.0, 8728.677613239131, 2198868135.646019, 497.0]
[2019-04-27 20:54:07,615] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.0336201e-27 1.0000000e+00 3.6977321e-31 1.1392522e-28 1.1639171e-14], sum to 1.0000
[2019-04-27 20:54:07,621] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8883
[2019-04-27 20:54:07,628] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.3, 64.33333333333333, 1.0, 2.0, 0.3372723778732024, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 426681.3902147502, 426681.3902147507, 120279.953259515], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1107600.0000, 
sim time next is 1108200.0000, 
raw observation next is [22.05, 65.66666666666667, 1.0, 2.0, 0.3340871028877356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 422823.4142250275, 422823.414225028, 119868.063315898], 
processed observation next is [1.0, 0.8260869565217391, 0.37222222222222223, 0.6566666666666667, 1.0, 1.0, 0.20724655105682807, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1510083622232241, 0.15100836222322428, 0.23051550637672694], 
reward next is 0.7695, 
noisyNet noise sample is [array([0.636039], dtype=float32), 0.5615272]. 
=============================================
[2019-04-27 20:54:10,362] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.7198671e-31 1.0000000e+00 6.1694963e-32 2.6296661e-32 5.9407397e-15], sum to 1.0000
[2019-04-27 20:54:10,370] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6974
[2019-04-27 20:54:10,376] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.66666666666667, 22.33333333333334, 1.0, 2.0, 0.3794715741277151, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 478964.2272798027, 478964.2272798027, 125918.1762407966], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 673800.0000, 
sim time next is 674400.0000, 
raw observation next is [32.43333333333334, 22.66666666666667, 1.0, 2.0, 0.377856463894927, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 477304.4490015315, 477304.4490015315, 125700.5140505548], 
processed observation next is [1.0, 0.8260869565217391, 0.7567901234567903, 0.2266666666666667, 1.0, 1.0, 0.25935293320824643, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.17046587464340412, 0.17046587464340412, 0.24173175778952846], 
reward next is 0.7583, 
noisyNet noise sample is [array([-1.4682508], dtype=float32), 0.047192898]. 
=============================================
[2019-04-27 20:54:15,226] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0760863e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0102561e-31], sum to 1.0000
[2019-04-27 20:54:15,234] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7847
[2019-04-27 20:54:15,241] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 67.0, 1.0, 2.0, 0.3336414516030382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 422512.0950723681, 422512.0950723681, 119812.7683802453], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1384200.0000, 
sim time next is 1384800.0000, 
raw observation next is [21.73333333333333, 67.0, 1.0, 2.0, 0.3296840149117203, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 417788.4807674947, 417788.4807674947, 119304.3253765155], 
processed observation next is [0.0, 0.0, 0.3604938271604937, 0.67, 1.0, 1.0, 0.20200477965680985, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.14921017170267667, 0.14921017170267667, 0.2294313949548375], 
reward next is 0.7706, 
noisyNet noise sample is [array([0.39767253], dtype=float32), 1.6336484]. 
=============================================
[2019-04-27 20:54:22,453] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.069205e-34 1.000000e+00 0.000000e+00 2.259285e-37 7.143196e-29], sum to 1.0000
[2019-04-27 20:54:22,464] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7001
[2019-04-27 20:54:22,468] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.1, 62.0, 1.0, 2.0, 0.390455666055733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 486182.2216009463, 486182.2216009463, 127333.4616915992], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 865800.0000, 
sim time next is 866400.0000, 
raw observation next is [23.93333333333334, 62.66666666666667, 1.0, 2.0, 0.3882338407358635, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 483727.6433094683, 483727.6433094678, 127030.5572707251], 
processed observation next is [0.0, 0.0, 0.4419753086419756, 0.6266666666666667, 1.0, 1.0, 0.27170695325698035, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1727598726105244, 0.17275987261052422, 0.24428953321293287], 
reward next is 0.7557, 
noisyNet noise sample is [array([-0.1750489], dtype=float32), -0.11678593]. 
=============================================
[2019-04-27 20:54:30,217] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-27 20:54:30,220] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 20:54:30,221] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:54:30,222] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 20:54:30,223] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 20:54:30,227] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:54:30,230] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:54:30,229] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 20:54:30,230] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 20:54:30,233] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:54:30,237] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:54:30,260] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run70
[2019-04-27 20:54:30,283] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run70
[2019-04-27 20:54:30,301] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run70
[2019-04-27 20:54:30,322] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run70
[2019-04-27 20:54:30,346] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run70
[2019-04-27 20:54:40,732] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04223787], dtype=float32), -0.03896221]
[2019-04-27 20:54:40,734] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [23.86666666666667, 47.16666666666666, 1.0, 2.0, 0.3577349622317351, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 458619.9961860183, 458619.9961860183, 123007.8150713748]
[2019-04-27 20:54:40,734] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 20:54:40,736] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [6.9132748e-33 1.0000000e+00 5.9558562e-36 4.4951527e-35 5.0710342e-26], sampled 0.8538499681099064
[2019-04-27 20:54:42,854] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04223787], dtype=float32), -0.03896221]
[2019-04-27 20:54:42,855] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [18.83333333333333, 36.16666666666666, 1.0, 2.0, 0.2292634428735248, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 295724.9593393236, 295724.9593393232, 79187.58411620664]
[2019-04-27 20:54:42,856] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:54:42,859] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [2.9242833e-34 1.0000000e+00 4.1886420e-38 4.6021337e-37 8.0878820e-29], sampled 0.49875910855848815
[2019-04-27 20:54:49,107] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04223787], dtype=float32), -0.03896221]
[2019-04-27 20:54:49,108] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [23.151573845, 55.53358825166667, 1.0, 2.0, 0.318895343876588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 405930.7867855986, 405930.7867855986, 117937.9698700975]
[2019-04-27 20:54:49,109] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 20:54:49,111] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [5.5612483e-33 1.0000000e+00 3.7820774e-36 5.7864595e-36 2.2958707e-22], sampled 0.933350873503496
[2019-04-27 20:54:53,177] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04223787], dtype=float32), -0.03896221]
[2019-04-27 20:54:53,178] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [31.83333333333334, 33.0, 1.0, 2.0, 0.4061857351969103, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 499508.2420076206, 499508.2420076206, 129402.9857392686]
[2019-04-27 20:54:53,181] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:54:53,184] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [9.8604821e-32 1.0000000e+00 1.5963959e-34 2.4292408e-34 9.9121718e-20], sampled 0.7201004406235951
[2019-04-27 20:55:18,913] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04223787], dtype=float32), -0.03896221]
[2019-04-27 20:55:18,915] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [25.7, 82.0, 1.0, 2.0, 0.5885646296616527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 678881.0534000405, 678881.0534000405, 156517.025659596]
[2019-04-27 20:55:18,917] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 20:55:18,920] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [8.8781753e-33 1.0000000e+00 1.2467644e-35 8.9296578e-35 1.9228404e-25], sampled 0.8596076842895889
[2019-04-27 20:55:43,693] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04223787], dtype=float32), -0.03896221]
[2019-04-27 20:55:43,693] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [22.0, 83.0, 1.0, 2.0, 0.8459959113171129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260096134638, 1037651.908105574, 1037651.908105574, 208829.9859817715]
[2019-04-27 20:55:43,694] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:55:43,699] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [7.2212706e-24 1.0000000e+00 4.4533594e-25 7.3366683e-24 2.6787397e-10], sampled 0.8213442068627437
[2019-04-27 20:55:53,396] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04223787], dtype=float32), -0.03896221]
[2019-04-27 20:55:53,398] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [23.0, 92.33333333333334, 1.0, 2.0, 0.5602023621747353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 662327.7005358806, 662327.7005358806, 152446.2774978302]
[2019-04-27 20:55:53,401] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:55:53,405] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.3808083e-32 1.0000000e+00 2.2916643e-35 1.3578239e-34 8.5143773e-26], sampled 0.33090993519739764
[2019-04-27 20:56:13,317] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8930.0894 2122952267.9487 401.0000
[2019-04-27 20:56:13,523] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8202.5736 2453449543.3351 492.0000
[2019-04-27 20:56:13,533] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8742.9243 2200368885.5767 477.0000
[2019-04-27 20:56:13,621] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8654.7524 2252578672.5441 403.0000
[2019-04-27 20:56:13,646] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8804.6776 2173094149.0614 431.0000
[2019-04-27 20:56:14,662] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 1725000, evaluation results [1725000.0, 8202.573587953693, 2453449543.335148, 492.0, 8804.677632727302, 2173094149.061429, 431.0, 8930.089357748579, 2122952267.9487174, 401.0, 8654.752384405658, 2252578672.544074, 403.0, 8742.92431457823, 2200368885.5767283, 477.0]
[2019-04-27 20:56:20,968] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.9009982e-34 1.0000000e+00 1.6890180e-37 1.5760272e-36 9.9219927e-23], sum to 1.0000
[2019-04-27 20:56:20,975] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1002
[2019-04-27 20:56:20,983] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.4, 82.5, 1.0, 2.0, 0.3951004080636598, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 489764.0622078954, 489764.0622078954, 127935.9275438229], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1290600.0000, 
sim time next is 1291200.0000, 
raw observation next is [21.2, 83.66666666666666, 1.0, 2.0, 0.3941111875981458, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 488844.535737899, 488844.535737899, 127804.273555484], 
processed observation next is [1.0, 0.9565217391304348, 0.34074074074074073, 0.8366666666666666, 1.0, 1.0, 0.27870379475969737, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.17458733419210679, 0.17458733419210679, 0.24577744914516156], 
reward next is 0.7542, 
noisyNet noise sample is [array([0.44307178], dtype=float32), 1.0991215]. 
=============================================
[2019-04-27 20:56:28,647] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.4923111e-28 1.0000000e+00 4.5037827e-29 1.2301542e-27 1.0830788e-14], sum to 1.0000
[2019-04-27 20:56:28,655] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4174
[2019-04-27 20:56:28,661] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.25, 66.5, 1.0, 2.0, 0.8439029894429422, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.926042608599, 1051613.354354949, 1051613.354354949, 208813.3153887548], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1251000.0000, 
sim time next is 1251600.0000, 
raw observation next is [23.43333333333333, 66.0, 1.0, 2.0, 0.8389075221416558, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156597, 1043966.269895872, 1043966.269895873, 207683.8925785494], 
processed observation next is [1.0, 0.4782608695652174, 0.42345679012345666, 0.66, 1.0, 1.0, 0.8082232406448283, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.809462128820122, 0.37284509639138286, 0.37284509639138325, 0.399392101112595], 
reward next is 0.6006, 
noisyNet noise sample is [array([0.7419805], dtype=float32), -0.34431764]. 
=============================================
[2019-04-27 20:56:33,213] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.0668236e-19 2.1148433e-05 2.4039051e-18 2.5072386e-18 9.9997890e-01], sum to 1.0000
[2019-04-27 20:56:33,220] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7700
[2019-04-27 20:56:33,233] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.4, 40.0, 1.0, 2.0, 0.3319022349036957, 1.0, 2.0, 0.3319022349036957, 1.0, 2.0, 0.5396471020050447, 6.911199999999999, 6.9112, 121.94756008, 1208058.821955227, 1208058.821955227, 267605.4675722824], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1339200.0000, 
sim time next is 1339800.0000, 
raw observation next is [28.58333333333334, 39.33333333333334, 1.0, 2.0, 0.2977555657463279, 1.0, 2.0, 0.2977555657463279, 1.0, 2.0, 0.4843866888492505, 6.911200000000001, 6.9112, 121.94756008, 1084471.423586018, 1084471.423586017, 254202.5958282429], 
processed observation next is [1.0, 0.5217391304347826, 0.6141975308641977, 0.3933333333333334, 1.0, 1.0, 0.1639947211265808, 1.0, 1.0, 0.1639947211265808, 1.0, 1.0, 0.3554833610615631, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.38731122270929214, 0.3873112227092918, 0.488851145823544], 
reward next is 0.5111, 
noisyNet noise sample is [array([3.2854724], dtype=float32), 0.07748991]. 
=============================================
[2019-04-27 20:56:35,234] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.9586083e-30 1.0000000e+00 2.5846538e-32 3.2749235e-30 3.2176091e-18], sum to 1.0000
[2019-04-27 20:56:35,243] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6214
[2019-04-27 20:56:35,248] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.03333333333333, 59.0, 1.0, 2.0, 0.3617843355449866, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 453667.6622166314, 453667.662216631, 123470.02657682], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1374600.0000, 
sim time next is 1375200.0000, 
raw observation next is [23.8, 60.0, 1.0, 2.0, 0.3591863863820412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 450700.817990955, 450700.817990955, 123126.1839754896], 
processed observation next is [1.0, 0.9565217391304348, 0.43703703703703706, 0.6, 1.0, 1.0, 0.23712665045481096, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16096457785391252, 0.16096457785391252, 0.2367811230297877], 
reward next is 0.7632, 
noisyNet noise sample is [array([0.7225628], dtype=float32), 0.80426735]. 
=============================================
[2019-04-27 20:56:36,016] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:56:36,027] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8535
[2019-04-27 20:56:36,035] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 67.0, 1.0, 2.0, 0.3231902784973242, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 410118.4835385374, 410118.4835385374, 118476.3741371587], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1386000.0000, 
sim time next is 1386600.0000, 
raw observation next is [21.51666666666667, 67.0, 1.0, 2.0, 0.3206613036968219, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 407230.302336377, 407230.3023363766, 118156.5028984461], 
processed observation next is [0.0, 0.043478260869565216, 0.35246913580246925, 0.67, 1.0, 1.0, 0.19126345678193082, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1454393936915632, 0.14543939369156308, 0.22722404403547325], 
reward next is 0.7728, 
noisyNet noise sample is [array([-0.85497695], dtype=float32), -0.99046385]. 
=============================================
[2019-04-27 20:56:37,662] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3113803e-37], sum to 1.0000
[2019-04-27 20:56:37,669] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0052
[2019-04-27 20:56:37,672] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.15, 23.5, 1.0, 2.0, 0.3841958911872081, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 481406.843893373, 481406.843893373, 126520.1668833508], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1429800.0000, 
sim time next is 1430400.0000, 
raw observation next is [33.3, 23.0, 1.0, 2.0, 0.3853154297372423, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 482983.6902552621, 482983.6902552617, 126678.0122922564], 
processed observation next is [0.0, 0.5652173913043478, 0.7888888888888888, 0.23, 1.0, 1.0, 0.268232654449098, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.17249417509116502, 0.1724941750911649, 0.2436115621004931], 
reward next is 0.7564, 
noisyNet noise sample is [array([0.80388445], dtype=float32), 1.8804861]. 
=============================================
[2019-04-27 20:56:40,486] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.1303155e-38], sum to 1.0000
[2019-04-27 20:56:40,493] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4694
[2019-04-27 20:56:40,498] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.53333333333333, 63.83333333333334, 1.0, 2.0, 0.3728538525038321, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 466111.421080178, 466111.421080178, 124944.0370999134], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1491000.0000, 
sim time next is 1491600.0000, 
raw observation next is [23.86666666666667, 62.66666666666667, 1.0, 2.0, 0.3775044911303331, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 471151.7059770997, 471151.7059770997, 125565.5370746192], 
processed observation next is [0.0, 0.2608695652173913, 0.4395061728395063, 0.6266666666666667, 1.0, 1.0, 0.2589339180123013, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16826846642039275, 0.16826846642039275, 0.24147218668196002], 
reward next is 0.7585, 
noisyNet noise sample is [array([0.9465386], dtype=float32), 1.4065857]. 
=============================================
[2019-04-27 20:56:42,299] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2990226e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1966309e-34], sum to 1.0000
[2019-04-27 20:56:42,310] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9582
[2019-04-27 20:56:42,314] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.95, 32.0, 1.0, 2.0, 0.4406419751899729, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 536600.2022084442, 536600.2022084438, 134246.3286060114], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1507800.0000, 
sim time next is 1508400.0000, 
raw observation next is [33.3, 31.0, 1.0, 2.0, 0.4413510452892903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 537319.9326424913, 537319.9326424913, 134346.7047152479], 
processed observation next is [0.0, 0.4782608695652174, 0.7888888888888888, 0.31, 1.0, 1.0, 0.3349417205824885, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1918999759437469, 0.1918999759437469, 0.25835904752932287], 
reward next is 0.7416, 
noisyNet noise sample is [array([-0.25123933], dtype=float32), -0.21488626]. 
=============================================
[2019-04-27 20:56:51,475] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:56:51,487] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9855
[2019-04-27 20:56:51,496] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.11666666666667, 88.5, 1.0, 2.0, 0.3231359153077465, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 412680.0997343975, 412680.0997343975, 118482.7402452412], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1659000.0000, 
sim time next is 1659600.0000, 
raw observation next is [17.9, 90.0, 1.0, 2.0, 0.3107073455456359, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 396984.9496668494, 396984.9496668494, 116909.7907213155], 
processed observation next is [1.0, 0.21739130434782608, 0.21851851851851847, 0.9, 1.0, 1.0, 0.1794135066019475, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.14178033916673194, 0.14178033916673194, 0.22482652061791442], 
reward next is 0.7752, 
noisyNet noise sample is [array([-1.252304], dtype=float32), -0.9613786]. 
=============================================
[2019-04-27 20:57:01,535] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:57:01,544] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5698
[2019-04-27 20:57:01,552] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.95, 89.16666666666667, 1.0, 2.0, 0.3538624312016554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 447712.1934603626, 447712.1934603626, 122463.9970362605], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1833000.0000, 
sim time next is 1833600.0000, 
raw observation next is [19.1, 88.33333333333334, 1.0, 2.0, 0.3376491575326398, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 426956.3076411831, 426956.3076411831, 120326.9729570052], 
processed observation next is [1.0, 0.21739130434782608, 0.262962962962963, 0.8833333333333334, 1.0, 1.0, 0.21148709230076168, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.15248439558613683, 0.15248439558613683, 0.2313980249173177], 
reward next is 0.7686, 
noisyNet noise sample is [array([1.4350023], dtype=float32), 0.31098223]. 
=============================================
[2019-04-27 20:57:07,528] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-27 20:57:07,531] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 20:57:07,533] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 20:57:07,533] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 20:57:07,534] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:57:07,534] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 20:57:07,536] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 20:57:07,535] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:57:07,533] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:57:07,539] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:57:07,539] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:57:07,564] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run71
[2019-04-27 20:57:07,565] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run71
[2019-04-27 20:57:07,612] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run71
[2019-04-27 20:57:07,646] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run71
[2019-04-27 20:57:07,647] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run71
[2019-04-27 20:57:17,573] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.03955531], dtype=float32), -0.039122783]
[2019-04-27 20:57:17,574] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [22.35, 49.0, 1.0, 2.0, 0.2426585519026581, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 313006.7218891536, 313006.7218891536, 103374.4906418937]
[2019-04-27 20:57:17,578] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 20:57:17,580] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.70865100230665
[2019-04-27 20:57:31,909] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03955531], dtype=float32), -0.039122783]
[2019-04-27 20:57:31,910] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [24.76666666666667, 67.83333333333334, 1.0, 2.0, 0.9883808876448897, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.486125992648981, 6.9112, 121.9235618457964, 1497827.201002194, 1203419.696751919, 241591.749187383]
[2019-04-27 20:57:31,912] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 20:57:31,914] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [3.4915077e-34 1.0000000e+00 2.3886852e-37 3.6902522e-37 6.0821878e-34], sampled 0.9373602693772995
[2019-04-27 20:57:31,916] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1497827.201002194 W.
[2019-04-27 20:58:29,645] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03955531], dtype=float32), -0.039122783]
[2019-04-27 20:58:29,646] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [23.1, 92.5, 1.0, 2.0, 0.5272550643793542, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 622223.7738556442, 622223.7738556442, 146985.4146262885]
[2019-04-27 20:58:29,647] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 20:58:29,651] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.37334308626680524
[2019-04-27 20:58:49,013] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.03955531], dtype=float32), -0.039122783]
[2019-04-27 20:58:49,014] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [26.84125756, 49.27412464, 1.0, 2.0, 0.5304178142001008, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 656507.7558117581, 656507.7558117576, 148487.680534062]
[2019-04-27 20:58:49,015] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 20:58:49,019] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.11021360315680706
[2019-04-27 20:58:51,957] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8922.0495 2120823351.2149 431.0000
[2019-04-27 20:58:52,468] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8096.9038 2445628494.3108 746.0000
[2019-04-27 20:58:52,576] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8696.3390 2195992493.1694 572.0000
[2019-04-27 20:58:52,634] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8581.3685 2248912489.3528 553.0000
[2019-04-27 20:58:52,642] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8768.5216 2170879392.7385 493.0000
[2019-04-27 20:58:53,660] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 1750000, evaluation results [1750000.0, 8096.903836826734, 2445628494.3108454, 746.0, 8768.521555547173, 2170879392.738508, 493.0, 8922.049506177987, 2120823351.2149136, 431.0, 8581.368543223192, 2248912489.352782, 553.0, 8696.339019432698, 2195992493.169353, 572.0]
[2019-04-27 20:58:58,098] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:58:58,105] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8997
[2019-04-27 20:58:58,113] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 65.0, 1.0, 2.0, 0.5173563695269268, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 611302.7931325737, 611302.7931325737, 145427.3277859599], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2034000.0000, 
sim time next is 2034600.0000, 
raw observation next is [27.35, 64.66666666666667, 1.0, 2.0, 0.5211466575179315, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 614769.9499536863, 614769.9499536863, 145993.2910345082], 
processed observation next is [0.0, 0.5652173913043478, 0.5685185185185185, 0.6466666666666667, 1.0, 1.0, 0.4299364970451566, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21956069641203083, 0.21956069641203083, 0.2807563289125158], 
reward next is 0.7192, 
noisyNet noise sample is [array([0.5870972], dtype=float32), -1.110385]. 
=============================================
[2019-04-27 20:59:04,966] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:59:04,977] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7233
[2019-04-27 20:59:04,981] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.03333333333333, 72.0, 1.0, 2.0, 0.6498012638056216, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 740561.9811512533, 740561.9811512533, 166856.8369321243], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2148000.0000, 
sim time next is 2148600.0000, 
raw observation next is [27.86666666666666, 72.5, 1.0, 2.0, 0.6464389445774006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 736728.1899442888, 736728.1899442893, 166250.2605322646], 
processed observation next is [0.0, 0.8695652173913043, 0.587654320987654, 0.725, 1.0, 1.0, 0.5790939816397626, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.26311721069438887, 0.26311721069438904, 0.3197120394851242], 
reward next is 0.6803, 
noisyNet noise sample is [array([-0.82508916], dtype=float32), 0.41723657]. 
=============================================
[2019-04-27 20:59:05,920] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:59:05,930] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1857
[2019-04-27 20:59:05,936] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.13333333333333, 82.66666666666667, 1.0, 2.0, 0.5697573624843475, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 663367.0101283867, 663367.0101283867, 153618.3407752422], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2160600.0000, 
sim time next is 2161200.0000, 
raw observation next is [25.06666666666667, 83.33333333333334, 1.0, 2.0, 0.5713933081040891, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 664882.7597820151, 664882.7597820151, 153876.1670335396], 
processed observation next is [1.0, 0.0, 0.4839506172839507, 0.8333333333333335, 1.0, 1.0, 0.48975393821915364, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.23745812849357684, 0.23745812849357684, 0.29591570583373], 
reward next is 0.7041, 
noisyNet noise sample is [array([-0.7061513], dtype=float32), 0.41873094]. 
=============================================
[2019-04-27 20:59:05,964] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3187604e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 20:59:05,973] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7621
[2019-04-27 20:59:05,978] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.73333333333333, 90.0, 1.0, 2.0, 0.6036872521478708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 708040.6065421755, 708040.6065421755, 159646.905159575], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2177400.0000, 
sim time next is 2178000.0000, 
raw observation next is [23.7, 90.0, 1.0, 2.0, 0.5756084282112078, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 675561.9888141528, 675561.9888141523, 154838.063656798], 
processed observation next is [1.0, 0.21739130434782608, 0.4333333333333333, 0.9, 1.0, 1.0, 0.494771938346676, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.24127213886219742, 0.24127213886219726, 0.29776550703230387], 
reward next is 0.7022, 
noisyNet noise sample is [array([-0.23261108], dtype=float32), -0.37325588]. 
=============================================
[2019-04-27 20:59:05,992] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[58.73434 ]
 [58.852627]
 [59.01107 ]
 [58.98406 ]
 [58.880512]], R is [[58.92015457]
 [59.02393723]
 [59.12654877]
 [59.23303986]
 [59.3368454 ]].
[2019-04-27 20:59:06,115] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 20:59:06,123] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9574
[2019-04-27 20:59:06,128] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.33333333333333, 1.0, 2.0, 0.484766413594218, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 581716.8443082741, 581716.8443082741, 140648.8810244325], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2338800.0000, 
sim time next is 2339400.0000, 
raw observation next is [22.0, 94.66666666666667, 1.0, 2.0, 0.4875357731067518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 584614.3232041098, 584614.3232041098, 141063.0932105848], 
processed observation next is [1.0, 0.043478260869565216, 0.37037037037037035, 0.9466666666666668, 1.0, 1.0, 0.3899235394127997, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2087908297157535, 0.2087908297157535, 0.2712751792511246], 
reward next is 0.7287, 
noisyNet noise sample is [array([-0.6016204], dtype=float32), -1.2378227]. 
=============================================
[2019-04-27 20:59:08,361] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.0519869e-29 1.0000000e+00 2.6608261e-32 4.1870436e-31 3.0414835e-25], sum to 1.0000
[2019-04-27 20:59:08,369] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7136
[2019-04-27 20:59:08,375] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1524843.371273298 W.
[2019-04-27 20:59:08,379] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.3, 89.0, 1.0, 2.0, 0.710550376617612, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.9260426156618, 1524843.371273298, 1524843.371273299, 319151.1423956566], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2203200.0000, 
sim time next is 2203800.0000, 
raw observation next is [25.46666666666667, 88.33333333333334, 1.0, 2.0, 0.7546607011101736, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.9260426156618, 1575192.238470501, 1575192.238470502, 327400.753363403], 
processed observation next is [1.0, 0.5217391304347826, 0.4987654320987655, 0.8833333333333334, 1.0, 1.0, 0.7079294060835399, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.9972168686025908, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.5625686565966075, 0.5625686565966078, 0.6296168333911597], 
reward next is 0.3704, 
noisyNet noise sample is [array([0.8022501], dtype=float32), 0.723092]. 
=============================================
[2019-04-27 20:59:16,404] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.1336032e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2782267e-34], sum to 1.0000
[2019-04-27 20:59:16,412] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0796
[2019-04-27 20:59:16,417] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.91666666666667, 92.5, 1.0, 2.0, 0.4944999073907056, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 596162.9489348423, 596162.9489348418, 142257.6005006791], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2351400.0000, 
sim time next is 2352000.0000, 
raw observation next is [22.23333333333333, 89.0, 1.0, 2.0, 0.4815997425007601, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 581788.3536907032, 581788.3536907032, 140290.4798850038], 
processed observation next is [1.0, 0.21739130434782608, 0.37901234567901226, 0.89, 1.0, 1.0, 0.38285683631042866, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.20778155488953684, 0.20778155488953684, 0.26978938439423805], 
reward next is 0.7302, 
noisyNet noise sample is [array([-0.47253793], dtype=float32), 0.0003963365]. 
=============================================
[2019-04-27 20:59:16,441] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[66.60386 ]
 [66.64829 ]
 [66.69522 ]
 [66.691734]
 [66.68125 ]], R is [[66.50215912]
 [66.56356049]
 [66.62954712]
 [66.69409943]
 [66.75733948]].
[2019-04-27 20:59:18,248] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.7515476e-33 1.0000000e+00 5.7607790e-36 3.4596958e-37 1.6184073e-28], sum to 1.0000
[2019-04-27 20:59:18,258] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0841
[2019-04-27 20:59:18,264] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.86666666666667, 39.66666666666667, 1.0, 2.0, 0.3970105772608505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 488410.3007714829, 488410.3007714829, 128114.4155270331], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2397000.0000, 
sim time next is 2397600.0000, 
raw observation next is [29.7, 40.0, 1.0, 2.0, 0.3987804793850078, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 491031.7558955056, 491031.7558955056, 128374.0475986343], 
processed observation next is [1.0, 0.782608695652174, 0.6555555555555556, 0.4, 1.0, 1.0, 0.28426247545834266, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.17536848424839485, 0.17536848424839485, 0.2468731684589121], 
reward next is 0.7531, 
noisyNet noise sample is [array([-0.0837748], dtype=float32), -1.0861812]. 
=============================================
[2019-04-27 20:59:20,618] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5655033e-31 1.0000000e+00 2.6499620e-36 1.5289852e-34 1.3256037e-30], sum to 1.0000
[2019-04-27 20:59:20,630] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6609
[2019-04-27 20:59:20,636] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.13333333333334, 63.0, 1.0, 2.0, 0.3226060872695969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 409693.2828025058, 409693.2828025058, 118404.1544020017], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2422200.0000, 
sim time next is 2422800.0000, 
raw observation next is [22.0, 63.0, 1.0, 2.0, 0.3181550113512155, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 404564.438648834, 404564.438648834, 117841.6251642741], 
processed observation next is [1.0, 0.043478260869565216, 0.37037037037037035, 0.63, 1.0, 1.0, 0.1882797754181137, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.14448729951744071, 0.14448729951744071, 0.22661850993129634], 
reward next is 0.7734, 
noisyNet noise sample is [array([-1.0215139], dtype=float32), 1.3040468]. 
=============================================
[2019-04-27 20:59:34,767] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.4065547e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.3543297e-31], sum to 1.0000
[2019-04-27 20:59:34,773] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7772
[2019-04-27 20:59:34,777] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.58333333333334, 75.83333333333334, 1.0, 2.0, 0.6126869808601799, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 706611.6885945037, 706611.6885945032, 160681.4792877728], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2848200.0000, 
sim time next is 2848800.0000, 
raw observation next is [25.66666666666667, 80.66666666666666, 1.0, 2.0, 0.601939400751108, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 696522.6598440871, 696522.6598440871, 158919.4429010194], 
processed observation next is [1.0, 1.0, 0.506172839506173, 0.8066666666666665, 1.0, 1.0, 0.5261183342275094, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.24875809280145966, 0.24875809280145966, 0.3056143132711912], 
reward next is 0.6944, 
noisyNet noise sample is [array([-0.48530525], dtype=float32), 0.46807423]. 
=============================================
[2019-04-27 20:59:36,642] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.1440183e-38], sum to 1.0000
[2019-04-27 20:59:36,651] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1225
[2019-04-27 20:59:36,659] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.53333333333334, 65.33333333333334, 1.0, 2.0, 0.6550489246891875, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 746545.5191854073, 746545.5191854073, 167808.2698594477], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2748000.0000, 
sim time next is 2748600.0000, 
raw observation next is [29.3, 67.0, 1.0, 2.0, 0.6627924649467495, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 755375.017961089, 755375.017961089, 169219.2101061512], 
processed observation next is [0.0, 0.8260869565217391, 0.6407407407407407, 0.67, 1.0, 1.0, 0.5985624582699398, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2697767921289604, 0.2697767921289604, 0.3254215578964446], 
reward next is 0.6746, 
noisyNet noise sample is [array([0.04495494], dtype=float32), 0.5332156]. 
=============================================
[2019-04-27 20:59:39,078] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.4871878e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 20:59:39,085] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8273
[2019-04-27 20:59:39,090] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.6518209374643252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 742864.8672195461, 742864.8672195461, 167221.2449869135], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2757600.0000, 
sim time next is 2758200.0000, 
raw observation next is [25.91666666666667, 84.83333333333333, 1.0, 2.0, 0.6449019684677452, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 734975.7019669173, 734975.7019669173, 165973.6906181919], 
processed observation next is [0.0, 0.9565217391304348, 0.5154320987654323, 0.8483333333333333, 1.0, 1.0, 0.5772642481758871, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.26249132213104187, 0.26249132213104187, 0.3191801742657537], 
reward next is 0.6808, 
noisyNet noise sample is [array([1.0514828], dtype=float32), -0.53446794]. 
=============================================
[2019-04-27 20:59:39,941] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.0823170e-30 1.0000000e+00 1.2182481e-31 1.0581820e-31 2.8162623e-25], sum to 1.0000
[2019-04-27 20:59:39,947] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8750
[2019-04-27 20:59:39,953] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 94.00000000000001, 1.0, 2.0, 0.8341251217074874, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156576, 962282.5433009259, 962282.5433009259, 203695.6465697796], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2776200.0000, 
sim time next is 2776800.0000, 
raw observation next is [24.0, 94.0, 1.0, 2.0, 0.7720708665777882, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 890773.5506793559, 890773.5506793559, 190788.230783176], 
processed observation next is [1.0, 0.13043478260869565, 0.4444444444444444, 0.94, 1.0, 1.0, 0.728655793544986, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.31813341095691283, 0.31813341095691283, 0.3669004438138], 
reward next is 0.6331, 
noisyNet noise sample is [array([-0.04205081], dtype=float32), -0.6911752]. 
=============================================
[2019-04-27 20:59:46,483] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-27 20:59:46,484] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 20:59:46,485] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 20:59:46,485] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:59:46,485] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 20:59:46,487] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 20:59:46,487] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 20:59:46,486] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:59:46,489] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:59:46,489] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:59:46,489] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 20:59:46,515] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run72
[2019-04-27 20:59:46,538] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run72
[2019-04-27 20:59:46,565] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run72
[2019-04-27 20:59:46,566] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run72
[2019-04-27 20:59:46,617] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run72
[2019-04-27 21:00:13,557] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04094451], dtype=float32), -0.039498974]
[2019-04-27 21:00:13,558] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [19.13333333333333, 95.33333333333334, 1.0, 2.0, 0.3621567188118567, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 453034.5405333558, 453034.5405333554, 123501.5126501602]
[2019-04-27 21:00:13,559] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 21:00:13,562] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8251688341322694
[2019-04-27 21:00:19,091] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04094451], dtype=float32), -0.039498974]
[2019-04-27 21:00:19,091] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [24.05928402666667, 66.39891068833333, 1.0, 2.0, 0.3826424889790399, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 473941.0162996934, 473941.0162996934, 126197.4891902398]
[2019-04-27 21:00:19,093] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 21:00:19,096] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.40379675993268915
[2019-04-27 21:00:19,714] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04094451], dtype=float32), -0.039498974]
[2019-04-27 21:00:19,714] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [33.67838163333333, 27.92658226666667, 1.0, 2.0, 0.8553096066573802, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.925984185135, 1047686.618865367, 1047686.618865367, 210840.2530106043]
[2019-04-27 21:00:19,715] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 21:00:19,718] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.4287746e-34 1.0000000e+00 3.4662594e-38 2.1615141e-38 4.1037151e-36], sampled 0.2617089149842411
[2019-04-27 21:01:23,991] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04094451], dtype=float32), -0.039498974]
[2019-04-27 21:01:23,995] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [20.15, 90.5, 1.0, 2.0, 0.3832073433515575, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 476645.2220110339, 476645.2220110334, 126318.0034948471]
[2019-04-27 21:01:23,996] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:01:23,999] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5710956605957762
[2019-04-27 21:01:30,815] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8768.6241 2170826060.4938 493.0000
[2019-04-27 21:01:31,014] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8581.2410 2248887715.3409 553.0000
[2019-04-27 21:01:31,086] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8922.2563 2120684861.2065 430.0000
[2019-04-27 21:01:31,156] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8097.6795 2445569542.8369 746.0000
[2019-04-27 21:01:31,337] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8698.5611 2195688889.4153 572.0000
[2019-04-27 21:01:32,354] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1775000, evaluation results [1775000.0, 8097.679510568991, 2445569542.8369265, 746.0, 8768.624117554622, 2170826060.49376, 493.0, 8922.256315340886, 2120684861.2065036, 430.0, 8581.240965317736, 2248887715.34094, 553.0, 8698.561088185956, 2195688889.4152822, 572.0]
[2019-04-27 21:01:37,042] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [7.6507643e-22 1.0000000e+00 4.2837997e-25 4.1402828e-24 1.2164834e-16], sum to 1.0000
[2019-04-27 21:01:37,052] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4614
[2019-04-27 21:01:37,060] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1753878.419204466 W.
[2019-04-27 21:01:37,063] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.9112077961969987, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.9260426156618, 1753878.419204466, 1753878.419204466, 359401.8705209899], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2973600.0000, 
sim time next is 2974200.0000, 
raw observation next is [28.06666666666667, 83.33333333333333, 1.0, 2.0, 0.5627873811426021, 1.0, 1.0, 0.5627873811426021, 1.0, 2.0, 0.8959758394844328, 6.9112, 6.9112, 121.94756008, 1925609.126453707, 1925609.126453707, 375473.3566739533], 
processed observation next is [1.0, 0.43478260869565216, 0.5950617283950619, 0.8333333333333333, 1.0, 1.0, 0.47950878707452627, 1.0, 0.5, 0.47950878707452627, 1.0, 1.0, 0.8699697993555411, 0.0, 0.0, 0.8096049824067558, 0.6877175451620382, 0.6877175451620382, 0.7220641474499102], 
reward next is 0.2779, 
noisyNet noise sample is [array([0.1227431], dtype=float32), 0.9631091]. 
=============================================
[2019-04-27 21:01:45,994] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5222243e-32 1.0000000e+00 1.6923402e-36 7.0677611e-37 3.9289205e-33], sum to 1.0000
[2019-04-27 21:01:46,000] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2214
[2019-04-27 21:01:46,006] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.13333333333333, 54.66666666666667, 1.0, 2.0, 0.5208021577626237, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 633892.1017427369, 633892.1017427369, 146630.7495870496], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3127200.0000, 
sim time next is 3127800.0000, 
raw observation next is [27.16666666666667, 52.83333333333333, 1.0, 2.0, 0.50270730359604, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 614694.7138523352, 614694.7138523352, 143816.4351091938], 
processed observation next is [1.0, 0.17391304347826086, 0.5617283950617286, 0.5283333333333333, 1.0, 1.0, 0.40798488523338095, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21953382637583402, 0.21953382637583402, 0.2765700675176804], 
reward next is 0.7234, 
noisyNet noise sample is [array([0.42642695], dtype=float32), 0.27743623]. 
=============================================
[2019-04-27 21:01:47,948] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.281662e-22 1.000000e+00 9.799774e-24 4.085886e-24 7.501041e-14], sum to 1.0000
[2019-04-27 21:01:47,955] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3218
[2019-04-27 21:01:47,965] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1733340.550726419 W.
[2019-04-27 21:01:47,970] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.0, 38.0, 1.0, 2.0, 0.878392041279915, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9829796637322552, 6.911199999999999, 6.9112, 121.9260426156618, 1733340.550726419, 1733340.55072642, 349926.0857675308], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3153600.0000, 
sim time next is 3154200.0000, 
raw observation next is [33.16666666666666, 37.66666666666667, 1.0, 2.0, 0.5033178210489835, 1.0, 1.0, 0.5033178210489835, 1.0, 2.0, 0.801298363027034, 6.9112, 6.9112, 121.94756008, 1721934.534171056, 1721934.534171056, 345220.451405912], 
processed observation next is [1.0, 0.5217391304347826, 0.7839506172839502, 0.3766666666666667, 1.0, 1.0, 0.40871169172498034, 1.0, 0.5, 0.40871169172498034, 1.0, 1.0, 0.7516229537837926, 0.0, 0.0, 0.8096049824067558, 0.6149766193468057, 0.6149766193468057, 0.6638854834729077], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.19990514], dtype=float32), -1.0994467]. 
=============================================
[2019-04-27 21:01:58,176] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.5374175e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 21:01:58,184] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3334
[2019-04-27 21:01:58,191] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.83333333333334, 91.66666666666667, 1.0, 2.0, 0.5867458169640085, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 683288.7418216369, 683288.7418216369, 156504.2895804916], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3375600.0000, 
sim time next is 3376200.0000, 
raw observation next is [23.71666666666667, 91.33333333333334, 1.0, 2.0, 0.5787987882306725, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 676247.3386554389, 676247.3386554389, 155248.1786727169], 
processed observation next is [1.0, 0.043478260869565216, 0.4339506172839507, 0.9133333333333334, 1.0, 1.0, 0.49856998598889585, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.24151690666265674, 0.24151690666265674, 0.2985541897552248], 
reward next is 0.7014, 
noisyNet noise sample is [array([-1.2159164], dtype=float32), -1.5722054]. 
=============================================
[2019-04-27 21:02:00,188] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.0993521e-14 4.2233950e-01 1.6233012e-13 3.5701834e-12 5.7766050e-01], sum to 1.0000
[2019-04-27 21:02:00,195] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4369
[2019-04-27 21:02:00,199] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.93333333333334, 94.0, 1.0, 2.0, 0.511732194477025, 1.0, 2.0, 0.511732194477025, 1.0, 1.0, 0.8146943195614871, 6.911200000000001, 6.9112, 121.94756008, 1750749.719095405, 1750749.719095404, 349384.6691652995], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4009800.0000, 
sim time next is 4010400.0000, 
raw observation next is [25.0, 94.0, 1.0, 2.0, 0.4953550928431945, 1.0, 2.0, 0.4953550928431945, 1.0, 2.0, 0.7886214403954649, 6.9112, 6.9112, 121.94756008, 1694666.87287788, 1694666.87287788, 341315.1048448741], 
processed observation next is [1.0, 0.43478260869565216, 0.48148148148148145, 0.94, 1.0, 1.0, 0.39923225338475543, 1.0, 1.0, 0.39923225338475543, 1.0, 1.0, 0.735776800494331, 0.0, 0.0, 0.8096049824067558, 0.6052381688849572, 0.6052381688849572, 0.6563752016247579], 
reward next is 0.3436, 
noisyNet noise sample is [array([-0.945638], dtype=float32), -1.0878209]. 
=============================================
[2019-04-27 21:02:01,552] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2433686e-20 1.0000000e+00 1.8209491e-22 4.6639905e-22 1.8775751e-13], sum to 1.0000
[2019-04-27 21:02:01,559] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1590
[2019-04-27 21:02:01,565] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1994391.806486511 W.
[2019-04-27 21:02:01,571] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.85, 70.0, 1.0, 2.0, 0.5828677059447482, 1.0, 2.0, 0.5828677059447482, 1.0, 1.0, 0.9279443705399723, 6.911199999999999, 6.9112, 121.94756008, 1994391.806486511, 1994391.806486512, 386120.0810966787], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3407400.0000, 
sim time next is 3408000.0000, 
raw observation next is [29.06666666666666, 69.0, 1.0, 2.0, 0.6102190174209386, 1.0, 2.0, 0.6102190174209386, 1.0, 2.0, 0.9714885491800942, 6.911200000000001, 6.9112, 121.94756008, 2088088.877924301, 2088088.877924301, 400971.5501714733], 
processed observation next is [1.0, 0.43478260869565216, 0.6320987654320985, 0.69, 1.0, 1.0, 0.5359750207392127, 1.0, 1.0, 0.5359750207392127, 1.0, 1.0, 0.9643606864751177, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.7457460278301075, 0.7457460278301075, 0.7710991349451409], 
reward next is 0.2289, 
noisyNet noise sample is [array([0.89247304], dtype=float32), 0.66457593]. 
=============================================
[2019-04-27 21:02:01,600] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[60.535336]
 [60.655537]
 [61.092564]
 [60.890957]
 [60.562504]], R is [[60.19498062]
 [59.59303284]
 [59.31685638]
 [59.03364944]
 [58.78809738]].
[2019-04-27 21:02:02,962] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.1036318e-27 1.0000000e+00 1.4528790e-29 8.7139076e-31 3.8219561e-24], sum to 1.0000
[2019-04-27 21:02:02,969] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7240
[2019-04-27 21:02:02,978] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2041078.553879545 W.
[2019-04-27 21:02:02,984] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.8, 59.66666666666667, 1.0, 2.0, 0.8947445583371895, 1.0, 2.0, 0.8947445583371895, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 2041078.553879545, 2041078.553879546, 384459.4920905498], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3421200.0000, 
sim time next is 3421800.0000, 
raw observation next is [31.1, 59.5, 1.0, 2.0, 0.9191221071973795, 1.0, 2.0, 0.9191221071973795, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 2096753.559988052, 2096753.559988052, 395436.202622264], 
processed observation next is [1.0, 0.6086956521739131, 0.7074074074074075, 0.595, 1.0, 1.0, 0.9037167942825947, 1.0, 1.0, 0.9037167942825947, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.74884055713859, 0.74884055713859, 0.7604542358120461], 
reward next is 0.2395, 
noisyNet noise sample is [array([-1.0676514], dtype=float32), -0.49687666]. 
=============================================
[2019-04-27 21:02:04,308] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.7005627e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0210864e-32], sum to 1.0000
[2019-04-27 21:02:04,319] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8763
[2019-04-27 21:02:04,322] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.33333333333334, 88.0, 1.0, 2.0, 0.6420333975844147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 732906.4918861141, 732906.4918861141, 165519.248136992], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3453600.0000, 
sim time next is 3454200.0000, 
raw observation next is [25.25, 89.5, 1.0, 2.0, 0.6470360822556298, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 737409.0582130648, 737409.0582130648, 166357.5927797963], 
processed observation next is [1.0, 1.0, 0.49074074074074076, 0.895, 1.0, 1.0, 0.5798048598281307, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2633603779332374, 0.2633603779332374, 0.31991844765345445], 
reward next is 0.6801, 
noisyNet noise sample is [array([0.6919956], dtype=float32), 0.28804877]. 
=============================================
[2019-04-27 21:02:06,529] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.2005230e-20 1.0000000e+00 2.2151123e-23 1.0529091e-20 6.8369754e-20], sum to 1.0000
[2019-04-27 21:02:06,535] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5448
[2019-04-27 21:02:06,550] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1979837.904267909 W.
[2019-04-27 21:02:06,557] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.08333333333333, 87.0, 1.0, 2.0, 0.8679283114536819, 1.0, 1.0, 0.8679283114536819, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9259335529134, 1979837.904267909, 1979837.904267909, 372623.5998439332], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3492600.0000, 
sim time next is 3493200.0000, 
raw observation next is [27.16666666666667, 85.0, 1.0, 2.0, 0.5690066115306457, 1.0, 2.0, 0.5690066115306457, 1.0, 1.0, 0.9058770568083915, 6.911199999999999, 6.9112, 121.94756008, 1946911.768168372, 1946911.768168373, 378747.5795725957], 
processed observation next is [1.0, 0.43478260869565216, 0.5617283950617286, 0.85, 1.0, 1.0, 0.4869126327745782, 1.0, 1.0, 0.4869126327745782, 1.0, 0.5, 0.8823463210104893, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.6953256314887043, 0.6953256314887046, 0.7283607299472995], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.96363765], dtype=float32), 0.9021507]. 
=============================================
[2019-04-27 21:02:09,899] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:02:09,908] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6038
[2019-04-27 21:02:09,912] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.06666666666667, 84.66666666666666, 1.0, 2.0, 0.5873024311488101, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 707265.8140526327, 707265.8140526327, 157528.0518444523], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3550800.0000, 
sim time next is 3551400.0000, 
raw observation next is [22.8, 88.5, 1.0, 2.0, 0.5418299974375347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 649858.249690774, 649858.2496907735, 149752.889173355], 
processed observation next is [1.0, 0.08695652173913043, 0.4, 0.885, 1.0, 1.0, 0.4545595207589698, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2320922320324193, 0.23209223203241913, 0.28798632533337504], 
reward next is 0.7120, 
noisyNet noise sample is [array([0.5616435], dtype=float32), -1.5156322]. 
=============================================
[2019-04-27 21:02:10,416] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.9104827e-19 1.0000000e+00 4.9320276e-20 6.8685766e-21 1.7761478e-09], sum to 1.0000
[2019-04-27 21:02:10,424] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6861
[2019-04-27 21:02:10,432] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1602584.049004287 W.
[2019-04-27 21:02:10,437] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [24.43333333333333, 83.33333333333334, 1.0, 2.0, 0.6966505686757661, 1.0, 1.0, 0.6966505686757661, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9250487310172, 1602584.049004287, 1602584.049004287, 303599.5228594794], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3590400.0000, 
sim time next is 3591000.0000, 
raw observation next is [24.65, 80.5, 1.0, 2.0, 0.6307908971451515, 1.0, 2.0, 0.6307908971451515, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260423125847, 1452977.795593752, 1452977.795593753, 279514.091891252], 
processed observation next is [1.0, 0.5652173913043478, 0.46851851851851845, 0.805, 1.0, 1.0, 0.5604653537442279, 1.0, 1.0, 0.5604653537442279, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621268080192, 0.5189206412834829, 0.5189206412834833, 0.5375270997908693], 
reward next is 0.4625, 
noisyNet noise sample is [array([0.46284434], dtype=float32), -1.0701966]. 
=============================================
[2019-04-27 21:02:10,450] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[57.513607]
 [55.760883]
 [57.25489 ]
 [57.371197]
 [57.00038 ]], R is [[58.63090897]
 [58.46075439]
 [57.87614822]
 [57.29738617]
 [57.31176376]].
[2019-04-27 21:02:10,562] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.9765721e-30 1.0000000e+00 5.6178987e-34 2.7396586e-34 1.0693597e-27], sum to 1.0000
[2019-04-27 21:02:10,568] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7895
[2019-04-27 21:02:10,577] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1830282.288164279 W.
[2019-04-27 21:02:10,583] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 46.0, 1.0, 2.0, 0.5349552332990584, 1.0, 1.0, 0.5349552332990584, 1.0, 2.0, 0.851666153687737, 6.911199999999999, 6.9112, 121.94756008, 1830282.288164279, 1830282.288164279, 361076.597489684], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4190400.0000, 
sim time next is 4191000.0000, 
raw observation next is [32.16666666666667, 44.33333333333334, 1.0, 2.0, 0.4871601120166791, 1.0, 2.0, 0.4871601120166791, 1.0, 2.0, 0.7755747640277596, 6.911200000000001, 6.9112, 121.94756008, 1666604.751837431, 1666604.751837431, 337331.7693909391], 
processed observation next is [1.0, 0.5217391304347826, 0.7469135802469138, 0.4433333333333334, 1.0, 1.0, 0.38947632382937986, 1.0, 1.0, 0.38947632382937986, 1.0, 1.0, 0.7194684550346994, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.5952159827990825, 0.5952159827990825, 0.6487149411364214], 
reward next is 0.3513, 
noisyNet noise sample is [array([-1.1613499], dtype=float32), -2.7987702]. 
=============================================
[2019-04-27 21:02:10,601] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[62.55619 ]
 [62.159683]
 [62.199635]
 [61.45749 ]
 [61.234   ]], R is [[62.72805786]
 [62.10077667]
 [61.76885605]
 [61.47381592]
 [61.20956802]].
[2019-04-27 21:02:16,962] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1167214e-23 1.0000000e+00 7.1875699e-26 5.4308876e-26 3.2020376e-16], sum to 1.0000
[2019-04-27 21:02:16,972] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9930
[2019-04-27 21:02:16,982] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1833673.49132435 W.
[2019-04-27 21:02:16,986] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.8039179523976545, 1.0, 1.0, 0.8039179523976545, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1833673.49132435, 1833673.49132435, 345376.8731843331], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3684600.0000, 
sim time next is 3685200.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 1.02, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9977734948820727, 7.294591544619916, 6.9112, 121.9246844493169, 2074611.551312498, 1878283.000299556, 382761.3237992988], 
processed observation next is [1.0, 0.6521739130434783, 0.5555555555555556, 0.89, 1.0, 1.0, 1.0238095238095237, 0.0, 0.5, -0.1904761904761905, 1.0, 0.5, 0.9972168686025908, 0.038339154461991584, 0.0, 0.8094531120079993, 0.7409326968973208, 0.6708153572498414, 0.7360794688448054], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.38788238], dtype=float32), -1.7465256]. 
=============================================
[2019-04-27 21:02:21,954] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6452516e-32 1.0000000e+00 6.9837902e-38 2.9462490e-36 9.3759652e-34], sum to 1.0000
[2019-04-27 21:02:21,960] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5573343e-27 1.0000000e+00 1.5674342e-30 5.6357262e-30 7.2113462e-28], sum to 1.0000
[2019-04-27 21:02:21,961] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6800
[2019-04-27 21:02:21,965] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.1, 77.5, 1.0, 2.0, 0.5465301217519308, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 677377.8455829934, 677377.8455829934, 151175.6642337254], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4248600.0000, 
sim time next is 4249200.0000, 
raw observation next is [22.2, 77.0, 1.0, 2.0, 0.4956386920677866, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 613995.0537195951, 613995.0537195947, 142903.2026076799], 
processed observation next is [1.0, 0.17391304347826086, 0.37777777777777777, 0.77, 1.0, 1.0, 0.3995698715092698, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.21928394775699825, 0.2192839477569981, 0.27481385116861523], 
reward next is 0.7252, 
noisyNet noise sample is [array([-0.03098181], dtype=float32), 1.0006983]. 
=============================================
[2019-04-27 21:02:21,969] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7823
[2019-04-27 21:02:21,977] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2328868.181627332 W.
[2019-04-27 21:02:21,983] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 79.0, 1.0, 2.0, 0.7342551867659145, 1.0, 2.0, 0.6804922553593918, 1.0, 2.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.94756008, 2328868.181627332, 2328868.181627332, 438571.8640433842], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3758400.0000, 
sim time next is 3759000.0000, 
raw observation next is [30.33333333333333, 77.66666666666667, 1.0, 2.0, 0.6296248217517826, 1.0, 2.0, 0.6281770728523258, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.94756008, 2149612.926941738, 2149612.926941738, 410864.4314279541], 
processed observation next is [1.0, 0.5217391304347826, 0.6790123456790121, 0.7766666666666667, 1.0, 1.0, 0.5590771687521221, 1.0, 1.0, 0.5573536581575307, 1.0, 1.0, 0.9972168686025908, 0.0, 0.0, 0.8096049824067558, 0.7677189024791922, 0.7677189024791922, 0.7901239065922194], 
reward next is 0.2099, 
noisyNet noise sample is [array([-2.1338823], dtype=float32), -1.0043983]. 
=============================================
[2019-04-27 21:02:21,996] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[59.211983]
 [58.87858 ]
 [58.69476 ]
 [58.433674]
 [57.78496 ]], R is [[59.22251129]
 [58.78688049]
 [58.414608  ]
 [58.04944611]
 [57.46895218]].
[2019-04-27 21:02:22,499] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.0607103e-33 1.0000000e+00 1.7078775e-38 3.5520773e-37 1.4893493e-28], sum to 1.0000
[2019-04-27 21:02:22,509] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8304
[2019-04-27 21:02:22,513] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.16666666666666, 59.0, 1.0, 2.0, 0.6425717629525376, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 732815.988911174, 732815.988911174, 165579.8690615533], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3793800.0000, 
sim time next is 3794400.0000, 
raw observation next is [30.0, 59.0, 1.0, 2.0, 0.6314531342314018, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 722533.7688282294, 722533.7688282294, 163716.3560948353], 
processed observation next is [1.0, 0.9565217391304348, 0.6666666666666666, 0.59, 1.0, 1.0, 0.5612537312278593, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2580477745815105, 0.2580477745815105, 0.31483914633622173], 
reward next is 0.6852, 
noisyNet noise sample is [array([1.1233332], dtype=float32), 0.59926313]. 
=============================================
[2019-04-27 21:02:25,359] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-27 21:02:25,362] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 21:02:25,363] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:02:25,363] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 21:02:25,364] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 21:02:25,365] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:02:25,366] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:02:25,367] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 21:02:25,368] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 21:02:25,370] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:02:25,371] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:02:25,394] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run73
[2019-04-27 21:02:25,394] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run73
[2019-04-27 21:02:25,416] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run73
[2019-04-27 21:02:25,465] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run73
[2019-04-27 21:02:25,487] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run73
[2019-04-27 21:02:31,116] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04098264], dtype=float32), -0.040602244]
[2019-04-27 21:02:31,117] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [25.00780276, 48.53303610833333, 1.0, 2.0, 0.3299479463820269, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 417412.2609689365, 417412.2609689365, 119331.1726852502]
[2019-04-27 21:02:31,119] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 21:02:31,121] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5178653067633715
[2019-04-27 21:02:48,145] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04098264], dtype=float32), -0.040602244]
[2019-04-27 21:02:48,146] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [30.5, 41.0, 1.0, 2.0, 0.4663726123284272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 566293.5873961325, 566293.5873961325, 138051.0753614467]
[2019-04-27 21:02:48,147] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 21:02:48,149] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [5.6199937e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.4721487e-37], sampled 0.7423729744947348
[2019-04-27 21:03:08,438] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04098264], dtype=float32), -0.040602244]
[2019-04-27 21:03:08,440] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [22.86666666666667, 91.33333333333334, 1.0, 2.0, 0.5263192626677027, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 626115.2584298587, 626115.2584298587, 147030.0706200924]
[2019-04-27 21:03:08,440] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:03:08,444] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.19408509842445631
[2019-04-27 21:03:12,897] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04098264], dtype=float32), -0.040602244]
[2019-04-27 21:03:12,899] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [23.0, 94.66666666666667, 1.0, 2.0, 0.5701406899286054, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 670785.1487239931, 670785.1487239926, 153982.6056231993]
[2019-04-27 21:03:12,899] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:03:12,902] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [4.0167173e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.4986950182597022
[2019-04-27 21:03:27,118] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04098264], dtype=float32), -0.040602244]
[2019-04-27 21:03:27,119] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [22.83333333333334, 80.33333333333334, 1.0, 2.0, 0.648776472565136, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 790321.0613470229, 790321.0613470234, 168794.329823035]
[2019-04-27 21:03:27,121] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 21:03:27,123] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.2085820e-33 1.0000000e+00 1.9293985e-36 7.9435007e-36 2.1637718e-33], sampled 0.9754611660926318
[2019-04-27 21:03:28,040] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04098264], dtype=float32), -0.040602244]
[2019-04-27 21:03:28,041] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [21.4, 84.66666666666666, 1.0, 2.0, 0.4087666597127884, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 504349.7522712384, 504349.7522712388, 129811.0438050429]
[2019-04-27 21:03:28,044] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 21:03:28,046] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.954931685437445
[2019-04-27 21:04:00,071] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04098264], dtype=float32), -0.040602244]
[2019-04-27 21:04:00,073] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [20.97109707, 85.94208718333333, 1.0, 2.0, 0.4053074545658999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 501376.9869726761, 501376.9869726761, 129350.3724897565]
[2019-04-27 21:04:00,075] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 21:04:00,079] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6800111060340994
[2019-04-27 21:04:09,642] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8097.6795 2445569542.8369 746.0000
[2019-04-27 21:04:09,820] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8923.1866 2120752895.7923 431.0000
[2019-04-27 21:04:09,916] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8581.2600 2248881556.8485 554.0000
[2019-04-27 21:04:10,112] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8768.5674 2170855574.5270 493.0000
[2019-04-27 21:04:10,322] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8697.5625 2195797676.9310 571.0000
[2019-04-27 21:04:11,342] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1800000, evaluation results [1800000.0, 8097.679510568991, 2445569542.8369265, 746.0, 8768.567359798457, 2170855574.526967, 493.0, 8923.18663134159, 2120752895.7922986, 431.0, 8581.259978242537, 2248881556.8485174, 554.0, 8697.562466558522, 2195797676.9309654, 571.0]
[2019-04-27 21:04:12,372] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.1587711e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.7298468e-38], sum to 1.0000
[2019-04-27 21:04:12,380] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4686
[2019-04-27 21:04:12,387] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.83333333333333, 58.83333333333334, 1.0, 2.0, 0.5996616702854795, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 690235.5865344255, 690235.5865344255, 158353.9563273824], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3833400.0000, 
sim time next is 3834000.0000, 
raw observation next is [30.0, 59.0, 1.0, 2.0, 0.6135837868754735, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 703500.6134347584, 703500.6134347584, 160639.2923472571], 
processed observation next is [0.0, 0.391304347826087, 0.6666666666666666, 0.59, 1.0, 1.0, 0.5399806986612781, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.25125021908384226, 0.25125021908384226, 0.3089217160524175], 
reward next is 0.6911, 
noisyNet noise sample is [array([-0.19946773], dtype=float32), 1.4796677]. 
=============================================
[2019-04-27 21:04:12,405] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[69.370895]
 [69.43168 ]
 [69.49356 ]
 [69.55835 ]
 [69.59708 ]], R is [[69.3580246 ]
 [69.35991669]
 [69.36516571]
 [69.37329865]
 [69.38405609]].
[2019-04-27 21:04:13,248] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.0040377e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3144196e-38], sum to 1.0000
[2019-04-27 21:04:13,259] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5798
[2019-04-27 21:04:13,263] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.33333333333334, 55.66666666666667, 1.0, 2.0, 0.7256740278688149, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 827078.8939563243, 827078.8939563248, 181066.905968692], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3867600.0000, 
sim time next is 3868200.0000, 
raw observation next is [33.0, 58.5, 1.0, 2.0, 0.7581783242060851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 864146.1885545212, 864146.1885545212, 187457.0145224831], 
processed observation next is [0.0, 0.782608695652174, 0.7777777777777778, 0.585, 1.0, 1.0, 0.7121170526262918, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.30862363876947185, 0.30862363876947185, 0.3604942586970829], 
reward next is 0.6395, 
noisyNet noise sample is [array([-0.8686418], dtype=float32), 0.39338207]. 
=============================================
[2019-04-27 21:04:21,299] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.5538955e-21 1.0000000e+00 3.8737277e-23 2.4492135e-23 2.3638719e-17], sum to 1.0000
[2019-04-27 21:04:21,307] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6841
[2019-04-27 21:04:21,315] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1798042.445428188 W.
[2019-04-27 21:04:21,320] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.8, 87.33333333333334, 1.0, 2.0, 0.9498953146925672, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9977734948820727, 6.9112, 6.9112, 121.9260426156618, 1798042.445428188, 1798042.445428188, 367957.6572418726], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4012800.0000, 
sim time next is 4013400.0000, 
raw observation next is [26.0, 85.66666666666666, 1.0, 2.0, 0.9493471076743092, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.9260426156618, 1797416.620352696, 1797416.620352696, 367834.4664062308], 
processed observation next is [1.0, 0.43478260869565216, 0.5185185185185185, 0.8566666666666666, 1.0, 1.0, 0.9396989377075109, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.9972168686025908, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.64193450726882, 0.64193450726882, 0.7073739738581362], 
reward next is 0.2926, 
noisyNet noise sample is [array([0.5709384], dtype=float32), -1.9546237]. 
=============================================
[2019-04-27 21:04:25,093] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.1458003e-23 1.0000000e+00 5.7713853e-27 2.3487259e-25 2.7301663e-18], sum to 1.0000
[2019-04-27 21:04:25,101] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3709
[2019-04-27 21:04:25,112] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1391632.624881865 W.
[2019-04-27 21:04:25,116] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [23.35, 83.5, 1.0, 2.0, 0.5908915578332543, 1.0, 1.0, 0.5908915578332543, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9257752023137, 1391632.624881865, 1391632.624881865, 266966.5144886479], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4091400.0000, 
sim time next is 4092000.0000, 
raw observation next is [23.56666666666667, 83.33333333333333, 1.0, 2.0, 0.582183434686091, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9319976229485007, 6.911200000000001, 6.9112, 121.9260425341197, 1370496.505211583, 1370496.505211583, 286112.6801698896], 
processed observation next is [1.0, 0.34782608695652173, 0.4283950617283952, 0.8333333333333333, 1.0, 1.0, 0.5025993270072512, 0.0, 0.5, -0.1904761904761905, 1.0, 0.5, 0.9149970286856259, 8.881784197001253e-17, 0.0, 0.8094621282787812, 0.4894630375755654, 0.4894630375755654, 0.5502166926344031], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.37324724], dtype=float32), 1.3707539]. 
=============================================
[2019-04-27 21:04:25,126] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[54.31889 ]
 [56.09288 ]
 [57.923763]
 [58.64429 ]
 [58.822224]], R is [[53.17315674]
 [53.12802887]
 [52.59674835]
 [52.6953392 ]
 [52.86714172]].
[2019-04-27 21:04:25,259] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.5064693e-34 1.0000000e+00 0.0000000e+00 1.2441164e-38 1.2905077e-37], sum to 1.0000
[2019-04-27 21:04:25,268] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8622
[2019-04-27 21:04:25,278] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.16666666666667, 100.0, 1.0, 2.0, 0.558067097513152, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 683215.0561454595, 683215.0561454595, 152896.478294007], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4083000.0000, 
sim time next is 4083600.0000, 
raw observation next is [20.33333333333334, 100.0, 1.0, 2.0, 0.5349849792198182, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 653149.4059390601, 653149.4059390601, 149001.8145038577], 
processed observation next is [1.0, 0.2608695652173913, 0.3086419753086422, 1.0, 1.0, 1.0, 0.44641068954740265, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.23326764497823577, 0.23326764497823577, 0.28654195096895707], 
reward next is 0.7135, 
noisyNet noise sample is [array([-0.9222961], dtype=float32), 0.86663914]. 
=============================================
[2019-04-27 21:04:35,369] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.9666375e-33 1.0000000e+00 4.0007657e-38 2.9532254e-36 5.3238445e-37], sum to 1.0000
[2019-04-27 21:04:35,380] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0069
[2019-04-27 21:04:35,386] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 78.0, 1.0, 2.0, 0.4399003549921038, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 545423.5711239739, 545423.5711239739, 134390.2130435126], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4246800.0000, 
sim time next is 4247400.0000, 
raw observation next is [22.0, 78.0, 1.0, 2.0, 0.4340890461225614, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 538235.5931422715, 538235.5931422715, 133533.5600256844], 
processed observation next is [1.0, 0.13043478260869565, 0.37037037037037035, 0.78, 1.0, 1.0, 0.32629648347923973, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.19222699755081124, 0.19222699755081124, 0.25679530774170073], 
reward next is 0.7432, 
noisyNet noise sample is [array([1.1418425], dtype=float32), -0.55012554]. 
=============================================
[2019-04-27 21:04:45,999] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:04:46,009] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3407
[2019-04-27 21:04:46,020] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.83333333333334, 75.66666666666667, 1.0, 2.0, 0.712090558662206, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 811589.0895740245, 811589.0895740245, 178448.263014462], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4471800.0000, 
sim time next is 4472400.0000, 
raw observation next is [28.66666666666667, 77.33333333333334, 1.0, 2.0, 0.7106293812982922, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 809922.8658805822, 809922.8658805822, 178169.4062062317], 
processed observation next is [0.0, 0.782608695652174, 0.6172839506172841, 0.7733333333333334, 1.0, 1.0, 0.6555111682122526, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2892581663859222, 0.2892581663859222, 0.3426334734735225], 
reward next is 0.6574, 
noisyNet noise sample is [array([0.30931583], dtype=float32), 0.67981035]. 
=============================================
[2019-04-27 21:04:48,168] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:04:48,176] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2073
[2019-04-27 21:04:48,182] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.03333333333333, 83.66666666666666, 1.0, 2.0, 0.7065642282954179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 805287.2709330095, 805287.2709330095, 177390.4616102606], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4481400.0000, 
sim time next is 4482000.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.7077329746065462, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 806620.0185117348, 806620.0185117348, 177613.2096852566], 
processed observation next is [0.0, 0.9130434782608695, 0.5555555555555556, 0.84, 1.0, 1.0, 0.652063065007793, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2880785780399053, 0.2880785780399053, 0.3415638647793396], 
reward next is 0.6584, 
noisyNet noise sample is [array([1.502802], dtype=float32), 1.4138125]. 
=============================================
[2019-04-27 21:04:48,201] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[66.76843 ]
 [66.763824]
 [66.74829 ]
 [66.7311  ]
 [66.72624 ]], R is [[66.87030792]
 [66.86047363]
 [66.85092163]
 [66.84134674]
 [66.83180237]].
[2019-04-27 21:04:49,473] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:04:49,481] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6391
[2019-04-27 21:04:49,486] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.66666666666667, 94.0, 1.0, 2.0, 0.6585278009620904, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 750512.2624758733, 750512.2624758729, 168438.8443565942], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4494000.0000, 
sim time next is 4494600.0000, 
raw observation next is [24.5, 94.0, 1.0, 2.0, 0.650431328611727, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 741602.720296786, 741602.720296786, 166985.8492888572], 
processed observation next is [0.0, 0.0, 0.46296296296296297, 0.94, 1.0, 1.0, 0.5838468197758655, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2648581143917093, 0.2648581143917093, 0.3211266332478023], 
reward next is 0.6789, 
noisyNet noise sample is [array([-0.57584256], dtype=float32), -0.8087872]. 
=============================================
[2019-04-27 21:04:51,668] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:04:51,675] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7482
[2019-04-27 21:04:51,680] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.8, 87.0, 1.0, 2.0, 0.5995676539194082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 693435.0974533957, 693435.0974533953, 158493.1203539531], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4531200.0000, 
sim time next is 4531800.0000, 
raw observation next is [24.75, 86.5, 1.0, 2.0, 0.5917260333575118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 686071.8443937078, 686071.8443937078, 157221.8747977821], 
processed observation next is [0.0, 0.43478260869565216, 0.4722222222222222, 0.865, 1.0, 1.0, 0.5139595635208474, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2450256587120385, 0.2450256587120385, 0.30234975922650403], 
reward next is 0.6977, 
noisyNet noise sample is [array([-2.0486462], dtype=float32), -0.7967152]. 
=============================================
[2019-04-27 21:04:53,900] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:04:53,912] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1243
[2019-04-27 21:04:53,916] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 100.0, 1.0, 2.0, 0.4883182726048977, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 589190.8895323267, 589190.8895323267, 141308.8042672926], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4591800.0000, 
sim time next is 4592400.0000, 
raw observation next is [21.0, 100.0, 1.0, 2.0, 0.4886906860646396, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 589640.9569290928, 589640.9569290928, 141366.7830916858], 
processed observation next is [1.0, 0.13043478260869565, 0.3333333333333333, 1.0, 1.0, 1.0, 0.39129843579123763, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21058605604610456, 0.21058605604610456, 0.2718591982532419], 
reward next is 0.7281, 
noisyNet noise sample is [array([-1.1619859], dtype=float32), -0.12798509]. 
=============================================
[2019-04-27 21:05:04,240] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-27 21:05:04,242] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 21:05:04,243] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 21:05:04,247] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 21:05:04,248] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:05:04,245] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 21:05:04,248] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 21:05:04,250] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:05:04,250] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:05:04,252] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:05:04,256] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:05:04,277] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run74
[2019-04-27 21:05:04,301] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run74
[2019-04-27 21:05:04,325] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run74
[2019-04-27 21:05:04,326] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run74
[2019-04-27 21:05:04,373] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run74
[2019-04-27 21:05:09,919] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04100419], dtype=float32), -0.038118705]
[2019-04-27 21:05:09,922] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [23.72977993, 57.01887375, 1.0, 2.0, 0.3106411916908509, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 392839.7903397346, 392839.7903397346, 116876.3387224575]
[2019-04-27 21:05:09,923] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 21:05:09,925] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9746915264102088
[2019-04-27 21:05:35,611] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04100419], dtype=float32), -0.038118705]
[2019-04-27 21:05:35,612] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [25.83333333333334, 69.0, 1.0, 2.0, 0.4874160626713384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 582447.4637222119, 582447.4637222114, 140972.1506083886]
[2019-04-27 21:05:35,612] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:05:35,615] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7232315178152829
[2019-04-27 21:06:20,908] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04100419], dtype=float32), -0.038118705]
[2019-04-27 21:06:20,909] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [21.52368574666667, 91.75960716666667, 1.0, 2.0, 0.4401568224142552, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 535749.9369933095, 535749.9369933095, 134167.3406218766]
[2019-04-27 21:06:20,911] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 21:06:20,916] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.37082241042678343
[2019-04-27 21:06:26,761] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04100419], dtype=float32), -0.038118705]
[2019-04-27 21:06:26,763] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [25.55467548666666, 92.45183448166668, 1.0, 2.0, 0.7651620189059546, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.9260426156618, 1587178.445222544, 1587178.445222545, 329416.1199844189]
[2019-04-27 21:06:26,764] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 21:06:26,768] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [4.1195567e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.15286444818291345
[2019-04-27 21:06:26,769] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1587178.445222544 W.
[2019-04-27 21:06:32,895] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04100419], dtype=float32), -0.038118705]
[2019-04-27 21:06:32,896] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [22.405173155, 70.55521498499999, 1.0, 2.0, 0.3689274218959096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 460961.1273002889, 460961.1273002889, 124405.6748477199]
[2019-04-27 21:06:32,897] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 21:06:32,900] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8663762459733095
[2019-04-27 21:06:48,406] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8923.1866 2120752895.7923 431.0000
[2019-04-27 21:06:48,646] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8097.6795 2445569542.8369 746.0000
[2019-04-27 21:06:48,717] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8698.8707 2195527909.4997 572.0000
[2019-04-27 21:06:48,733] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8581.2410 2248887715.3409 553.0000
[2019-04-27 21:06:48,808] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8768.6241 2170826060.4938 493.0000
[2019-04-27 21:06:49,822] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1825000, evaluation results [1825000.0, 8097.679510568991, 2445569542.8369265, 746.0, 8768.624117554622, 2170826060.49376, 493.0, 8923.18663134159, 2120752895.7922986, 431.0, 8581.240965317736, 2248887715.34094, 553.0, 8698.870664946635, 2195527909.4997315, 572.0]
[2019-04-27 21:07:00,044] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9747524e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 21:07:00,052] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4205
[2019-04-27 21:07:00,060] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.41666666666667, 79.66666666666667, 1.0, 2.0, 0.6618298358293246, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 787307.9628257001, 787307.9628257001, 170544.3810163243], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4943400.0000, 
sim time next is 4944000.0000, 
raw observation next is [24.33333333333334, 80.33333333333334, 1.0, 2.0, 0.5480442848775924, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 651795.624629113, 651795.624629113, 150576.236484532], 
processed observation next is [1.0, 0.21739130434782608, 0.4567901234567903, 0.8033333333333335, 1.0, 1.0, 0.46195748199713377, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.23278415165325464, 0.23278415165325464, 0.28956968554717694], 
reward next is 0.7104, 
noisyNet noise sample is [array([0.85160553], dtype=float32), 0.26008725]. 
=============================================
[2019-04-27 21:07:00,072] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[64.78583 ]
 [64.93176 ]
 [65.27425 ]
 [65.425995]
 [65.499985]], R is [[64.85317993]
 [64.87667847]
 [64.93547058]
 [64.99079132]
 [65.04361725]].
[2019-04-27 21:07:02,912] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:07:02,925] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4297
[2019-04-27 21:07:02,932] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 84.83333333333333, 1.0, 2.0, 0.6683032867126416, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 761658.7424722443, 761658.7424722438, 170231.5408243275], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4990200.0000, 
sim time next is 4990800.0000, 
raw observation next is [26.66666666666667, 85.66666666666667, 1.0, 2.0, 0.6852771126488032, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 781013.5019618964, 781013.5019618964, 173375.9017401146], 
processed observation next is [1.0, 0.782608695652174, 0.5432098765432101, 0.8566666666666667, 1.0, 1.0, 0.62532989601048, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.27893339355782015, 0.27893339355782015, 0.3334151956540666], 
reward next is 0.6666, 
noisyNet noise sample is [array([0.14961982], dtype=float32), -0.31860965]. 
=============================================
[2019-04-27 21:07:07,900] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:07:07,907] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2110
[2019-04-27 21:07:07,917] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.83333333333334, 70.83333333333334, 1.0, 2.0, 0.7758058220970323, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 884249.0088389845, 884249.0088389842, 190999.7918336705], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5073000.0000, 
sim time next is 5073600.0000, 
raw observation next is [30.66666666666667, 71.66666666666667, 1.0, 2.0, 0.7764247673474751, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 884954.8783917081, 884954.8783917081, 191125.0235837742], 
processed observation next is [0.0, 0.7391304347826086, 0.6913580246913582, 0.7166666666666667, 1.0, 1.0, 0.7338390087469941, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.31605531371132434, 0.31605531371132434, 0.3675481222764888], 
reward next is 0.6325, 
noisyNet noise sample is [array([1.7605948], dtype=float32), -0.8376496]. 
=============================================
[2019-04-27 21:07:08,563] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:07:08,577] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5618
[2019-04-27 21:07:08,580] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 97.0, 1.0, 2.0, 0.7643834988794155, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 871222.6580554914, 871222.6580554909, 188694.5627080549], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5099400.0000, 
sim time next is 5100000.0000, 
raw observation next is [26.0, 96.0, 1.0, 2.0, 0.754939488117123, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 860452.5997402449, 860452.5997402449, 186808.6023960677], 
processed observation next is [0.0, 0.0, 0.5185185185185185, 0.96, 1.0, 1.0, 0.7082612953775274, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3073044999072303, 0.3073044999072303, 0.3592473123001302], 
reward next is 0.6408, 
noisyNet noise sample is [array([1.8873053], dtype=float32), 1.9442954]. 
=============================================
[2019-04-27 21:07:08,594] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[64.3675  ]
 [64.46017 ]
 [64.96111 ]
 [66.518776]
 [69.193115]], R is [[64.37162781]
 [64.36503601]
 [64.3553009 ]
 [64.34326935]
 [64.33106232]].
[2019-04-27 21:07:09,325] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:07:09,337] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1414
[2019-04-27 21:07:09,342] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 100.0, 1.0, 2.0, 0.7795783963826081, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 888551.411305141, 888551.4113051415, 191762.4324402517], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5097600.0000, 
sim time next is 5098200.0000, 
raw observation next is [26.0, 99.00000000000001, 1.0, 2.0, 0.7788311850780892, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 887699.2581724576, 887699.2581724576, 191610.0010683335], 
processed observation next is [0.0, 0.0, 0.5185185185185185, 0.9900000000000001, 1.0, 1.0, 0.7367037917596301, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.31703544934730626, 0.31703544934730626, 0.36848077128525675], 
reward next is 0.6315, 
noisyNet noise sample is [array([1.2101964], dtype=float32), -0.087411925]. 
=============================================
[2019-04-27 21:07:19,535] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2782488e-30 1.0000000e+00 3.3405538e-34 7.4813328e-34 7.6182356e-29], sum to 1.0000
[2019-04-27 21:07:19,552] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0956
[2019-04-27 21:07:19,557] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.68333333333334, 84.83333333333334, 1.0, 2.0, 0.6797782018645007, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 793071.1890626182, 793071.1890626182, 173235.6663589595], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5284200.0000, 
sim time next is 5284800.0000, 
raw observation next is [24.6, 85.0, 1.0, 2.0, 0.6716486894716904, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 784542.5142433527, 784542.5142433527, 171761.328908037], 
processed observation next is [1.0, 0.17391304347826086, 0.46666666666666673, 0.85, 1.0, 1.0, 0.6091055827043933, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2801937550869117, 0.2801937550869117, 0.33031024790007113], 
reward next is 0.6697, 
noisyNet noise sample is [array([-1.292896], dtype=float32), -1.0824168]. 
=============================================
[2019-04-27 21:07:29,070] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.11054116e-29 1.00000000e+00 1.80376578e-33 1.29387765e-32
 2.89634736e-29], sum to 1.0000
[2019-04-27 21:07:29,080] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5718
[2019-04-27 21:07:29,088] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333333, 92.16666666666667, 1.0, 2.0, 0.8877424738357537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 1011916.552038062, 1011916.552038062, 214742.4420913097], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5471400.0000, 
sim time next is 5472000.0000, 
raw observation next is [27.4, 92.0, 1.0, 2.0, 0.9115813774102135, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1039108.376563733, 1039108.376563733, 220079.6289754228], 
processed observation next is [1.0, 0.34782608695652173, 0.5703703703703703, 0.92, 1.0, 1.0, 0.894739735012159, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3711101344870475, 0.3711101344870475, 0.4232300557219669], 
reward next is 0.5768, 
noisyNet noise sample is [array([1.4514407], dtype=float32), -0.08251039]. 
=============================================
[2019-04-27 21:07:29,104] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[54.411346]
 [54.4377  ]
 [54.495953]
 [54.670776]
 [54.99131 ]], R is [[54.44917297]
 [54.49171448]
 [54.52893066]
 [54.55106735]
 [54.59412003]].
[2019-04-27 21:07:30,421] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.4178442e-20 1.0000000e+00 1.6421937e-22 4.3953395e-23 2.3653075e-16], sum to 1.0000
[2019-04-27 21:07:30,428] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2051
[2019-04-27 21:07:30,437] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1897849.718085407 W.
[2019-04-27 21:07:30,444] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.85, 90.0, 1.0, 2.0, 0.554682898158212, 1.0, 1.0, 0.554682898158212, 1.0, 2.0, 0.8830732386286999, 6.911199999999999, 6.9112, 121.94756008, 1897849.718085407, 1897849.718085407, 371237.9472019181], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5473800.0000, 
sim time next is 5474400.0000, 
raw observation next is [28.0, 89.33333333333334, 1.0, 2.0, 1.02, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9977734948820727, 7.326068702850216, 6.9112, 121.9247127407182, 2090747.490197364, 1878299.95103283, 382651.4916551239], 
processed observation next is [1.0, 0.34782608695652173, 0.5925925925925926, 0.8933333333333334, 1.0, 1.0, 1.0238095238095237, 0.0, 0.5, -0.1904761904761905, 1.0, 1.0, 0.9972168686025908, 0.04148687028502156, 0.0, 0.8094532998334829, 0.7466955322133443, 0.6708214110831535, 0.7358682531829306], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.41800153], dtype=float32), -0.11946588]. 
=============================================
[2019-04-27 21:07:31,559] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.1933792e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 21:07:31,567] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8737
[2019-04-27 21:07:31,582] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2339973.18885811 W.
[2019-04-27 21:07:31,589] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.03333333333333, 65.0, 1.0, 2.0, 0.7407364311319641, 1.0, 2.0, 0.6837328775424169, 1.0, 2.0, 0.9977734948820727, 6.911200000000001, 6.9112, 121.94756008, 2339973.18885811, 2339973.18885811, 440366.0229634021], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5492400.0000, 
sim time next is 5493000.0000, 
raw observation next is [33.21666666666666, 64.0, 1.0, 2.0, 0.7188821918036471, 1.0, 2.0, 0.6728057578782584, 1.0, 2.0, 0.9977734948820727, 6.911200000000001, 6.9112, 121.94756008, 2302528.542068682, 2302528.542068681, 434352.578459513], 
processed observation next is [1.0, 0.5652173913043478, 0.7858024691358023, 0.64, 1.0, 1.0, 0.6653359426233894, 1.0, 1.0, 0.6104830450931646, 1.0, 1.0, 0.9972168686025908, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.8223316221673865, 0.8223316221673862, 0.8352934201144481], 
reward next is 0.1647, 
noisyNet noise sample is [array([-0.10350841], dtype=float32), 0.76623183]. 
=============================================
[2019-04-27 21:07:31,598] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[56.07863 ]
 [55.794914]
 [56.39173 ]
 [57.249424]
 [55.15813 ]], R is [[56.25317383]
 [55.84378433]
 [55.4080658 ]
 [55.03950882]
 [54.80298233]].
[2019-04-27 21:07:35,215] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1386637e-24 1.0000000e+00 1.4500614e-28 1.2092313e-27 5.8268471e-26], sum to 1.0000
[2019-04-27 21:07:35,220] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2266
[2019-04-27 21:07:35,229] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1554438.79132198 W.
[2019-04-27 21:07:35,233] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.03333333333333, 82.33333333333334, 1.0, 2.0, 0.7364790885314083, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.9260426156618, 1554438.79132198, 1554438.79132198, 323958.9716630964], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5563200.0000, 
sim time next is 5563800.0000, 
raw observation next is [26.1, 82.0, 1.0, 2.0, 0.7680915473485284, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.9260426156618, 1590521.969965149, 1590521.96996515, 329978.7935069462], 
processed observation next is [1.0, 0.391304347826087, 0.5222222222222223, 0.82, 1.0, 1.0, 0.723918508748248, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.9972168686025908, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.5680435607018389, 0.5680435607018394, 0.6345746028979734], 
reward next is 0.3654, 
noisyNet noise sample is [array([0.6369513], dtype=float32), -0.5834288]. 
=============================================
[2019-04-27 21:07:42,737] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:07:42,747] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4785
[2019-04-27 21:07:42,748] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-27 21:07:42,750] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 21:07:42,751] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.85, 97.0, 1.0, 2.0, 0.5048038846648878, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 603519.1075789253, 603519.1075789249, 143704.3081923995], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5711400.0000, 
sim time next is 5712000.0000, 
raw observation next is [21.8, 97.0, 1.0, 2.0, 0.501740181524466, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 600404.4032058716, 600404.4032058719, 143240.6141835789], 
processed observation next is [0.0, 0.08695652173913043, 0.362962962962963, 0.97, 1.0, 1.0, 0.40683354943388805, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.214430144002097, 0.21443014400209712, 0.2754627195838056], 
reward next is 0.7245, 
noisyNet noise sample is [array([-0.01614674], dtype=float32), -1.9083816]. 
=============================================
[2019-04-27 21:07:42,752] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:07:42,753] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 21:07:42,753] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:07:42,754] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 21:07:42,755] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 21:07:42,756] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:07:42,758] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:07:42,756] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 21:07:42,760] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:07:42,785] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run75
[2019-04-27 21:07:42,811] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run75
[2019-04-27 21:07:42,812] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run75
[2019-04-27 21:07:42,863] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run75
[2019-04-27 21:07:42,891] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run75
[2019-04-27 21:08:09,015] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03886215], dtype=float32), -0.036723893]
[2019-04-27 21:08:09,017] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [19.1, 88.33333333333334, 1.0, 2.0, 0.3376491575326398, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 426956.3076411831, 426956.3076411831, 120326.9729570052]
[2019-04-27 21:08:09,017] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:08:09,022] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.43214987293069096
[2019-04-27 21:08:12,579] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03886215], dtype=float32), -0.036723893]
[2019-04-27 21:08:12,580] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [30.5, 55.0, 1.0, 2.0, 0.5861896552986827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 675850.2590118239, 675850.2590118239, 156098.2256104898]
[2019-04-27 21:08:12,580] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:08:12,583] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.07251710749157725
[2019-04-27 21:08:23,159] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03886215], dtype=float32), -0.036723893]
[2019-04-27 21:08:23,160] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [28.66666666666666, 80.66666666666667, 1.0, 2.0, 0.7605999235374568, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.9260426156618, 1581971.666707403, 1581971.666707403, 328543.1582478967]
[2019-04-27 21:08:23,162] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:08:23,165] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.5950604e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.943651783521812
[2019-04-27 21:08:23,167] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1581971.666707403 W.
[2019-04-27 21:08:43,678] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03886215], dtype=float32), -0.036723893]
[2019-04-27 21:08:43,680] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [26.2, 90.0, 1.0, 2.0, 0.6947271333229492, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 791789.2971734446, 791789.2971734442, 175148.961310586]
[2019-04-27 21:08:43,682] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:08:43,684] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8284749545931966
[2019-04-27 21:08:57,901] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03886215], dtype=float32), -0.036723893]
[2019-04-27 21:08:57,902] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [25.77335854, 52.35127796, 1.0, 2.0, 0.3876282973444481, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 482496.9043819892, 482496.9043819892, 126936.6069059308]
[2019-04-27 21:08:57,903] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 21:08:57,906] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9249922838959161
[2019-04-27 21:08:59,487] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.03886215], dtype=float32), -0.036723893]
[2019-04-27 21:08:59,489] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [22.1654307, 87.44098602, 1.0, 2.0, 0.4545158588251114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 551878.8862843581, 551878.8862843581, 136261.3476165744]
[2019-04-27 21:08:59,491] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 21:08:59,496] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5949374374676
[2019-04-27 21:09:15,009] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03886215], dtype=float32), -0.036723893]
[2019-04-27 21:09:15,010] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [23.74840521333333, 84.44349589666666, 1.0, 2.0, 0.5207554001329663, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 618846.2263988578, 618846.2263988578, 146108.4712869246]
[2019-04-27 21:09:15,011] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 21:09:15,016] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2816511175595189
[2019-04-27 21:09:16,975] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.03886215], dtype=float32), -0.036723893]
[2019-04-27 21:09:16,978] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [19.32198622, 105.6326499, 1.0, 2.0, 0.4085261003866879, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 506374.3274632, 506374.3274632, 129831.251476981]
[2019-04-27 21:09:16,979] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 21:09:16,981] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.40943812568260785
[2019-04-27 21:09:20,437] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03886215], dtype=float32), -0.036723893]
[2019-04-27 21:09:20,440] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [17.24820075333333, 95.77153716000001, 1.0, 2.0, 0.289177502403674, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 370013.8989572081, 370013.8989572081, 114250.4282863422]
[2019-04-27 21:09:20,442] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 21:09:20,445] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.45940222410242126
[2019-04-27 21:09:26,639] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8769.4422 2170768739.9937 493.0000
[2019-04-27 21:09:27,477] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8923.1866 2120752895.7923 431.0000
[2019-04-27 21:09:27,569] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8097.6795 2445569542.8369 746.0000
[2019-04-27 21:09:27,707] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8582.0590 2248830394.8409 553.0000
[2019-04-27 21:09:27,790] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8698.8707 2195527909.4997 572.0000
[2019-04-27 21:09:28,806] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1850000, evaluation results [1850000.0, 8097.679510568991, 2445569542.8369265, 746.0, 8769.442185004564, 2170768739.993712, 493.0, 8923.18663134159, 2120752895.7922986, 431.0, 8582.059032767682, 2248830394.840892, 553.0, 8698.870664946635, 2195527909.4997315, 572.0]
[2019-04-27 21:09:28,833] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[70.55768 ]
 [70.51352 ]
 [70.20578 ]
 [69.244316]
 [69.03676 ]], R is [[70.58976746]
 [70.60752106]
 [70.62435913]
 [70.6404953 ]
 [70.65576935]].
[2019-04-27 21:09:31,665] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:09:31,676] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3751
[2019-04-27 21:09:31,681] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.66666666666667, 63.0, 1.0, 2.0, 0.4857074740787273, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 582361.5866653157, 582361.5866653157, 140777.6042582216], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5748000.0000, 
sim time next is 5748600.0000, 
raw observation next is [26.83333333333333, 62.5, 1.0, 2.0, 0.4889010260998961, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 585472.7702864391, 585472.7702864391, 141247.5806403522], 
processed observation next is [0.0, 0.5217391304347826, 0.5493827160493825, 0.625, 1.0, 1.0, 0.39154884059511447, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.20909741795944253, 0.20909741795944253, 0.27162996276990803], 
reward next is 0.7284, 
noisyNet noise sample is [array([-1.4893453], dtype=float32), -1.9307934]. 
=============================================
[2019-04-27 21:09:36,955] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.6656621e-32 1.0000000e+00 3.4856071e-37 5.7939693e-36 3.9658596e-32], sum to 1.0000
[2019-04-27 21:09:36,965] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8111
[2019-04-27 21:09:36,976] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1314090.335140673 W.
[2019-04-27 21:09:36,979] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.9, 45.0, 1.0, 2.0, 0.5424906630808433, 1.0, 2.0, 0.5424906630808433, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156339, 1314090.335140673, 1314090.335140673, 252050.6924385724], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5842800.0000, 
sim time next is 5843400.0000, 
raw observation next is [27.91666666666666, 44.66666666666666, 1.0, 2.0, 0.4945181647906294, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8072309503149047, 6.911199999999999, 6.9112, 121.9260426156618, 1206083.422036516, 1206083.422036516, 252361.5938513058], 
processed observation next is [1.0, 0.6521739130434783, 0.5895061728395059, 0.44666666666666655, 1.0, 1.0, 0.3982359104650349, 0.0, 0.5, -0.1904761904761905, 1.0, 0.5, 0.7590386878936308, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.43074407929875574, 0.43074407929875574, 0.4853107574063573], 
reward next is 0.5147, 
noisyNet noise sample is [array([0.07198241], dtype=float32), -0.19565418]. 
=============================================
[2019-04-27 21:09:37,066] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:09:37,074] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3527
[2019-04-27 21:09:37,080] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.86666666666667, 74.5, 1.0, 2.0, 0.66256337265155, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 755113.7961713052, 755113.7961713048, 169176.7654311855], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6479400.0000, 
sim time next is 6480000.0000, 
raw observation next is [27.8, 75.0, 1.0, 2.0, 0.6632009443893638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 755840.7856186043, 755840.7856186043, 169293.4168550111], 
processed observation next is [1.0, 0.0, 0.5851851851851853, 0.75, 1.0, 1.0, 0.5990487433206713, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2699431377209301, 0.2699431377209301, 0.32556426318271364], 
reward next is 0.6744, 
noisyNet noise sample is [array([-0.60871136], dtype=float32), 0.22155493]. 
=============================================
[2019-04-27 21:09:37,099] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[83.54305 ]
 [83.44727 ]
 [83.33643 ]
 [83.230316]
 [83.09289 ]], R is [[77.11521912]
 [77.01873016]
 [76.92305756]
 [76.82806396]
 [76.73429108]].
[2019-04-27 21:09:39,082] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:09:39,090] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5651
[2019-04-27 21:09:39,094] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.3, 84.0, 1.0, 2.0, 0.3622563322720143, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 454261.7934544161, 454261.7934544161, 123533.4954460577], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5880600.0000, 
sim time next is 5881200.0000, 
raw observation next is [20.2, 84.33333333333334, 1.0, 2.0, 0.3598852076289441, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 451609.4863726892, 451609.4863726892, 123220.304684636], 
processed observation next is [1.0, 0.043478260869565216, 0.3037037037037037, 0.8433333333333334, 1.0, 1.0, 0.23795858051064775, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16128910227596044, 0.16128910227596044, 0.23696212439353076], 
reward next is 0.7630, 
noisyNet noise sample is [array([1.2726839], dtype=float32), 0.21921657]. 
=============================================
[2019-04-27 21:09:48,777] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:09:48,783] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4363
[2019-04-27 21:09:48,789] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.43333333333333, 79.33333333333334, 1.0, 2.0, 0.5131289063037733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 610664.5473227823, 610664.5473227823, 144922.507659563], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6045600.0000, 
sim time next is 6046200.0000, 
raw observation next is [24.3, 80.0, 1.0, 2.0, 0.5120312608866895, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 609750.712859758, 609750.7128597575, 144762.4964460452], 
processed observation next is [1.0, 1.0, 0.4555555555555556, 0.8, 1.0, 1.0, 0.419084834388916, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.21776811173562785, 0.21776811173562768, 0.2783894162423946], 
reward next is 0.7216, 
noisyNet noise sample is [array([1.2400312], dtype=float32), 2.647923]. 
=============================================
[2019-04-27 21:09:53,492] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.5981279e-32 1.0000000e+00 1.3206849e-34 5.9801507e-35 1.0968914e-29], sum to 1.0000
[2019-04-27 21:09:53,498] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4846
[2019-04-27 21:09:53,502] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.46666666666667, 89.33333333333334, 1.0, 2.0, 0.5326617428667029, 0.0, 2.0, 0.0, 1.0, 1.0, 0.8504616526940293, 6.9112, 6.9112, 121.9255644473683, 1241693.829395173, 1241693.829395173, 268079.2860023506], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6142800.0000, 
sim time next is 6143400.0000, 
raw observation next is [23.4, 89.5, 1.0, 2.0, 0.8364722343699761, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260424698526, 984160.3312552908, 984160.3312552912, 205129.6273581096], 
processed observation next is [1.0, 0.08695652173913043, 0.42222222222222217, 0.895, 1.0, 1.0, 0.8053240885356858, 0.0, 1.0, -0.1904761904761905, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.8094621278521145, 0.3514858325911753, 0.35148583259117544, 0.39448005261174923], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5364934], dtype=float32), 0.96774304]. 
=============================================
[2019-04-27 21:09:58,364] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:09:58,372] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4890
[2019-04-27 21:09:58,376] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.4, 79.0, 1.0, 2.0, 0.6899288462812887, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 786317.8237007672, 786317.8237007672, 174245.1253066433], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6391200.0000, 
sim time next is 6391800.0000, 
raw observation next is [27.3, 79.5, 1.0, 2.0, 0.689184536121613, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 785469.0923787908, 785469.0923787908, 174105.4848360732], 
processed observation next is [0.0, 1.0, 0.5666666666666667, 0.795, 1.0, 1.0, 0.6299815906209678, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.28052467584956814, 0.28052467584956814, 0.33481824006937155], 
reward next is 0.6652, 
noisyNet noise sample is [array([0.96510726], dtype=float32), 2.6516588]. 
=============================================
[2019-04-27 21:09:58,713] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:09:58,725] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4966
[2019-04-27 21:09:58,733] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.2, 82.33333333333334, 1.0, 2.0, 0.4758133419726134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 574227.635243448, 574227.635243448, 139381.159370818], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6232800.0000, 
sim time next is 6233400.0000, 
raw observation next is [23.1, 83.0, 1.0, 2.0, 0.4751342853538689, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 573487.5604613783, 573487.5604613783, 139279.6605055186], 
processed observation next is [0.0, 0.13043478260869565, 0.41111111111111115, 0.83, 1.0, 1.0, 0.3751598635165106, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2048169858790637, 0.2048169858790637, 0.26784550097215115], 
reward next is 0.7322, 
noisyNet noise sample is [array([1.544037], dtype=float32), -0.86958885]. 
=============================================
[2019-04-27 21:10:12,685] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.2930733e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.5589254e-36], sum to 1.0000
[2019-04-27 21:10:12,694] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0910
[2019-04-27 21:10:12,702] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.63333333333333, 76.0, 1.0, 2.0, 0.6644040016231963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 757212.5702492299, 757212.5702492299, 169513.5720693338], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6481200.0000, 
sim time next is 6481800.0000, 
raw observation next is [27.55, 76.5, 1.0, 2.0, 0.6653070274408106, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 758242.2460146851, 758242.2460146851, 169678.9873844772], 
processed observation next is [1.0, 0.0, 0.575925925925926, 0.765, 1.0, 1.0, 0.601555985048584, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2708008021481018, 0.2708008021481018, 0.32630574497014847], 
reward next is 0.6737, 
noisyNet noise sample is [array([-0.3648117], dtype=float32), 0.69667774]. 
=============================================
[2019-04-27 21:10:15,770] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.7767841e-24 1.0000000e+00 2.9190679e-26 3.2122458e-28 1.5768209e-21], sum to 1.0000
[2019-04-27 21:10:15,778] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3537
[2019-04-27 21:10:15,784] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.2, 80.0, 1.0, 2.0, 0.9013134368917247, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9259959713014, 1092843.41677925, 1092843.41677925, 220823.0624745169], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7030800.0000, 
sim time next is 7031400.0000, 
raw observation next is [23.36666666666667, 79.33333333333334, 1.0, 2.0, 0.9677936542147358, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.317590876277044, 6.9112, 121.9244895575976, 1380252.679291987, 1172146.878658025, 236370.1279675151], 
processed observation next is [1.0, 0.391304347826087, 0.4209876543209878, 0.7933333333333334, 1.0, 1.0, 0.9616591121603998, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.040639087627704386, 0.0, 0.8094518181296562, 0.49294738546142397, 0.41862388523500893, 0.4545579383990675], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.38103405], dtype=float32), -1.7645777]. 
=============================================
[2019-04-27 21:10:16,354] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3152183e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 21:10:16,361] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9868
[2019-04-27 21:10:16,368] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.36666666666667, 87.0, 1.0, 2.0, 0.3688842297068451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 460141.8893956085, 460141.8893956085, 124385.743545986], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7195200.0000, 
sim time next is 7195800.0000, 
raw observation next is [20.43333333333334, 86.5, 1.0, 2.0, 0.3726426703360987, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 464796.6551620236, 464796.6551620236, 124895.6968283708], 
processed observation next is [1.0, 0.2608695652173913, 0.31234567901234594, 0.865, 1.0, 1.0, 0.25314603611440323, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16599880541500844, 0.16599880541500844, 0.24018403236225153], 
reward next is 0.7598, 
noisyNet noise sample is [array([-0.14458232], dtype=float32), 0.9860801]. 
=============================================
[2019-04-27 21:10:19,298] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.2767057e-34 1.0000000e+00 0.0000000e+00 1.8018364e-37 5.3225558e-33], sum to 1.0000
[2019-04-27 21:10:19,312] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1231
[2019-04-27 21:10:19,318] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 44.5, 1.0, 2.0, 0.6741456802161254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 853339.6604784119, 853339.6604784115, 174243.0422386104], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6600600.0000, 
sim time next is 6601200.0000, 
raw observation next is [25.96666666666667, 43.66666666666667, 1.0, 2.0, 0.5777301983110922, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 731495.7052364501, 731495.7052364501, 156734.0499765849], 
processed observation next is [1.0, 0.391304347826087, 0.517283950617284, 0.4366666666666667, 1.0, 1.0, 0.49729785513225255, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.26124846615587505, 0.26124846615587505, 0.3014116345703556], 
reward next is 0.6986, 
noisyNet noise sample is [array([-0.6493734], dtype=float32), -1.631734]. 
=============================================
[2019-04-27 21:10:20,446] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8324104e-31 1.0000000e+00 2.2984505e-34 4.3976751e-36 2.3700413e-30], sum to 1.0000
[2019-04-27 21:10:20,452] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1430
[2019-04-27 21:10:20,456] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.66666666666666, 36.66666666666667, 1.0, 2.0, 0.7328950985717562, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156256, 918926.6363014303, 918926.6363014298, 185597.6267007806], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6612000.0000, 
sim time next is 6612600.0000, 
raw observation next is [28.83333333333334, 35.83333333333334, 1.0, 2.0, 0.7201356267832832, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 903550.2039376388, 903550.2039376388, 183065.2746589648], 
processed observation next is [1.0, 0.5217391304347826, 0.623456790123457, 0.35833333333333345, 1.0, 1.0, 0.6668281271229562, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3226965014062996, 0.3226965014062996, 0.35204860511339386], 
reward next is 0.6480, 
noisyNet noise sample is [array([-1.4630426], dtype=float32), 0.22942102]. 
=============================================
[2019-04-27 21:10:21,452] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:10:21,461] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8001
[2019-04-27 21:10:21,469] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 45.83333333333334, 1.0, 2.0, 0.3643249584942124, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 469993.0400836995, 469993.0400836995, 121112.3599290975], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6664200.0000, 
sim time next is 6664800.0000, 
raw observation next is [23.0, 45.66666666666667, 1.0, 2.0, 0.3454598072372529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 445649.2062959917, 445649.2062959917, 118346.2693432619], 
processed observation next is [1.0, 0.13043478260869565, 0.4074074074074074, 0.4566666666666667, 1.0, 1.0, 0.22078548480625346, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.15916043081999703, 0.15916043081999703, 0.22758897950627288], 
reward next is 0.7724, 
noisyNet noise sample is [array([0.0282559], dtype=float32), -0.3915199]. 
=============================================
[2019-04-27 21:10:21,678] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-27 21:10:21,681] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 21:10:21,682] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 21:10:21,682] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 21:10:21,683] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 21:10:21,684] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:10:21,685] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:10:21,683] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:10:21,685] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 21:10:21,689] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:10:21,691] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:10:21,706] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run76
[2019-04-27 21:10:21,733] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run76
[2019-04-27 21:10:21,758] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run76
[2019-04-27 21:10:21,778] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run76
[2019-04-27 21:10:21,798] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run76
[2019-04-27 21:10:34,576] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03673721], dtype=float32), -0.03391081]
[2019-04-27 21:10:34,578] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [29.26666666666667, 39.66666666666667, 1.0, 2.0, 0.4005263449718034, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 495716.3703255517, 495716.3703255513, 128680.7546200353]
[2019-04-27 21:10:34,579] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:10:34,581] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.17726542595328643
[2019-04-27 21:10:48,066] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03673721], dtype=float32), -0.03391081]
[2019-04-27 21:10:48,067] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [18.46666666666667, 86.33333333333334, 1.0, 2.0, 0.3050828216639407, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 389394.9542131661, 389394.9542131661, 116206.1309845573]
[2019-04-27 21:10:48,068] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:10:48,070] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.34584625476643927
[2019-04-27 21:11:05,807] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.03673721], dtype=float32), -0.03391081]
[2019-04-27 21:11:05,808] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [22.96741393333333, 102.81121032, 1.0, 2.0, 0.5890634545206523, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 683454.7577569302, 683454.7577569302, 156786.9439711384]
[2019-04-27 21:11:05,809] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 21:11:05,812] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7220909039263781
[2019-04-27 21:11:07,917] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03673721], dtype=float32), -0.03391081]
[2019-04-27 21:11:07,918] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [31.3, 61.5, 1.0, 2.0, 0.9586073259780146, 1.0, 2.0, 0.9586073259780146, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 2186939.795250417, 2186939.795250417, 413652.6391709945]
[2019-04-27 21:11:07,919] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:11:07,923] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.619903622077427
[2019-04-27 21:11:07,924] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2186939.795250417 W.
[2019-04-27 21:11:35,291] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03673721], dtype=float32), -0.03391081]
[2019-04-27 21:11:35,292] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [26.8, 84.66666666666667, 1.0, 2.0, 0.6860411942090732, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999997, 6.9112, 121.9260426156618, 781884.7734126083, 781884.7734126096, 173518.5770050304]
[2019-04-27 21:11:35,293] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:11:35,295] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5668886632441783
[2019-04-27 21:11:39,457] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.03673721], dtype=float32), -0.03391081]
[2019-04-27 21:11:39,458] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [21.73333333333333, 88.0, 1.0, 2.0, 0.4982987795964892, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 608275.0913236995, 608275.0913236995, 143089.5464879736]
[2019-04-27 21:11:39,459] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 21:11:39,461] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.21657493233838665
[2019-04-27 21:11:51,909] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03673721], dtype=float32), -0.03391081]
[2019-04-27 21:11:51,910] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [22.83333333333333, 94.0, 1.0, 2.0, 0.5199162056638819, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 614868.8417563922, 614868.8417563922, 145858.1831540104]
[2019-04-27 21:11:51,911] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 21:11:51,913] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8974750681517111
[2019-04-27 21:11:57,242] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03673721], dtype=float32), -0.03391081]
[2019-04-27 21:11:57,244] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [31.0, 29.0, 1.0, 2.0, 0.3833930088397338, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 481117.8048786521, 481117.8048786521, 126420.7648737745]
[2019-04-27 21:11:57,245] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 21:11:57,249] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9498646909279994
[2019-04-27 21:12:03,854] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03673721], dtype=float32), -0.03391081]
[2019-04-27 21:12:03,855] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [29.16666666666666, 50.33333333333334, 1.0, 2.0, 0.7070706238157646, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9715446295920056, 6.911199999999999, 6.9112, 121.9260426156618, 1547391.044779211, 1547391.044779212, 313269.2610280123]
[2019-04-27 21:12:03,856] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:12:03,859] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.23589355661607514
[2019-04-27 21:12:03,860] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1547391.044779211 W.
[2019-04-27 21:12:05,650] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8922.8943 2120741344.6244 431.0000
[2019-04-27 21:12:06,193] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8769.4422 2170768739.9937 493.0000
[2019-04-27 21:12:06,330] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8699.2440 2195333791.6118 572.0000
[2019-04-27 21:12:06,345] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8098.4495 2445508074.6415 746.0000
[2019-04-27 21:12:06,441] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8582.0590 2248830394.8409 553.0000
[2019-04-27 21:12:07,461] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 1875000, evaluation results [1875000.0, 8098.4494892610255, 2445508074.64147, 746.0, 8769.442185004564, 2170768739.993712, 493.0, 8922.894346631983, 2120741344.624408, 431.0, 8582.059032767682, 2248830394.840892, 553.0, 8699.243968577355, 2195333791.6117573, 572.0]
[2019-04-27 21:12:09,022] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:12:09,035] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5881
[2019-04-27 21:12:09,040] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.9, 88.0, 1.0, 2.0, 0.3622031950541166, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 453781.0385418292, 453781.0385418292, 123519.6229014753], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7160400.0000, 
sim time next is 7161000.0000, 
raw observation next is [19.88333333333333, 88.16666666666667, 1.0, 2.0, 0.3626759997507057, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 454357.9447866943, 454357.9447866943, 123582.944157269], 
processed observation next is [1.0, 0.9130434782608695, 0.2919753086419752, 0.8816666666666667, 1.0, 1.0, 0.24128095208417344, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16227069456667653, 0.16227069456667653, 0.23765950799474808], 
reward next is 0.7623, 
noisyNet noise sample is [array([-0.97081006], dtype=float32), 0.48360655]. 
=============================================
[2019-04-27 21:12:09,051] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[80.75042 ]
 [80.33192 ]
 [80.145065]
 [79.92957 ]
 [79.72403 ]], R is [[80.95188904]
 [80.90483093]
 [80.85830688]
 [80.81227875]
 [80.76662445]].
[2019-04-27 21:12:14,374] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:12:14,381] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4702
[2019-04-27 21:12:14,390] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.16666666666667, 84.33333333333334, 1.0, 2.0, 0.3329347918984714, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 426913.1180678604, 426913.11806786, 119742.4029301858], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6756000.0000, 
sim time next is 6756600.0000, 
raw observation next is [18.08333333333333, 84.66666666666667, 1.0, 2.0, 0.3268963072547721, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 419347.5798206844, 419347.5798206844, 118963.5320092202], 
processed observation next is [1.0, 0.17391304347826086, 0.22530864197530848, 0.8466666666666667, 1.0, 1.0, 0.19868608006520488, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.14976699279310157, 0.14976699279310157, 0.22877602309465422], 
reward next is 0.7712, 
noisyNet noise sample is [array([-0.8020751], dtype=float32), -1.0702358]. 
=============================================
[2019-04-27 21:12:21,398] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:12:21,406] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4321
[2019-04-27 21:12:21,411] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 49.0, 1.0, 2.0, 0.502982387580882, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 598547.712896449, 598547.7128964486, 143313.6712528823], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6872400.0000, 
sim time next is 6873000.0000, 
raw observation next is [30.16666666666666, 48.83333333333334, 1.0, 2.0, 0.507783598663727, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 603097.0513937526, 603097.0513937526, 144027.8340835352], 
processed observation next is [0.0, 0.5652173913043478, 0.6728395061728393, 0.48833333333333345, 1.0, 1.0, 0.41402809364729404, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21539180406919733, 0.21539180406919733, 0.27697660400679847], 
reward next is 0.7230, 
noisyNet noise sample is [array([0.7646196], dtype=float32), 1.475753]. 
=============================================
[2019-04-27 21:12:21,424] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[72.19699]
 [72.10974]
 [72.11193]
 [72.10846]
 [72.10329]], R is [[72.22301483]
 [72.22518158]
 [72.22844696]
 [72.23300171]
 [72.23905945]].
[2019-04-27 21:12:26,150] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:12:26,159] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2128
[2019-04-27 21:12:26,162] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 49.0, 1.0, 2.0, 0.5580195494863893, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 652648.6042078888, 652648.6042078884, 151786.5286853809], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6969600.0000, 
sim time next is 6970200.0000, 
raw observation next is [31.03333333333333, 48.16666666666667, 1.0, 2.0, 0.5624632860316662, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 659537.6177843225, 659537.6177843229, 152599.3318927402], 
processed observation next is [0.0, 0.6956521739130435, 0.7049382716049382, 0.4816666666666667, 1.0, 1.0, 0.47912295956150736, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2355491492086866, 0.23554914920868678, 0.29346025363988504], 
reward next is 0.7065, 
noisyNet noise sample is [array([-0.6982756], dtype=float32), 0.51096845]. 
=============================================
[2019-04-27 21:12:26,742] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:12:26,754] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4743
[2019-04-27 21:12:26,759] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.2, 63.33333333333334, 1.0, 2.0, 0.4411992800384067, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 539999.9680569153, 539999.9680569153, 134406.502393395], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6988800.0000, 
sim time next is 6989400.0000, 
raw observation next is [25.05, 64.0, 1.0, 2.0, 0.4395417753620542, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 538232.7389857145, 538232.7389857145, 134168.9580189884], 
processed observation next is [0.0, 0.9130434782608695, 0.48333333333333334, 0.64, 1.0, 1.0, 0.33278782781196925, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.19222597820918375, 0.19222597820918375, 0.25801722695959306], 
reward next is 0.7420, 
noisyNet noise sample is [array([1.7608525], dtype=float32), -1.4211373]. 
=============================================
[2019-04-27 21:12:33,230] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:12:33,239] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8501
[2019-04-27 21:12:33,244] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.8, 84.66666666666667, 1.0, 2.0, 0.5138450620538777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 610349.8076975867, 610349.8076975867, 144992.3456740532], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7593600.0000, 
sim time next is 7594200.0000, 
raw observation next is [23.75, 85.0, 1.0, 2.0, 0.5148685469613936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 611611.6334843005, 611611.6334843005, 145157.2901063694], 
processed observation next is [0.0, 0.9130434782608695, 0.4351851851851852, 0.85, 1.0, 1.0, 0.42246255590642096, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21843272624439303, 0.21843272624439303, 0.2791486348199411], 
reward next is 0.7209, 
noisyNet noise sample is [array([0.8616136], dtype=float32), -0.521154]. 
=============================================
[2019-04-27 21:12:34,002] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2092513e-32 1.0000000e+00 2.2277058e-37 6.1103411e-37 6.6637673e-33], sum to 1.0000
[2019-04-27 21:12:34,016] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9549
[2019-04-27 21:12:34,020] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.65, 79.0, 1.0, 2.0, 0.632629261401902, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 786096.906766881, 786096.906766881, 166253.3973632899], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7119000.0000, 
sim time next is 7119600.0000, 
raw observation next is [21.76666666666667, 78.66666666666667, 1.0, 2.0, 0.6555154906893286, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 813799.1866184906, 813799.1866184906, 170456.1721635787], 
processed observation next is [1.0, 0.391304347826087, 0.3617283950617285, 0.7866666666666667, 1.0, 1.0, 0.5898993936777721, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2906425666494609, 0.2906425666494609, 0.3278003310838052], 
reward next is 0.6722, 
noisyNet noise sample is [array([0.65295666], dtype=float32), 1.1967962]. 
=============================================
[2019-04-27 21:12:39,805] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:12:39,815] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3378
[2019-04-27 21:12:39,819] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.36666666666667, 87.0, 1.0, 2.0, 0.3688842297068451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 460141.8893956085, 460141.8893956085, 124385.743545986], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7195200.0000, 
sim time next is 7195800.0000, 
raw observation next is [20.43333333333334, 86.5, 1.0, 2.0, 0.3726426703360987, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 464796.6551620236, 464796.6551620236, 124895.6968283708], 
processed observation next is [1.0, 0.2608695652173913, 0.31234567901234594, 0.865, 1.0, 1.0, 0.25314603611440323, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16599880541500844, 0.16599880541500844, 0.24018403236225153], 
reward next is 0.7598, 
noisyNet noise sample is [array([-0.13284014], dtype=float32), 0.7815428]. 
=============================================
[2019-04-27 21:12:45,087] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:12:45,095] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5483
[2019-04-27 21:12:45,101] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.16666666666666, 72.33333333333333, 1.0, 2.0, 0.4486950694861547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 545686.5581658657, 545686.5581658657, 135418.4745850023], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7329000.0000, 
sim time next is 7329600.0000, 
raw observation next is [24.0, 73.0, 1.0, 2.0, 0.4456425775408738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 542498.7842246711, 542498.7842246711, 134980.486429471], 
processed observation next is [1.0, 0.8695652173913043, 0.4444444444444444, 0.73, 1.0, 1.0, 0.34005068754865925, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.19374956579452537, 0.19374956579452537, 0.25957785851821347], 
reward next is 0.7404, 
noisyNet noise sample is [array([-0.03251045], dtype=float32), 0.52874553]. 
=============================================
[2019-04-27 21:12:45,887] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:12:45,887] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:12:45,939] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res14/Eplus-env-sub_run10
[2019-04-27 21:12:51,205] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:12:51,214] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7418
[2019-04-27 21:12:51,222] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.08333333333334, 91.16666666666667, 1.0, 2.0, 0.3823609136777921, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 475520.8360949864, 475520.8360949859, 126199.7925486079], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7431000.0000, 
sim time next is 7431600.0000, 
raw observation next is [20.06666666666667, 91.33333333333334, 1.0, 2.0, 0.381958393854153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 475001.9825132458, 475001.9825132458, 126143.9455419241], 
processed observation next is [0.0, 0.0, 0.29876543209876555, 0.9133333333333334, 1.0, 1.0, 0.26423618315970593, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16964356518330206, 0.16964356518330206, 0.24258451065754635], 
reward next is 0.7574, 
noisyNet noise sample is [array([0.0942465], dtype=float32), -0.095386125]. 
=============================================
[2019-04-27 21:12:54,492] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:12:54,496] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:12:54,562] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res3/Eplus-env-sub_run10
[2019-04-27 21:12:55,519] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:12:55,524] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9113
[2019-04-27 21:12:55,528] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.3, 95.66666666666667, 1.0, 2.0, 0.4581901174338989, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 554741.6327025421, 554741.6327025421, 136763.0832682126], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7519200.0000, 
sim time next is 7519800.0000, 
raw observation next is [21.25, 95.83333333333333, 1.0, 2.0, 0.4562096922479887, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 552625.2322766407, 552625.2322766407, 136474.4812817969], 
processed observation next is [0.0, 0.0, 0.3425925925925926, 0.9583333333333333, 1.0, 1.0, 0.3526305860095103, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.19736615438451455, 0.19736615438451455, 0.2624509255419171], 
reward next is 0.7375, 
noisyNet noise sample is [array([-1.1640426], dtype=float32), 0.018462924]. 
=============================================
[2019-04-27 21:12:56,499] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:12:56,507] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6566
[2019-04-27 21:12:56,511] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.08333333333334, 75.83333333333334, 1.0, 2.0, 0.3261487005281855, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 420730.7062652097, 420730.7062652097, 113262.8290068343], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 15000.0000, 
sim time next is 15600.0000, 
raw observation next is [18.06666666666667, 75.66666666666667, 1.0, 2.0, 0.290960663736095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 375327.1513664683, 375327.1513664683, 108192.1938665509], 
processed observation next is [1.0, 0.17391304347826086, 0.22469135802469148, 0.7566666666666667, 1.0, 1.0, 0.1559055520667798, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1340454112023101, 0.1340454112023101, 0.20806191128182863], 
reward next is 0.7919, 
noisyNet noise sample is [array([1.3144193], dtype=float32), 0.45237517]. 
=============================================
[2019-04-27 21:12:59,548] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:12:59,557] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6641
[2019-04-27 21:12:59,562] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.9, 84.0, 1.0, 2.0, 0.5138330715122956, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 610196.2859273349, 610196.2859273349, 144985.0780227908], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7592400.0000, 
sim time next is 7593000.0000, 
raw observation next is [23.85, 84.33333333333333, 1.0, 2.0, 0.5138182897846096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 610271.5596980184, 610271.5596980184, 144986.300734854], 
processed observation next is [0.0, 0.9130434782608695, 0.43888888888888894, 0.8433333333333333, 1.0, 1.0, 0.4212122497435829, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21795412846357798, 0.21795412846357798, 0.2788198091054885], 
reward next is 0.7212, 
noisyNet noise sample is [array([-1.0534577], dtype=float32), -0.70065933]. 
=============================================
[2019-04-27 21:12:59,590] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[77.253006]
 [77.129326]
 [77.10413 ]
 [77.08576 ]
 [77.0675  ]], R is [[77.25756073]
 [77.20616913]
 [77.15511322]
 [77.1042099 ]
 [77.05324554]].
[2019-04-27 21:12:59,804] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-27 21:12:59,806] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 21:12:59,807] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:12:59,808] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 21:12:59,810] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 21:12:59,810] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:12:59,812] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:12:59,811] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 21:12:59,813] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 21:12:59,816] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:12:59,816] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:12:59,836] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run77
[2019-04-27 21:12:59,837] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run77
[2019-04-27 21:12:59,860] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run77
[2019-04-27 21:12:59,862] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run77
[2019-04-27 21:12:59,939] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run77
[2019-04-27 21:13:38,950] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0390326], dtype=float32), -0.03075509]
[2019-04-27 21:13:38,952] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [29.33333333333334, 66.66666666666667, 1.0, 2.0, 0.6434700491610188, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 733343.0052627617, 733343.0052627613, 165718.6057785166]
[2019-04-27 21:13:38,952] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 21:13:38,958] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6987541630289764
[2019-04-27 21:14:04,527] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0390326], dtype=float32), -0.03075509]
[2019-04-27 21:14:04,528] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [31.13333333333333, 78.66666666666667, 1.0, 2.0, 0.883859425764085, 1.0, 2.0, 0.7552943748584772, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.94756008, 2585236.292113334, 2585236.292113334, 482300.3114415606]
[2019-04-27 21:14:04,529] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:14:04,531] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.19903548329777054
[2019-04-27 21:14:04,533] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2585236.292113334 W.
[2019-04-27 21:14:16,101] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0390326], dtype=float32), -0.03075509]
[2019-04-27 21:14:16,101] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [26.4, 45.0, 1.0, 2.0, 0.7826737750205895, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 983270.732296659, 983270.7322966595, 195830.7924064914]
[2019-04-27 21:14:16,102] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:14:16,105] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.7267441e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.4512384119752114
[2019-04-27 21:14:32,687] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0390326], dtype=float32), -0.03075509]
[2019-04-27 21:14:32,689] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [19.72337183, 52.69358079, 1.0, 2.0, 0.245349926991371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 316456.6825516001, 316456.6825516006, 91015.10739254812]
[2019-04-27 21:14:32,691] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 21:14:32,693] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4587651914002263
[2019-04-27 21:14:35,107] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0390326], dtype=float32), -0.03075509]
[2019-04-27 21:14:35,110] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [18.98080771, 98.92660495999999, 1.0, 2.0, 0.3889823122136059, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 484097.0310243928, 484097.0310243928, 127122.860275896]
[2019-04-27 21:14:35,112] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 21:14:35,116] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6099623472304639
[2019-04-27 21:14:44,808] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8699.1525 2195381338.3658 572.0000
[2019-04-27 21:14:44,827] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8769.4422 2170768739.9937 493.0000
[2019-04-27 21:14:44,832] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8921.8784 2120668147.5309 430.0000
[2019-04-27 21:14:44,895] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8098.4495 2445508074.6415 746.0000
[2019-04-27 21:14:44,974] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8582.0590 2248830394.8409 553.0000
[2019-04-27 21:14:45,994] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1900000, evaluation results [1900000.0, 8098.4494892610255, 2445508074.64147, 746.0, 8769.442185004564, 2170768739.993712, 493.0, 8921.87835320127, 2120668147.5309098, 430.0, 8582.059032767682, 2248830394.840892, 553.0, 8699.15253251195, 2195381338.3657684, 572.0]
[2019-04-27 21:14:50,401] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.5226184e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3929251e-37], sum to 1.0000
[2019-04-27 21:14:50,409] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5655
[2019-04-27 21:14:50,414] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.15, 66.0, 1.0, 2.0, 0.5408137062910265, 1.0, 1.0, 0.5408137062910265, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9257559130249, 1290507.699301004, 1290507.699301004, 250809.2666851035], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7661400.0000, 
sim time next is 7662000.0000, 
raw observation next is [25.0, 66.0, 1.0, 2.0, 0.9621176229528493, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.282646068222801, 6.9112, 121.9244461769064, 1355874.569804145, 1165663.473962996, 235021.3880728565], 
processed observation next is [1.0, 0.6956521739130435, 0.48148148148148145, 0.66, 1.0, 1.0, 0.9549019320867254, 0.0, 0.5, -0.1904761904761905, 0.0, 1.0, -0.25, 0.037144606822280136, 0.0, 0.8094515301269793, 0.48424091778719464, 0.4163083835582128, 0.45196420783241636], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.95378613], dtype=float32), 0.025330078]. 
=============================================
[2019-04-27 21:14:50,426] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[62.499546]
 [61.250122]
 [61.005596]
 [60.629288]
 [59.834282]], R is [[61.5893631 ]
 [60.97346878]
 [60.3637352 ]
 [60.22931671]
 [59.6270256 ]].
[2019-04-27 21:14:52,464] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:14:52,473] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6991
[2019-04-27 21:14:52,480] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.18333333333334, 92.16666666666667, 1.0, 2.0, 0.3103585014018716, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 394639.0130860892, 394639.0130860892, 116858.779770845], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7707000.0000, 
sim time next is 7707600.0000, 
raw observation next is [18.1, 94.0, 1.0, 2.0, 0.3147703728346104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 399717.8601995404, 399717.8601995404, 117410.1471549808], 
processed observation next is [1.0, 0.21739130434782608, 0.22592592592592597, 0.94, 1.0, 1.0, 0.18425044385072667, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.142756378642693, 0.142756378642693, 0.22578874452880923], 
reward next is 0.7742, 
noisyNet noise sample is [array([0.14556994], dtype=float32), 0.07857338]. 
=============================================
[2019-04-27 21:14:56,405] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:14:56,412] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2171
[2019-04-27 21:14:56,418] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.4, 74.0, 1.0, 2.0, 0.3499820938754528, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 440207.4466638992, 440207.4466638992, 121917.0039296335], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7776000.0000, 
sim time next is 7776600.0000, 
raw observation next is [21.36666666666667, 73.83333333333334, 1.0, 2.0, 0.3482817038363532, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 438319.4495411519, 438319.4495411519, 121695.7746161226], 
processed observation next is [1.0, 0.0, 0.3469135802469137, 0.7383333333333334, 1.0, 1.0, 0.2241448855194681, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.15654266055041138, 0.15654266055041138, 0.23403033580023577], 
reward next is 0.7660, 
noisyNet noise sample is [array([-0.17173912], dtype=float32), 0.63218784]. 
=============================================
[2019-04-27 21:14:58,937] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:14:58,938] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:14:59,010] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res11/Eplus-env-sub_run10
[2019-04-27 21:14:59,561] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0942440e-25 1.0000000e+00 1.2047403e-30 1.5252484e-30 1.7523831e-25], sum to 1.0000
[2019-04-27 21:14:59,564] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7415
[2019-04-27 21:14:59,568] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.88333333333333, 42.5, 1.0, 2.0, 0.9193575267888099, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.172851259538144, 6.9112, 121.924908120674, 1279278.483654642, 1145290.899383485, 225910.0034807839], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7812600.0000, 
sim time next is 7813200.0000, 
raw observation next is [28.06666666666667, 42.0, 1.0, 2.0, 0.9270773410494202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.220091505410555, 6.9112, 121.9244721446513, 1312235.219028276, 1154057.197290251, 227696.4708145636], 
processed observation next is [1.0, 0.43478260869565216, 0.5950617283950619, 0.42, 1.0, 1.0, 0.9131873107731193, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.03088915054105552, 0.0, 0.809451702525801, 0.4686554353672414, 0.4121632847465182, 0.4378778284895454], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.3639755], dtype=float32), -2.1556642]. 
=============================================
[2019-04-27 21:14:59,888] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:14:59,896] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4897
[2019-04-27 21:14:59,901] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.98333333333333, 45.5, 1.0, 2.0, 0.4271968067845469, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 521671.87296828, 521671.8729682795, 132320.8265954159], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7841400.0000, 
sim time next is 7842000.0000, 
raw observation next is [28.66666666666667, 47.0, 1.0, 2.0, 0.4310269318093365, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 526107.7406058445, 526107.7406058445, 132871.5222661059], 
processed observation next is [1.0, 0.782608695652174, 0.6172839506172841, 0.47, 1.0, 1.0, 0.3226511092968291, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.18789562164494447, 0.18789562164494447, 0.2555221582040498], 
reward next is 0.7445, 
noisyNet noise sample is [array([0.59715366], dtype=float32), 0.18056592]. 
=============================================
[2019-04-27 21:14:59,908] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[78.19321]
 [78.26995]
 [77.4513 ]
 [77.11387]
 [76.7492 ]], R is [[77.71446991]
 [77.68286133]
 [77.65346527]
 [77.62594604]
 [77.60018158]].
[2019-04-27 21:15:00,000] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:15:00,006] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8757
[2019-04-27 21:15:00,012] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1549578.909226806 W.
[2019-04-27 21:15:00,017] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.03333333333333, 33.5, 1.0, 2.0, 0.9893825595253735, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.560309335347849, 6.9112, 121.9233528062426, 1549578.909226806, 1217184.245912351, 242257.4771247279], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7834200.0000, 
sim time next is 7834800.0000, 
raw observation next is [31.06666666666667, 34.0, 1.0, 2.0, 0.5828765173015651, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9500313548949425, 6.9112, 6.9112, 121.9256517328287, 1419124.178474132, 1419124.178474132, 284446.1437262858], 
processed observation next is [1.0, 0.6956521739130435, 0.7061728395061729, 0.34, 1.0, 1.0, 0.503424425359006, 0.0, 1.0, -0.1904761904761905, 1.0, 0.5, 0.937539193618678, 0.0, 0.0, 0.8094595337646137, 0.5068300637407614, 0.5068300637407614, 0.5470118148582419], 
reward next is 0.4530, 
noisyNet noise sample is [array([4.202516], dtype=float32), -0.35724077]. 
=============================================
[2019-04-27 21:15:01,184] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:15:01,197] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7182
[2019-04-27 21:15:01,202] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.71666666666667, 51.5, 1.0, 2.0, 0.4329901239856415, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 528172.4074693026, 528172.4074693026, 133148.7141979707], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7843800.0000, 
sim time next is 7844400.0000, 
raw observation next is [27.4, 53.0, 1.0, 2.0, 0.4349388112036908, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 530562.0642133465, 530562.0642133465, 133434.3721541441], 
processed observation next is [1.0, 0.8260869565217391, 0.5703703703703703, 0.53, 1.0, 1.0, 0.3273081085758224, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1894864515047666, 0.1894864515047666, 0.2566045618348925], 
reward next is 0.7434, 
noisyNet noise sample is [array([-0.79636914], dtype=float32), -1.995471]. 
=============================================
[2019-04-27 21:15:02,516] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.6190979e-24 1.0000000e+00 6.2903068e-27 3.1718716e-26 1.8907971e-20], sum to 1.0000
[2019-04-27 21:15:02,528] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5530
[2019-04-27 21:15:02,540] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666667, 59.0, 1.0, 2.0, 0.9010040759534188, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.924262434958927, 6.9112, 121.9259419125413, 1105854.28322347, 1099165.154542109, 220987.3903903789], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7899600.0000, 
sim time next is 7900200.0000, 
raw observation next is [26.45, 58.0, 1.0, 2.0, 0.8686550410790924, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260347496819, 1058238.369859791, 1058238.369859791, 213619.1091369558], 
processed observation next is [1.0, 0.43478260869565216, 0.5351851851851852, 0.58, 1.0, 1.0, 0.8436369536655862, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094620765982095, 0.3779422749499253, 0.3779422749499253, 0.41080597910953043], 
reward next is 0.5892, 
noisyNet noise sample is [array([1.5593554], dtype=float32), -1.0549737]. 
=============================================
[2019-04-27 21:15:03,803] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.8621521e-24 1.0000000e+00 2.5747619e-26 1.7418121e-26 8.0256392e-23], sum to 1.0000
[2019-04-27 21:15:03,810] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7530
[2019-04-27 21:15:03,817] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666667, 59.0, 1.0, 2.0, 0.9010042972031194, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.924263871027539, 6.9112, 121.92594190492, 1105855.285085538, 1099165.421009718, 220987.4409481427], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7899600.0000, 
sim time next is 7900200.0000, 
raw observation next is [26.45, 58.0, 1.0, 2.0, 0.8686551901638535, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260347488171, 1058238.548477376, 1058238.548477376, 213619.142326097], 
processed observation next is [1.0, 0.43478260869565216, 0.5351851851851852, 0.58, 1.0, 1.0, 0.8436371311474447, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.809462076592468, 0.37794233874192, 0.37794233874192, 0.41080604293480194], 
reward next is 0.5892, 
noisyNet noise sample is [array([-0.19003078], dtype=float32), 1.6023666]. 
=============================================
[2019-04-27 21:15:06,071] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:15:06,071] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:15:06,123] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res12/Eplus-env-sub_run10
[2019-04-27 21:15:06,293] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:15:06,293] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:15:06,319] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res2/Eplus-env-sub_run10
[2019-04-27 21:15:06,339] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:15:06,340] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:15:06,361] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res16/Eplus-env-sub_run10
[2019-04-27 21:15:06,507] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:15:06,507] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:15:06,522] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res10/Eplus-env-sub_run10
[2019-04-27 21:15:06,621] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:15:06,621] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:15:06,643] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res7/Eplus-env-sub_run10
[2019-04-27 21:15:06,982] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:15:06,983] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:15:06,994] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res17/Eplus-env-sub_run10
[2019-04-27 21:15:07,033] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:15:07,034] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:15:07,041] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:15:07,042] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8586
[2019-04-27 21:15:07,043] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res4/Eplus-env-sub_run10
[2019-04-27 21:15:07,077] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.26666666666667, 65.33333333333334, 1.0, 2.0, 0.3248620300789506, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 410959.4815183475, 410959.4815183475, 118678.7026695065], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 199200.0000, 
sim time next is 199800.0000, 
raw observation next is [22.45, 66.0, 1.0, 2.0, 0.3357220510766953, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 423398.9442650486, 423398.9442650482, 120063.6607916377], 
processed observation next is [0.0, 0.30434782608695654, 0.387037037037037, 0.66, 1.0, 1.0, 0.20919291794844677, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.15121390866608878, 0.15121390866608864, 0.23089165536853404], 
reward next is 0.7691, 
noisyNet noise sample is [array([1.7853119], dtype=float32), 0.57489794]. 
=============================================
[2019-04-27 21:15:07,101] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:15:07,101] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:15:07,105] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res9/Eplus-env-sub_run10
[2019-04-27 21:15:07,130] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:15:07,131] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:15:07,141] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res6/Eplus-env-sub_run10
[2019-04-27 21:15:07,163] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:15:07,163] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:15:07,172] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res13/Eplus-env-sub_run10
[2019-04-27 21:15:07,202] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:15:07,202] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:15:07,209] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res15/Eplus-env-sub_run10
[2019-04-27 21:15:07,257] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:15:07,258] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:15:07,289] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:15:07,289] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:15:07,293] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res8/Eplus-env-sub_run10
[2019-04-27 21:15:07,342] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res5/Eplus-env-sub_run10
[2019-04-27 21:15:16,863] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:15:16,869] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7810
[2019-04-27 21:15:16,876] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.18333333333333, 11.33333333333333, 1.0, 2.0, 0.3797562843917118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 489906.3961256933, 489906.3961256933, 125743.0170376293], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 154200.0000, 
sim time next is 154800.0000, 
raw observation next is [33.9, 12.0, 1.0, 2.0, 0.3775605947700827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 487000.2339408466, 487000.2339408466, 125679.8147105362], 
processed observation next is [1.0, 0.8260869565217391, 0.811111111111111, 0.12, 1.0, 1.0, 0.2590007080596223, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.17392865497887378, 0.17392865497887378, 0.24169195136641577], 
reward next is 0.7583, 
noisyNet noise sample is [array([0.62556654], dtype=float32), -0.15478773]. 
=============================================
[2019-04-27 21:15:25,077] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:15:25,085] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0906
[2019-04-27 21:15:25,091] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 30.0, 1.0, 2.0, 0.3047116897681467, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 393069.9487109467, 393069.9487109467, 105556.2794029256], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 297000.0000, 
sim time next is 297600.0000, 
raw observation next is [25.83333333333334, 29.66666666666666, 1.0, 2.0, 0.3056912864413071, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 394333.9271658415, 394333.9271658415, 105963.9426582767], 
processed observation next is [0.0, 0.43478260869565216, 0.5123456790123458, 0.29666666666666663, 1.0, 1.0, 0.17344200766822276, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.14083354541637197, 0.14083354541637197, 0.2037768128043783], 
reward next is 0.7962, 
noisyNet noise sample is [array([0.05260894], dtype=float32), -0.62515485]. 
=============================================
[2019-04-27 21:15:28,171] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:15:28,177] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6487
[2019-04-27 21:15:28,181] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 51.0, 1.0, 2.0, 0.2678388048109389, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 345494.1969839053, 345494.1969839049, 105130.1605125173], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 349200.0000, 
sim time next is 349800.0000, 
raw observation next is [21.65, 51.5, 1.0, 2.0, 0.2664900113402932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 343753.9534359644, 343753.9534359644, 104321.3716915351], 
processed observation next is [1.0, 0.043478260869565216, 0.35740740740740734, 0.515, 1.0, 1.0, 0.12677382302415857, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.122769269084273, 0.122769269084273, 0.20061802248372135], 
reward next is 0.7994, 
noisyNet noise sample is [array([-2.1763387], dtype=float32), 2.0823276]. 
=============================================
[2019-04-27 21:15:32,824] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:15:32,832] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9646
[2019-04-27 21:15:32,839] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1619079.987450351 W.
[2019-04-27 21:15:32,844] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [35.6, 17.33333333333333, 1.0, 2.0, 0.4453272858421333, 1.0, 1.0, 0.4453272858421333, 1.0, 2.0, 0.7233432658754204, 6.9112, 6.9112, 121.94756008, 1619079.987450351, 1619079.987450351, 316775.944406324], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 664800.0000, 
sim time next is 665400.0000, 
raw observation next is [35.6, 17.16666666666667, 1.0, 2.0, 0.6614921524958179, 1.0, 2.0, 0.6614921524958179, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 1615608.166305807, 1615608.166305806, 294320.5502916753], 
processed observation next is [1.0, 0.6956521739130435, 0.8740740740740741, 0.17166666666666672, 1.0, 1.0, 0.5970144672569261, 1.0, 1.0, 0.5970144672569261, 0.0, 0.5, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.5770029165377882, 0.5770029165377879, 0.5660010582532218], 
reward next is 0.4340, 
noisyNet noise sample is [array([0.31248045], dtype=float32), -1.3826578]. 
=============================================
[2019-04-27 21:15:39,150] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-27 21:15:39,152] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 21:15:39,153] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 21:15:39,153] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:15:39,157] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 21:15:39,155] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 21:15:39,158] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 21:15:39,160] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:15:39,158] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:15:39,162] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:15:39,162] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:15:39,184] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run78
[2019-04-27 21:15:39,185] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run78
[2019-04-27 21:15:39,230] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run78
[2019-04-27 21:15:39,262] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run78
[2019-04-27 21:15:39,283] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run78
[2019-04-27 21:15:41,269] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.02789518], dtype=float32), -0.026040005]
[2019-04-27 21:15:41,271] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [25.88947856, 47.14260598666667, 1.0, 2.0, 0.333882418033655, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 421003.185784638, 421003.1857846376, 119824.1556248766]
[2019-04-27 21:15:41,271] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 21:15:41,274] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.2594402e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.0523323e-37], sampled 0.5469630412803367
[2019-04-27 21:16:23,656] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.02789518], dtype=float32), -0.026040005]
[2019-04-27 21:16:23,657] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [28.53333333333334, 60.0, 1.0, 2.0, 0.5395198122411642, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 632932.8110710856, 632932.8110710856, 148822.5439883122]
[2019-04-27 21:16:23,658] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 21:16:23,661] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [3.8082272e-34 1.0000000e+00 7.8250300e-37 1.2111140e-36 5.4842741e-31], sampled 0.9300817890745436
[2019-04-27 21:16:31,926] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02789518], dtype=float32), -0.026040005]
[2019-04-27 21:16:31,927] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [26.0, 94.0, 1.0, 2.0, 0.7271866314658577, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 828803.7991768759, 828803.7991768759, 181356.499783127]
[2019-04-27 21:16:31,927] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:16:31,930] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.0441989e-32 1.0000000e+00 5.7507916e-35 1.1299433e-34 7.4533180e-30], sampled 0.07119780641395335
[2019-04-27 21:17:03,079] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.02789518], dtype=float32), -0.026040005]
[2019-04-27 21:17:03,081] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [27.83333333333334, 70.0, 1.0, 2.0, 0.6058832795163984, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 694600.9767031603, 694600.9767031599, 159295.3607197337]
[2019-04-27 21:17:03,084] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 21:17:03,086] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [4.1053612e-35 1.0000000e+00 2.2317788e-38 0.0000000e+00 1.8941064e-32], sampled 0.5702765259989079
[2019-04-27 21:17:05,627] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.02789518], dtype=float32), -0.026040005]
[2019-04-27 21:17:05,628] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [21.0, 94.0, 1.0, 2.0, 0.4439903810017736, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 542020.3287748623, 542020.3287748623, 134780.4353811206]
[2019-04-27 21:17:05,629] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 21:17:05,632] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [2.6633712e-35 1.0000000e+00 1.2447144e-38 0.0000000e+00 8.2248884e-33], sampled 0.06583173965886746
[2019-04-27 21:17:16,934] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.02789518], dtype=float32), -0.026040005]
[2019-04-27 21:17:16,936] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [25.135914405, 70.56517066, 1.0, 2.0, 0.4623056209665543, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 556416.4699204377, 556416.4699204377, 137275.0358591047]
[2019-04-27 21:17:16,939] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 21:17:16,942] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.0291391e-34 1.0000000e+00 1.9333244e-37 3.8980105e-37 2.8235239e-32], sampled 0.06690925334172637
[2019-04-27 21:17:23,358] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8582.0197 2248850823.1596 553.0000
[2019-04-27 21:17:23,428] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8923.7317 2120762698.2138 427.0000
[2019-04-27 21:17:23,540] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8696.9122 2195747098.0441 568.0000
[2019-04-27 21:17:23,589] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8099.2280 2445610565.2976 741.0000
[2019-04-27 21:17:23,650] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8770.3581 2171099266.0742 492.0000
[2019-04-27 21:17:24,668] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1925000, evaluation results [1925000.0, 8099.227978261261, 2445610565.297572, 741.0, 8770.358066832798, 2171099266.0742073, 492.0, 8923.73168364284, 2120762698.2138011, 427.0, 8582.019747545304, 2248850823.159563, 553.0, 8696.912158563538, 2195747098.0441313, 568.0]
[2019-04-27 21:17:32,949] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.4065352e-32 1.0000000e+00 2.7174560e-35 1.3538444e-37 2.0536182e-25], sum to 1.0000
[2019-04-27 21:17:32,958] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2434
[2019-04-27 21:17:32,967] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.8, 35.5, 1.0, 2.0, 0.3315409882164033, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 420734.7684051611, 420734.7684051611, 119548.7886606814], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 689400.0000, 
sim time next is 690000.0000, 
raw observation next is [27.66666666666666, 35.66666666666667, 1.0, 2.0, 0.329818417077115, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 418860.1419106686, 418860.1419106686, 119328.9234228422], 
processed observation next is [1.0, 1.0, 0.5802469135802467, 0.3566666666666667, 1.0, 1.0, 0.20216478223466072, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1495929078252388, 0.1495929078252388, 0.22947869889008116], 
reward next is 0.7705, 
noisyNet noise sample is [array([0.48602197], dtype=float32), -0.49146926]. 
=============================================
[2019-04-27 21:17:32,987] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[82.564964]
 [82.55169 ]
 [82.497505]
 [82.41941 ]
 [82.19961 ]], R is [[82.52377319]
 [82.46863556]
 [82.41360474]
 [82.35866547]
 [82.30383301]].
[2019-04-27 21:17:41,364] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:17:41,372] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3734
[2019-04-27 21:17:41,378] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.2, 36.5, 1.0, 2.0, 0.4325094501183941, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 528740.574143094, 528740.5741430935, 133111.2850237224], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 840600.0000, 
sim time next is 841200.0000, 
raw observation next is [31.13333333333333, 36.33333333333334, 1.0, 2.0, 0.4297822265363352, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 526124.9856398124, 526124.9856398124, 132733.1549402879], 
processed observation next is [0.0, 0.7391304347826086, 0.7086419753086418, 0.36333333333333345, 1.0, 1.0, 0.32116931730516096, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1879017805856473, 0.1879017805856473, 0.25525606719286137], 
reward next is 0.7447, 
noisyNet noise sample is [array([0.4571344], dtype=float32), 0.083517686]. 
=============================================
[2019-04-27 21:17:43,021] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:17:43,033] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5222
[2019-04-27 21:17:43,043] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.93333333333334, 62.66666666666667, 1.0, 2.0, 0.3882338407358635, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 483727.6433094683, 483727.6433094678, 127030.5572707251], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 866400.0000, 
sim time next is 867000.0000, 
raw observation next is [23.76666666666667, 63.33333333333334, 1.0, 2.0, 0.3861053772508486, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 481392.2799525632, 481392.2799525632, 126741.4162570162], 
processed observation next is [0.0, 0.0, 0.43580246913580256, 0.6333333333333334, 1.0, 1.0, 0.26917306815577213, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.17192581426877257, 0.17192581426877257, 0.24373349280195422], 
reward next is 0.7563, 
noisyNet noise sample is [array([0.04958684], dtype=float32), -1.2601632]. 
=============================================
[2019-04-27 21:17:43,055] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[74.40079 ]
 [74.030075]
 [73.53254 ]
 [73.22216 ]
 [73.76393 ]], R is [[74.69345856]
 [74.70223236]
 [74.71033478]
 [74.71774292]
 [74.72451019]].
[2019-04-27 21:17:43,587] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:17:43,594] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6343
[2019-04-27 21:17:43,601] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.03333333333333, 62.66666666666666, 1.0, 2.0, 0.3845374173725907, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 478746.9214633277, 478746.9214633277, 126510.8721767014], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 900600.0000, 
sim time next is 901200.0000, 
raw observation next is [24.16666666666666, 62.33333333333334, 1.0, 2.0, 0.3871632582537007, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 481562.4204816634, 481562.4204816638, 126865.3075409971], 
processed observation next is [0.0, 0.43478260869565216, 0.45061728395061706, 0.6233333333333334, 1.0, 1.0, 0.2704324503020246, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1719865787434512, 0.17198657874345136, 0.2439717452711483], 
reward next is 0.7560, 
noisyNet noise sample is [array([-0.9747071], dtype=float32), -0.8914238]. 
=============================================
[2019-04-27 21:17:50,279] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.3314921e-27 1.0000000e+00 2.4517168e-32 1.2348487e-31 4.9361393e-24], sum to 1.0000
[2019-04-27 21:17:50,287] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6080
[2019-04-27 21:17:50,292] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.8, 57.5, 1.0, 2.0, 0.6925667522248117, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 872267.0560279211, 872267.0560279211, 177721.2334631999], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 984600.0000, 
sim time next is 985200.0000, 
raw observation next is [23.93333333333333, 57.33333333333334, 1.0, 2.0, 0.7590469896772275, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 954890.3840004185, 954890.384000418, 190954.9248929136], 
processed observation next is [1.0, 0.391304347826087, 0.4419753086419752, 0.5733333333333335, 1.0, 1.0, 0.7131511781871755, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.34103228000014946, 0.3410322800001493, 0.36722100940944924], 
reward next is 0.6328, 
noisyNet noise sample is [array([0.516891], dtype=float32), -1.2808459]. 
=============================================
[2019-04-27 21:18:09,766] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.0644886e-23 1.0000000e+00 4.4696104e-26 1.5514945e-26 1.7408675e-20], sum to 1.0000
[2019-04-27 21:18:09,774] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6125
[2019-04-27 21:18:09,780] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.76666666666667, 38.66666666666667, 1.0, 2.0, 0.8062020677581836, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260425321668, 1000700.980002375, 1000700.980002375, 200552.2376198172], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1340400.0000, 
sim time next is 1341000.0000, 
raw observation next is [28.95, 38.0, 1.0, 2.0, 0.7382217959600677, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156363, 920396.0770844731, 920396.0770844731, 186565.860866107], 
processed observation next is [1.0, 0.5217391304347826, 0.6277777777777778, 0.38, 1.0, 1.0, 0.6883592809048424, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288199667, 0.3287128846730261, 0.3287128846730261, 0.3587805016655904], 
reward next is 0.6412, 
noisyNet noise sample is [array([0.13164371], dtype=float32), 0.28346702]. 
=============================================
[2019-04-27 21:18:09,793] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[56.010605]
 [55.416153]
 [55.49346 ]
 [54.764458]
 [53.96911 ]], R is [[56.67232132]
 [56.10559845]
 [56.10669327]
 [55.54562759]
 [54.99017334]].
[2019-04-27 21:18:09,949] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3811073e-26 1.0000000e+00 1.5181803e-27 7.4413618e-29 1.2517241e-22], sum to 1.0000
[2019-04-27 21:18:09,962] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1424
[2019-04-27 21:18:09,968] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.13333333333333, 37.33333333333334, 1.0, 2.0, 0.7282532031935107, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156615, 908094.5584662941, 908094.5584662941, 184571.4625894768], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1341600.0000, 
sim time next is 1342200.0000, 
raw observation next is [29.31666666666667, 36.66666666666666, 1.0, 2.0, 0.7382802294885119, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 920575.1121416889, 920575.1121416889, 186579.9060578909], 
processed observation next is [1.0, 0.5217391304347826, 0.6413580246913582, 0.3666666666666666, 1.0, 1.0, 0.6884288446291809, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3287768257648889, 0.3287768257648889, 0.3588075116497902], 
reward next is 0.6412, 
noisyNet noise sample is [array([-0.01020715], dtype=float32), -0.79139644]. 
=============================================
[2019-04-27 21:18:15,896] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:18:15,905] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1838
[2019-04-27 21:18:15,909] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.73333333333333, 47.0, 1.0, 2.0, 0.3442633634765695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 434446.66292571, 434446.6629257095, 121182.0482586733], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1470000.0000, 
sim time next is 1470600.0000, 
raw observation next is [25.45, 48.0, 1.0, 2.0, 0.3434382786192593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 433708.5133276213, 433708.5133276218, 121077.3027865631], 
processed observation next is [0.0, 0.0, 0.4981481481481481, 0.48, 1.0, 1.0, 0.21837890311816582, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1548958976170076, 0.15489589761700778, 0.23284096689723674], 
reward next is 0.7672, 
noisyNet noise sample is [array([0.84264404], dtype=float32), 1.8761789]. 
=============================================
[2019-04-27 21:18:17,597] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-27 21:18:17,598] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 21:18:17,600] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:18:17,600] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 21:18:17,601] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 21:18:17,602] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:18:17,603] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:18:17,604] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 21:18:17,603] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 21:18:17,606] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:18:17,607] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:18:17,631] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run79
[2019-04-27 21:18:17,656] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run79
[2019-04-27 21:18:17,657] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run79
[2019-04-27 21:18:17,677] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run79
[2019-04-27 21:18:17,720] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run79
[2019-04-27 21:18:33,082] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.02916477], dtype=float32), -0.023932116]
[2019-04-27 21:18:33,083] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [24.0, 41.0, 1.0, 2.0, 0.2888416682344489, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 372566.6011798582, 372566.6011798582, 110690.0767056075]
[2019-04-27 21:18:33,083] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 21:18:33,087] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.21424881408256125
[2019-04-27 21:18:45,731] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.02916477], dtype=float32), -0.023932116]
[2019-04-27 21:18:45,734] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [19.16666666666667, 99.0, 1.0, 2.0, 0.3792088979899408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 471616.5156378556, 471616.5156378556, 125766.4558049902]
[2019-04-27 21:18:45,735] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 21:18:45,737] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2606370605590248
[2019-04-27 21:20:01,136] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8098.4495 2445508074.6415 746.0000
[2019-04-27 21:20:01,211] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8700.1130 2195220806.1349 572.0000
[2019-04-27 21:20:01,306] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8923.0130 2120630287.7220 430.0000
[2019-04-27 21:20:01,438] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8582.0590 2248830394.8409 553.0000
[2019-04-27 21:20:01,490] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8769.4422 2170768739.9937 493.0000
[2019-04-27 21:20:02,510] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 1950000, evaluation results [1950000.0, 8098.4494892610255, 2445508074.64147, 746.0, 8769.442185004564, 2170768739.993712, 493.0, 8923.012977399014, 2120630287.722009, 430.0, 8582.059032767682, 2248830394.840892, 553.0, 8700.112961499688, 2195220806.134913, 572.0]
[2019-04-27 21:20:02,817] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:20:02,828] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7048
[2019-04-27 21:20:02,834] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.71666666666667, 69.0, 1.0, 2.0, 0.3414860457589032, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 431632.8994712498, 431632.8994712498, 120825.9742070084], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1483800.0000, 
sim time next is 1484400.0000, 
raw observation next is [21.63333333333333, 70.0, 1.0, 2.0, 0.3437196635316701, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 434153.1812973912, 434153.1812973908, 121115.3740492681], 
processed observation next is [0.0, 0.17391304347826086, 0.35679012345678995, 0.7, 1.0, 1.0, 0.21871388515675014, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.15505470760621115, 0.15505470760621098, 0.2329141808639771], 
reward next is 0.7671, 
noisyNet noise sample is [array([-0.28551745], dtype=float32), -2.8811426]. 
=============================================
[2019-04-27 21:20:12,225] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.9316938e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 21:20:12,234] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5794
[2019-04-27 21:20:12,239] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.2, 81.0, 1.0, 2.0, 0.330535859007274, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 421382.3717020401, 421382.3717020401, 119430.3595543057], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1656000.0000, 
sim time next is 1656600.0000, 
raw observation next is [18.98333333333333, 82.5, 1.0, 2.0, 0.3467417275070088, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 442165.939018779, 442165.939018779, 121544.8955610541], 
processed observation next is [1.0, 0.17391304347826086, 0.25864197530864186, 0.825, 1.0, 1.0, 0.2223115803654867, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.15791640679242108, 0.15791640679242108, 0.23374018377125788], 
reward next is 0.7663, 
noisyNet noise sample is [array([-0.32787457], dtype=float32), 1.5596622]. 
=============================================
[2019-04-27 21:20:21,316] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.507534e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-27 21:20:21,324] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1026
[2019-04-27 21:20:21,328] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.26666666666667, 91.66666666666666, 1.0, 2.0, 0.3154428391806077, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 400792.4425301905, 400792.4425301905, 117496.4483165857], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1820400.0000, 
sim time next is 1821000.0000, 
raw observation next is [18.23333333333333, 91.83333333333333, 1.0, 2.0, 0.3144572908683863, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 399603.6666947314, 399603.6666947319, 117372.4987119776], 
processed observation next is [1.0, 0.043478260869565216, 0.2308641975308641, 0.9183333333333333, 1.0, 1.0, 0.18387772722426943, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.14271559524811836, 0.14271559524811855, 0.22571634367688], 
reward next is 0.7743, 
noisyNet noise sample is [array([0.5965945], dtype=float32), -1.7114667]. 
=============================================
[2019-04-27 21:20:21,345] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[65.767685]
 [65.69659 ]
 [65.58849 ]
 [65.293755]
 [65.01809 ]], R is [[65.91270447]
 [66.02762604]
 [66.14109802]
 [66.25312805]
 [66.36377716]].
[2019-04-27 21:20:30,697] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:20:30,707] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6883
[2019-04-27 21:20:30,712] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.5, 91.0, 1.0, 2.0, 0.3629255962178805, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 454934.7871474361, 454934.7871474356, 123620.8794279519], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1987200.0000, 
sim time next is 1987800.0000, 
raw observation next is [19.5, 91.00000000000001, 1.0, 2.0, 0.3634319201879735, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 455564.3234768051, 455564.3234768047, 123688.9470067443], 
processed observation next is [0.0, 0.0, 0.2777777777777778, 0.9100000000000001, 1.0, 1.0, 0.2421808573666351, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.16270154409885898, 0.1627015440988588, 0.23786335962835442], 
reward next is 0.7621, 
noisyNet noise sample is [array([0.15777051], dtype=float32), -0.15525292]. 
=============================================
[2019-04-27 21:20:32,845] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:20:32,856] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4083
[2019-04-27 21:20:32,863] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.8, 84.0, 1.0, 2.0, 0.3722370767566662, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 464033.1602484185, 464033.1602484185, 124835.5308662179], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2012400.0000, 
sim time next is 2013000.0000, 
raw observation next is [20.91666666666667, 83.16666666666667, 1.0, 2.0, 0.373120185447367, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 465047.9171999443, 465047.9171999443, 124954.0982908087], 
processed observation next is [0.0, 0.30434782608695654, 0.3302469135802471, 0.8316666666666667, 1.0, 1.0, 0.25371450648496074, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16608854185712296, 0.16608854185712296, 0.24029634286693982], 
reward next is 0.7597, 
noisyNet noise sample is [array([0.04144867], dtype=float32), 1.8594791]. 
=============================================
[2019-04-27 21:20:32,883] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[71.15455 ]
 [71.12242 ]
 [71.14344 ]
 [71.16826 ]
 [71.189606]], R is [[71.18942261]
 [71.2374649 ]
 [71.28517914]
 [71.33267212]
 [71.38007355]].
[2019-04-27 21:20:36,332] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.9676383e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.9104025e-34], sum to 1.0000
[2019-04-27 21:20:36,344] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8431
[2019-04-27 21:20:36,352] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2579092.683166934 W.
[2019-04-27 21:20:36,359] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 0.8802773532618485, 1.0, 2.0, 0.7535033386073589, 1.0, 1.0, 0.9977734948820727, 6.911199999999999, 6.9112, 122.0903777698222, 2579092.683166934, 2579092.683166934, 481238.531571033], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2808600.0000, 
sim time next is 2809200.0000, 
raw observation next is [32.0, 63.00000000000001, 1.0, 2.0, 0.6075216122726245, 1.0, 2.0, 0.6075216122726245, 1.0, 2.0, 0.9671941923356255, 6.9112, 6.9112, 121.94756008, 2078847.971387434, 2078847.971387434, 399488.9860349956], 
processed observation next is [1.0, 0.5217391304347826, 0.7407407407407407, 0.6300000000000001, 1.0, 1.0, 0.5327638241340767, 1.0, 1.0, 0.5327638241340767, 1.0, 1.0, 0.958992740419532, 0.0, 0.0, 0.8096049824067558, 0.7424457040669408, 0.7424457040669408, 0.7682480500672992], 
reward next is 0.2318, 
noisyNet noise sample is [array([1.2570161], dtype=float32), -1.4992492]. 
=============================================
[2019-04-27 21:20:36,608] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:20:36,622] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3999
[2019-04-27 21:20:36,627] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.2, 66.0, 1.0, 2.0, 0.5818998062894046, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 672431.8460157218, 672431.8460157213, 155440.6988998796], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2116800.0000, 
sim time next is 2117400.0000, 
raw observation next is [28.38333333333333, 65.0, 1.0, 2.0, 0.582057457202338, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 672539.5021959675, 672539.5021959675, 155463.9663015378], 
processed observation next is [0.0, 0.5217391304347826, 0.60679012345679, 0.65, 1.0, 1.0, 0.5024493538123072, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.24019267935570268, 0.24019267935570268, 0.29896916596449574], 
reward next is 0.7010, 
noisyNet noise sample is [array([-0.8091296], dtype=float32), -1.2313776]. 
=============================================
[2019-04-27 21:20:36,769] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:20:36,777] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8208
[2019-04-27 21:20:36,784] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.46666666666667, 91.33333333333334, 1.0, 2.0, 0.4378600167830093, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 533580.449489345, 533580.449489345, 133847.5455396984], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2096400.0000, 
sim time next is 2097000.0000, 
raw observation next is [21.6, 91.0, 1.0, 2.0, 0.4425016102615175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 538439.472315028, 538439.472315028, 134508.4061835143], 
processed observation next is [0.0, 0.2608695652173913, 0.3555555555555556, 0.91, 1.0, 1.0, 0.3363114407875209, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.19229981154108142, 0.19229981154108142, 0.2586700118913737], 
reward next is 0.7413, 
noisyNet noise sample is [array([0.49024934], dtype=float32), 0.63664716]. 
=============================================
[2019-04-27 21:20:36,800] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[73.93226 ]
 [73.95094 ]
 [73.956116]
 [73.91822 ]
 [73.93957 ]], R is [[73.90180206]
 [73.90538788]
 [73.9099884 ]
 [73.91546631]
 [73.92149353]].
[2019-04-27 21:20:37,339] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:20:37,346] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5758
[2019-04-27 21:20:37,352] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.85, 93.5, 1.0, 2.0, 0.4241409264303296, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 519658.4891345546, 519658.4891345546, 131925.744499537], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2093400.0000, 
sim time next is 2094000.0000, 
raw observation next is [20.96666666666667, 93.0, 1.0, 2.0, 0.426147419611376, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 521657.8986743604, 521657.8986743604, 132204.1054493735], 
processed observation next is [0.0, 0.21739130434782608, 0.3320987654320988, 0.93, 1.0, 1.0, 0.31684216620401906, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.18630639238370014, 0.18630639238370014, 0.2542386643257183], 
reward next is 0.7458, 
noisyNet noise sample is [array([1.3944188], dtype=float32), 1.4664711]. 
=============================================
[2019-04-27 21:20:37,370] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[75.4127  ]
 [75.42829 ]
 [75.433556]
 [75.41452 ]
 [75.32772 ]], R is [[75.38575745]
 [75.37819672]
 [75.3711853 ]
 [75.36466217]
 [75.3584137 ]].
[2019-04-27 21:20:41,675] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:20:41,684] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8338
[2019-04-27 21:20:41,693] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1322464.743181536 W.
[2019-04-27 21:20:41,697] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.8, 91.0, 1.0, 2.0, 0.3866493267559149, 1.0, 2.0, 0.3866493267559149, 1.0, 1.0, 0.6155583204848755, 6.911199999999999, 6.9112, 121.94756008, 1322464.743181536, 1322464.743181536, 291283.1993591462], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2201400.0000, 
sim time next is 2202000.0000, 
raw observation next is [24.96666666666667, 90.33333333333333, 1.0, 2.0, 0.4156879784480479, 1.0, 2.0, 0.4156879784480479, 1.0, 2.0, 0.6617887997016125, 6.9112, 6.9112, 121.94756008, 1421878.425320246, 1421878.425320246, 304022.5162320596], 
processed observation next is [1.0, 0.4782608695652174, 0.48024691358024696, 0.9033333333333333, 1.0, 1.0, 0.30439045053339037, 1.0, 1.0, 0.30439045053339037, 1.0, 1.0, 0.5772359996270157, 0.0, 0.0, 0.8096049824067558, 0.5078137233286593, 0.5078137233286593, 0.5846586850616531], 
reward next is 0.4153, 
noisyNet noise sample is [array([-0.4541243], dtype=float32), -0.82025576]. 
=============================================
[2019-04-27 21:20:41,715] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[59.003952]
 [58.92686 ]
 [58.75835 ]
 [58.600132]
 [58.659992]], R is [[59.56074524]
 [58.96513748]
 [58.86545944]
 [58.70211792]
 [58.51708221]].
[2019-04-27 21:20:46,923] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.5851492e-34 1.0000000e+00 6.3143643e-37 7.1595663e-36 1.4907708e-35], sum to 1.0000
[2019-04-27 21:20:46,928] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9062
[2019-04-27 21:20:46,935] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.4, 96.0, 1.0, 2.0, 0.4213850374793857, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 517407.3622732415, 517407.3622732415, 131557.3662005997], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2269800.0000, 
sim time next is 2270400.0000, 
raw observation next is [20.4, 96.0, 1.0, 2.0, 0.4227916523796962, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 519134.5993493839, 519134.5993493839, 131760.6692666854], 
processed observation next is [1.0, 0.2608695652173913, 0.31111111111111106, 0.96, 1.0, 1.0, 0.312847205213924, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1854052140533514, 0.1854052140533514, 0.25338590243593345], 
reward next is 0.7466, 
noisyNet noise sample is [array([1.4839953], dtype=float32), 1.1083504]. 
=============================================
[2019-04-27 21:20:50,283] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8979619e-24 1.0000000e+00 6.4963022e-26 1.6657556e-26 1.1576641e-15], sum to 1.0000
[2019-04-27 21:20:50,290] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6443
[2019-04-27 21:20:50,297] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.33333333333334, 92.5, 1.0, 2.0, 0.7299244345148564, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 840185.3155779114, 840185.3155779114, 182303.4484185748], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3052200.0000, 
sim time next is 3052800.0000, 
raw observation next is [24.6, 91.0, 1.0, 2.0, 0.7772663656961858, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 893304.9950060829, 893304.9950060825, 191668.018604476], 
processed observation next is [1.0, 0.34782608695652173, 0.46666666666666673, 0.91, 1.0, 1.0, 0.7348409115430783, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.3190374982164582, 0.31903749821645805, 0.3685923434701462], 
reward next is 0.6314, 
noisyNet noise sample is [array([-0.6938318], dtype=float32), 0.058424648]. 
=============================================
[2019-04-27 21:20:52,775] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.5844895e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.2050104e-29], sum to 1.0000
[2019-04-27 21:20:52,786] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0316
[2019-04-27 21:20:52,789] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.53333333333333, 40.5, 1.0, 2.0, 0.4023095756333384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 495646.9097925081, 495646.9097925081, 128877.7524951843], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2398200.0000, 
sim time next is 2398800.0000, 
raw observation next is [29.36666666666667, 41.0, 1.0, 2.0, 0.4043665546833753, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 498394.2374046114, 498394.2374046114, 129173.7418066512], 
processed observation next is [1.0, 0.782608695652174, 0.64320987654321, 0.41, 1.0, 1.0, 0.2909125650992563, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.17799794193021837, 0.17799794193021837, 0.2484110419358677], 
reward next is 0.7516, 
noisyNet noise sample is [array([2.8037133], dtype=float32), -0.21598247]. 
=============================================
[2019-04-27 21:20:53,923] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.17833284e-30 1.00000000e+00 1.13374395e-32 1.18540612e-31
 4.78791700e-26], sum to 1.0000
[2019-04-27 21:20:53,932] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4277
[2019-04-27 21:20:53,939] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.5509425922943642, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 645424.7445407242, 645424.7445407242, 150659.8103019774], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2628000.0000, 
sim time next is 2628600.0000, 
raw observation next is [23.91666666666667, 88.33333333333334, 1.0, 2.0, 0.54967429161408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 644767.6823800015, 644767.6823800015, 150485.1496465795], 
processed observation next is [0.0, 0.43478260869565216, 0.4413580246913582, 0.8833333333333334, 1.0, 1.0, 0.46389796620723805, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.23027417227857197, 0.23027417227857197, 0.2893945185511144], 
reward next is 0.7106, 
noisyNet noise sample is [array([0.32906717], dtype=float32), -0.9223254]. 
=============================================
[2019-04-27 21:20:54,028] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1130472e-30 1.0000000e+00 9.1736365e-33 8.5336012e-32 4.9472567e-26], sum to 1.0000
[2019-04-27 21:20:54,032] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3549
[2019-04-27 21:20:54,038] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.66666666666666, 86.33333333333334, 1.0, 2.0, 0.5287274879437627, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 626340.9963300426, 626340.9963300426, 147317.5950603888], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2630400.0000, 
sim time next is 2631000.0000, 
raw observation next is [23.58333333333334, 85.66666666666667, 1.0, 2.0, 0.5203877377128171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 618440.391247483, 618440.391247483, 146050.6400991032], 
processed observation next is [0.0, 0.43478260869565216, 0.4290123456790126, 0.8566666666666667, 1.0, 1.0, 0.429033021086687, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2208715683026725, 0.2208715683026725, 0.2808666155751985], 
reward next is 0.7191, 
noisyNet noise sample is [array([0.32906717], dtype=float32), -0.9223254]. 
=============================================
[2019-04-27 21:20:54,071] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[67.8471  ]
 [67.743   ]
 [67.677895]
 [67.63785 ]
 [67.64818 ]], R is [[68.01594543]
 [68.0524826 ]
 [68.08630371]
 [68.1177063 ]
 [68.14713287]].
[2019-04-27 21:20:55,298] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-27 21:20:55,300] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 21:20:55,301] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:20:55,301] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 21:20:55,302] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 21:20:55,303] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 21:20:55,304] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:20:55,305] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 21:20:55,303] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:20:55,306] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:20:55,310] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:20:55,329] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run80
[2019-04-27 21:20:55,353] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run80
[2019-04-27 21:20:55,354] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run80
[2019-04-27 21:20:55,406] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run80
[2019-04-27 21:20:55,432] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run80
[2019-04-27 21:21:13,998] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02764368], dtype=float32), -0.026317267]
[2019-04-27 21:21:14,001] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [26.3, 55.0, 1.0, 2.0, 0.4873150479384163, 0.0, 2.0, 0.0, 1.0, 1.0, 0.7935440317607613, 6.911200000000001, 6.9112, 121.9257051979274, 1184875.967859861, 1184875.96785986, 250112.0691127044]
[2019-04-27 21:21:14,003] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:21:14,005] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [9.3799885e-23 1.0000000e+00 4.0472596e-24 2.4320523e-25 3.0263118e-13], sampled 0.3898232538302401
[2019-04-27 21:21:33,413] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02764368], dtype=float32), -0.026317267]
[2019-04-27 21:21:33,414] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [30.76088166, 87.22319194, 1.0, 2.0, 0.9846645956919422, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.050177003038779, 6.9112, 121.9253496677682, 1193697.083590895, 1122528.840887512, 237069.9793181412]
[2019-04-27 21:21:33,415] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 21:21:33,418] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [3.4453803e-29 1.0000000e+00 2.9503392e-31 4.8561453e-32 1.7351971e-22], sampled 0.5106619156937656
[2019-04-27 21:21:37,731] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02764368], dtype=float32), -0.026317267]
[2019-04-27 21:21:37,734] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [25.75, 89.0, 1.0, 2.0, 0.6709496335375297, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 764676.2625457158, 764676.2625457158, 170716.1712893358]
[2019-04-27 21:21:37,736] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:21:37,739] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [3.0879417e-33 1.0000000e+00 3.5467037e-36 1.2517194e-37 1.6779678e-27], sampled 0.9607546237951616
[2019-04-27 21:21:52,855] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02764368], dtype=float32), -0.026317267]
[2019-04-27 21:21:52,857] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [25.73333333333333, 59.5, 1.0, 2.0, 0.4299740325188032, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 527622.3514421702, 527622.3514421702, 132795.4385943961]
[2019-04-27 21:21:52,858] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:21:52,859] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [6.2865392e-35 1.0000000e+00 5.0634261e-38 3.2192461e-38 2.3262106e-33], sampled 0.7774822047848521
[2019-04-27 21:22:01,483] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.02764368], dtype=float32), -0.026317267]
[2019-04-27 21:22:01,484] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [24.23649897, 81.27347764, 1.0, 2.0, 0.4929616542248736, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 586888.7154511773, 586888.7154511773, 141754.191046193]
[2019-04-27 21:22:01,485] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 21:22:01,487] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.5441380e-33 1.0000000e+00 1.9156923e-35 1.1083117e-35 2.8188684e-29], sampled 0.4525327356648351
[2019-04-27 21:22:08,777] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02764368], dtype=float32), -0.026317267]
[2019-04-27 21:22:08,778] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [25.86666666666667, 90.33333333333334, 1.0, 2.0, 0.6872504383905208, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 783263.6595596975, 783263.6595596975, 173744.0494788332]
[2019-04-27 21:22:08,779] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:22:08,782] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.3433422e-33 1.0000000e+00 2.0883907e-36 5.0466879e-38 4.0636209e-27], sampled 0.542020834069921
[2019-04-27 21:22:27,639] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.02764368], dtype=float32), -0.026317267]
[2019-04-27 21:22:27,640] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [29.0, 70.66666666666667, 1.0, 2.0, 0.7547567125591783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 860244.1618881093, 860244.1618881088, 186767.7948851134]
[2019-04-27 21:22:27,640] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 21:22:27,643] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.1001984e-28 1.0000000e+00 1.8152061e-30 6.2394384e-31 1.2533875e-21], sampled 0.7455807696289957
[2019-04-27 21:22:38,545] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.02764368], dtype=float32), -0.026317267]
[2019-04-27 21:22:38,546] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [19.73333333333333, 88.0, 1.0, 2.0, 0.3467847497588579, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 435148.9260940978, 435148.9260940978, 121479.1955714683]
[2019-04-27 21:22:38,547] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 21:22:38,551] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [2.4962475e-35 1.0000000e+00 1.7966751e-37 1.4792243e-37 2.8984981e-32], sampled 0.9434708523166347
[2019-04-27 21:22:39,604] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8581.4901 2248842826.9369 553.0000
[2019-04-27 21:22:40,102] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8923.7430 2120926957.9541 429.0000
[2019-04-27 21:22:40,107] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8696.2611 2195954547.0608 565.0000
[2019-04-27 21:22:40,180] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8771.7465 2171161827.3120 491.0000
[2019-04-27 21:22:40,198] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8102.6163 2445599954.2360 739.0000
[2019-04-27 21:22:41,217] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1975000, evaluation results [1975000.0, 8102.616340258612, 2445599954.2360272, 739.0, 8771.74649650807, 2171161827.3120356, 491.0, 8923.742992577647, 2120926957.9541352, 429.0, 8581.490054714895, 2248842826.93692, 553.0, 8696.261130041059, 2195954547.060849, 565.0]
[2019-04-27 21:22:44,858] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0640070e-23 1.0000000e+00 1.9012291e-25 2.8858415e-26 2.6714422e-11], sum to 1.0000
[2019-04-27 21:22:44,868] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2561
[2019-04-27 21:22:44,877] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.33333333333334, 49.66666666666667, 1.0, 2.0, 0.3763826136360297, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 469340.1937705354, 469340.1937705354, 125403.9215805897], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2506800.0000, 
sim time next is 2507400.0000, 
raw observation next is [26.25, 50.0, 1.0, 2.0, 0.3771551060960203, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 470376.8291250364, 470376.829125036, 125511.0956400542], 
processed observation next is [1.0, 0.0, 0.5277777777777778, 0.5, 1.0, 1.0, 0.25851798344764326, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.167991724687513, 0.16799172468751286, 0.24136749161548884], 
reward next is 0.7586, 
noisyNet noise sample is [array([-0.43893617], dtype=float32), 0.7000731]. 
=============================================
[2019-04-27 21:22:55,342] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.4507652e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.1733728e-36], sum to 1.0000
[2019-04-27 21:22:55,352] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9085
[2019-04-27 21:22:55,356] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.66666666666667, 96.0, 1.0, 2.0, 0.5394742976783091, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 636564.8291496973, 636564.8291496973, 148967.0205802352], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2690400.0000, 
sim time next is 2691000.0000, 
raw observation next is [22.5, 97.0, 1.0, 2.0, 0.5383783991913562, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 635819.5482339564, 635819.5482339564, 148810.0212394636], 
processed observation next is [0.0, 0.13043478260869565, 0.3888888888888889, 0.97, 1.0, 1.0, 0.450450475227805, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.22707841008355586, 0.22707841008355586, 0.2861731177681992], 
reward next is 0.7138, 
noisyNet noise sample is [array([1.8474354], dtype=float32), -0.19262716]. 
=============================================
[2019-04-27 21:22:55,370] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[73.74656]
 [73.71784]
 [73.62896]
 [73.59069]
 [73.50253]], R is [[73.85830688]
 [73.83325195]
 [73.80834198]
 [73.78396606]
 [73.7612915 ]].
[2019-04-27 21:22:55,952] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.1613793e-33 1.0000000e+00 0.0000000e+00 1.0603689e-37 4.4332050e-33], sum to 1.0000
[2019-04-27 21:22:55,960] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7207
[2019-04-27 21:22:55,965] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 96.0, 1.0, 2.0, 0.5628325230445325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 659397.9430266212, 659397.9430266212, 152636.7268356235], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2680800.0000, 
sim time next is 2681400.0000, 
raw observation next is [23.0, 95.0, 1.0, 2.0, 0.5562835667111733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 653352.4441937996, 653352.4441937996, 151614.2824598191], 
processed observation next is [0.0, 0.0, 0.4074074074074074, 0.95, 1.0, 1.0, 0.47176615084663487, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.23334015864064273, 0.23334015864064273, 0.2915659278073444], 
reward next is 0.7084, 
noisyNet noise sample is [array([0.55141735], dtype=float32), 0.37237525]. 
=============================================
[2019-04-27 21:22:56,389] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.427382e-37 1.000000e+00 0.000000e+00 0.000000e+00 3.353292e-36], sum to 1.0000
[2019-04-27 21:22:56,397] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3322
[2019-04-27 21:22:56,400] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.26666666666667, 96.66666666666666, 1.0, 2.0, 0.5729390715749271, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 667192.2198485465, 667192.2198485465, 154159.2903250666], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2677200.0000, 
sim time next is 2677800.0000, 
raw observation next is [23.13333333333333, 98.33333333333334, 1.0, 2.0, 0.5767360261515798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 670596.7307071849, 670596.7307071849, 154755.3264859008], 
processed observation next is [0.0, 1.0, 0.41234567901234553, 0.9833333333333334, 1.0, 1.0, 0.4961143168471188, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.23949883239542316, 0.23949883239542316, 0.29760639708827075], 
reward next is 0.7024, 
noisyNet noise sample is [array([-1.2900612], dtype=float32), 1.4601415]. 
=============================================
[2019-04-27 21:22:57,582] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.5145677e-21 1.0000000e+00 9.2402036e-23 1.2264452e-24 3.4159090e-14], sum to 1.0000
[2019-04-27 21:22:57,593] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4313
[2019-04-27 21:22:57,603] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2106326.463958512 W.
[2019-04-27 21:22:57,609] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.83333333333334, 63.16666666666666, 1.0, 2.0, 0.9233134926089477, 1.0, 2.0, 0.9233134926089477, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 2106326.463958512, 2106326.463958512, 397344.5945026441], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3426600.0000, 
sim time next is 3427200.0000, 
raw observation next is [30.6, 64.0, 1.0, 2.0, 0.6071594325266282, 1.0, 2.0, 0.6071594325266282, 1.0, 1.0, 0.9666175903846285, 6.911199999999999, 6.9112, 121.94756008, 2077607.205036619, 2077607.205036619, 399290.2207365354], 
processed observation next is [1.0, 0.6956521739130435, 0.688888888888889, 0.64, 1.0, 1.0, 0.5323326577697954, 1.0, 1.0, 0.5323326577697954, 1.0, 0.5, 0.9582719879807855, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.742002573227364, 0.742002573227364, 0.7678658091087219], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.9434462], dtype=float32), -1.3626218]. 
=============================================
[2019-04-27 21:22:59,411] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1203403e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.1234569e-35], sum to 1.0000
[2019-04-27 21:22:59,420] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8396
[2019-04-27 21:22:59,426] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.83333333333334, 70.33333333333333, 1.0, 2.0, 0.6759704613931857, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000002, 6.9112, 121.9260426156618, 770401.3379961839, 770401.3379961831, 171644.2548960155], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2749800.0000, 
sim time next is 2750400.0000, 
raw observation next is [28.6, 72.0, 1.0, 2.0, 0.6811770963657765, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 776338.3280064305, 776338.3280064305, 172610.7052965619], 
processed observation next is [0.0, 0.8695652173913043, 0.6148148148148148, 0.72, 1.0, 1.0, 0.620448924244972, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2772636885737252, 0.2772636885737252, 0.3319436640318498], 
reward next is 0.6681, 
noisyNet noise sample is [array([1.2457778], dtype=float32), -1.0413827]. 
=============================================
[2019-04-27 21:23:07,393] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.0272490e-11 4.5474970e-01 7.9701402e-12 3.9562083e-11 5.4525030e-01], sum to 1.0000
[2019-04-27 21:23:07,400] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5432
[2019-04-27 21:23:07,409] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3785973218809336, 1.0, 2.0, 0.3785973218809336, 1.0, 2.0, 0.602739266488414, 6.911199999999999, 6.9112, 121.94756008, 1294901.028296149, 1294901.028296149, 287833.5530364023], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2884800.0000, 
sim time next is 2885400.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3938224035196189, 1.0, 2.0, 0.3938224035196189, 1.0, 2.0, 0.6269780923034933, 6.9112, 6.9112, 121.94756008, 1347020.536622916, 1347020.536622916, 294386.5794668831], 
processed observation next is [1.0, 0.391304347826087, 0.4074074074074074, 0.94, 1.0, 1.0, 0.2783600041900225, 1.0, 1.0, 0.2783600041900225, 1.0, 1.0, 0.5337226153793666, 0.0, 0.0, 0.8096049824067558, 0.4810787630796129, 0.4810787630796129, 0.5661280374363137], 
reward next is 0.4339, 
noisyNet noise sample is [array([-0.21526416], dtype=float32), -0.79182994]. 
=============================================
[2019-04-27 21:23:10,695] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.5419871e-20 1.0000000e+00 1.6824322e-23 3.5918117e-21 5.5112925e-10], sum to 1.0000
[2019-04-27 21:23:10,707] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1697
[2019-04-27 21:23:10,712] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 94.0, 1.0, 2.0, 0.7563079578788232, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000002, 6.9112, 121.9260426156618, 862013.2089937247, 862013.2089937237, 187074.1400492771], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2950800.0000, 
sim time next is 2951400.0000, 
raw observation next is [25.0, 94.0, 1.0, 2.0, 0.7465314062799003, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 850864.0540189222, 850864.0540189222, 185136.5179772108], 
processed observation next is [1.0, 0.13043478260869565, 0.48148148148148145, 0.94, 1.0, 1.0, 0.6982516741427385, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3038800192924722, 0.3038800192924722, 0.35603176534079], 
reward next is 0.6440, 
noisyNet noise sample is [array([-2.1639628], dtype=float32), 0.027937563]. 
=============================================
[2019-04-27 21:23:12,614] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0992542e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 21:23:12,624] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4215
[2019-04-27 21:23:12,629] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.66666666666667, 84.83333333333334, 1.0, 2.0, 0.5215520193988897, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 620471.3096483819, 620471.3096483814, 146262.3809168734], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3208200.0000, 
sim time next is 3208800.0000, 
raw observation next is [23.33333333333333, 86.66666666666667, 1.0, 2.0, 0.5159053532112415, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 614708.8968826829, 614708.8968826829, 145393.2360475748], 
processed observation next is [0.0, 0.13043478260869565, 0.4197530864197529, 0.8666666666666667, 1.0, 1.0, 0.4236968490610018, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21953889174381533, 0.21953889174381533, 0.27960237701456697], 
reward next is 0.7204, 
noisyNet noise sample is [array([-0.3096373], dtype=float32), -0.65244937]. 
=============================================
[2019-04-27 21:23:16,524] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.6817736e-22 1.0000000e+00 2.7876108e-22 2.9755533e-25 1.9898444e-16], sum to 1.0000
[2019-04-27 21:23:16,532] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5693
[2019-04-27 21:23:16,536] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.6, 96.33333333333333, 1.0, 2.0, 0.5398340212596153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 635463.370786024, 635463.3707860236, 148962.9332239446], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3044400.0000, 
sim time next is 3045000.0000, 
raw observation next is [21.8, 98.16666666666667, 1.0, 2.0, 0.5101875044030053, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 607290.5358897154, 607290.5358897154, 144458.6980109983], 
processed observation next is [1.0, 0.21739130434782608, 0.362962962962963, 0.9816666666666667, 1.0, 1.0, 0.4168898861940539, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21688947710346979, 0.21688947710346979, 0.27780518848268904], 
reward next is 0.7222, 
noisyNet noise sample is [array([-0.5193008], dtype=float32), 0.34572145]. 
=============================================
[2019-04-27 21:23:16,554] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[54.108707]
 [53.99522 ]
 [53.62755 ]
 [53.50783 ]
 [53.2802  ]], R is [[54.45669937]
 [54.62566376]
 [54.78237534]
 [54.9239502 ]
 [55.03650665]].
[2019-04-27 21:23:22,417] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.5014966e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 21:23:22,423] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8653
[2019-04-27 21:23:22,432] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1344145.092684093 W.
[2019-04-27 21:23:22,436] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.05, 35.16666666666666, 1.0, 2.0, 0.5797213082459938, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9245871278690375, 6.911199999999999, 6.9112, 121.9260426156618, 1344145.092684093, 1344145.092684093, 285690.1368016346], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3157800.0000, 
sim time next is 3158400.0000, 
raw observation next is [34.1, 34.33333333333334, 1.0, 2.0, 0.7080070651578398, 1.0, 1.0, 0.7080070651578398, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1638968.704733592, 1638968.704733592, 308426.0390796667], 
processed observation next is [1.0, 0.5652173913043478, 0.8185185185185185, 0.34333333333333343, 1.0, 1.0, 0.6523893632831427, 1.0, 0.5, 0.6523893632831427, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.5853459659762829, 0.5853459659762829, 0.5931269982301283], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.04301], dtype=float32), -1.2706883]. 
=============================================
[2019-04-27 21:23:26,058] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.2381311e-30 1.0000000e+00 3.8125172e-33 5.4636736e-35 3.1568462e-24], sum to 1.0000
[2019-04-27 21:23:26,070] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6844
[2019-04-27 21:23:26,076] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.33333333333334, 88.0, 1.0, 2.0, 0.6420333975844147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 732906.4918861141, 732906.4918861141, 165519.248136992], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3453600.0000, 
sim time next is 3454200.0000, 
raw observation next is [25.25, 89.5, 1.0, 2.0, 0.6470360822556298, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 737409.0582130648, 737409.0582130648, 166357.5927797963], 
processed observation next is [1.0, 1.0, 0.49074074074074076, 0.895, 1.0, 1.0, 0.5798048598281307, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2633603779332374, 0.2633603779332374, 0.31991844765345445], 
reward next is 0.6801, 
noisyNet noise sample is [array([-0.7091827], dtype=float32), -0.74757004]. 
=============================================
[2019-04-27 21:23:26,691] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3552364e-34 1.0000000e+00 7.0149231e-38 0.0000000e+00 3.2349672e-30], sum to 1.0000
[2019-04-27 21:23:26,699] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5755
[2019-04-27 21:23:26,705] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.5, 59.0, 1.0, 2.0, 0.6637572086087636, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 756475.0649514459, 756475.0649514459, 169394.0642132334], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3792600.0000, 
sim time next is 3793200.0000, 
raw observation next is [30.33333333333334, 59.0, 1.0, 2.0, 0.6533285818489063, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 744583.9282880032, 744583.9282880032, 167494.3070759128], 
processed observation next is [1.0, 0.9130434782608695, 0.6790123456790126, 0.59, 1.0, 1.0, 0.5872959307725075, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2659228315314297, 0.2659228315314297, 0.3221044366844477], 
reward next is 0.6779, 
noisyNet noise sample is [array([-0.88035744], dtype=float32), -1.2376322]. 
=============================================
[2019-04-27 21:23:34,118] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-27 21:23:34,119] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 21:23:34,120] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 21:23:34,120] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:23:34,121] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 21:23:34,121] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:23:34,123] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 21:23:34,125] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 21:23:34,125] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:23:34,128] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:23:34,126] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:23:34,553] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run81
[2019-04-27 21:23:34,553] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run81
[2019-04-27 21:23:34,564] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run81
[2019-04-27 21:23:34,588] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run81
[2019-04-27 21:23:34,606] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run81
[2019-04-27 21:23:37,315] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03002717], dtype=float32), -0.029667703]
[2019-04-27 21:23:37,316] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [28.33333333333334, 26.5, 1.0, 2.0, 0.3279131709514931, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 422226.8571279844, 422226.8571279844, 119085.8078776454]
[2019-04-27 21:23:37,317] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 21:23:37,320] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8584119691243338
[2019-04-27 21:24:40,162] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03002717], dtype=float32), -0.029667703]
[2019-04-27 21:24:40,163] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [25.92101873, 81.57764395, 1.0, 2.0, 0.8343078503582434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156612, 955290.7422257675, 955290.7422257675, 203356.4801114109]
[2019-04-27 21:24:40,164] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 21:24:40,166] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [2.25989541e-31 1.00000000e+00 2.99025238e-34 5.02358933e-35
 1.00431324e-28], sampled 0.9974395752588087
[2019-04-27 21:25:02,685] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03002717], dtype=float32), -0.029667703]
[2019-04-27 21:25:02,685] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [26.3, 42.0, 1.0, 2.0, 0.6117112565522325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000002, 6.9112, 121.9260426156618, 774932.2371543171, 774932.2371543163, 162720.8829894366]
[2019-04-27 21:25:02,687] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:25:02,691] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [4.6984445e-34 1.0000000e+00 3.7457923e-37 8.9505501e-38 3.8188050e-33], sampled 0.994830955202809
[2019-04-27 21:25:20,390] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8923.6626 2120699284.3595 431.0000
[2019-04-27 21:25:20,434] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8768.3985 2170876753.9502 493.0000
[2019-04-27 21:25:20,445] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8582.0590 2248830394.8409 553.0000
[2019-04-27 21:25:20,533] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8699.1592 2195751671.2584 572.0000
[2019-04-27 21:25:20,540] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8098.4495 2445508074.6415 746.0000
[2019-04-27 21:25:21,558] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 2000000, evaluation results [2000000.0, 8098.4494892610255, 2445508074.64147, 746.0, 8768.398458133903, 2170876753.9501715, 493.0, 8923.662597507513, 2120699284.3595483, 431.0, 8582.059032767682, 2248830394.840892, 553.0, 8699.159241284628, 2195751671.2584224, 572.0]
[2019-04-27 21:25:23,404] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0574907e-17 9.9993443e-01 6.4692447e-18 4.5550841e-19 6.5590488e-05], sum to 1.0000
[2019-04-27 21:25:23,412] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6937
[2019-04-27 21:25:23,418] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.95, 97.0, 1.0, 2.0, 0.6282935468326117, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 736016.1108567854, 736016.1108567854, 163953.2151419474], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3389400.0000, 
sim time next is 3390000.0000, 
raw observation next is [22.96666666666667, 98.0, 1.0, 2.0, 0.6241011871769118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 729036.1236619884, 729036.1236619884, 163115.0821180541], 
processed observation next is [1.0, 0.21739130434782608, 0.4061728395061729, 0.98, 1.0, 1.0, 0.5525014133058473, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2603700441649959, 0.2603700441649959, 0.31368285022702713], 
reward next is 0.6863, 
noisyNet noise sample is [array([0.31129432], dtype=float32), -0.5099568]. 
=============================================
[2019-04-27 21:25:23,427] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[61.90354 ]
 [61.447495]
 [61.240005]
 [61.105335]
 [60.985703]], R is [[62.21982574]
 [62.28233337]
 [62.34516907]
 [62.39846802]
 [62.47047806]].
[2019-04-27 21:25:24,458] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.9843564e-30 1.0000000e+00 3.3876649e-33 2.2112755e-34 2.8087777e-22], sum to 1.0000
[2019-04-27 21:25:24,465] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0565
[2019-04-27 21:25:24,469] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.45, 83.5, 1.0, 2.0, 0.6668605088682805, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 760013.6074413107, 760013.6074413107, 169963.7855563505], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3443400.0000, 
sim time next is 3444000.0000, 
raw observation next is [26.3, 85.33333333333333, 1.0, 2.0, 0.6721665693800213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 766063.8882530314, 766063.8882530314, 170940.6611769492], 
processed observation next is [1.0, 0.8695652173913043, 0.5296296296296297, 0.8533333333333333, 1.0, 1.0, 0.6097221064047872, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2735942458046541, 0.2735942458046541, 0.3287320407249023], 
reward next is 0.6713, 
noisyNet noise sample is [array([-0.3669281], dtype=float32), 0.9301261]. 
=============================================
[2019-04-27 21:25:24,481] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[74.66085 ]
 [74.972664]
 [75.15139 ]
 [75.11147 ]
 [75.04455 ]], R is [[74.41490173]
 [74.34390259]
 [74.27558136]
 [74.20995331]
 [74.14567566]].
[2019-04-27 21:25:30,030] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8804678e-20 1.0000000e+00 1.5101604e-23 2.9228594e-24 2.3352509e-12], sum to 1.0000
[2019-04-27 21:25:30,038] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3390
[2019-04-27 21:25:30,047] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1922434.962783209 W.
[2019-04-27 21:25:30,052] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.5, 79.0, 1.0, 2.0, 0.8427908641204415, 1.0, 2.0, 0.8427908641204415, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1922434.962783209, 1922434.962783209, 361754.3544740011], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3506400.0000, 
sim time next is 3507000.0000, 
raw observation next is [28.41666666666666, 80.66666666666667, 1.0, 2.0, 0.7869376920360254, 1.0, 2.0, 0.7869376920360254, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1794903.941225112, 1794903.941225113, 338387.4241919582], 
processed observation next is [1.0, 0.6086956521739131, 0.6080246913580245, 0.8066666666666668, 1.0, 1.0, 0.7463543952809826, 1.0, 1.0, 0.7463543952809826, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.6410371218661114, 0.6410371218661118, 0.6507450465229966], 
reward next is 0.3493, 
noisyNet noise sample is [array([0.12114596], dtype=float32), -0.45919684]. 
=============================================
[2019-04-27 21:25:30,069] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[64.739   ]
 [64.77751 ]
 [64.399376]
 [63.735794]
 [62.586872]], R is [[65.35553741]
 [65.00630188]
 [64.67208862]
 [64.32407379]
 [63.68083191]].
[2019-04-27 21:25:42,067] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.0155881e-21 1.0000000e+00 2.1428365e-23 6.8548018e-23 1.1118336e-11], sum to 1.0000
[2019-04-27 21:25:42,076] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9298
[2019-04-27 21:25:42,081] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 100.0, 1.0, 2.0, 0.7115210916734526, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 810939.7091208806, 810939.709120881, 178332.4423425056], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3738600.0000, 
sim time next is 3739200.0000, 
raw observation next is [24.0, 100.0, 1.0, 2.0, 0.7042707718813074, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 802671.9979848696, 802671.9979848696, 176950.1710607985], 
processed observation next is [1.0, 0.2608695652173913, 0.4444444444444444, 1.0, 1.0, 1.0, 0.6479413950967945, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.286668570708882, 0.286668570708882, 0.34028879050153554], 
reward next is 0.6597, 
noisyNet noise sample is [array([-0.47033337], dtype=float32), -0.593178]. 
=============================================
[2019-04-27 21:25:45,638] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.6608546e-18 4.6337178e-01 1.3948881e-17 1.5555278e-18 5.3662819e-01], sum to 1.0000
[2019-04-27 21:25:45,647] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9033
[2019-04-27 21:25:45,652] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.56666666666667, 46.83333333333333, 1.0, 2.0, 0.6154875209805232, 1.0, 2.0, 0.6154875209805232, 1.0, 2.0, 0.979876178430135, 6.9112, 6.9112, 121.94756008, 2106138.25877629, 2106138.25877629, 403878.545694956], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3773400.0000, 
sim time next is 3774000.0000, 
raw observation next is [34.13333333333333, 51.66666666666667, 1.0, 2.0, 0.5641283421761804, 1.0, 2.0, 0.5641283421761804, 1.0, 2.0, 0.8981106931219414, 6.911199999999999, 6.9112, 121.94756008, 1930202.258318502, 1930202.258318502, 376177.5636244162], 
processed observation next is [1.0, 0.6956521739130435, 0.8197530864197531, 0.5166666666666667, 1.0, 1.0, 0.4811051692573575, 1.0, 1.0, 0.4811051692573575, 1.0, 1.0, 0.8726383664024268, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.689357949399465, 0.689357949399465, 0.7234183915854158], 
reward next is 0.2766, 
noisyNet noise sample is [array([-0.48917687], dtype=float32), -0.28011176]. 
=============================================
[2019-04-27 21:25:45,670] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[78.68088 ]
 [77.93164 ]
 [77.19214 ]
 [76.719894]
 [76.02493 ]], R is [[78.92092896]
 [78.35503387]
 [77.75997162]
 [77.16719818]
 [76.58241272]].
[2019-04-27 21:25:46,654] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.2717204e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 21:25:46,661] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0462
[2019-04-27 21:25:46,666] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666667, 82.33333333333333, 1.0, 2.0, 0.6371217921849336, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 726818.5871893178, 726818.5871893173, 164616.2822000471], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3804600.0000, 
sim time next is 3805200.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.6436731303408879, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 733574.5613766143, 733574.5613766143, 165752.751530244], 
processed observation next is [0.0, 0.043478260869565216, 0.5185185185185185, 0.84, 1.0, 1.0, 0.5758013456439142, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.26199091477736225, 0.26199091477736225, 0.31875529140431536], 
reward next is 0.6812, 
noisyNet noise sample is [array([1.2552137], dtype=float32), -0.6309989]. 
=============================================
[2019-04-27 21:25:57,598] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2795546e-11 1.7334053e-02 9.5383059e-12 2.3043134e-10 9.8266590e-01], sum to 1.0000
[2019-04-27 21:25:57,606] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2701
[2019-04-27 21:25:57,612] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.63333333333333, 92.66666666666667, 1.0, 2.0, 0.3149432112064488, 1.0, 2.0, 0.3149432112064488, 1.0, 2.0, 0.5013998492249782, 6.9112, 6.9112, 121.94756008, 1077034.448908766, 1077034.448908766, 261830.3695738897], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3994800.0000, 
sim time next is 3995400.0000, 
raw observation next is [24.61666666666667, 92.83333333333333, 1.0, 2.0, 0.3060436492507626, 1.0, 2.0, 0.3060436492507626, 1.0, 2.0, 0.4872314567530274, 6.911200000000001, 6.9112, 121.94756008, 1046579.181113319, 1046579.181113318, 258374.4040602708], 
processed observation next is [1.0, 0.21739130434782608, 0.4672839506172841, 0.9283333333333332, 1.0, 1.0, 0.1738614872032888, 1.0, 1.0, 0.1738614872032888, 1.0, 1.0, 0.35903932094128427, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.3737782789690425, 0.37377827896904214, 0.49687385396205924], 
reward next is 0.5031, 
noisyNet noise sample is [array([0.27637407], dtype=float32), -0.57909995]. 
=============================================
[2019-04-27 21:25:58,170] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8305575e-13 2.2010240e-03 9.6524015e-13 3.2667589e-13 9.9779898e-01], sum to 1.0000
[2019-04-27 21:25:58,178] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7606
[2019-04-27 21:25:58,182] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.5, 94.0, 1.0, 2.0, 0.4653697159040872, 1.0, 2.0, 0.4653697159040872, 1.0, 2.0, 0.740883743752856, 6.911200000000001, 6.9112, 121.94756008, 1591992.057241601, 1591992.057241601, 326900.304322332], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4005600.0000, 
sim time next is 4006200.0000, 
raw observation next is [24.55, 94.0, 1.0, 2.0, 0.4652597962619299, 1.0, 2.0, 0.4652597962619299, 1.0, 2.0, 0.7407087481027088, 6.9112, 6.9112, 121.94756008, 1591615.696182211, 1591615.696182211, 326848.1821154496], 
processed observation next is [1.0, 0.34782608695652173, 0.46481481481481485, 0.94, 1.0, 1.0, 0.36340451935944035, 1.0, 1.0, 0.36340451935944035, 1.0, 1.0, 0.675885935128386, 0.0, 0.0, 0.8096049824067558, 0.5684341772079324, 0.5684341772079324, 0.6285541963758646], 
reward next is 0.3714, 
noisyNet noise sample is [array([1.2279145], dtype=float32), -1.3679065]. 
=============================================
[2019-04-27 21:26:00,936] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.9755825e-24 1.0000000e+00 3.2627725e-26 3.4802837e-27 2.0382795e-20], sum to 1.0000
[2019-04-27 21:26:00,948] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0618
[2019-04-27 21:26:00,960] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.33333333333334, 97.33333333333333, 1.0, 2.0, 0.575824250184171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 668126.8204659502, 668126.8204659502, 154537.4885336524], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4771200.0000, 
sim time next is 4771800.0000, 
raw observation next is [23.25, 98.0, 1.0, 2.0, 0.5720730893353871, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 663816.6199634614, 663816.6199634614, 153907.1141087218], 
processed observation next is [1.0, 0.21739130434782608, 0.4166666666666667, 0.98, 1.0, 1.0, 0.4905632015897466, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2370773642726648, 0.2370773642726648, 0.2959752194398496], 
reward next is 0.7040, 
noisyNet noise sample is [array([-1.2465491], dtype=float32), 0.26623708]. 
=============================================
[2019-04-27 21:26:03,622] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8798458e-33 1.0000000e+00 4.5506690e-37 0.0000000e+00 2.6298013e-36], sum to 1.0000
[2019-04-27 21:26:03,628] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7274
[2019-04-27 21:26:03,635] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1670657.37750888 W.
[2019-04-27 21:26:03,641] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.96666666666667, 69.0, 1.0, 2.0, 0.4883436161373503, 1.0, 2.0, 0.4883436161373503, 1.0, 2.0, 0.7774589411319065, 6.9112, 6.9112, 121.94756008, 1670657.37750888, 1670657.37750888, 337904.7835072604], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4108200.0000, 
sim time next is 4108800.0000, 
raw observation next is [28.93333333333334, 68.0, 1.0, 2.0, 0.5549537554995502, 1.0, 2.0, 0.5549537554995502, 1.0, 2.0, 0.8835044523373181, 6.911200000000001, 6.9112, 121.94756008, 1898777.442486877, 1898777.442486877, 371378.9244916411], 
processed observation next is [1.0, 0.5652173913043478, 0.6271604938271608, 0.68, 1.0, 1.0, 0.47018304226136926, 1.0, 1.0, 0.47018304226136926, 1.0, 1.0, 0.8543805654216476, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.6781348008881704, 0.6781348008881704, 0.7141902394070021], 
reward next is 0.2858, 
noisyNet noise sample is [array([-0.57103676], dtype=float32), 1.2672797]. 
=============================================
[2019-04-27 21:26:04,719] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.3120322e-23 1.0000000e+00 4.3039423e-27 7.2530599e-28 1.6437538e-15], sum to 1.0000
[2019-04-27 21:26:04,725] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5123
[2019-04-27 21:26:04,734] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2013007.089010784 W.
[2019-04-27 21:26:04,738] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.0, 72.0, 1.0, 2.0, 0.8824527663182234, 1.0, 2.0, 0.8824527663182234, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 2013007.089010784, 2013007.089010784, 379003.3800450449], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4116600.0000, 
sim time next is 4117200.0000, 
raw observation next is [29.0, 71.33333333333333, 1.0, 2.0, 0.5889082994298255, 1.0, 2.0, 0.5889082994298255, 1.0, 1.0, 0.9375611920966107, 6.911200000000001, 6.9112, 121.94756008, 2015084.152049417, 2015084.152049416, 389365.3891841951], 
processed observation next is [1.0, 0.6521739130434783, 0.6296296296296297, 0.7133333333333333, 1.0, 1.0, 0.5106051183688398, 1.0, 1.0, 0.5106051183688398, 1.0, 0.5, 0.9219514901207634, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.7196729114462204, 0.71967291144622, 0.7487795945849905], 
reward next is 0.2512, 
noisyNet noise sample is [array([-0.5043726], dtype=float32), -0.96283823]. 
=============================================
[2019-04-27 21:26:14,483] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0341675e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.4743859e-36], sum to 1.0000
[2019-04-27 21:26:14,492] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3222
[2019-04-27 21:26:14,502] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 100.0, 1.0, 2.0, 0.5755096133797039, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 668463.7569617374, 668463.756961737, 154516.2886701329], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5011200.0000, 
sim time next is 5011800.0000, 
raw observation next is [23.0, 100.0, 1.0, 2.0, 0.5777342261666353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 670528.9499301063, 670528.9499301063, 154868.8205309012], 
processed observation next is [0.0, 0.0, 0.4074074074074074, 1.0, 1.0, 1.0, 0.49730265019837533, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.23947462497503796, 0.23947462497503796, 0.2978246548671177], 
reward next is 0.7022, 
noisyNet noise sample is [array([-1.2472172], dtype=float32), -0.3406247]. 
=============================================
[2019-04-27 21:26:14,511] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-27 21:26:14,512] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 21:26:14,514] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 21:26:14,515] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:26:14,517] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:26:14,519] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 21:26:14,519] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:26:14,520] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 21:26:14,520] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 21:26:14,521] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:26:14,521] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:26:14,550] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run82
[2019-04-27 21:26:14,574] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run82
[2019-04-27 21:26:14,602] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run82
[2019-04-27 21:26:14,627] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run82
[2019-04-27 21:26:14,627] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run82
[2019-04-27 21:26:35,374] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02705983], dtype=float32), -0.025927333]
[2019-04-27 21:26:35,376] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [21.74053159, 42.92131546333334, 1.0, 2.0, 0.2577161204078778, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 332433.779213188, 332433.779213188, 92946.05355726936]
[2019-04-27 21:26:35,378] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 21:26:35,380] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7613431494677952
[2019-04-27 21:26:49,642] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.02705983], dtype=float32), -0.025927333]
[2019-04-27 21:26:49,644] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [26.32707205166666, 41.14012606833333, 1.0, 2.0, 0.3117313466852708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 396213.1988935402, 396213.1988935402, 117029.9417452395]
[2019-04-27 21:26:49,645] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 21:26:49,649] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.798023346469008
[2019-04-27 21:27:14,641] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02705983], dtype=float32), -0.025927333]
[2019-04-27 21:27:14,642] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [25.99276516, 93.25000875666667, 1.0, 2.0, 0.7179466876827452, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 818267.0427646856, 818267.0427646851, 179570.8183079332]
[2019-04-27 21:27:14,643] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 21:27:14,645] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [2.8317347e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.9125282014860782
[2019-04-27 21:27:15,291] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.02705983], dtype=float32), -0.025927333]
[2019-04-27 21:27:15,295] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [25.6, 73.0, 1.0, 2.0, 0.5162927005695435, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 611603.0794448921, 611603.0794448921, 145318.5900180394]
[2019-04-27 21:27:15,296] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 21:27:15,300] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5220171472331764
[2019-04-27 21:27:39,581] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.02705983], dtype=float32), -0.025927333]
[2019-04-27 21:27:39,582] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [28.781134555, 75.86201253833335, 1.0, 2.0, 0.7916524739363917, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.9260426156618, 1617412.944227905, 1617412.944227905, 334582.5596721594]
[2019-04-27 21:27:39,583] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 21:27:39,587] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [9.5227722e-28 1.0000000e+00 1.6967541e-30 7.4258146e-31 1.6143201e-27], sampled 0.9808965174248941
[2019-04-27 21:27:39,589] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1617412.944227905 W.
[2019-04-27 21:27:46,135] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.02705983], dtype=float32), -0.025927333]
[2019-04-27 21:27:46,137] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [22.66666666666667, 94.0, 1.0, 2.0, 0.5179200523559807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 613902.8406880331, 613902.8406880331, 145593.3112898288]
[2019-04-27 21:27:46,137] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 21:27:46,141] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4333930493778284
[2019-04-27 21:27:59,059] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8098.4495 2445508074.6415 746.0000
[2019-04-27 21:27:59,189] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8582.0590 2248830394.8409 553.0000
[2019-04-27 21:27:59,201] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8699.3316 2195627096.8477 572.0000
[2019-04-27 21:27:59,263] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8923.9572 2120691103.1707 431.0000
[2019-04-27 21:27:59,451] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8769.4309 2170774608.7126 493.0000
[2019-04-27 21:28:00,469] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 2025000, evaluation results [2025000.0, 8098.4494892610255, 2445508074.64147, 746.0, 8769.430899008301, 2170774608.7126427, 493.0, 8923.957176355885, 2120691103.1707451, 431.0, 8582.059032767682, 2248830394.840892, 553.0, 8699.331633205775, 2195627096.847748, 572.0]
[2019-04-27 21:28:01,496] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:28:01,503] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9320
[2019-04-27 21:28:01,507] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.76666666666667, 69.66666666666666, 1.0, 2.0, 0.6059778391947741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 697333.3758454359, 697333.3758454359, 159438.6791382179], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4308000.0000, 
sim time next is 4308600.0000, 
raw observation next is [27.38333333333333, 71.83333333333334, 1.0, 2.0, 0.6074364374574986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 699149.687478294, 699149.687478294, 159698.673301988], 
processed observation next is [1.0, 0.8695652173913043, 0.569753086419753, 0.7183333333333334, 1.0, 1.0, 0.5326624255446413, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.24969631695653358, 0.24969631695653358, 0.30711283327305383], 
reward next is 0.6929, 
noisyNet noise sample is [array([-0.7339288], dtype=float32), 1.7795324]. 
=============================================
[2019-04-27 21:28:03,584] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.06833275e-26 1.00000000e+00 8.72642349e-29 4.70046224e-30
 1.23153280e-30], sum to 1.0000
[2019-04-27 21:28:03,597] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5792
[2019-04-27 21:28:03,607] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1948472.289869445 W.
[2019-04-27 21:28:03,611] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5694621944536055, 1.0, 2.0, 0.5694621944536055, 1.0, 2.0, 0.9066023596590435, 6.911200000000001, 6.9112, 121.94756008, 1948472.289869445, 1948472.289869445, 378988.2497996959], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4356000.0000, 
sim time next is 4356600.0000, 
raw observation next is [28.2, 76.83333333333334, 1.0, 2.0, 0.5397631690075667, 1.0, 2.0, 0.5397631690075667, 1.0, 2.0, 0.8593205439192168, 6.911199999999999, 6.9112, 121.94756008, 1846749.040319049, 1846749.040319049, 363533.6833626267], 
processed observation next is [1.0, 0.43478260869565216, 0.6, 0.7683333333333334, 1.0, 1.0, 0.45209901072329367, 1.0, 1.0, 0.45209901072329367, 1.0, 1.0, 0.8241506798990208, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.6595532286853747, 0.6595532286853747, 0.6991032372358206], 
reward next is 0.3009, 
noisyNet noise sample is [array([0.38835225], dtype=float32), -0.25458604]. 
=============================================
[2019-04-27 21:28:08,074] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:28:08,084] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1918
[2019-04-27 21:28:08,091] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.2, 92.5, 1.0, 2.0, 0.4905025441756574, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 588683.007064521, 588683.0070645206, 141542.0786474962], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4426200.0000, 
sim time next is 4426800.0000, 
raw observation next is [22.13333333333333, 93.0, 1.0, 2.0, 0.490122371264707, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 588272.6634561362, 588272.6634561362, 141484.4883310192], 
processed observation next is [0.0, 0.21739130434782608, 0.3753086419753085, 0.93, 1.0, 1.0, 0.39300282293417504, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21009737980576296, 0.21009737980576296, 0.27208555448272925], 
reward next is 0.7279, 
noisyNet noise sample is [array([-0.13732359], dtype=float32), 0.74273115]. 
=============================================
[2019-04-27 21:28:20,605] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:28:20,615] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4724
[2019-04-27 21:28:20,628] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2041516.184990924 W.
[2019-04-27 21:28:20,637] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.8, 67.5, 1.0, 2.0, 0.8949361829909229, 1.0, 2.0, 0.8949361829909229, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 2041516.184990924, 2041516.184990924, 384545.4108371054], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4638600.0000, 
sim time next is 4639200.0000, 
raw observation next is [29.73333333333333, 66.66666666666666, 1.0, 2.0, 0.5459417603925162, 1.0, 2.0, 0.5459417603925162, 1.0, 1.0, 0.8691570626267295, 6.9112, 6.9112, 121.94756008, 1867910.611920723, 1867910.611920723, 366709.5942410276], 
processed observation next is [1.0, 0.6956521739130435, 0.65679012345679, 0.6666666666666665, 1.0, 1.0, 0.45945447665775735, 1.0, 1.0, 0.45945447665775735, 1.0, 0.5, 0.8364463282834119, 0.0, 0.0, 0.8096049824067558, 0.6671109328288296, 0.6671109328288296, 0.7052107581558223], 
reward next is 0.2948, 
noisyNet noise sample is [array([-1.3830377], dtype=float32), 2.299147]. 
=============================================
[2019-04-27 21:28:24,902] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4443678e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 21:28:24,914] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1632
[2019-04-27 21:28:24,922] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2219915.275164877 W.
[2019-04-27 21:28:24,929] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.83333333333334, 84.83333333333333, 1.0, 2.0, 0.6706624422442623, 1.0, 2.0, 0.648695883098566, 1.0, 1.0, 0.9977734948820727, 6.911200000000001, 6.9112, 121.94756008, 2219915.275164877, 2219915.275164877, 421449.6112382776], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4722600.0000, 
sim time next is 4723200.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.9190617678193289, 1.0, 2.0, 0.9190617678193289, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 2096615.74876953, 2096615.748769531, 395409.808199351], 
processed observation next is [1.0, 0.6956521739130435, 0.5925925925925926, 0.84, 1.0, 1.0, 0.9036449616896772, 1.0, 1.0, 0.9036449616896772, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.7487913388462607, 0.748791338846261, 0.7604034773064443], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7720729], dtype=float32), -0.6059193]. 
=============================================
[2019-04-27 21:28:27,940] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3853857e-32 1.0000000e+00 3.9632200e-36 9.2214049e-37 4.8936834e-35], sum to 1.0000
[2019-04-27 21:28:27,951] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7330
[2019-04-27 21:28:27,956] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 96.0, 1.0, 2.0, 0.6382943551693466, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 740337.4349601323, 740337.4349601323, 165415.2934002409], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4770000.0000, 
sim time next is 4770600.0000, 
raw observation next is [23.41666666666667, 96.66666666666666, 1.0, 2.0, 0.6015323530118108, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 697899.7788070549, 697899.7788070549, 158934.2712014076], 
processed observation next is [1.0, 0.21739130434782608, 0.42283950617283966, 0.9666666666666666, 1.0, 1.0, 0.5256337535854891, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.24924992100251958, 0.24924992100251958, 0.30564282923347613], 
reward next is 0.6944, 
noisyNet noise sample is [array([-1.1004589], dtype=float32), -2.36721]. 
=============================================
[2019-04-27 21:28:30,553] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.5167709e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.9239817e-35], sum to 1.0000
[2019-04-27 21:28:30,564] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3526
[2019-04-27 21:28:30,576] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1885927.95030194 W.
[2019-04-27 21:28:30,581] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.6, 84.0, 1.0, 2.0, 0.5512022076155624, 1.0, 2.0, 0.5512022076155624, 1.0, 1.0, 0.8775318659266247, 6.911200000000001, 6.9112, 121.94756008, 1885927.95030194, 1885927.95030194, 369429.8258545852], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4813200.0000, 
sim time next is 4813800.0000, 
raw observation next is [28.33333333333334, 85.66666666666667, 1.0, 2.0, 0.4605038978177366, 1.0, 2.0, 0.4605038978177366, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1049860.878501295, 1049860.878501296, 223440.6307638766], 
processed observation next is [1.0, 0.7391304347826086, 0.6049382716049385, 0.8566666666666667, 1.0, 1.0, 0.35774273549730545, 1.0, 1.0, 0.35774273549730545, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.37495031375046256, 0.3749503137504629, 0.4296935206997627], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.11186916], dtype=float32), -1.3259555]. 
=============================================
[2019-04-27 21:28:30,728] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:28:30,734] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0351
[2019-04-27 21:28:30,738] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.26666666666667, 92.33333333333334, 1.0, 2.0, 0.7457997438220292, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 850029.6745062042, 850029.6745062042, 185003.2800596722], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4816200.0000, 
sim time next is 4816800.0000, 
raw observation next is [27.0, 94.0, 1.0, 2.0, 0.7820151006977833, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 891330.3442817377, 891330.3442817377, 192258.9681071985], 
processed observation next is [1.0, 0.782608695652174, 0.5555555555555556, 0.94, 1.0, 1.0, 0.7404941674973611, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3183322658149063, 0.3183322658149063, 0.3697287848215356], 
reward next is 0.6303, 
noisyNet noise sample is [array([-1.7812662], dtype=float32), 0.19551833]. 
=============================================
[2019-04-27 21:28:31,125] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.1656582e-33 1.0000000e+00 5.5419229e-37 2.8778865e-38 3.6423655e-37], sum to 1.0000
[2019-04-27 21:28:31,136] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6847
[2019-04-27 21:28:31,142] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.36666666666667, 88.0, 1.0, 2.0, 0.6622101813099844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 754711.0709974088, 754711.0709974088, 169110.0552901584], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5553600.0000, 
sim time next is 5554200.0000, 
raw observation next is [25.38333333333333, 87.5, 1.0, 2.0, 0.6444664657036133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 735153.4002993043, 735153.4002993043, 165929.0557760294], 
processed observation next is [1.0, 0.2608695652173913, 0.49567901234567885, 0.875, 1.0, 1.0, 0.5767457925043016, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2625547858211801, 0.2625547858211801, 0.31909433803082576], 
reward next is 0.6809, 
noisyNet noise sample is [array([-0.49040553], dtype=float32), -1.135578]. 
=============================================
[2019-04-27 21:28:33,043] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.6236754e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 21:28:33,050] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3312
[2019-04-27 21:28:33,059] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2554654.422056018 W.
[2019-04-27 21:28:33,065] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.26666666666667, 85.16666666666667, 1.0, 2.0, 0.866015557360038, 1.0, 2.0, 0.7463724406564538, 1.0, 1.0, 0.9977734948820727, 6.9112, 6.9112, 121.94756008, 2554654.422056018, 2554654.422056018, 476830.5077929081], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4896600.0000, 
sim time next is 4897200.0000, 
raw observation next is [30.53333333333334, 81.33333333333334, 1.0, 2.0, 0.8962304389904427, 1.0, 2.0, 0.7614798814716561, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.94756008, 2606439.076453718, 2606439.076453718, 486132.8880173845], 
processed observation next is [1.0, 0.6956521739130435, 0.6864197530864199, 0.8133333333333335, 1.0, 1.0, 0.8764648083219556, 1.0, 1.0, 0.7160474779424477, 1.0, 1.0, 0.9972168686025908, 0.0, 0.0, 0.8096049824067558, 0.9308710987334706, 0.9308710987334706, 0.9348709384949702], 
reward next is 0.0651, 
noisyNet noise sample is [array([1.621923], dtype=float32), -0.2275901]. 
=============================================
[2019-04-27 21:28:35,180] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:28:35,190] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5095
[2019-04-27 21:28:35,193] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.8168667763384675, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 931077.9481623777, 931077.9481623777, 199457.3090159736], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5155200.0000, 
sim time next is 5155800.0000, 
raw observation next is [30.98333333333333, 69.83333333333334, 1.0, 2.0, 0.7893666508072518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 899714.4611758761, 899714.4611758761, 193760.4670229657], 
processed observation next is [0.0, 0.6956521739130435, 0.7030864197530863, 0.6983333333333335, 1.0, 1.0, 0.7492460128657759, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3213265932770986, 0.3213265932770986, 0.37261628273647246], 
reward next is 0.6274, 
noisyNet noise sample is [array([-0.7564302], dtype=float32), 0.7697567]. 
=============================================
[2019-04-27 21:28:40,957] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:28:40,966] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4160
[2019-04-27 21:28:40,972] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 88.0, 1.0, 2.0, 0.602782341517014, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 692849.5956301463, 692849.5956301463, 158846.1882912112], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5038800.0000, 
sim time next is 5039400.0000, 
raw observation next is [25.25, 86.5, 1.0, 2.0, 0.606334805278666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 696282.987665306, 696282.987665306, 159430.4356071799], 
processed observation next is [0.0, 0.30434782608695654, 0.49074074074074076, 0.865, 1.0, 1.0, 0.5313509586650785, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.24867249559475213, 0.24867249559475213, 0.30659699155226905], 
reward next is 0.6934, 
noisyNet noise sample is [array([-1.9422807], dtype=float32), -1.5053852]. 
=============================================
[2019-04-27 21:28:43,033] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:28:43,057] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2563
[2019-04-27 21:28:43,062] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.16666666666667, 94.00000000000001, 1.0, 2.0, 0.5424196175897927, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 637582.8454560855, 637582.8454560855, 149348.8565323888], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5033400.0000, 
sim time next is 5034000.0000, 
raw observation next is [23.33333333333334, 94.0, 1.0, 2.0, 0.5504464433078793, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 644887.750816083, 644887.750816083, 150579.7237685493], 
processed observation next is [0.0, 0.2608695652173913, 0.4197530864197533, 0.94, 1.0, 1.0, 0.464817194414142, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2303170538628868, 0.2303170538628868, 0.2895763918625948], 
reward next is 0.7104, 
noisyNet noise sample is [array([0.923432], dtype=float32), -1.4682462]. 
=============================================
[2019-04-27 21:28:43,077] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[73.16836 ]
 [73.15535 ]
 [73.03102 ]
 [73.01907 ]
 [72.991394]], R is [[73.12018585]
 [73.10177612]
 [73.08508301]
 [73.06797791]
 [73.05036926]].
[2019-04-27 21:28:46,988] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.4890168e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7027752e-33], sum to 1.0000
[2019-04-27 21:28:46,997] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4938
[2019-04-27 21:28:47,004] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1314060.486237459 W.
[2019-04-27 21:28:47,008] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.9, 45.0, 1.0, 2.0, 0.5434932410340152, 1.0, 2.0, 0.5434932410340152, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 121.926042615629, 1314060.486237459, 1314060.486237459, 252300.9254291678], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5842800.0000, 
sim time next is 5843400.0000, 
raw observation next is [27.91666666666666, 44.66666666666666, 1.0, 2.0, 0.3341435020834002, 1.0, 2.0, 0.3341435020834002, 1.0, 1.0, 0.5402936876131161, 6.9112, 6.9112, 121.94756008, 1206112.094687539, 1206112.094687539, 268824.4328374348], 
processed observation next is [1.0, 0.6521739130434783, 0.5895061728395059, 0.44666666666666655, 1.0, 1.0, 0.20731369295642882, 1.0, 1.0, 0.20731369295642882, 1.0, 0.5, 0.4253671095163951, 0.0, 0.0, 0.8096049824067558, 0.4307543195312639, 0.4307543195312639, 0.5169700631489131], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.317509], dtype=float32), 0.1155866]. 
=============================================
[2019-04-27 21:28:49,616] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:28:49,625] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5073
[2019-04-27 21:28:49,630] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.63333333333333, 85.83333333333334, 1.0, 2.0, 0.7092656194179138, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 808367.7312074881, 808367.7312074881, 177905.1066345203], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5172600.0000, 
sim time next is 5173200.0000, 
raw observation next is [26.5, 86.0, 1.0, 2.0, 0.7035659875714706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 801868.3205233983, 801868.3205233983, 176819.4119684989], 
processed observation next is [0.0, 0.9130434782608695, 0.5370370370370371, 0.86, 1.0, 1.0, 0.6471023661565126, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2863815430440708, 0.2863815430440708, 0.3400373307086517], 
reward next is 0.6600, 
noisyNet noise sample is [array([-0.33987135], dtype=float32), 0.41642663]. 
=============================================
[2019-04-27 21:28:52,878] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:28:52,883] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9961
[2019-04-27 21:28:52,891] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666667, 69.0, 1.0, 2.0, 0.460361621490991, 1.0, 2.0, 0.460361621490991, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1049536.293520563, 1049536.293520564, 223397.3409467013], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5245800.0000, 
sim time next is 5246400.0000, 
raw observation next is [29.53333333333334, 70.0, 1.0, 2.0, 0.6563317402613252, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 748008.230518904, 748008.230518904, 168045.1712711373], 
processed observation next is [1.0, 0.7391304347826086, 0.6493827160493829, 0.7, 1.0, 1.0, 0.5908711193587205, 0.0, 0.5, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2671457966138943, 0.2671457966138943, 0.3231637909060333], 
reward next is 0.6768, 
noisyNet noise sample is [array([1.3962913], dtype=float32), -1.4063563]. 
=============================================
[2019-04-27 21:28:53,377] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-27 21:28:53,378] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 21:28:53,379] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 21:28:53,379] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 21:28:53,380] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:28:53,380] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 21:28:53,379] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:28:53,382] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 21:28:53,383] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:28:53,380] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:28:53,384] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:28:53,415] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run83
[2019-04-27 21:28:53,440] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run83
[2019-04-27 21:28:53,480] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run83
[2019-04-27 21:28:53,505] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run83
[2019-04-27 21:28:53,505] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run83
[2019-04-27 21:29:11,567] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02555838], dtype=float32), -0.025955584]
[2019-04-27 21:29:11,567] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [18.05, 93.0, 1.0, 2.0, 0.3220639535258955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 409462.8708974149, 409462.8708974149, 118338.208481309]
[2019-04-27 21:29:11,568] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:29:11,571] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.33708961889030087
[2019-04-27 21:29:35,122] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02555838], dtype=float32), -0.025955584]
[2019-04-27 21:29:35,123] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [22.4, 92.0, 1.0, 2.0, 0.5700988328057325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 682561.4900241259, 682561.4900241259, 154437.0480765185]
[2019-04-27 21:29:35,124] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:29:35,126] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [2.3968372e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.3922472098877692
[2019-04-27 21:30:23,463] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.02555838], dtype=float32), -0.025955584]
[2019-04-27 21:30:23,464] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [17.41740781, 83.81338737, 1.0, 2.0, 0.2587266172227677, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 333653.1176117933, 333653.1176117933, 108065.4657776253]
[2019-04-27 21:30:23,464] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 21:30:23,467] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.28423307452413593
[2019-04-27 21:30:25,893] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02555838], dtype=float32), -0.025955584]
[2019-04-27 21:30:25,894] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [31.13333333333333, 45.66666666666667, 1.0, 2.0, 0.5304304024066674, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 627150.8974876625, 627150.8974876625, 147545.7707461921]
[2019-04-27 21:30:25,897] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:30:25,900] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.012669987950411521
[2019-04-27 21:30:30,946] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.02555838], dtype=float32), -0.025955584]
[2019-04-27 21:30:30,948] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [29.0, 23.0, 1.0, 2.0, 0.3296725802951756, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 425277.7595789446, 425277.7595789446, 117925.9131156861]
[2019-04-27 21:30:30,948] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 21:30:30,950] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.11943316650782199
[2019-04-27 21:30:37,250] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02555838], dtype=float32), -0.025955584]
[2019-04-27 21:30:37,250] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [28.98333333333333, 45.5, 1.0, 2.0, 0.4271968073497106, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 521671.87296828, 521671.8729682795, 132320.8266579834]
[2019-04-27 21:30:37,252] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:30:37,257] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3708818787551059
[2019-04-27 21:30:37,587] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8769.4422 2170768739.9937 493.0000
[2019-04-27 21:30:37,936] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8923.6649 2120679552.0029 431.0000
[2019-04-27 21:30:38,194] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8582.0590 2248830394.8409 553.0000
[2019-04-27 21:30:38,263] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8098.4495 2445508074.6415 746.0000
[2019-04-27 21:30:38,346] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8699.6412 2195466116.9322 572.0000
[2019-04-27 21:30:39,364] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 2050000, evaluation results [2050000.0, 8098.4494892610255, 2445508074.64147, 746.0, 8769.442185004564, 2170768739.993712, 493.0, 8923.664891646278, 2120679552.0028546, 431.0, 8582.059032767682, 2248830394.840892, 553.0, 8699.64120996645, 2195466116.9321966, 572.0]
[2019-04-27 21:30:46,233] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:30:46,239] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8169
[2019-04-27 21:30:46,243] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 69.33333333333333, 1.0, 2.0, 0.6136554353580197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 701129.8838804301, 701129.8838804297, 160530.7633930692], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5337600.0000, 
sim time next is 5338200.0000, 
raw observation next is [28.13333333333333, 69.66666666666667, 1.0, 2.0, 0.6168987647255814, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 704465.0311161361, 704465.0311161361, 161079.0827694647], 
processed observation next is [1.0, 0.782608695652174, 0.5975308641975308, 0.6966666666666668, 1.0, 1.0, 0.5439271008637874, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2515946539700486, 0.2515946539700486, 0.3097674668643552], 
reward next is 0.6902, 
noisyNet noise sample is [array([0.7905141], dtype=float32), 1.8053482]. 
=============================================
[2019-04-27 21:31:10,149] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:31:10,154] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5096
[2019-04-27 21:31:10,159] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.45, 92.5, 1.0, 2.0, 0.4988567020168338, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 606330.9244808882, 606330.9244808882, 143099.4483326828], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5803800.0000, 
sim time next is 5804400.0000, 
raw observation next is [21.4, 93.0, 1.0, 2.0, 0.4643251452435682, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 564253.9347890046, 564253.9347890042, 137754.1487920007], 
processed observation next is [1.0, 0.17391304347826086, 0.3481481481481481, 0.93, 1.0, 1.0, 0.3622918395756765, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.20151926242464452, 0.20151926242464435, 0.26491182460000134], 
reward next is 0.7351, 
noisyNet noise sample is [array([-1.9411299], dtype=float32), 0.10437109]. 
=============================================
[2019-04-27 21:31:10,434] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:31:10,441] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5617
[2019-04-27 21:31:10,445] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.3, 81.66666666666667, 1.0, 2.0, 0.5220997903382565, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 618988.3209184683, 618988.3209184683, 146268.5616748449], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5781000.0000, 
sim time next is 5781600.0000, 
raw observation next is [24.2, 82.0, 1.0, 2.0, 0.5193102156899587, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 616271.2921081213, 616271.2921081213, 145843.826759305], 
processed observation next is [0.0, 0.9565217391304348, 0.45185185185185184, 0.82, 1.0, 1.0, 0.4277502567737604, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.22009689003861477, 0.22009689003861477, 0.2804688976140481], 
reward next is 0.7195, 
noisyNet noise sample is [array([0.3237343], dtype=float32), -0.7355122]. 
=============================================
[2019-04-27 21:31:11,687] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:31:11,697] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5397
[2019-04-27 21:31:11,703] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.4, 93.0, 1.0, 2.0, 0.4643251452435682, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 564253.9347890046, 564253.9347890042, 137754.1487920007], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5804400.0000, 
sim time next is 5805000.0000, 
raw observation next is [21.35, 93.5, 1.0, 2.0, 0.4575427643836938, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 555942.4647455825, 555942.4647455821, 136727.6329437856], 
processed observation next is [1.0, 0.17391304347826086, 0.3462962962962963, 0.935, 1.0, 1.0, 0.3542175766472545, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.19855088026627948, 0.19855088026627932, 0.26293775566112615], 
reward next is 0.7371, 
noisyNet noise sample is [array([0.17545204], dtype=float32), 0.1983741]. 
=============================================
[2019-04-27 21:31:11,722] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[67.020065]
 [66.98036 ]
 [67.3047  ]
 [67.6519  ]
 [67.76016 ]], R is [[67.02441406]
 [67.08925629]
 [67.14317322]
 [67.20783997]
 [67.2721405 ]].
[2019-04-27 21:31:12,691] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.4807889e-30 1.0000000e+00 7.9245966e-33 1.1956778e-33 4.9622653e-33], sum to 1.0000
[2019-04-27 21:31:12,695] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9961
[2019-04-27 21:31:12,703] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.96666666666667, 46.33333333333334, 1.0, 2.0, 0.66537448713132, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156422, 829370.0498986829, 829370.0498986829, 172375.0250677503], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5833200.0000, 
sim time next is 5833800.0000, 
raw observation next is [27.05, 46.5, 1.0, 2.0, 0.6444868982330618, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 802481.9344933457, 802481.9344933457, 168463.8000005724], 
processed observation next is [1.0, 0.5217391304347826, 0.5574074074074075, 0.465, 1.0, 1.0, 0.5767701169441212, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2866006908904806, 0.2866006908904806, 0.3239688461549469], 
reward next is 0.6760, 
noisyNet noise sample is [array([0.7492092], dtype=float32), 0.49109918]. 
=============================================
[2019-04-27 21:31:13,534] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:31:13,542] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0692
[2019-04-27 21:31:13,546] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.45, 77.0, 1.0, 2.0, 0.4100794309879831, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 506163.8990373791, 506163.8990373791, 130002.9075089636], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5869800.0000, 
sim time next is 5870400.0000, 
raw observation next is [22.26666666666667, 78.0, 1.0, 2.0, 0.4086833190615934, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 504735.347332637, 504735.347332637, 129811.1434540821], 
processed observation next is [1.0, 0.9565217391304348, 0.38024691358024704, 0.78, 1.0, 1.0, 0.2960515703114207, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.18026262404737037, 0.18026262404737037, 0.24963681433477328], 
reward next is 0.7504, 
noisyNet noise sample is [array([-0.23147222], dtype=float32), 0.17409456]. 
=============================================
[2019-04-27 21:31:23,593] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.7826662e-31 1.0000000e+00 5.1258816e-33 2.5226680e-35 1.0259403e-27], sum to 1.0000
[2019-04-27 21:31:23,603] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9956
[2019-04-27 21:31:23,613] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1665689.256333778 W.
[2019-04-27 21:31:23,618] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.0, 58.66666666666667, 1.0, 2.0, 0.8302142951503547, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9935678492063699, 6.911199999999999, 6.9112, 121.9260426156618, 1665689.256333778, 1665689.256333778, 341656.8152581209], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6013200.0000, 
sim time next is 6013800.0000, 
raw observation next is [29.0, 58.5, 1.0, 2.0, 0.7343831775245037, 1.0, 1.0, 0.7343831775245037, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1674921.45622802, 1674921.456228021, 317384.8784117809], 
processed observation next is [1.0, 0.6086956521739131, 0.6296296296296297, 0.585, 1.0, 1.0, 0.6837894970529805, 1.0, 0.5, 0.6837894970529805, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.59818623436715, 0.5981862343671503, 0.610355535407271], 
reward next is 0.3896, 
noisyNet noise sample is [array([1.1952095], dtype=float32), 0.6939934]. 
=============================================
[2019-04-27 21:31:27,769] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5924072e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 21:31:27,776] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3741
[2019-04-27 21:31:27,779] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 45.5, 1.0, 2.0, 0.3473259553067215, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 448057.2733211736, 448057.2733211732, 118241.3590813175], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6665400.0000, 
sim time next is 6666000.0000, 
raw observation next is [23.0, 45.33333333333334, 1.0, 2.0, 0.3256986452492784, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 420149.9774547277, 420149.9774547277, 114872.973945257], 
processed observation next is [1.0, 0.13043478260869565, 0.4074074074074074, 0.4533333333333334, 1.0, 1.0, 0.19726029196342665, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.15005356337668846, 0.15005356337668846, 0.2209095652793404], 
reward next is 0.7791, 
noisyNet noise sample is [array([1.0168079], dtype=float32), 0.152525]. 
=============================================
[2019-04-27 21:31:27,797] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[64.217285]
 [64.582275]
 [64.805916]
 [65.10822 ]
 [65.04615 ]], R is [[64.09850311]
 [64.23013306]
 [64.36024475]
 [64.48373413]
 [64.62464905]].
[2019-04-27 21:31:32,169] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-27 21:31:32,171] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 21:31:32,172] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 21:31:32,173] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:31:32,173] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 21:31:32,174] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 21:31:32,175] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 21:31:32,176] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:31:32,174] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:31:32,177] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:31:32,180] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:31:32,206] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run84
[2019-04-27 21:31:32,228] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run84
[2019-04-27 21:31:32,265] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run84
[2019-04-27 21:31:32,289] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run84
[2019-04-27 21:31:32,317] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run84
[2019-04-27 21:31:43,955] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.02407856], dtype=float32), -0.024900587]
[2019-04-27 21:31:43,957] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [37.91666666666666, 15.5, 1.0, 2.0, 0.9668027191783748, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.436858422682111, 6.9112, 121.9238518356539, 1463456.943579385, 1194277.685077705, 236912.5386668648]
[2019-04-27 21:31:43,959] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 21:31:43,962] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [4.3776568e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3637030e-37], sampled 0.9943920638189576
[2019-04-27 21:31:43,964] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1463456.943579385 W.
[2019-04-27 21:31:49,172] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02407856], dtype=float32), -0.024900587]
[2019-04-27 21:31:49,173] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [19.2, 74.83333333333333, 1.0, 2.0, 0.2815990463554197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 361674.4464131907, 361674.4464131903, 113330.2143063304]
[2019-04-27 21:31:49,174] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:31:49,175] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.0903145344286258
[2019-04-27 21:32:47,711] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02407856], dtype=float32), -0.024900587]
[2019-04-27 21:32:47,711] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [25.3, 90.5, 1.0, 2.0, 0.6657803393039041, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 758781.9407214485, 758781.9407214485, 169764.8773232345]
[2019-04-27 21:32:47,712] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:32:47,715] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4938612979436673
[2019-04-27 21:32:59,272] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02407856], dtype=float32), -0.024900587]
[2019-04-27 21:32:59,274] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [25.87209798, 53.00888069, 1.0, 2.0, 0.4219850388119551, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 522453.7762422662, 522453.7762422658, 131749.3128536979]
[2019-04-27 21:32:59,276] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 21:32:59,279] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6755308928309182
[2019-04-27 21:33:16,332] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.02407856], dtype=float32), -0.024900587]
[2019-04-27 21:33:16,335] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [24.12815514, 48.90016597, 1.0, 2.0, 0.3571545547576233, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 456360.812335275, 456360.8123352755, 122930.9030536038]
[2019-04-27 21:33:16,338] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 21:33:16,341] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7929623705056148
[2019-04-27 21:33:16,629] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8922.6489 2120606354.9094 430.0000
[2019-04-27 21:33:16,994] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8699.9231 2195319545.7982 572.0000
[2019-04-27 21:33:17,089] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02407856], dtype=float32), -0.024900587]
[2019-04-27 21:33:17,090] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [28.2, 52.0, 1.0, 2.0, 0.5744240109350655, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9232658507385819, 6.9112, 6.9112, 121.9260426156618, 1366520.697215673, 1366520.697215673, 282696.6680783434]
[2019-04-27 21:33:17,090] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:33:17,092] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [2.8434677e-31 1.0000000e+00 9.5051363e-35 1.6786909e-35 2.3146225e-34], sampled 0.20461818773070484
[2019-04-27 21:33:17,093] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1366520.697215673 W.
[2019-04-27 21:33:17,150] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8770.2626 2170712315.3188 493.0000
[2019-04-27 21:33:17,167] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8582.0590 2248830394.8409 553.0000
[2019-04-27 21:33:17,270] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8098.4495 2445508074.6415 746.0000
[2019-04-27 21:33:18,288] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 2075000, evaluation results [2075000.0, 8098.4494892610255, 2445508074.64147, 746.0, 8770.262620159681, 2170712315.3187604, 493.0, 8922.648898215564, 2120606354.9093559, 430.0, 8582.059032767682, 2248830394.840892, 553.0, 8699.923077531761, 2195319545.7982335, 572.0]
[2019-04-27 21:33:20,138] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:33:20,147] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9950
[2019-04-27 21:33:20,150] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 65.66666666666667, 1.0, 2.0, 0.5568560573125135, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 651059.150198973, 651059.150198973, 151583.3842757899], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6207600.0000, 
sim time next is 6208200.0000, 
raw observation next is [27.45, 66.0, 1.0, 2.0, 0.5534182329296136, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 648113.5129784897, 648113.5129784893, 151059.6848483814], 
processed observation next is [1.0, 0.8695652173913043, 0.5722222222222222, 0.66, 1.0, 1.0, 0.4683550392019209, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.23146911177803206, 0.2314691117780319, 0.290499393939195], 
reward next is 0.7095, 
noisyNet noise sample is [array([-0.52763635], dtype=float32), 0.7428133]. 
=============================================
[2019-04-27 21:33:28,708] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:33:28,716] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9554
[2019-04-27 21:33:28,721] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.75, 56.83333333333333, 1.0, 2.0, 0.6753830641255097, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 769731.5471045189, 769731.5471045189, 171536.797595237], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6361800.0000, 
sim time next is 6362400.0000, 
raw observation next is [31.8, 56.66666666666667, 1.0, 2.0, 0.6706835469251736, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 764372.8544254999, 764372.8544254994, 170669.2806698438], 
processed observation next is [0.0, 0.6521739130434783, 0.7333333333333334, 0.5666666666666668, 1.0, 1.0, 0.6079566034823496, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.27299030515196426, 0.2729903051519641, 0.328210155134315], 
reward next is 0.6718, 
noisyNet noise sample is [array([0.24807744], dtype=float32), -0.78906125]. 
=============================================
[2019-04-27 21:33:31,648] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.5616431e-22 1.0000000e+00 1.4019341e-22 1.1616136e-24 9.7138052e-15], sum to 1.0000
[2019-04-27 21:33:31,657] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0547
[2019-04-27 21:33:31,664] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2237807.784081055 W.
[2019-04-27 21:33:31,669] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.63333333333333, 72.33333333333334, 1.0, 2.0, 0.681106328546409, 1.0, 2.0, 0.653917826249639, 1.0, 1.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.94756008, 2237807.784081055, 2237807.784081055, 424201.6104528655], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6434400.0000, 
sim time next is 6435000.0000, 
raw observation next is [29.8, 71.0, 1.0, 2.0, 0.7180785146261743, 1.0, 2.0, 0.6724039192895218, 1.0, 2.0, 0.9977734948820727, 6.911200000000001, 6.9112, 121.94756008, 2301151.566490802, 2301151.566490802, 434133.4058532177], 
processed observation next is [1.0, 0.4782608695652174, 0.6592592592592593, 0.71, 1.0, 1.0, 0.6643791840787789, 1.0, 1.0, 0.6100046658208593, 1.0, 1.0, 0.9972168686025908, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.8218398451752864, 0.8218398451752864, 0.8348719343331109], 
reward next is 0.1651, 
noisyNet noise sample is [array([0.94331753], dtype=float32), -0.14095822]. 
=============================================
[2019-04-27 21:33:31,689] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[66.58234 ]
 [66.27849 ]
 [66.17319 ]
 [65.944595]
 [65.32469 ]], R is [[66.37923431]
 [65.89966583]
 [65.42720032]
 [64.99108887]
 [64.5771637 ]].
[2019-04-27 21:33:34,907] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:33:34,918] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2719
[2019-04-27 21:33:34,925] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.85, 65.5, 1.0, 2.0, 0.6822291442234287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 777537.9563136111, 777537.9563136111, 172806.8649638458], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6467400.0000, 
sim time next is 6468000.0000, 
raw observation next is [29.73333333333333, 66.0, 1.0, 2.0, 0.6832778930129592, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 778733.8246306609, 778733.8246306609, 173002.1404880387], 
processed observation next is [1.0, 0.8695652173913043, 0.65679012345679, 0.66, 1.0, 1.0, 0.6229498726344752, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2781192230823789, 0.2781192230823789, 0.33269642401545907], 
reward next is 0.6673, 
noisyNet noise sample is [array([1.2214131], dtype=float32), -0.17012337]. 
=============================================
[2019-04-27 21:33:34,938] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[84.9487  ]
 [84.94088 ]
 [84.90185 ]
 [84.761284]
 [84.48051 ]], R is [[84.84153748]
 [84.66080475]
 [84.48509979]
 [84.30938721]
 [84.13152313]].
[2019-04-27 21:33:36,396] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.3493792e-33 1.0000000e+00 4.1304279e-37 2.1953185e-38 0.0000000e+00], sum to 1.0000
[2019-04-27 21:33:36,406] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9875
[2019-04-27 21:33:36,410] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.03333333333333, 80.0, 1.0, 2.0, 0.6672034570204474, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 760404.6555569951, 760404.6555569951, 170027.0185829971], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6486000.0000, 
sim time next is 6486600.0000, 
raw observation next is [26.96666666666667, 80.5, 1.0, 2.0, 0.6674911915013906, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 760732.7461988628, 760732.7461988628, 170079.9011788352], 
processed observation next is [1.0, 0.043478260869565216, 0.554320987654321, 0.805, 1.0, 1.0, 0.6041561803587983, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2716902664995939, 0.2716902664995939, 0.32707673303622153], 
reward next is 0.6729, 
noisyNet noise sample is [array([-0.64397794], dtype=float32), -0.26833877]. 
=============================================
[2019-04-27 21:33:37,709] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.4035604e-30 1.0000000e+00 6.6082500e-34 2.8672069e-35 4.2993770e-36], sum to 1.0000
[2019-04-27 21:33:37,720] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3563
[2019-04-27 21:33:37,730] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2025698.701321033 W.
[2019-04-27 21:33:37,734] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.61666666666667, 79.16666666666667, 1.0, 2.0, 0.8880101536161181, 1.0, 2.0, 0.8880101536161181, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 2025698.701321033, 2025698.701321034, 381464.626888853], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6520200.0000, 
sim time next is 6520800.0000, 
raw observation next is [28.53333333333333, 79.33333333333334, 1.0, 2.0, 0.9175298834748828, 1.0, 2.0, 0.9175298834748828, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 2093117.031595447, 2093117.031595448, 394713.8437601413], 
processed observation next is [1.0, 0.4782608695652174, 0.6123456790123456, 0.7933333333333334, 1.0, 1.0, 0.901821289851051, 1.0, 1.0, 0.901821289851051, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.747541796998374, 0.7475417969983743, 0.7590650841541179], 
reward next is 0.2409, 
noisyNet noise sample is [array([-0.29975814], dtype=float32), 0.5309772]. 
=============================================
[2019-04-27 21:33:53,608] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:33:53,619] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6690
[2019-04-27 21:33:53,626] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.16666666666667, 76.33333333333334, 1.0, 2.0, 0.4773715710497107, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 575203.4672745423, 575203.4672745423, 139590.3417817167], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6817800.0000, 
sim time next is 6818400.0000, 
raw observation next is [24.1, 77.0, 1.0, 2.0, 0.4788311913210218, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 576656.8201737145, 576656.8201737141, 139804.4468923738], 
processed observation next is [1.0, 0.9565217391304348, 0.4481481481481482, 0.77, 1.0, 1.0, 0.37956094204883545, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2059488643477552, 0.20594886434775503, 0.26885470556225727], 
reward next is 0.7311, 
noisyNet noise sample is [array([0.5192915], dtype=float32), -1.3830723]. 
=============================================
[2019-04-27 21:33:59,930] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:33:59,939] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9044
[2019-04-27 21:33:59,943] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.8, 64.0, 1.0, 2.0, 0.4252317168601469, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 522453.933686141, 522453.933686141, 132122.5590049238], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6903600.0000, 
sim time next is 6904200.0000, 
raw observation next is [24.65, 64.5, 1.0, 2.0, 0.4233873475125478, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 520648.1975017693, 520648.1975017689, 131867.1965551804], 
processed observation next is [0.0, 0.9130434782608695, 0.46851851851851845, 0.645, 1.0, 1.0, 0.3135563660863664, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.18594578482206045, 0.1859457848220603, 0.2535907626061161], 
reward next is 0.7464, 
noisyNet noise sample is [array([-0.1901693], dtype=float32), 1.0745513]. 
=============================================
[2019-04-27 21:34:00,045] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:34:00,056] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0779
[2019-04-27 21:34:00,064] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.65, 64.5, 1.0, 2.0, 0.4233873475125478, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 520648.1975017693, 520648.1975017689, 131867.1965551804], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6904200.0000, 
sim time next is 6904800.0000, 
raw observation next is [24.5, 65.0, 1.0, 2.0, 0.4219579989451635, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 519351.7907946567, 519351.7907946563, 131672.2314362967], 
processed observation next is [0.0, 0.9565217391304348, 0.46296296296296297, 0.65, 1.0, 1.0, 0.31185476064900414, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1854827824266631, 0.18548278242666297, 0.2532158296851859], 
reward next is 0.7468, 
noisyNet noise sample is [array([-0.39462566], dtype=float32), -2.1838105]. 
=============================================
[2019-04-27 21:34:00,879] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:34:00,890] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4490
[2019-04-27 21:34:00,897] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 61.5, 1.0, 2.0, 0.5002822338425454, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 597239.643988091, 597239.643988091, 142959.664396102], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6949800.0000, 
sim time next is 6950400.0000, 
raw observation next is [27.46666666666667, 60.33333333333333, 1.0, 2.0, 0.50187747440798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 598660.9105309446, 598660.9105309446, 143192.891736222], 
processed observation next is [0.0, 0.43478260869565216, 0.5728395061728396, 0.6033333333333333, 1.0, 1.0, 0.40699699334283335, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21380746804676592, 0.21380746804676592, 0.27537094564658077], 
reward next is 0.7246, 
noisyNet noise sample is [array([1.1283265], dtype=float32), -0.42399406]. 
=============================================
[2019-04-27 21:34:01,006] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:34:01,013] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8570
[2019-04-27 21:34:01,018] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.25, 62.5, 1.0, 2.0, 0.4327716351873896, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 530333.2018952505, 530333.2018952505, 133184.8437259588], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6901800.0000, 
sim time next is 6902400.0000, 
raw observation next is [25.1, 63.0, 1.0, 2.0, 0.430588326295304, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 528110.4046949166, 528110.4046949166, 132877.9353066579], 
processed observation next is [0.0, 0.9130434782608695, 0.4851851851851852, 0.63, 1.0, 1.0, 0.322128959875362, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.18861085881961306, 0.18861085881961306, 0.2555344909743421], 
reward next is 0.7445, 
noisyNet noise sample is [array([-1.4431819], dtype=float32), 1.0650808]. 
=============================================
[2019-04-27 21:34:06,693] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.00263684e-35 1.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00], sum to 1.0000
[2019-04-27 21:34:06,703] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1510
[2019-04-27 21:34:06,710] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.93333333333334, 88.33333333333334, 1.0, 2.0, 0.5228988950756835, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 645433.9186486366, 645433.9186486366, 147217.531665837], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7014000.0000, 
sim time next is 7014600.0000, 
raw observation next is [20.85, 89.0, 1.0, 2.0, 0.5102496969860351, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 629849.4226416467, 629849.4226416467, 145175.6741814865], 
processed observation next is [1.0, 0.17391304347826086, 0.32777777777777783, 0.89, 1.0, 1.0, 0.41696392498337503, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.22494622237201667, 0.22494622237201667, 0.27918398881055095], 
reward next is 0.7208, 
noisyNet noise sample is [array([-0.6640275], dtype=float32), -1.7329862]. 
=============================================
[2019-04-27 21:34:11,399] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-27 21:34:11,400] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 21:34:11,400] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 21:34:11,401] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:34:11,402] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:34:11,401] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 21:34:11,405] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 21:34:11,407] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:34:11,408] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:34:11,403] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 21:34:11,411] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:34:11,433] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run85
[2019-04-27 21:34:11,456] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run85
[2019-04-27 21:34:11,480] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run85
[2019-04-27 21:34:11,481] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run85
[2019-04-27 21:34:11,525] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run85
[2019-04-27 21:34:13,769] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01359302], dtype=float32), -0.02076717]
[2019-04-27 21:34:13,771] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [19.83333333333334, 53.5, 1.0, 2.0, 0.2420952772119576, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 312280.0025699891, 312280.0025699887, 90488.9530578745]
[2019-04-27 21:34:13,771] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 21:34:13,774] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7964085622692145
[2019-04-27 21:34:22,648] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01359302], dtype=float32), -0.02076717]
[2019-04-27 21:34:22,652] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [26.5, 43.5, 1.0, 2.0, 0.3197650932477369, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 403889.3074135773, 403889.3074135773, 118022.1572172754]
[2019-04-27 21:34:22,654] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 21:34:22,657] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5071325338386498
[2019-04-27 21:34:50,386] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.01359302], dtype=float32), -0.02076717]
[2019-04-27 21:34:50,387] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [30.31161795, 34.125765565, 1.0, 2.0, 0.4248126802822126, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 525009.7786612583, 525009.7786612583, 132136.8154200308]
[2019-04-27 21:34:50,388] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 21:34:50,392] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5970618050139025
[2019-04-27 21:35:02,803] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01359302], dtype=float32), -0.02076717]
[2019-04-27 21:35:02,804] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [25.5539445, 84.80781407666666, 1.0, 2.0, 0.5918311327743564, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 679748.7792668606, 679748.7792668606, 156937.9468947961]
[2019-04-27 21:35:02,806] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 21:35:02,809] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6512994305387095
[2019-04-27 21:35:11,537] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.01359302], dtype=float32), -0.02076717]
[2019-04-27 21:35:11,539] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [23.0, 94.0, 1.0, 2.0, 0.5644321758038726, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 664930.55244788, 664930.55244788, 153057.9142538675]
[2019-04-27 21:35:11,540] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 21:35:11,547] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.32151316529638685
[2019-04-27 21:35:23,543] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01359302], dtype=float32), -0.02076717]
[2019-04-27 21:35:23,544] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [32.13333333333334, 61.0, 1.0, 2.0, 0.6899270710408016, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.9260426156618, 1501304.438142344, 1501304.438142345, 315415.6449703826]
[2019-04-27 21:35:23,545] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 21:35:23,547] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.16062712612942265
[2019-04-27 21:35:23,549] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1501304.438142344 W.
[2019-04-27 21:35:55,306] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01359302], dtype=float32), -0.02076717]
[2019-04-27 21:35:55,307] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [27.21196874333333, 49.29655200000001, 1.0, 2.0, 0.7409985741258577, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.926042615659, 913059.6429180708, 913059.6429180708, 186861.5359705988]
[2019-04-27 21:35:55,308] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 21:35:55,310] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7173788444449841
[2019-04-27 21:35:55,517] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8922.6628 2120599135.7723 430.0000
[2019-04-27 21:35:55,590] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8582.8795 2248773970.1662 553.0000
[2019-04-27 21:35:55,627] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8770.2626 2170712315.3188 493.0000
[2019-04-27 21:35:55,696] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8098.4495 2445508074.6415 746.0000
[2019-04-27 21:35:55,710] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8700.0145 2195271999.0442 572.0000
[2019-04-27 21:35:56,764] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 2100000, evaluation results [2100000.0, 8098.4494892610255, 2445508074.64147, 746.0, 8770.262620159681, 2170712315.3187604, 493.0, 8922.662781171732, 2120599135.7722971, 430.0, 8582.879467926026, 2248773970.1662407, 553.0, 8700.014513597167, 2195271999.0442224, 572.0]
[2019-04-27 21:36:01,323] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.8627733e-34 1.0000000e+00 2.4415494e-35 4.2484266e-37 5.7556755e-31], sum to 1.0000
[2019-04-27 21:36:01,328] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2913
[2019-04-27 21:36:01,333] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.6, 75.0, 1.0, 2.0, 0.7515130804270602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 928675.2123514254, 928675.2123514254, 189058.9727252414], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7212000.0000, 
sim time next is 7212600.0000, 
raw observation next is [22.7, 74.5, 1.0, 2.0, 0.7561325571454169, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 934099.6569732078, 934099.6569732078, 189992.3297408723], 
processed observation next is [1.0, 0.4782608695652174, 0.39629629629629626, 0.745, 1.0, 1.0, 0.7096816156493059, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3336070203475742, 0.3336070203475742, 0.36536986488629286], 
reward next is 0.6346, 
noisyNet noise sample is [array([0.7514864], dtype=float32), -0.27402475]. 
=============================================
[2019-04-27 21:36:01,495] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:36:01,505] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0811
[2019-04-27 21:36:01,509] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.58333333333334, 70.66666666666667, 1.0, 2.0, 0.5915496334568758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 728228.5679224565, 728228.5679224565, 158748.8176587292], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7218600.0000, 
sim time next is 7219200.0000, 
raw observation next is [23.66666666666667, 70.33333333333334, 1.0, 2.0, 0.7438052597897375, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 914986.0720293343, 914986.0720293343, 187384.9362588544], 
processed observation next is [1.0, 0.5652173913043478, 0.43209876543209896, 0.7033333333333335, 1.0, 1.0, 0.6950062616544495, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3267807400104765, 0.3267807400104765, 0.3603556466516431], 
reward next is 0.6396, 
noisyNet noise sample is [array([-0.9504054], dtype=float32), -1.2621325]. 
=============================================
[2019-04-27 21:36:01,953] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4414142e-35 1.0000000e+00 0.0000000e+00 7.2648744e-38 2.1641845e-33], sum to 1.0000
[2019-04-27 21:36:01,966] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1320
[2019-04-27 21:36:01,972] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.83333333333334, 78.0, 1.0, 2.0, 0.8295511077946167, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1016203.440701442, 1016203.440701442, 205202.5843017852], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7206600.0000, 
sim time next is 7207200.0000, 
raw observation next is [23.0, 78.0, 1.0, 2.0, 0.8105242384878372, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 990473.2170547874, 990473.2170547874, 201035.9900852104], 
processed observation next is [1.0, 0.43478260869565216, 0.4074074074074074, 0.78, 1.0, 1.0, 0.7744336172474252, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3537404346624241, 0.3537404346624241, 0.3866076732407892], 
reward next is 0.6134, 
noisyNet noise sample is [array([-1.3738306], dtype=float32), 0.65163344]. 
=============================================
[2019-04-27 21:36:03,678] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:36:03,679] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:36:03,751] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res14/Eplus-env-sub_run11
[2019-04-27 21:36:05,508] A3C_AGENT_WORKER-Thread-19 INFO:Local step 132500, global step 2104274: loss 2.0103
[2019-04-27 21:36:05,510] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 132500, global step 2104274: learning rate 0.0001
[2019-04-27 21:36:09,790] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:36:09,800] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5787
[2019-04-27 21:36:09,810] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.2, 87.0, 1.0, 2.0, 0.4136590595633177, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 509992.1769441743, 509992.1769441743, 130499.9192044718], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7340400.0000, 
sim time next is 7341000.0000, 
raw observation next is [21.16666666666667, 86.83333333333333, 1.0, 2.0, 0.4113007545556291, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 507461.3409748423, 507461.3409748423, 130172.0346358785], 
processed observation next is [1.0, 1.0, 0.33950617283950635, 0.8683333333333333, 1.0, 1.0, 0.29916756494717744, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.18123619320530082, 0.18123619320530082, 0.2503308358382279], 
reward next is 0.7497, 
noisyNet noise sample is [array([-0.83750534], dtype=float32), -1.69878]. 
=============================================
[2019-04-27 21:36:09,826] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[89.98844]
 [89.7296 ]
 [89.70694]
 [89.66779]
 [89.62089]], R is [[89.93727112]
 [89.78694153]
 [89.63758087]
 [89.48912048]
 [89.34153748]].
[2019-04-27 21:36:10,301] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:36:10,314] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6448
[2019-04-27 21:36:10,318] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.33333333333333, 86.33333333333334, 1.0, 2.0, 0.3929517132257688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 490905.4678988244, 490905.4678988244, 127712.7277259831], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7353600.0000, 
sim time next is 7354200.0000, 
raw observation next is [20.26666666666667, 87.16666666666667, 1.0, 2.0, 0.3895647469726781, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 486458.2937883403, 486458.2937883403, 127236.1665439605], 
processed observation next is [1.0, 0.08695652173913043, 0.3061728395061729, 0.8716666666666667, 1.0, 1.0, 0.27329136544366445, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.17373510492440725, 0.17373510492440725, 0.2446849356614625], 
reward next is 0.7553, 
noisyNet noise sample is [array([-0.5081266], dtype=float32), 0.24564254]. 
=============================================
[2019-04-27 21:36:11,802] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:36:11,803] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:36:11,881] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res3/Eplus-env-sub_run11
[2019-04-27 21:36:13,433] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:36:13,439] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6086
[2019-04-27 21:36:13,444] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.13333333333334, 90.83333333333334, 1.0, 2.0, 0.3834973303631854, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 476837.6561346826, 476837.6561346826, 126354.5436506285], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7429800.0000, 
sim time next is 7430400.0000, 
raw observation next is [20.1, 91.0, 1.0, 2.0, 0.3828670459072693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 476140.5873112579, 476140.5873112579, 126269.3668881913], 
processed observation next is [0.0, 0.0, 0.30000000000000004, 0.91, 1.0, 1.0, 0.2653179117943682, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1700502097540207, 0.1700502097540207, 0.24282570555421404], 
reward next is 0.7572, 
noisyNet noise sample is [array([-2.5769653], dtype=float32), -2.5311346]. 
=============================================
[2019-04-27 21:36:13,649] A3C_AGENT_WORKER-Thread-3 INFO:Local step 132500, global step 2108270: loss 0.2968
[2019-04-27 21:36:13,652] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 132500, global step 2108273: learning rate 0.0001
[2019-04-27 21:36:18,553] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:36:18,559] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4910
[2019-04-27 21:36:18,563] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.95, 96.0, 1.0, 2.0, 0.4382123090340981, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 533425.6217944389, 533425.6217944389, 133881.9930066861], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7540200.0000, 
sim time next is 7540800.0000, 
raw observation next is [20.96666666666667, 96.0, 1.0, 2.0, 0.4383477626756594, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 533445.6106477823, 533445.6106477823, 133897.5954242102], 
processed observation next is [0.0, 0.2608695652173913, 0.3320987654320988, 0.96, 1.0, 1.0, 0.3313663841376898, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.19051628951706512, 0.19051628951706512, 0.2574953758157888], 
reward next is 0.7425, 
noisyNet noise sample is [array([-0.9841454], dtype=float32), 0.7674659]. 
=============================================
[2019-04-27 21:36:22,120] A3C_AGENT_WORKER-Thread-19 INFO:Local step 133000, global step 2112299: loss 0.0625
[2019-04-27 21:36:22,121] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 133000, global step 2112299: learning rate 0.0001
[2019-04-27 21:36:24,150] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0483296e-25 1.0000000e+00 9.0172868e-28 2.4569379e-27 2.4605471e-21], sum to 1.0000
[2019-04-27 21:36:24,162] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5388
[2019-04-27 21:36:24,169] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1307118.55189039 W.
[2019-04-27 21:36:24,174] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.0, 82.0, 1.0, 2.0, 0.9673400387820368, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.212757470413331, 6.9112, 121.9246107076428, 1307118.55189039, 1152695.987708657, 235485.2394792231], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7639200.0000, 
sim time next is 7639800.0000, 
raw observation next is [24.26666666666667, 81.33333333333334, 1.0, 2.0, 0.5512200205780138, 0.0, 2.0, 0.0, 1.0, 1.0, 0.8807709293145052, 6.9112, 6.9112, 121.9258609893915, 1289114.336815178, 1289114.336815178, 274749.9959791228], 
processed observation next is [1.0, 0.43478260869565216, 0.4543209876543211, 0.8133333333333335, 1.0, 1.0, 0.46573811973573065, 0.0, 1.0, -0.1904761904761905, 1.0, 0.5, 0.8509636616431315, 0.0, 0.0, 0.8094609230105791, 0.46039797743399213, 0.46039797743399213, 0.5283653768829285], 
reward next is 0.4716, 
noisyNet noise sample is [array([1.080412], dtype=float32), -0.19506955]. 
=============================================
[2019-04-27 21:36:24,486] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:36:24,494] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5657
[2019-04-27 21:36:24,498] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.31666666666667, 85.16666666666667, 1.0, 2.0, 0.4076721427197446, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 503278.9890624441, 503278.9890624441, 129662.2266479852], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7609800.0000, 
sim time next is 7610400.0000, 
raw observation next is [21.2, 85.0, 1.0, 2.0, 0.4014721582153116, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 496591.7599133955, 496591.7599133955, 128807.2846330274], 
processed observation next is [1.0, 0.08695652173913043, 0.34074074074074073, 0.85, 1.0, 1.0, 0.2874668550182281, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.17735419996906981, 0.17735419996906981, 0.24770631660197578], 
reward next is 0.7523, 
noisyNet noise sample is [array([-0.1183171], dtype=float32), 0.28911024]. 
=============================================
[2019-04-27 21:36:29,165] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:36:29,172] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3150
[2019-04-27 21:36:29,175] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.2, 71.0, 1.0, 2.0, 0.2549896013900424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 328916.0272857442, 328916.0272857442, 109818.198302787], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7700400.0000, 
sim time next is 7701000.0000, 
raw observation next is [19.1, 73.0, 1.0, 2.0, 0.2654754295834408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 342147.3830642386, 342147.3830642386, 111405.6482483123], 
processed observation next is [1.0, 0.13043478260869565, 0.262962962962963, 0.73, 1.0, 1.0, 0.12556598759933427, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.12219549395151379, 0.12219549395151379, 0.2142416312467544], 
reward next is 0.7858, 
noisyNet noise sample is [array([-1.8039938], dtype=float32), 1.5398034]. 
=============================================
[2019-04-27 21:36:29,189] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[66.881645]
 [66.88144 ]
 [67.26842 ]
 [67.942795]
 [68.43825 ]], R is [[66.87756348]
 [66.99759674]
 [67.12236023]
 [67.25094604]
 [67.38236237]].
[2019-04-27 21:36:30,433] A3C_AGENT_WORKER-Thread-3 INFO:Local step 133000, global step 2116260: loss 0.0600
[2019-04-27 21:36:30,434] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 133000, global step 2116260: learning rate 0.0001
[2019-04-27 21:36:30,928] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:36:30,929] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:36:30,996] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res11/Eplus-env-sub_run11
[2019-04-27 21:36:32,711] A3C_AGENT_WORKER-Thread-16 INFO:Local step 132500, global step 2117419: loss 0.1652
[2019-04-27 21:36:32,714] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 132500, global step 2117421: learning rate 0.0001
[2019-04-27 21:36:35,688] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3120508e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 21:36:35,698] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7194
[2019-04-27 21:36:35,703] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.11666666666667, 58.0, 1.0, 2.0, 0.3520785537122555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 446602.3152498361, 446602.3152498361, 122237.3794916017], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7798200.0000, 
sim time next is 7798800.0000, 
raw observation next is [23.33333333333334, 57.0, 1.0, 2.0, 0.3194812387424169, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 405081.6681245176, 405081.6681245171, 118001.4888134094], 
processed observation next is [1.0, 0.2608695652173913, 0.4197530864197533, 0.57, 1.0, 1.0, 0.18985861755049632, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.14467202433018486, 0.14467202433018467, 0.2269259400257873], 
reward next is 0.7731, 
noisyNet noise sample is [array([1.8478432], dtype=float32), -2.392953]. 
=============================================
[2019-04-27 21:36:38,666] A3C_AGENT_WORKER-Thread-19 INFO:Local step 133500, global step 2120248: loss 0.1505
[2019-04-27 21:36:38,670] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 133500, global step 2120248: learning rate 0.0001
[2019-04-27 21:36:42,204] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:36:42,205] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:36:42,209] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:36:42,210] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:36:42,249] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:36:42,250] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:36:42,260] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:36:42,260] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:36:42,277] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res12/Eplus-env-sub_run11
[2019-04-27 21:36:42,281] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res16/Eplus-env-sub_run11
[2019-04-27 21:36:42,366] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res7/Eplus-env-sub_run11
[2019-04-27 21:36:42,404] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res10/Eplus-env-sub_run11
[2019-04-27 21:36:42,525] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:36:42,525] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:36:42,561] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res2/Eplus-env-sub_run11
[2019-04-27 21:36:42,791] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:36:42,791] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:36:42,795] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res4/Eplus-env-sub_run11
[2019-04-27 21:36:43,012] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:36:43,013] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:36:43,026] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res9/Eplus-env-sub_run11
[2019-04-27 21:36:43,173] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:36:43,173] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:36:43,180] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res13/Eplus-env-sub_run11
[2019-04-27 21:36:43,220] A3C_AGENT_WORKER-Thread-3 INFO:Local step 133500, global step 2122395: loss 0.0565
[2019-04-27 21:36:43,220] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 133500, global step 2122395: learning rate 0.0001
[2019-04-27 21:36:43,228] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:36:43,228] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:36:43,232] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res6/Eplus-env-sub_run11
[2019-04-27 21:36:43,257] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:36:43,258] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:36:43,262] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res17/Eplus-env-sub_run11
[2019-04-27 21:36:43,361] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:36:43,361] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:36:43,364] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res15/Eplus-env-sub_run11
[2019-04-27 21:36:43,509] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:36:43,509] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:36:43,513] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res8/Eplus-env-sub_run11
[2019-04-27 21:36:43,704] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:36:43,706] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:36:43,718] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res5/Eplus-env-sub_run11
[2019-04-27 21:36:43,934] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:36:43,934] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4352
[2019-04-27 21:36:43,940] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.2, 49.5, 1.0, 2.0, 0.2523798290333188, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 325548.9168890951, 325548.9168890951, 90805.12667294705], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 279000.0000, 
sim time next is 279600.0000, 
raw observation next is [20.43333333333333, 48.33333333333333, 1.0, 2.0, 0.2545765649639217, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 328383.1295367532, 328383.1295367532, 91269.9272443438], 
processed observation next is [0.0, 0.21739130434782608, 0.31234567901234556, 0.4833333333333333, 1.0, 1.0, 0.11259114876657349, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.117279689120269, 0.117279689120269, 0.1755190908545073], 
reward next is 0.8245, 
noisyNet noise sample is [array([0.07203263], dtype=float32), -1.3852897]. 
=============================================
[2019-04-27 21:36:44,075] A3C_AGENT_WORKER-Thread-21 INFO:Local step 132500, global step 2122575: loss 0.9427
[2019-04-27 21:36:44,077] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 132500, global step 2122576: learning rate 0.0001
[2019-04-27 21:36:44,093] A3C_AGENT_WORKER-Thread-17 INFO:Local step 132500, global step 2122592: loss 0.8614
[2019-04-27 21:36:44,119] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 132500, global step 2122597: learning rate 0.0001
[2019-04-27 21:36:44,236] A3C_AGENT_WORKER-Thread-15 INFO:Local step 132500, global step 2122658: loss 0.5623
[2019-04-27 21:36:44,241] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 132500, global step 2122659: learning rate 0.0001
[2019-04-27 21:36:44,253] A3C_AGENT_WORKER-Thread-12 INFO:Local step 132500, global step 2122664: loss 0.2572
[2019-04-27 21:36:44,257] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 132500, global step 2122664: learning rate 0.0001
[2019-04-27 21:36:44,284] A3C_AGENT_WORKER-Thread-2 INFO:Local step 132500, global step 2122679: loss 0.2068
[2019-04-27 21:36:44,286] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 132500, global step 2122679: learning rate 0.0001
[2019-04-27 21:36:44,556] A3C_AGENT_WORKER-Thread-9 INFO:Local step 132500, global step 2122826: loss 0.1400
[2019-04-27 21:36:44,556] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 132500, global step 2122826: learning rate 0.0001
[2019-04-27 21:36:44,775] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2774764e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 21:36:44,775] A3C_AGENT_WORKER-Thread-14 INFO:Local step 132500, global step 2122966: loss 1.1450
[2019-04-27 21:36:44,777] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 132500, global step 2122966: learning rate 0.0001
[2019-04-27 21:36:44,780] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2030
[2019-04-27 21:36:44,788] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1584866.754104606 W.
[2019-04-27 21:36:44,792] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [35.7, 20.0, 1.0, 2.0, 0.7147346030382905, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9567469287150907, 6.9112, 6.9112, 121.9260426156618, 1584866.754104606, 1584866.754104606, 308674.8854991555], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 655200.0000, 
sim time next is 655800.0000, 
raw observation next is [35.7, 19.83333333333334, 1.0, 2.0, 0.7167854140042433, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9566245915536471, 6.911199999999999, 6.9112, 121.9260426156618, 1587911.277202085, 1587911.277202085, 308940.4512750455], 
processed observation next is [1.0, 0.6086956521739131, 0.8777777777777779, 0.1983333333333334, 1.0, 1.0, 0.6628397785764801, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.9457807394420589, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.567111170429316, 0.567111170429316, 0.5941162524520106], 
reward next is 0.4059, 
noisyNet noise sample is [array([-1.2141597], dtype=float32), 0.24869421]. 
=============================================
[2019-04-27 21:36:44,905] A3C_AGENT_WORKER-Thread-16 INFO:Local step 133000, global step 2123046: loss 0.0410
[2019-04-27 21:36:44,908] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 133000, global step 2123046: learning rate 0.0001
[2019-04-27 21:36:45,005] A3C_AGENT_WORKER-Thread-18 INFO:Local step 132500, global step 2123109: loss 4.8002
[2019-04-27 21:36:45,007] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 132500, global step 2123110: learning rate 0.0001
[2019-04-27 21:36:45,046] A3C_AGENT_WORKER-Thread-11 INFO:Local step 132500, global step 2123133: loss 3.8122
[2019-04-27 21:36:45,048] A3C_AGENT_WORKER-Thread-22 INFO:Local step 132500, global step 2123134: loss 5.5674
[2019-04-27 21:36:45,049] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 132500, global step 2123134: learning rate 0.0001
[2019-04-27 21:36:45,052] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 132500, global step 2123134: learning rate 0.0001
[2019-04-27 21:36:45,243] A3C_AGENT_WORKER-Thread-20 INFO:Local step 132500, global step 2123232: loss 2.3481
[2019-04-27 21:36:45,244] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 132500, global step 2123232: learning rate 0.0001
[2019-04-27 21:36:45,458] A3C_AGENT_WORKER-Thread-13 INFO:Local step 132500, global step 2123333: loss 0.7233
[2019-04-27 21:36:45,458] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 132500, global step 2123333: learning rate 0.0001
[2019-04-27 21:36:45,660] A3C_AGENT_WORKER-Thread-10 INFO:Local step 132500, global step 2123429: loss 0.6757
[2019-04-27 21:36:45,663] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 132500, global step 2123430: learning rate 0.0001
[2019-04-27 21:36:48,924] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-27 21:36:48,925] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 21:36:48,926] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 21:36:48,926] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 21:36:48,928] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 21:36:48,929] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 21:36:48,929] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:36:48,930] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:36:48,931] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:36:48,928] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:36:48,929] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:36:48,962] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run86
[2019-04-27 21:36:48,962] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run86
[2019-04-27 21:36:49,020] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run86
[2019-04-27 21:36:49,045] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run86
[2019-04-27 21:36:49,070] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run86
[2019-04-27 21:36:52,622] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.01250236], dtype=float32), -0.018312067]
[2019-04-27 21:36:52,624] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [13.66666666666667, 73.66666666666667, 1.0, 2.0, 0.1682609535167364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 217027.2365776231, 217027.2365776226, 70811.4331230249]
[2019-04-27 21:36:52,625] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 21:36:52,627] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7301183830747626
[2019-04-27 21:37:12,968] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01250236], dtype=float32), -0.018312067]
[2019-04-27 21:37:12,969] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [19.57513178, 78.982934895, 1.0, 2.0, 0.3027882440702061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 385053.390055465, 385053.390055465, 115914.3302825563]
[2019-04-27 21:37:12,970] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 21:37:12,973] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.25354028287523844
[2019-04-27 21:37:31,699] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.01250236], dtype=float32), -0.018312067]
[2019-04-27 21:37:31,701] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [23.01822042, 88.13225963, 1.0, 2.0, 0.5017588471145764, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 598994.8655256841, 598994.8655256841, 143191.4829413673]
[2019-04-27 21:37:31,702] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 21:37:31,706] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8894633122236721
[2019-04-27 21:37:58,568] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01250236], dtype=float32), -0.018312067]
[2019-04-27 21:37:58,570] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [22.93333333333333, 73.0, 1.0, 2.0, 0.4078145046296879, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 504152.3065149585, 504152.3065149585, 129699.2048564748]
[2019-04-27 21:37:58,572] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 21:37:58,574] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.14566777355629767
[2019-04-27 21:38:09,675] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01250236], dtype=float32), -0.018312067]
[2019-04-27 21:38:09,675] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [26.91666666666666, 63.83333333333333, 1.0, 2.0, 0.5862730935347223, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 697844.5137136865, 697844.513713686, 157050.8746128345]
[2019-04-27 21:38:09,676] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 21:38:09,678] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.396890221924626
[2019-04-27 21:38:12,119] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.01250236], dtype=float32), -0.018312067]
[2019-04-27 21:38:12,120] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [25.40898484833333, 54.88610835, 1.0, 2.0, 0.4414166720864592, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 547464.1986682363, 547464.1986682358, 134617.5285917812]
[2019-04-27 21:38:12,122] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 21:38:12,124] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3835559022976893
[2019-04-27 21:38:12,453] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.01250236], dtype=float32), -0.018312067]
[2019-04-27 21:38:12,455] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [27.0, 87.33333333333334, 1.0, 2.0, 0.9835847124433381, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.043524473677556, 6.9112, 121.9254055731217, 1189055.970907027, 1121294.367401406, 236791.1891494809]
[2019-04-27 21:38:12,456] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 21:38:12,458] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2864764783093853
[2019-04-27 21:38:25,704] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01250236], dtype=float32), -0.018312067]
[2019-04-27 21:38:25,706] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [21.95664028, 71.89392573, 1.0, 2.0, 0.3991524534548847, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 499893.0524077345, 499893.0524077345, 128604.6302443959]
[2019-04-27 21:38:25,708] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 21:38:25,711] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.011218218826218096
[2019-04-27 21:38:33,500] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8770.2626 2170712315.3188 493.0000
[2019-04-27 21:38:33,585] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8700.0145 2195271999.0442 572.0000
[2019-04-27 21:38:33,642] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8922.6628 2120599135.7723 430.0000
[2019-04-27 21:38:33,786] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8098.4495 2445508074.6415 746.0000
[2019-04-27 21:38:33,810] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8582.8795 2248773970.1662 553.0000
[2019-04-27 21:38:34,831] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 2125000, evaluation results [2125000.0, 8098.4494892610255, 2445508074.64147, 746.0, 8770.262620159681, 2170712315.3187604, 493.0, 8922.662781171732, 2120599135.7722971, 430.0, 8582.879467926026, 2248773970.1662407, 553.0, 8700.014513597167, 2195271999.0442224, 572.0]
[2019-04-27 21:38:36,164] A3C_AGENT_WORKER-Thread-19 INFO:Local step 134000, global step 2125640: loss 0.0048
[2019-04-27 21:38:36,167] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 134000, global step 2125640: learning rate 0.0001
[2019-04-27 21:38:36,376] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.455369e-34 1.000000e+00 4.422183e-37 0.000000e+00 3.439179e-31], sum to 1.0000
[2019-04-27 21:38:36,384] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0295
[2019-04-27 21:38:36,396] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1395659.185421556 W.
[2019-04-27 21:38:36,399] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [35.86666666666667, 21.0, 1.0, 2.0, 0.3883855821798568, 1.0, 1.0, 0.3883855821798568, 1.0, 1.0, 0.6263743261767046, 6.911200000000001, 6.9112, 121.94756008, 1395659.185421556, 1395659.185421555, 291626.0279411229], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 134400.0000, 
sim time next is 135000.0000, 
raw observation next is [36.25, 19.5, 1.0, 2.0, 0.5849735461291838, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9505772482910019, 6.911200000000001, 6.9112, 121.9260425222466, 1418533.827649422, 1418533.827649422, 285538.3702875629], 
processed observation next is [1.0, 0.5652173913043478, 0.8981481481481481, 0.195, 1.0, 1.0, 0.5059208882490284, 0.0, 0.5, -0.1904761904761905, 1.0, 1.0, 0.9382215603637523, 8.881784197001253e-17, 0.0, 0.8094621281999562, 0.5066192241605079, 0.5066192241605079, 0.5491122505530056], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.49664858], dtype=float32), 1.0469632]. 
=============================================
[2019-04-27 21:38:36,408] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[71.94634 ]
 [72.55599 ]
 [72.14214 ]
 [72.19782 ]
 [72.111465]], R is [[72.26303864]
 [71.97958374]
 [71.25978851]
 [70.54718781]
 [70.31237793]].
[2019-04-27 21:38:44,652] A3C_AGENT_WORKER-Thread-3 INFO:Local step 134000, global step 2129671: loss 0.0102
[2019-04-27 21:38:44,654] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 134000, global step 2129671: learning rate 0.0001
[2019-04-27 21:38:45,359] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:38:45,363] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4395
[2019-04-27 21:38:45,368] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.7, 30.16666666666666, 1.0, 2.0, 0.3195674631771831, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 412238.6507264663, 412238.6507264663, 115532.5916176425], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 303000.0000, 
sim time next is 303600.0000, 
raw observation next is [26.8, 30.33333333333334, 1.0, 2.0, 0.317767662704615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 409916.3066156639, 409916.3066156639, 116732.6948921115], 
processed observation next is [0.0, 0.5217391304347826, 0.5481481481481482, 0.3033333333333334, 1.0, 1.0, 0.1878186460769226, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1463986809341657, 0.1463986809341657, 0.22448595171559904], 
reward next is 0.7755, 
noisyNet noise sample is [array([-1.1048282], dtype=float32), -0.46437344]. 
=============================================
[2019-04-27 21:38:45,845] A3C_AGENT_WORKER-Thread-21 INFO:Local step 133000, global step 2130226: loss 0.0007
[2019-04-27 21:38:45,848] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 133000, global step 2130229: learning rate 0.0001
[2019-04-27 21:38:46,255] A3C_AGENT_WORKER-Thread-12 INFO:Local step 133000, global step 2130413: loss 0.0127
[2019-04-27 21:38:46,260] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 133000, global step 2130415: learning rate 0.0001
[2019-04-27 21:38:46,321] A3C_AGENT_WORKER-Thread-17 INFO:Local step 133000, global step 2130444: loss 0.0016
[2019-04-27 21:38:46,326] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 133000, global step 2130444: learning rate 0.0001
[2019-04-27 21:38:46,397] A3C_AGENT_WORKER-Thread-15 INFO:Local step 133000, global step 2130481: loss 0.0011
[2019-04-27 21:38:46,401] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 133000, global step 2130481: learning rate 0.0001
[2019-04-27 21:38:46,469] A3C_AGENT_WORKER-Thread-2 INFO:Local step 133000, global step 2130521: loss 0.0076
[2019-04-27 21:38:46,471] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 133000, global step 2130523: learning rate 0.0001
[2019-04-27 21:38:46,921] A3C_AGENT_WORKER-Thread-9 INFO:Local step 133000, global step 2130740: loss 0.0008
[2019-04-27 21:38:46,925] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 133000, global step 2130740: learning rate 0.0001
[2019-04-27 21:38:47,529] A3C_AGENT_WORKER-Thread-14 INFO:Local step 133000, global step 2131018: loss 0.0021
[2019-04-27 21:38:47,532] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 133000, global step 2131020: learning rate 0.0001
[2019-04-27 21:38:47,593] A3C_AGENT_WORKER-Thread-18 INFO:Local step 133000, global step 2131051: loss 0.0107
[2019-04-27 21:38:47,595] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 133000, global step 2131052: learning rate 0.0001
[2019-04-27 21:38:47,769] A3C_AGENT_WORKER-Thread-22 INFO:Local step 133000, global step 2131134: loss 0.0006
[2019-04-27 21:38:47,770] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 133000, global step 2131134: learning rate 0.0001
[2019-04-27 21:38:47,783] A3C_AGENT_WORKER-Thread-16 INFO:Local step 133500, global step 2131140: loss 0.0893
[2019-04-27 21:38:47,786] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 133500, global step 2131140: learning rate 0.0001
[2019-04-27 21:38:47,903] A3C_AGENT_WORKER-Thread-11 INFO:Local step 133000, global step 2131196: loss 0.0083
[2019-04-27 21:38:47,907] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 133000, global step 2131196: learning rate 0.0001
[2019-04-27 21:38:48,124] A3C_AGENT_WORKER-Thread-20 INFO:Local step 133000, global step 2131305: loss 0.0057
[2019-04-27 21:38:48,128] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 133000, global step 2131308: learning rate 0.0001
[2019-04-27 21:38:48,351] A3C_AGENT_WORKER-Thread-13 INFO:Local step 133000, global step 2131418: loss 0.0024
[2019-04-27 21:38:48,355] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 133000, global step 2131418: learning rate 0.0001
[2019-04-27 21:38:48,488] A3C_AGENT_WORKER-Thread-10 INFO:Local step 133000, global step 2131479: loss 0.0132
[2019-04-27 21:38:48,489] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 133000, global step 2131479: learning rate 0.0001
[2019-04-27 21:38:51,806] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:38:51,816] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0262
[2019-04-27 21:38:51,819] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.65, 27.33333333333333, 1.0, 2.0, 0.2578166369787893, 1.0, 2.0, 0.2578166369787893, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260425076617, 641823.5184360252, 641823.5184360257, 172759.3176813691], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 407400.0000, 
sim time next is 408000.0000, 
raw observation next is [30.5, 27.66666666666667, 1.0, 2.0, 0.3652202104765608, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156289, 460355.248044136, 460355.248044136, 123966.263234369], 
processed observation next is [1.0, 0.7391304347826086, 0.6851851851851852, 0.2766666666666667, 1.0, 1.0, 0.24430977437685808, 0.0, 0.5, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288199175, 0.16441258858719143, 0.16441258858719143, 0.23839666006609425], 
reward next is 0.7616, 
noisyNet noise sample is [array([0.14272965], dtype=float32), 1.2619792]. 
=============================================
[2019-04-27 21:38:51,834] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[75.16224 ]
 [73.76653 ]
 [72.692055]
 [72.71678 ]
 [71.95826 ]], R is [[76.08447266]
 [75.99140167]
 [75.23149109]
 [74.47917938]
 [74.20615387]].
[2019-04-27 21:38:51,855] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:38:51,866] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0703
[2019-04-27 21:38:51,872] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.95, 34.33333333333334, 1.0, 2.0, 0.3329472870055464, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 423051.2240079078, 423051.2240079078, 119734.494002285], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 418200.0000, 
sim time next is 418800.0000, 
raw observation next is [27.8, 34.66666666666667, 1.0, 2.0, 0.3295868928892844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 418983.9382123344, 418983.9382123344, 119301.9522239365], 
processed observation next is [1.0, 0.8695652173913043, 0.5851851851851853, 0.34666666666666673, 1.0, 1.0, 0.20188915820152903, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.14963712079011943, 0.14963712079011943, 0.2294268311998779], 
reward next is 0.7706, 
noisyNet noise sample is [array([-0.61175555], dtype=float32), -0.20431343]. 
=============================================
[2019-04-27 21:38:52,657] A3C_AGENT_WORKER-Thread-19 INFO:Local step 134500, global step 2133476: loss 60.1598
[2019-04-27 21:38:52,658] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 134500, global step 2133476: learning rate 0.0001
[2019-04-27 21:38:56,620] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [6.1644258e-21 1.0000000e+00 2.6307920e-23 5.0670538e-25 1.3411207e-17], sum to 1.0000
[2019-04-27 21:38:56,630] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6886
[2019-04-27 21:38:56,634] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.68333333333333, 38.83333333333334, 1.0, 2.0, 0.9185295905513485, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.153145127307548, 6.9112, 121.9249874527693, 1265530.875843683, 1141634.418076511, 225652.5152665732], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 471000.0000, 
sim time next is 471600.0000, 
raw observation next is [29.0, 38.0, 1.0, 2.0, 0.9560595324983454, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.418996338605633, 6.9112, 121.92379758647, 1450996.116858498, 1190963.793244622, 234593.218196394], 
processed observation next is [1.0, 0.4782608695652174, 0.6296296296296297, 0.38, 1.0, 1.0, 0.9476899196408874, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.05077963386056332, 0.0, 0.8094472241609683, 0.518212898878035, 0.42534421187307925, 0.4511408042238346], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.996626], dtype=float32), 0.8326409]. 
=============================================
[2019-04-27 21:38:57,659] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:38:57,668] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3656
[2019-04-27 21:38:57,672] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.1, 35.0, 1.0, 2.0, 0.3462055673262171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 434944.1649709155, 434944.1649709155, 121411.1316450534], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 502200.0000, 
sim time next is 502800.0000, 
raw observation next is [28.86666666666667, 36.0, 1.0, 2.0, 0.344533132770746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 432678.9477112515, 432678.9477112515, 121188.7617148748], 
processed observation next is [1.0, 0.8260869565217391, 0.6246913580246916, 0.36, 1.0, 1.0, 0.21968230091755475, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.15452819561116124, 0.15452819561116124, 0.23305531099014384], 
reward next is 0.7669, 
noisyNet noise sample is [array([-0.9397831], dtype=float32), 0.024431229]. 
=============================================
[2019-04-27 21:39:01,211] A3C_AGENT_WORKER-Thread-3 INFO:Local step 134500, global step 2137523: loss 19.6858
[2019-04-27 21:39:01,213] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 134500, global step 2137525: learning rate 0.0001
[2019-04-27 21:39:02,742] A3C_AGENT_WORKER-Thread-21 INFO:Local step 133500, global step 2138249: loss 0.2090
[2019-04-27 21:39:02,745] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 133500, global step 2138249: learning rate 0.0001
[2019-04-27 21:39:03,156] A3C_AGENT_WORKER-Thread-15 INFO:Local step 133500, global step 2138439: loss 0.2514
[2019-04-27 21:39:03,157] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 133500, global step 2138439: learning rate 0.0001
[2019-04-27 21:39:03,282] A3C_AGENT_WORKER-Thread-17 INFO:Local step 133500, global step 2138499: loss 0.1668
[2019-04-27 21:39:03,286] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 133500, global step 2138501: learning rate 0.0001
[2019-04-27 21:39:03,287] A3C_AGENT_WORKER-Thread-12 INFO:Local step 133500, global step 2138501: loss 0.1307
[2019-04-27 21:39:03,290] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 133500, global step 2138502: learning rate 0.0001
[2019-04-27 21:39:03,441] A3C_AGENT_WORKER-Thread-2 INFO:Local step 133500, global step 2138575: loss 0.2069
[2019-04-27 21:39:03,444] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 133500, global step 2138576: learning rate 0.0001
[2019-04-27 21:39:03,842] A3C_AGENT_WORKER-Thread-9 INFO:Local step 133500, global step 2138760: loss 0.1896
[2019-04-27 21:39:03,843] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 133500, global step 2138760: learning rate 0.0001
[2019-04-27 21:39:04,370] A3C_AGENT_WORKER-Thread-18 INFO:Local step 133500, global step 2139014: loss 0.2015
[2019-04-27 21:39:04,372] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 133500, global step 2139014: learning rate 0.0001
[2019-04-27 21:39:04,583] A3C_AGENT_WORKER-Thread-22 INFO:Local step 133500, global step 2139118: loss 0.1942
[2019-04-27 21:39:04,586] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 133500, global step 2139119: learning rate 0.0001
[2019-04-27 21:39:04,590] A3C_AGENT_WORKER-Thread-14 INFO:Local step 133500, global step 2139120: loss 0.1760
[2019-04-27 21:39:04,593] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 133500, global step 2139121: learning rate 0.0001
[2019-04-27 21:39:04,635] A3C_AGENT_WORKER-Thread-16 INFO:Local step 134000, global step 2139140: loss 0.0020
[2019-04-27 21:39:04,639] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 134000, global step 2139142: learning rate 0.0001
[2019-04-27 21:39:04,817] A3C_AGENT_WORKER-Thread-11 INFO:Local step 133500, global step 2139224: loss 0.1078
[2019-04-27 21:39:04,819] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 133500, global step 2139225: learning rate 0.0001
[2019-04-27 21:39:05,048] A3C_AGENT_WORKER-Thread-20 INFO:Local step 133500, global step 2139332: loss 0.0587
[2019-04-27 21:39:05,051] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 133500, global step 2139332: learning rate 0.0001
[2019-04-27 21:39:05,347] A3C_AGENT_WORKER-Thread-13 INFO:Local step 133500, global step 2139481: loss 0.0611
[2019-04-27 21:39:05,351] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 133500, global step 2139481: learning rate 0.0001
[2019-04-27 21:39:05,452] A3C_AGENT_WORKER-Thread-10 INFO:Local step 133500, global step 2139527: loss 0.0940
[2019-04-27 21:39:05,453] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 133500, global step 2139527: learning rate 0.0001
[2019-04-27 21:39:05,510] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:39:05,518] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7979
[2019-04-27 21:39:05,521] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.7, 25.0, 1.0, 2.0, 0.3829381785031569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 479411.8104910203, 479411.8104910203, 126339.2833141712], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1447200.0000, 
sim time next is 1447800.0000, 
raw observation next is [32.55, 25.33333333333334, 1.0, 2.0, 0.3871524511423226, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 484795.6218310964, 484795.6218310964, 126924.6857038722], 
processed observation next is [0.0, 0.782608695652174, 0.761111111111111, 0.2533333333333334, 1.0, 1.0, 0.2704195846932412, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.17314129351110585, 0.17314129351110585, 0.24408593404590806], 
reward next is 0.7559, 
noisyNet noise sample is [array([0.10099868], dtype=float32), -0.34334067]. 
=============================================
[2019-04-27 21:39:06,352] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.6515078e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 21:39:06,359] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2893
[2019-04-27 21:39:06,363] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.3, 87.0, 1.0, 2.0, 0.3514324460026781, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 444038.8394040149, 444038.8394040149, 122134.6287127744], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1305000.0000, 
sim time next is 1305600.0000, 
raw observation next is [19.23333333333333, 87.33333333333334, 1.0, 2.0, 0.3408228909110523, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 430791.3083508848, 430791.3083508848, 120739.0953581111], 
processed observation next is [1.0, 0.08695652173913043, 0.26790123456790116, 0.8733333333333334, 1.0, 1.0, 0.2152653463226813, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.15385403869674458, 0.15385403869674458, 0.2321905679963675], 
reward next is 0.7678, 
noisyNet noise sample is [array([2.7047281], dtype=float32), 0.73263997]. 
=============================================
[2019-04-27 21:39:08,877] A3C_AGENT_WORKER-Thread-19 INFO:Local step 135000, global step 2141147: loss 0.0663
[2019-04-27 21:39:08,878] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 135000, global step 2141147: learning rate 0.0001
[2019-04-27 21:39:10,448] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.3978689e-30 1.0000000e+00 9.9298841e-35 4.3356768e-36 9.8841546e-35], sum to 1.0000
[2019-04-27 21:39:10,455] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3976
[2019-04-27 21:39:10,460] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.05, 23.5, 1.0, 2.0, 0.511939020394135, 1.0, 2.0, 0.511939020394135, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.926042538785, 1266719.213319599, 1266719.213319599, 242873.4595982668], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 747000.0000, 
sim time next is 747600.0000, 
raw observation next is [32.03333333333333, 23.66666666666667, 1.0, 2.0, 0.9337386446895442, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.322489537283717, 6.9112, 121.924292551139, 1383670.428933538, 1173056.448292127, 229489.9715788806], 
processed observation next is [1.0, 0.6521739130434783, 0.7419753086419753, 0.23666666666666672, 1.0, 1.0, 0.9211174341542192, 0.0, 0.5, -0.1904761904761905, 0.0, 1.0, -0.25, 0.041128953728371706, 0.0, 0.8094505102116434, 0.49416801033340646, 0.4189487315329025, 0.4413268684209242], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.38823494], dtype=float32), 0.7310533]. 
=============================================
[2019-04-27 21:39:12,083] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:39:12,093] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1563
[2019-04-27 21:39:12,099] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.93333333333333, 28.33333333333334, 1.0, 2.0, 0.3464880502968761, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 439587.1093489051, 439587.1093489051, 121498.261594389], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 762000.0000, 
sim time next is 762600.0000, 
raw observation next is [29.81666666666667, 28.66666666666667, 1.0, 2.0, 0.3453722250897368, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 438211.0683574979, 438211.0683574979, 121351.6083655912], 
processed observation next is [1.0, 0.8260869565217391, 0.6598765432098767, 0.28666666666666674, 1.0, 1.0, 0.22068122034492477, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1565039529848207, 0.1565039529848207, 0.2333684776261369], 
reward next is 0.7666, 
noisyNet noise sample is [array([0.1361069], dtype=float32), -1.2246522]. 
=============================================
[2019-04-27 21:39:17,820] A3C_AGENT_WORKER-Thread-3 INFO:Local step 135000, global step 2145386: loss 0.1003
[2019-04-27 21:39:17,822] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 135000, global step 2145386: learning rate 0.0001
[2019-04-27 21:39:18,175] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:39:18,181] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9378
[2019-04-27 21:39:18,186] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 67.66666666666667, 1.0, 2.0, 0.3695517423694246, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 463737.3764016865, 463737.3764016865, 124524.2389937723], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 872400.0000, 
sim time next is 873000.0000, 
raw observation next is [22.4, 67.5, 1.0, 2.0, 0.3657118297325338, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 459572.1962164839, 459572.1962164839, 124014.1939249357], 
processed observation next is [0.0, 0.08695652173913043, 0.38518518518518513, 0.675, 1.0, 1.0, 0.24489503539587357, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1641329272201728, 0.1641329272201728, 0.23848883447103017], 
reward next is 0.7615, 
noisyNet noise sample is [array([1.186886], dtype=float32), -0.11927077]. 
=============================================
[2019-04-27 21:39:18,201] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[73.49085]
 [73.63128]
 [73.2116 ]
 [73.14597]
 [73.12931]], R is [[73.28826141]
 [73.31591034]
 [73.34260559]
 [73.36843109]
 [73.39363098]].
[2019-04-27 21:39:18,250] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:39:18,256] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6071
[2019-04-27 21:39:18,265] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 51.0, 1.0, 2.0, 0.3966062300460454, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 491960.3757269894, 491960.3757269898, 128154.3212919825], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 856800.0000, 
sim time next is 857400.0000, 
raw observation next is [26.33333333333334, 52.0, 1.0, 2.0, 0.3975761901926947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 492968.5379115993, 492968.5379115993, 128286.2189179703], 
processed observation next is [0.0, 0.9565217391304348, 0.5308641975308644, 0.52, 1.0, 1.0, 0.28282879784844606, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.17606019211128549, 0.17606019211128549, 0.24670426714994287], 
reward next is 0.7533, 
noisyNet noise sample is [array([0.83095974], dtype=float32), 1.4526585]. 
=============================================
[2019-04-27 21:39:19,533] A3C_AGENT_WORKER-Thread-21 INFO:Local step 134000, global step 2146186: loss 0.0007
[2019-04-27 21:39:19,535] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 134000, global step 2146186: learning rate 0.0001
[2019-04-27 21:39:20,136] A3C_AGENT_WORKER-Thread-12 INFO:Local step 134000, global step 2146468: loss 0.0166
[2019-04-27 21:39:20,138] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 134000, global step 2146469: learning rate 0.0001
[2019-04-27 21:39:20,210] A3C_AGENT_WORKER-Thread-17 INFO:Local step 134000, global step 2146504: loss 0.0112
[2019-04-27 21:39:20,212] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 134000, global step 2146504: learning rate 0.0001
[2019-04-27 21:39:20,232] A3C_AGENT_WORKER-Thread-15 INFO:Local step 134000, global step 2146514: loss 0.0253
[2019-04-27 21:39:20,239] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 134000, global step 2146515: learning rate 0.0001
[2019-04-27 21:39:20,414] A3C_AGENT_WORKER-Thread-2 INFO:Local step 134000, global step 2146604: loss 0.0148
[2019-04-27 21:39:20,415] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 134000, global step 2146604: learning rate 0.0001
[2019-04-27 21:39:20,956] A3C_AGENT_WORKER-Thread-9 INFO:Local step 134000, global step 2146861: loss 0.0047
[2019-04-27 21:39:20,960] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 134000, global step 2146863: learning rate 0.0001
[2019-04-27 21:39:21,118] A3C_AGENT_WORKER-Thread-16 INFO:Local step 134500, global step 2146936: loss 161.7750
[2019-04-27 21:39:21,121] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 134500, global step 2146936: learning rate 0.0001
[2019-04-27 21:39:21,412] A3C_AGENT_WORKER-Thread-18 INFO:Local step 134000, global step 2147079: loss 0.0153
[2019-04-27 21:39:21,414] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 134000, global step 2147079: learning rate 0.0001
[2019-04-27 21:39:21,496] A3C_AGENT_WORKER-Thread-22 INFO:Local step 134000, global step 2147125: loss 0.0016
[2019-04-27 21:39:21,500] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 134000, global step 2147126: learning rate 0.0001
[2019-04-27 21:39:21,803] A3C_AGENT_WORKER-Thread-11 INFO:Local step 134000, global step 2147276: loss 0.0061
[2019-04-27 21:39:21,804] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 134000, global step 2147276: learning rate 0.0001
[2019-04-27 21:39:21,835] A3C_AGENT_WORKER-Thread-14 INFO:Local step 134000, global step 2147290: loss 0.0042
[2019-04-27 21:39:21,838] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 134000, global step 2147291: learning rate 0.0001
[2019-04-27 21:39:21,953] A3C_AGENT_WORKER-Thread-20 INFO:Local step 134000, global step 2147346: loss 0.0019
[2019-04-27 21:39:21,955] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 134000, global step 2147346: learning rate 0.0001
[2019-04-27 21:39:22,327] A3C_AGENT_WORKER-Thread-13 INFO:Local step 134000, global step 2147518: loss 0.0082
[2019-04-27 21:39:22,330] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 134000, global step 2147518: learning rate 0.0001
[2019-04-27 21:39:22,459] A3C_AGENT_WORKER-Thread-10 INFO:Local step 134000, global step 2147586: loss 0.0011
[2019-04-27 21:39:22,460] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 134000, global step 2147586: learning rate 0.0001
[2019-04-27 21:39:23,550] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.3579167e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.0604807e-38], sum to 1.0000
[2019-04-27 21:39:23,557] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2048
[2019-04-27 21:39:23,565] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.63333333333333, 59.33333333333333, 1.0, 2.0, 0.3151048593531182, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 404214.4367449772, 404214.4367449767, 117462.4373036727], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 974400.0000, 
sim time next is 975000.0000, 
raw observation next is [21.76666666666667, 59.16666666666667, 1.0, 2.0, 0.3091733446577255, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 396281.4574865083, 396281.4574865083, 116717.435431299], 
processed observation next is [1.0, 0.2608695652173913, 0.3617283950617285, 0.5916666666666667, 1.0, 1.0, 0.17758731506872083, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.14152909195946725, 0.14152909195946725, 0.22445660659865194], 
reward next is 0.7755, 
noisyNet noise sample is [array([1.2530953], dtype=float32), 2.1298475]. 
=============================================
[2019-04-27 21:39:23,586] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[75.63194]
 [75.49566]
 [75.49294]
 [75.39933]
 [75.59615]], R is [[75.76348114]
 [75.779953  ]
 [75.79034424]
 [75.80297852]
 [75.80498505]].
[2019-04-27 21:39:25,861] A3C_AGENT_WORKER-Thread-19 INFO:Local step 135500, global step 2149183: loss 0.1576
[2019-04-27 21:39:25,865] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 135500, global step 2149183: learning rate 0.0001
[2019-04-27 21:39:27,630] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-04-27 21:39:27,632] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 21:39:27,634] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:39:27,634] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 21:39:27,635] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 21:39:27,635] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:39:27,636] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 21:39:27,637] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 21:39:27,636] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:39:27,637] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:39:27,639] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:39:27,664] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run87
[2019-04-27 21:39:27,690] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run87
[2019-04-27 21:39:27,720] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run87
[2019-04-27 21:39:27,721] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run87
[2019-04-27 21:39:27,763] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run87
[2019-04-27 21:39:59,160] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.01409912], dtype=float32), -0.015835423]
[2019-04-27 21:39:59,161] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [24.38333333333333, 88.5, 1.0, 2.0, 0.5742141803560972, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 667504.5867575529, 667504.5867575525, 154321.9702432745]
[2019-04-27 21:39:59,162] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:39:59,165] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.027988673838575173
[2019-04-27 21:40:11,668] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.01409912], dtype=float32), -0.015835423]
[2019-04-27 21:40:11,669] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [27.00631104, 91.90565539, 1.0, 2.0, 0.7210112369691734, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 821761.6810181522, 821761.6810181522, 180165.9487882523]
[2019-04-27 21:40:11,670] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 21:40:11,673] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.608509181857708
[2019-04-27 21:40:13,932] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01409912], dtype=float32), -0.015835423]
[2019-04-27 21:40:13,934] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [25.62850708, 72.62901228, 1.0, 2.0, 0.5434251856735715, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 642467.5775258985, 642467.5775258985, 149663.3441909832]
[2019-04-27 21:40:13,936] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 21:40:13,940] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6809530343223072
[2019-04-27 21:40:22,949] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.01409912], dtype=float32), -0.015835423]
[2019-04-27 21:40:22,950] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [33.35032950166666, 59.77902672333333, 1.0, 2.0, 0.7807799545610302, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 889921.7238851062, 889921.7238851062, 192010.3881812005]
[2019-04-27 21:40:22,951] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 21:40:22,953] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3643006306432951
[2019-04-27 21:40:40,068] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.01409912], dtype=float32), -0.015835423]
[2019-04-27 21:40:40,070] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [28.43333333333333, 66.33333333333334, 1.0, 2.0, 0.9705667385114105, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.911200000000001, 6.9112, 121.9260426156618, 1821640.959064078, 1821640.959064078, 372636.3589497258]
[2019-04-27 21:40:40,070] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:40:40,073] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [2.4395092e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.5963463e-38], sampled 0.7179959441250909
[2019-04-27 21:40:40,073] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1821640.959064078 W.
[2019-04-27 21:40:43,338] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.01409912], dtype=float32), -0.015835423]
[2019-04-27 21:40:43,340] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [31.0, 53.0, 1.0, 2.0, 0.5801893205416961, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 669051.6246440116, 669051.6246440116, 155085.3942426376]
[2019-04-27 21:40:43,341] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 21:40:43,344] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7765087272208714
[2019-04-27 21:40:49,424] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.01409912], dtype=float32), -0.015835423]
[2019-04-27 21:40:49,425] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [28.5, 62.5, 1.0, 2.0, 0.8851176241319545, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.9260426156618, 1724096.220793693, 1724096.220793693, 353766.8349254742]
[2019-04-27 21:40:49,426] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:40:49,428] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [3.6834207e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.15452482206942364
[2019-04-27 21:40:49,429] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1724096.220793693 W.
[2019-04-27 21:40:55,653] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01409912], dtype=float32), -0.015835423]
[2019-04-27 21:40:55,654] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [27.51204849, 35.092855145, 1.0, 2.0, 0.4257664971465171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 541495.2225733598, 541495.2225733598, 132518.0110125436]
[2019-04-27 21:40:55,655] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 21:40:55,659] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.35308216029932904
[2019-04-27 21:41:10,124] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8769.4422 2170768739.9937 493.0000
[2019-04-27 21:41:10,387] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8582.8795 2248773970.1662 553.0000
[2019-04-27 21:41:10,449] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8098.4495 2445508074.6415 746.0000
[2019-04-27 21:41:10,524] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8700.1130 2195220806.1349 572.0000
[2019-04-27 21:41:10,629] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8922.6628 2120599135.7723 430.0000
[2019-04-27 21:41:11,646] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 2150000, evaluation results [2150000.0, 8098.4494892610255, 2445508074.64147, 746.0, 8769.442185004564, 2170768739.993712, 493.0, 8922.662781171732, 2120599135.7722971, 430.0, 8582.879467926026, 2248773970.1662407, 553.0, 8700.112961499688, 2195220806.134913, 572.0]
[2019-04-27 21:41:18,884] A3C_AGENT_WORKER-Thread-3 INFO:Local step 135500, global step 2153586: loss 0.0144
[2019-04-27 21:41:18,886] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 135500, global step 2153586: learning rate 0.0001
[2019-04-27 21:41:20,139] A3C_AGENT_WORKER-Thread-21 INFO:Local step 134500, global step 2154163: loss 129.9623
[2019-04-27 21:41:20,142] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 134500, global step 2154164: learning rate 0.0001
[2019-04-27 21:41:20,654] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.4711465e-33], sum to 1.0000
[2019-04-27 21:41:20,662] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4070
[2019-04-27 21:41:20,666] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.1, 77.66666666666667, 1.0, 2.0, 0.347906797347068, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 436640.6138340661, 436640.6138340661, 121628.5640244995], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1842000.0000, 
sim time next is 1842600.0000, 
raw observation next is [21.25, 76.83333333333333, 1.0, 2.0, 0.3496408414912117, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 438628.0898494672, 438628.0898494672, 121854.5935392862], 
processed observation next is [1.0, 0.30434782608695654, 0.3425925925925926, 0.7683333333333333, 1.0, 1.0, 0.2257629065371568, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.15665288923195259, 0.15665288923195259, 0.23433575680631963], 
reward next is 0.7657, 
noisyNet noise sample is [array([-0.8793064], dtype=float32), -0.16517298]. 
=============================================
[2019-04-27 21:41:20,699] A3C_AGENT_WORKER-Thread-12 INFO:Local step 134500, global step 2154426: loss 140.3360
[2019-04-27 21:41:20,702] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 134500, global step 2154427: learning rate 0.0001
[2019-04-27 21:41:20,803] A3C_AGENT_WORKER-Thread-15 INFO:Local step 134500, global step 2154473: loss 138.9635
[2019-04-27 21:41:20,809] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 134500, global step 2154474: learning rate 0.0001
[2019-04-27 21:41:20,866] A3C_AGENT_WORKER-Thread-17 INFO:Local step 134500, global step 2154503: loss 126.3125
[2019-04-27 21:41:20,874] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 134500, global step 2154503: learning rate 0.0001
[2019-04-27 21:41:21,010] A3C_AGENT_WORKER-Thread-2 INFO:Local step 134500, global step 2154573: loss 126.8493
[2019-04-27 21:41:21,012] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 134500, global step 2154573: learning rate 0.0001
[2019-04-27 21:41:21,565] A3C_AGENT_WORKER-Thread-9 INFO:Local step 134500, global step 2154852: loss 104.5568
[2019-04-27 21:41:21,567] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 134500, global step 2154852: learning rate 0.0001
[2019-04-27 21:41:21,720] A3C_AGENT_WORKER-Thread-16 INFO:Local step 135000, global step 2154929: loss 0.0115
[2019-04-27 21:41:21,722] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 135000, global step 2154929: learning rate 0.0001
[2019-04-27 21:41:22,023] A3C_AGENT_WORKER-Thread-18 INFO:Local step 134500, global step 2155073: loss 124.1880
[2019-04-27 21:41:22,032] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 134500, global step 2155077: learning rate 0.0001
[2019-04-27 21:41:22,060] A3C_AGENT_WORKER-Thread-22 INFO:Local step 134500, global step 2155092: loss 109.9367
[2019-04-27 21:41:22,064] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 134500, global step 2155092: learning rate 0.0001
[2019-04-27 21:41:22,394] A3C_AGENT_WORKER-Thread-20 INFO:Local step 134500, global step 2155251: loss 109.4599
[2019-04-27 21:41:22,397] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 134500, global step 2155254: learning rate 0.0001
[2019-04-27 21:41:22,428] A3C_AGENT_WORKER-Thread-11 INFO:Local step 134500, global step 2155271: loss 97.4387
[2019-04-27 21:41:22,430] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 134500, global step 2155271: learning rate 0.0001
[2019-04-27 21:41:22,431] A3C_AGENT_WORKER-Thread-14 INFO:Local step 134500, global step 2155271: loss 102.5385
[2019-04-27 21:41:22,433] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 134500, global step 2155271: learning rate 0.0001
[2019-04-27 21:41:23,028] A3C_AGENT_WORKER-Thread-13 INFO:Local step 134500, global step 2155556: loss 102.1007
[2019-04-27 21:41:23,030] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 134500, global step 2155556: learning rate 0.0001
[2019-04-27 21:41:23,135] A3C_AGENT_WORKER-Thread-10 INFO:Local step 134500, global step 2155601: loss 112.0192
[2019-04-27 21:41:23,139] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 134500, global step 2155604: learning rate 0.0001
[2019-04-27 21:41:25,164] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.560368e-36 1.000000e+00 0.000000e+00 0.000000e+00 1.429128e-35], sum to 1.0000
[2019-04-27 21:41:25,171] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5237
[2019-04-27 21:41:25,175] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.06666666666667, 91.66666666666667, 1.0, 2.0, 0.3763008874103568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 467742.8858127754, 467742.8858127754, 125362.7133763666], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1923600.0000, 
sim time next is 1924200.0000, 
raw observation next is [20.1, 91.5, 1.0, 2.0, 0.3768110927489469, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 468295.8093931106, 468295.8093931106, 125430.805986504], 
processed observation next is [1.0, 0.2608695652173913, 0.30000000000000004, 0.915, 1.0, 1.0, 0.25810844374874636, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16724850335468236, 0.16724850335468236, 0.24121308843558462], 
reward next is 0.7588, 
noisyNet noise sample is [array([0.31026918], dtype=float32), -0.22435875]. 
=============================================
[2019-04-27 21:41:26,395] A3C_AGENT_WORKER-Thread-19 INFO:Local step 136000, global step 2157141: loss 0.0935
[2019-04-27 21:41:26,396] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 136000, global step 2157141: learning rate 0.0001
[2019-04-27 21:41:29,645] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [8.6276824e-30 1.0000000e+00 1.3189787e-33 1.1132815e-36 3.3844702e-28], sum to 1.0000
[2019-04-27 21:41:29,655] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8349
[2019-04-27 21:41:29,660] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1407127.417327813 W.
[2019-04-27 21:41:29,666] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.7, 33.0, 1.0, 2.0, 0.3892248967822274, 1.0, 2.0, 0.3892248967822274, 1.0, 1.0, 0.6299061353383928, 6.911200000000001, 6.9112, 121.94756008, 1407127.417327813, 1407127.417327812, 291795.7747363123], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1346400.0000, 
sim time next is 1347000.0000, 
raw observation next is [30.71666666666667, 32.83333333333334, 1.0, 2.0, 0.3092100101457059, 1.0, 2.0, 0.3092100101457059, 1.0, 2.0, 0.5007319623851585, 6.9112, 6.9112, 121.94756008, 1118781.743489476, 1118781.743489476, 258877.3363314321], 
processed observation next is [1.0, 0.6086956521739131, 0.69320987654321, 0.3283333333333334, 1.0, 1.0, 0.17763096445917373, 1.0, 1.0, 0.17763096445917373, 1.0, 1.0, 0.37591495298144806, 0.0, 0.0, 0.8096049824067558, 0.3995649083890985, 0.3995649083890985, 0.4978410314066002], 
reward next is 0.5022, 
noisyNet noise sample is [array([-1.1676917], dtype=float32), 0.18418594]. 
=============================================
[2019-04-27 21:41:29,687] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[70.89017]
 [70.7772 ]
 [70.00595]
 [69.69474]
 [69.0706 ]], R is [[71.34423828]
 [70.63079834]
 [69.92449188]
 [69.67397308]
 [69.47061157]].
[2019-04-27 21:41:35,711] A3C_AGENT_WORKER-Thread-3 INFO:Local step 136000, global step 2161550: loss 0.0548
[2019-04-27 21:41:35,712] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 136000, global step 2161550: learning rate 0.0001
[2019-04-27 21:41:37,013] A3C_AGENT_WORKER-Thread-21 INFO:Local step 135000, global step 2162174: loss 0.1522
[2019-04-27 21:41:37,015] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 135000, global step 2162174: learning rate 0.0001
[2019-04-27 21:41:37,523] A3C_AGENT_WORKER-Thread-12 INFO:Local step 135000, global step 2162424: loss 0.0918
[2019-04-27 21:41:37,526] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 135000, global step 2162424: learning rate 0.0001
[2019-04-27 21:41:37,568] A3C_AGENT_WORKER-Thread-17 INFO:Local step 135000, global step 2162445: loss 0.0702
[2019-04-27 21:41:37,570] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 135000, global step 2162445: learning rate 0.0001
[2019-04-27 21:41:37,572] A3C_AGENT_WORKER-Thread-15 INFO:Local step 135000, global step 2162446: loss 0.0793
[2019-04-27 21:41:37,574] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 135000, global step 2162446: learning rate 0.0001
[2019-04-27 21:41:37,784] A3C_AGENT_WORKER-Thread-2 INFO:Local step 135000, global step 2162547: loss 0.0839
[2019-04-27 21:41:37,788] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 135000, global step 2162548: learning rate 0.0001
[2019-04-27 21:41:37,897] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:41:37,904] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7397
[2019-04-27 21:41:37,907] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.4, 22.0, 1.0, 2.0, 0.4136899427819726, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 510516.1453257408, 510516.1453257408, 130516.2013522478], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1517400.0000, 
sim time next is 1518000.0000, 
raw observation next is [35.53333333333333, 21.33333333333333, 1.0, 2.0, 0.4143239774341992, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 512058.5365688694, 512058.5365688694, 130625.2514068128], 
processed observation next is [0.0, 0.5652173913043478, 0.8716049382716049, 0.2133333333333333, 1.0, 1.0, 0.30276663980261814, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1828780487745962, 0.1828780487745962, 0.25120240655156306], 
reward next is 0.7488, 
noisyNet noise sample is [array([-0.46266752], dtype=float32), 0.3851089]. 
=============================================
[2019-04-27 21:41:37,922] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[80.238754]
 [80.22364 ]
 [80.19271 ]
 [80.188416]
 [80.072235]], R is [[80.20747375]
 [80.15441132]
 [80.10173035]
 [80.04759216]
 [79.99690247]].
[2019-04-27 21:41:38,649] A3C_AGENT_WORKER-Thread-9 INFO:Local step 135000, global step 2162962: loss 0.0796
[2019-04-27 21:41:38,653] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 135000, global step 2162963: learning rate 0.0001
[2019-04-27 21:41:38,697] A3C_AGENT_WORKER-Thread-16 INFO:Local step 135500, global step 2162981: loss 0.4157
[2019-04-27 21:41:38,698] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 135500, global step 2162981: learning rate 0.0001
[2019-04-27 21:41:38,827] A3C_AGENT_WORKER-Thread-18 INFO:Local step 135000, global step 2163044: loss 0.0659
[2019-04-27 21:41:38,832] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 135000, global step 2163047: learning rate 0.0001
[2019-04-27 21:41:38,926] A3C_AGENT_WORKER-Thread-22 INFO:Local step 135000, global step 2163085: loss 0.0481
[2019-04-27 21:41:38,927] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 135000, global step 2163085: learning rate 0.0001
[2019-04-27 21:41:39,175] A3C_AGENT_WORKER-Thread-11 INFO:Local step 135000, global step 2163208: loss 0.0519
[2019-04-27 21:41:39,177] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 135000, global step 2163208: learning rate 0.0001
[2019-04-27 21:41:39,248] A3C_AGENT_WORKER-Thread-20 INFO:Local step 135000, global step 2163237: loss 0.0523
[2019-04-27 21:41:39,249] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 135000, global step 2163237: learning rate 0.0001
[2019-04-27 21:41:39,393] A3C_AGENT_WORKER-Thread-14 INFO:Local step 135000, global step 2163307: loss 0.0457
[2019-04-27 21:41:39,396] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 135000, global step 2163308: learning rate 0.0001
[2019-04-27 21:41:39,613] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:41:39,620] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6415
[2019-04-27 21:41:39,624] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.8, 20.0, 1.0, 2.0, 0.4154023829005045, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 514950.3441350589, 514950.3441350584, 130815.7064806316], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1519200.0000, 
sim time next is 1519800.0000, 
raw observation next is [35.63333333333333, 21.16666666666667, 1.0, 2.0, 0.4301012811385961, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 532126.7258112546, 532126.7258112546, 132922.7652851475], 
processed observation next is [0.0, 0.6086956521739131, 0.8753086419753087, 0.21166666666666673, 1.0, 1.0, 0.32154914421261444, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1900452592183052, 0.1900452592183052, 0.2556207024714375], 
reward next is 0.7444, 
noisyNet noise sample is [array([1.1457511], dtype=float32), 0.61825687]. 
=============================================
[2019-04-27 21:41:39,876] A3C_AGENT_WORKER-Thread-13 INFO:Local step 135000, global step 2163546: loss 0.0566
[2019-04-27 21:41:39,878] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 135000, global step 2163547: learning rate 0.0001
[2019-04-27 21:41:39,927] A3C_AGENT_WORKER-Thread-10 INFO:Local step 135000, global step 2163571: loss 0.0716
[2019-04-27 21:41:39,929] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 135000, global step 2163571: learning rate 0.0001
[2019-04-27 21:41:43,630] A3C_AGENT_WORKER-Thread-19 INFO:Local step 136500, global step 2165324: loss 0.1662
[2019-04-27 21:41:43,632] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 136500, global step 2165324: learning rate 0.0001
[2019-04-27 21:41:48,706] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:41:48,714] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0637
[2019-04-27 21:41:48,720] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.55, 72.5, 1.0, 2.0, 0.4216992872350666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 517542.7395184682, 517542.7395184682, 131596.1705675684], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1707000.0000, 
sim time next is 1707600.0000, 
raw observation next is [23.5, 73.0, 1.0, 2.0, 0.4268867787227399, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 523705.7989789845, 523705.7989789845, 132342.4494365678], 
processed observation next is [1.0, 0.782608695652174, 0.42592592592592593, 0.73, 1.0, 1.0, 0.31772235562230944, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1870377853496373, 0.1870377853496373, 0.2545047104549381], 
reward next is 0.7455, 
noisyNet noise sample is [array([0.25358313], dtype=float32), 0.016913382]. 
=============================================
[2019-04-27 21:41:50,770] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:41:50,778] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1405
[2019-04-27 21:41:50,783] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.7, 70.66666666666667, 1.0, 2.0, 0.4055108741072926, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 498661.6125196404, 498661.61251964, 129306.8762056291], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1705200.0000, 
sim time next is 1705800.0000, 
raw observation next is [23.65, 71.33333333333333, 1.0, 2.0, 0.4095201677084043, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 503216.7171282189, 503216.7171282189, 129866.4523323771], 
processed observation next is [1.0, 0.7391304347826086, 0.4314814814814814, 0.7133333333333333, 1.0, 1.0, 0.29704781870048136, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.17972025611722103, 0.17972025611722103, 0.24974317756226366], 
reward next is 0.7503, 
noisyNet noise sample is [array([0.56181335], dtype=float32), -0.13783686]. 
=============================================
[2019-04-27 21:41:52,859] A3C_AGENT_WORKER-Thread-3 INFO:Local step 136500, global step 2169694: loss 0.2859
[2019-04-27 21:41:52,862] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 136500, global step 2169695: learning rate 0.0001
[2019-04-27 21:41:53,934] A3C_AGENT_WORKER-Thread-21 INFO:Local step 135500, global step 2170206: loss 0.5941
[2019-04-27 21:41:53,937] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 135500, global step 2170206: learning rate 0.0001
[2019-04-27 21:41:54,305] A3C_AGENT_WORKER-Thread-12 INFO:Local step 135500, global step 2170377: loss 0.2033
[2019-04-27 21:41:54,306] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 135500, global step 2170377: learning rate 0.0001
[2019-04-27 21:41:54,426] A3C_AGENT_WORKER-Thread-17 INFO:Local step 135500, global step 2170435: loss 0.1609
[2019-04-27 21:41:54,427] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 135500, global step 2170436: learning rate 0.0001
[2019-04-27 21:41:54,461] A3C_AGENT_WORKER-Thread-15 INFO:Local step 135500, global step 2170450: loss 0.0927
[2019-04-27 21:41:54,464] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 135500, global step 2170451: learning rate 0.0001
[2019-04-27 21:41:54,690] A3C_AGENT_WORKER-Thread-2 INFO:Local step 135500, global step 2170561: loss 0.1002
[2019-04-27 21:41:54,692] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 135500, global step 2170562: learning rate 0.0001
[2019-04-27 21:41:55,385] A3C_AGENT_WORKER-Thread-16 INFO:Local step 136000, global step 2170891: loss 0.0230
[2019-04-27 21:41:55,387] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 136000, global step 2170891: learning rate 0.0001
[2019-04-27 21:41:55,468] A3C_AGENT_WORKER-Thread-9 INFO:Local step 135500, global step 2170926: loss 0.2067
[2019-04-27 21:41:55,469] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 135500, global step 2170926: learning rate 0.0001
[2019-04-27 21:41:55,634] A3C_AGENT_WORKER-Thread-18 INFO:Local step 135500, global step 2171006: loss 0.1294
[2019-04-27 21:41:55,636] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 135500, global step 2171006: learning rate 0.0001
[2019-04-27 21:41:55,753] A3C_AGENT_WORKER-Thread-22 INFO:Local step 135500, global step 2171063: loss 0.1660
[2019-04-27 21:41:55,755] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 135500, global step 2171064: learning rate 0.0001
[2019-04-27 21:41:55,992] A3C_AGENT_WORKER-Thread-20 INFO:Local step 135500, global step 2171174: loss 0.1710
[2019-04-27 21:41:55,994] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 135500, global step 2171174: learning rate 0.0001
[2019-04-27 21:41:56,069] A3C_AGENT_WORKER-Thread-11 INFO:Local step 135500, global step 2171208: loss 0.1245
[2019-04-27 21:41:56,071] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 135500, global step 2171208: learning rate 0.0001
[2019-04-27 21:41:56,322] A3C_AGENT_WORKER-Thread-14 INFO:Local step 135500, global step 2171332: loss 0.0983
[2019-04-27 21:41:56,323] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 135500, global step 2171332: learning rate 0.0001
[2019-04-27 21:41:56,520] A3C_AGENT_WORKER-Thread-13 INFO:Local step 135500, global step 2171428: loss 0.2147
[2019-04-27 21:41:56,526] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 135500, global step 2171429: learning rate 0.0001
[2019-04-27 21:41:56,751] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.4692109e-34 1.0000000e+00 0.0000000e+00 1.4284664e-38 1.6142741e-38], sum to 1.0000
[2019-04-27 21:41:56,758] A3C_AGENT_WORKER-Thread-10 INFO:Local step 135500, global step 2171546: loss 0.1870
[2019-04-27 21:41:56,760] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 135500, global step 2171547: learning rate 0.0001
[2019-04-27 21:41:56,762] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0595
[2019-04-27 21:41:56,767] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.63333333333334, 90.83333333333334, 1.0, 2.0, 0.3889382948671427, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 492694.9808900431, 492694.9808900431, 127248.0297610252], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1829400.0000, 
sim time next is 1830000.0000, 
raw observation next is [18.66666666666667, 90.66666666666667, 1.0, 2.0, 0.3345415781205069, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 423708.548619583, 423708.5486195825, 119930.0063055773], 
processed observation next is [1.0, 0.17391304347826086, 0.24691358024691376, 0.9066666666666667, 1.0, 1.0, 0.20778759300060345, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.15132448164985107, 0.15132448164985088, 0.23063462751072558], 
reward next is 0.7694, 
noisyNet noise sample is [array([-0.3664822], dtype=float32), 0.25167084]. 
=============================================
[2019-04-27 21:41:56,778] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[66.95736 ]
 [67.59527 ]
 [67.57876 ]
 [67.784996]
 [68.00356 ]], R is [[67.1682663 ]
 [67.25187683]
 [67.35062408]
 [67.44828033]
 [67.54488373]].
[2019-04-27 21:41:57,110] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:41:57,122] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8485
[2019-04-27 21:41:57,127] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 75.0, 1.0, 2.0, 0.5740595251817807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 667359.8175147277, 667359.8175147277, 154297.6525344536], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2644200.0000, 
sim time next is 2644800.0000, 
raw observation next is [26.6, 74.66666666666666, 1.0, 2.0, 0.5825780834839299, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 675290.7745588664, 675290.7745588664, 155651.6325729326], 
processed observation next is [0.0, 0.6086956521739131, 0.5407407407407407, 0.7466666666666666, 1.0, 1.0, 0.5030691470046784, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.24117527662816657, 0.24117527662816657, 0.299330062640255], 
reward next is 0.7007, 
noisyNet noise sample is [array([0.06062684], dtype=float32), -0.16291438]. 
=============================================
[2019-04-27 21:42:00,234] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:42:00,243] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1160
[2019-04-27 21:42:00,248] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.03333333333333, 90.83333333333334, 1.0, 2.0, 0.4231051435619206, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 519320.3118002451, 519320.3118002447, 131800.7473331296], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1885800.0000, 
sim time next is 1886400.0000, 
raw observation next is [21.0, 91.0, 1.0, 2.0, 0.4225686768681827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 518762.935848996, 518762.935848996, 131725.8206348486], 
processed observation next is [1.0, 0.8695652173913043, 0.3333333333333333, 0.91, 1.0, 1.0, 0.31258175817640804, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.18527247708892713, 0.18527247708892713, 0.25331888583624734], 
reward next is 0.7467, 
noisyNet noise sample is [array([0.6648403], dtype=float32), -0.998261]. 
=============================================
[2019-04-27 21:42:00,971] A3C_AGENT_WORKER-Thread-19 INFO:Local step 137000, global step 2173521: loss 0.0071
[2019-04-27 21:42:00,973] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 137000, global step 2173522: learning rate 0.0001
[2019-04-27 21:42:04,083] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-27 21:42:04,086] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 21:42:04,087] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 21:42:04,088] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 21:42:04,089] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 21:42:04,091] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:42:04,091] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 21:42:04,093] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:42:04,094] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:42:04,095] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:42:04,092] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:42:04,122] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run88
[2019-04-27 21:42:04,148] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run88
[2019-04-27 21:42:04,173] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run88
[2019-04-27 21:42:04,197] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run88
[2019-04-27 21:42:04,197] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run88
[2019-04-27 21:42:53,370] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01729345], dtype=float32), -0.01517486]
[2019-04-27 21:42:53,371] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [23.41666666666667, 86.0, 1.0, 2.0, 0.9651619683192214, 1.0, 2.0, 0.9651619683192214, 0.0, 2.0, 0.0, 6.9112, 6.9112, 131.9481188174244, 2220864.522920178, 2220864.522920178, 419939.6459347517]
[2019-04-27 21:42:53,372] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 21:42:53,373] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.3741463e-23 1.0000000e+00 4.1183833e-26 1.8217582e-26 7.6651663e-22], sampled 0.14802259613791413
[2019-04-27 21:42:53,374] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2220864.522920178 W.
[2019-04-27 21:43:15,989] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.01729345], dtype=float32), -0.01517486]
[2019-04-27 21:43:15,990] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [24.11412757333333, 102.6986298666667, 1.0, 2.0, 0.6873077732639515, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 783329.0378665886, 783329.0378665886, 173752.3294999005]
[2019-04-27 21:43:15,991] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 21:43:15,994] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9283408981786724
[2019-04-27 21:43:36,575] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.01729345], dtype=float32), -0.01517486]
[2019-04-27 21:43:36,576] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [23.8, 89.0, 1.0, 2.0, 0.537939358294641, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 632056.5178986893, 632056.5178986893, 148605.7775624987]
[2019-04-27 21:43:36,578] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 21:43:36,580] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.23452248970879297
[2019-04-27 21:43:44,385] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01729345], dtype=float32), -0.01517486]
[2019-04-27 21:43:44,387] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [24.57396234666667, 87.78136636, 1.0, 2.0, 0.5692396095386206, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 659478.6802629775, 659478.6802629775, 153383.0476315279]
[2019-04-27 21:43:44,388] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 21:43:44,390] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9069145852671194
[2019-04-27 21:43:48,896] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8098.4495 2445508074.6415 746.0000
[2019-04-27 21:43:49,175] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8770.2626 2170712315.3188 493.0000
[2019-04-27 21:43:49,241] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8582.8795 2248773970.1662 553.0000
[2019-04-27 21:43:49,401] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8700.1130 2195220806.1349 572.0000
[2019-04-27 21:43:49,418] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8922.6628 2120599135.7723 430.0000
[2019-04-27 21:43:50,436] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 2175000, evaluation results [2175000.0, 8098.4494892610255, 2445508074.64147, 746.0, 8770.262620159681, 2170712315.3187604, 493.0, 8922.662781171732, 2120599135.7722971, 430.0, 8582.879467926026, 2248773970.1662407, 553.0, 8700.112961499688, 2195220806.134913, 572.0]
[2019-04-27 21:43:56,012] A3C_AGENT_WORKER-Thread-3 INFO:Local step 137000, global step 2177923: loss 0.0470
[2019-04-27 21:43:56,013] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 137000, global step 2177923: learning rate 0.0001
[2019-04-27 21:43:56,409] A3C_AGENT_WORKER-Thread-21 INFO:Local step 136000, global step 2178105: loss 0.0084
[2019-04-27 21:43:56,413] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 136000, global step 2178106: learning rate 0.0001
[2019-04-27 21:43:56,885] A3C_AGENT_WORKER-Thread-15 INFO:Local step 136000, global step 2178341: loss 0.0008
[2019-04-27 21:43:56,887] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 136000, global step 2178342: learning rate 0.0001
[2019-04-27 21:43:56,903] A3C_AGENT_WORKER-Thread-12 INFO:Local step 136000, global step 2178350: loss 0.0016
[2019-04-27 21:43:56,904] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 136000, global step 2178350: learning rate 0.0001
[2019-04-27 21:43:57,121] A3C_AGENT_WORKER-Thread-17 INFO:Local step 136000, global step 2178452: loss 0.0028
[2019-04-27 21:43:57,123] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 136000, global step 2178452: learning rate 0.0001
[2019-04-27 21:43:57,211] A3C_AGENT_WORKER-Thread-2 INFO:Local step 136000, global step 2178491: loss 0.0022
[2019-04-27 21:43:57,211] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 136000, global step 2178491: learning rate 0.0001
[2019-04-27 21:43:57,957] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:43:57,968] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9889
[2019-04-27 21:43:57,971] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.85, 79.5, 1.0, 2.0, 0.521978745822649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 616866.1303252892, 616866.1303252892, 146171.444403132], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2107800.0000, 
sim time next is 2108400.0000, 
raw observation next is [25.1, 78.66666666666666, 1.0, 2.0, 0.5285388142612357, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 623145.9660635698, 623145.9660635698, 147168.9451665284], 
processed observation next is [0.0, 0.391304347826087, 0.4851851851851852, 0.7866666666666666, 1.0, 1.0, 0.4387366836443282, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2225521307369892, 0.2225521307369892, 0.28301720224332383], 
reward next is 0.7170, 
noisyNet noise sample is [array([0.556582], dtype=float32), -0.071457125]. 
=============================================
[2019-04-27 21:43:58,178] A3C_AGENT_WORKER-Thread-9 INFO:Local step 136000, global step 2178945: loss 0.0034
[2019-04-27 21:43:58,181] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 136000, global step 2178946: learning rate 0.0001
[2019-04-27 21:43:58,237] A3C_AGENT_WORKER-Thread-18 INFO:Local step 136000, global step 2178975: loss 0.0011
[2019-04-27 21:43:58,238] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 136000, global step 2178975: learning rate 0.0001
[2019-04-27 21:43:58,268] A3C_AGENT_WORKER-Thread-22 INFO:Local step 136000, global step 2178989: loss 0.0019
[2019-04-27 21:43:58,270] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 136000, global step 2178989: learning rate 0.0001
[2019-04-27 21:43:58,442] A3C_AGENT_WORKER-Thread-20 INFO:Local step 136000, global step 2179068: loss 0.0036
[2019-04-27 21:43:58,445] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 136000, global step 2179069: learning rate 0.0001
[2019-04-27 21:43:58,457] A3C_AGENT_WORKER-Thread-16 INFO:Local step 136500, global step 2179075: loss 0.1396
[2019-04-27 21:43:58,459] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 136500, global step 2179075: learning rate 0.0001
[2019-04-27 21:43:58,668] A3C_AGENT_WORKER-Thread-11 INFO:Local step 136000, global step 2179173: loss 0.0035
[2019-04-27 21:43:58,671] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 136000, global step 2179175: learning rate 0.0001
[2019-04-27 21:43:58,937] A3C_AGENT_WORKER-Thread-14 INFO:Local step 136000, global step 2179302: loss 0.0037
[2019-04-27 21:43:58,941] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 136000, global step 2179302: learning rate 0.0001
[2019-04-27 21:43:59,168] A3C_AGENT_WORKER-Thread-13 INFO:Local step 136000, global step 2179407: loss 0.0011
[2019-04-27 21:43:59,170] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 136000, global step 2179407: learning rate 0.0001
[2019-04-27 21:43:59,293] A3C_AGENT_WORKER-Thread-10 INFO:Local step 136000, global step 2179469: loss 0.0039
[2019-04-27 21:43:59,296] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 136000, global step 2179470: learning rate 0.0001
[2019-04-27 21:44:04,246] A3C_AGENT_WORKER-Thread-19 INFO:Local step 137500, global step 2181823: loss 0.1372
[2019-04-27 21:44:04,249] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 137500, global step 2181824: learning rate 0.0001
[2019-04-27 21:44:05,466] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.586214e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-27 21:44:05,474] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3619
[2019-04-27 21:44:05,478] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.4, 95.16666666666667, 1.0, 2.0, 0.4164290415373986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 512100.9669835112, 512100.9669835112, 130864.1754999739], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2265000.0000, 
sim time next is 2265600.0000, 
raw observation next is [20.4, 95.33333333333334, 1.0, 2.0, 0.4120643149659357, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 506610.2435691757, 506610.2435691757, 130236.0139397872], 
processed observation next is [1.0, 0.21739130434782608, 0.31111111111111106, 0.9533333333333335, 1.0, 1.0, 0.30007656543563777, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1809322298461342, 0.1809322298461342, 0.2504538729611292], 
reward next is 0.7495, 
noisyNet noise sample is [array([-0.4196739], dtype=float32), -0.7779343]. 
=============================================
[2019-04-27 21:44:11,856] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.6325639e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.6997920e-36], sum to 1.0000
[2019-04-27 21:44:11,865] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2471
[2019-04-27 21:44:11,868] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.16666666666666, 60.0, 1.0, 2.0, 0.4004276999869927, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 494178.6341608276, 494178.6341608276, 128633.3166660188], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2357400.0000, 
sim time next is 2358000.0000, 
raw observation next is [25.5, 57.0, 1.0, 2.0, 0.4185944983717875, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 517974.8417851378, 517974.8417851378, 131253.9065429663], 
processed observation next is [1.0, 0.30434782608695654, 0.5, 0.57, 1.0, 1.0, 0.307850593299747, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1849910149232635, 0.1849910149232635, 0.25241135873647363], 
reward next is 0.7476, 
noisyNet noise sample is [array([-1.105508], dtype=float32), -0.9293737]. 
=============================================
[2019-04-27 21:44:11,883] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[67.544044]
 [67.516136]
 [67.55707 ]
 [67.54806 ]
 [67.6454  ]], R is [[67.51689911]
 [67.59435272]
 [67.66944885]
 [67.74233246]
 [67.81245422]].
[2019-04-27 21:44:13,137] A3C_AGENT_WORKER-Thread-21 INFO:Local step 136500, global step 2186053: loss 0.0650
[2019-04-27 21:44:13,140] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 136500, global step 2186054: learning rate 0.0001
[2019-04-27 21:44:13,293] A3C_AGENT_WORKER-Thread-3 INFO:Local step 137500, global step 2186107: loss 0.1693
[2019-04-27 21:44:13,298] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 137500, global step 2186107: learning rate 0.0001
[2019-04-27 21:44:13,703] A3C_AGENT_WORKER-Thread-15 INFO:Local step 136500, global step 2186297: loss 0.1597
[2019-04-27 21:44:13,704] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 136500, global step 2186297: learning rate 0.0001
[2019-04-27 21:44:13,816] A3C_AGENT_WORKER-Thread-12 INFO:Local step 136500, global step 2186351: loss 0.1024
[2019-04-27 21:44:13,822] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 136500, global step 2186351: learning rate 0.0001
[2019-04-27 21:44:13,878] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:44:13,887] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3305
[2019-04-27 21:44:13,889] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.2, 89.33333333333334, 1.0, 2.0, 0.5250614722883021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 623030.8490510861, 623030.8490510861, 146765.8202634611], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2684400.0000, 
sim time next is 2685000.0000, 
raw observation next is [23.25, 88.16666666666666, 1.0, 2.0, 0.5193964062270096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 617486.7309674809, 617486.7309674805, 145900.2420274882], 
processed observation next is [0.0, 0.043478260869565216, 0.4166666666666667, 0.8816666666666666, 1.0, 1.0, 0.4278528645559637, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2205309753455289, 0.22053097534552873, 0.28057738851440034], 
reward next is 0.7194, 
noisyNet noise sample is [array([-0.31226486], dtype=float32), -0.6951468]. 
=============================================
[2019-04-27 21:44:13,903] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[70.047485]
 [70.09914 ]
 [70.08319 ]
 [70.11599 ]
 [70.1834  ]], R is [[70.11732483]
 [70.13391113]
 [70.14865112]
 [70.1615448 ]
 [70.17251587]].
[2019-04-27 21:44:14,013] A3C_AGENT_WORKER-Thread-17 INFO:Local step 136500, global step 2186441: loss 0.0747
[2019-04-27 21:44:14,017] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 136500, global step 2186442: learning rate 0.0001
[2019-04-27 21:44:14,196] A3C_AGENT_WORKER-Thread-2 INFO:Local step 136500, global step 2186524: loss 0.0987
[2019-04-27 21:44:14,199] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 136500, global step 2186524: learning rate 0.0001
[2019-04-27 21:44:15,013] A3C_AGENT_WORKER-Thread-9 INFO:Local step 136500, global step 2186911: loss 0.1118
[2019-04-27 21:44:15,015] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 136500, global step 2186911: learning rate 0.0001
[2019-04-27 21:44:15,073] A3C_AGENT_WORKER-Thread-18 INFO:Local step 136500, global step 2186938: loss 0.0942
[2019-04-27 21:44:15,074] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 136500, global step 2186938: learning rate 0.0001
[2019-04-27 21:44:15,211] A3C_AGENT_WORKER-Thread-22 INFO:Local step 136500, global step 2187001: loss 0.0384
[2019-04-27 21:44:15,214] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 136500, global step 2187001: learning rate 0.0001
[2019-04-27 21:44:15,278] A3C_AGENT_WORKER-Thread-20 INFO:Local step 136500, global step 2187032: loss 0.0248
[2019-04-27 21:44:15,283] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 136500, global step 2187034: learning rate 0.0001
[2019-04-27 21:44:15,393] A3C_AGENT_WORKER-Thread-16 INFO:Local step 137000, global step 2187085: loss 0.0398
[2019-04-27 21:44:15,394] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 137000, global step 2187086: learning rate 0.0001
[2019-04-27 21:44:15,544] A3C_AGENT_WORKER-Thread-11 INFO:Local step 136500, global step 2187159: loss 0.0143
[2019-04-27 21:44:15,548] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 136500, global step 2187159: learning rate 0.0001
[2019-04-27 21:44:15,850] A3C_AGENT_WORKER-Thread-14 INFO:Local step 136500, global step 2187302: loss 0.0810
[2019-04-27 21:44:15,852] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 136500, global step 2187303: learning rate 0.0001
[2019-04-27 21:44:16,083] A3C_AGENT_WORKER-Thread-13 INFO:Local step 136500, global step 2187414: loss 0.0807
[2019-04-27 21:44:16,085] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 136500, global step 2187415: learning rate 0.0001
[2019-04-27 21:44:16,121] A3C_AGENT_WORKER-Thread-10 INFO:Local step 136500, global step 2187431: loss 0.0863
[2019-04-27 21:44:16,123] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 136500, global step 2187431: learning rate 0.0001
[2019-04-27 21:44:17,716] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.7822197e-22 1.0000000e+00 3.7323912e-26 4.4357411e-26 4.1986034e-18], sum to 1.0000
[2019-04-27 21:44:17,722] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3840
[2019-04-27 21:44:17,730] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1314168.255047965 W.
[2019-04-27 21:44:17,738] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.15, 26.5, 1.0, 2.0, 0.3599877672942516, 1.0, 1.0, 0.3599877672942516, 1.0, 2.0, 0.5865938391518276, 6.9112, 6.9112, 121.94756008, 1314168.255047965, 1314168.255047965, 278965.8513886689], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2457000.0000, 
sim time next is 2457600.0000, 
raw observation next is [32.4, 25.66666666666667, 1.0, 2.0, 0.5200890858294521, 0.0, 1.0, 0.0, 1.0, 2.0, 0.8557439027043915, 6.911199999999999, 6.9112, 121.9260426156618, 1279610.74767653, 1279610.74767653, 260581.49509209], 
processed observation next is [1.0, 0.43478260869565216, 0.7555555555555555, 0.2566666666666667, 1.0, 1.0, 0.42867748313030013, 0.0, 0.5, -0.1904761904761905, 1.0, 1.0, 0.8196798783804893, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.45700383845590353, 0.45700383845590353, 0.5011182597924807], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.63721645], dtype=float32), 0.6301282]. 
=============================================
[2019-04-27 21:44:17,779] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2483037e-16 9.9999952e-01 3.0653928e-19 1.6125718e-19 5.1904624e-07], sum to 1.0000
[2019-04-27 21:44:17,785] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3401
[2019-04-27 21:44:17,791] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1321180.636407749 W.
[2019-04-27 21:44:17,795] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.6, 32.0, 1.0, 2.0, 0.9312843279907241, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.232914906224266, 6.9112, 121.9246835635676, 1321180.636407749, 1156435.686223628, 228613.2713850571], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2453400.0000, 
sim time next is 2454000.0000, 
raw observation next is [30.86666666666667, 31.0, 1.0, 2.0, 0.3525372052707372, 1.0, 1.0, 0.3525372052707372, 1.0, 1.0, 0.5745884255017004, 6.9112, 6.9112, 121.94756008, 1287332.270882124, 1287332.270882124, 275861.1588634209], 
processed observation next is [1.0, 0.391304347826087, 0.6987654320987656, 0.31, 1.0, 1.0, 0.2292109586556395, 1.0, 0.5, 0.2292109586556395, 1.0, 0.5, 0.46823553187712547, 0.0, 0.0, 0.8096049824067558, 0.45976152531504433, 0.45976152531504433, 0.5305022285835017], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.185501], dtype=float32), -1.8431702]. 
=============================================
[2019-04-27 21:44:17,813] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[57.370018]
 [56.99639 ]
 [56.598507]
 [56.18337 ]
 [55.545406]], R is [[57.12539673]
 [56.554142  ]
 [55.98860168]
 [55.91828918]
 [55.35910797]].
[2019-04-27 21:44:21,054] A3C_AGENT_WORKER-Thread-19 INFO:Local step 138000, global step 2189772: loss 0.0229
[2019-04-27 21:44:21,056] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 138000, global step 2189772: learning rate 0.0001
[2019-04-27 21:44:23,253] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:44:23,260] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5915
[2019-04-27 21:44:23,265] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.1, 39.0, 1.0, 2.0, 0.4715433000287685, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 564922.1597401334, 564922.1597401334, 138585.9414036685], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2570400.0000, 
sim time next is 2571000.0000, 
raw observation next is [31.83333333333334, 40.16666666666666, 1.0, 2.0, 0.4756446967895193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 569371.4441434431, 569371.4441434427, 139195.6146460963], 
processed observation next is [1.0, 0.782608695652174, 0.7345679012345682, 0.40166666666666656, 1.0, 1.0, 0.3757674961779992, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.20334694433694397, 0.2033469443369438, 0.267683874319416], 
reward next is 0.7323, 
noisyNet noise sample is [array([-0.57671016], dtype=float32), -0.68470645]. 
=============================================
[2019-04-27 21:44:23,281] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[68.76091]
 [68.45686]
 [68.1727 ]
 [66.88691]
 [64.40918]], R is [[69.568573  ]
 [69.60637665]
 [69.6450882 ]
 [69.68474579]
 [69.72530365]].
[2019-04-27 21:44:23,493] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.821783e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-27 21:44:23,502] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2201
[2019-04-27 21:44:23,507] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.76666666666667, 49.33333333333334, 1.0, 2.0, 0.5007967545481882, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 597275.8698285886, 597275.8698285886, 143019.2486999102], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2576400.0000, 
sim time next is 2577000.0000, 
raw observation next is [29.58333333333333, 50.16666666666666, 1.0, 2.0, 0.5022524246710173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 598942.5097923034, 598942.5097923034, 143245.7681656547], 
processed observation next is [1.0, 0.8260869565217391, 0.6512345679012344, 0.5016666666666666, 1.0, 1.0, 0.407443362703592, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21390803921153695, 0.21390803921153695, 0.2754726310877975], 
reward next is 0.7245, 
noisyNet noise sample is [array([-0.8636671], dtype=float32), 0.17353067]. 
=============================================
[2019-04-27 21:44:23,524] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[76.6669 ]
 [76.94575]
 [76.98533]
 [76.85299]
 [77.15512]], R is [[76.44173431]
 [76.40227509]
 [76.36380005]
 [76.32624817]
 [76.28947449]].
[2019-04-27 21:44:27,106] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.594625e-36 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-27 21:44:27,114] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6653
[2019-04-27 21:44:27,122] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2096753.573690245 W.
[2019-04-27 21:44:27,126] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.1, 59.5, 1.0, 2.0, 0.9191221131967587, 1.0, 2.0, 0.9191221131967587, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 2096753.573690245, 2096753.573690245, 395436.2034340668], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3421800.0000, 
sim time next is 3422400.0000, 
raw observation next is [31.4, 59.33333333333333, 1.0, 2.0, 0.9343743593229281, 1.0, 2.0, 0.9343743593229281, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 2131589.394940077, 2131589.394940077, 402408.78092367], 
processed observation next is [1.0, 0.6086956521739131, 0.7185185185185184, 0.5933333333333333, 1.0, 1.0, 0.9218742372892001, 1.0, 1.0, 0.9218742372892001, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.7612819267643132, 0.7612819267643132, 0.7738630402378269], 
reward next is 0.2261, 
noisyNet noise sample is [array([-1.7622321], dtype=float32), 0.5703834]. 
=============================================
[2019-04-27 21:44:29,987] A3C_AGENT_WORKER-Thread-21 INFO:Local step 137000, global step 2194019: loss 0.0036
[2019-04-27 21:44:29,992] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 137000, global step 2194019: learning rate 0.0001
[2019-04-27 21:44:30,138] A3C_AGENT_WORKER-Thread-3 INFO:Local step 138000, global step 2194092: loss 0.0047
[2019-04-27 21:44:30,142] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 138000, global step 2194092: learning rate 0.0001
[2019-04-27 21:44:30,576] A3C_AGENT_WORKER-Thread-15 INFO:Local step 137000, global step 2194299: loss 0.0017
[2019-04-27 21:44:30,578] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 137000, global step 2194300: learning rate 0.0001
[2019-04-27 21:44:30,746] A3C_AGENT_WORKER-Thread-12 INFO:Local step 137000, global step 2194380: loss 0.0057
[2019-04-27 21:44:30,747] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 137000, global step 2194380: learning rate 0.0001
[2019-04-27 21:44:30,993] A3C_AGENT_WORKER-Thread-2 INFO:Local step 137000, global step 2194494: loss 0.0020
[2019-04-27 21:44:30,995] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 137000, global step 2194495: learning rate 0.0001
[2019-04-27 21:44:31,002] A3C_AGENT_WORKER-Thread-17 INFO:Local step 137000, global step 2194497: loss 0.0016
[2019-04-27 21:44:31,004] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 137000, global step 2194497: learning rate 0.0001
[2019-04-27 21:44:31,729] A3C_AGENT_WORKER-Thread-9 INFO:Local step 137000, global step 2194857: loss 0.0166
[2019-04-27 21:44:31,734] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 137000, global step 2194857: learning rate 0.0001
[2019-04-27 21:44:31,966] A3C_AGENT_WORKER-Thread-20 INFO:Local step 137000, global step 2194967: loss 0.0032
[2019-04-27 21:44:31,969] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 137000, global step 2194967: learning rate 0.0001
[2019-04-27 21:44:31,996] A3C_AGENT_WORKER-Thread-22 INFO:Local step 137000, global step 2194978: loss 0.0016
[2019-04-27 21:44:32,000] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 137000, global step 2194978: learning rate 0.0001
[2019-04-27 21:44:32,074] A3C_AGENT_WORKER-Thread-18 INFO:Local step 137000, global step 2195020: loss 0.0020
[2019-04-27 21:44:32,076] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 137000, global step 2195020: learning rate 0.0001
[2019-04-27 21:44:32,224] A3C_AGENT_WORKER-Thread-11 INFO:Local step 137000, global step 2195089: loss 0.0012
[2019-04-27 21:44:32,228] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 137000, global step 2195089: learning rate 0.0001
[2019-04-27 21:44:32,593] A3C_AGENT_WORKER-Thread-14 INFO:Local step 137000, global step 2195265: loss 0.0032
[2019-04-27 21:44:32,596] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 137000, global step 2195266: learning rate 0.0001
[2019-04-27 21:44:32,626] A3C_AGENT_WORKER-Thread-16 INFO:Local step 137500, global step 2195277: loss 0.4204
[2019-04-27 21:44:32,627] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 137500, global step 2195277: learning rate 0.0001
[2019-04-27 21:44:32,842] A3C_AGENT_WORKER-Thread-13 INFO:Local step 137000, global step 2195376: loss 0.0037
[2019-04-27 21:44:32,844] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 137000, global step 2195376: learning rate 0.0001
[2019-04-27 21:44:32,900] A3C_AGENT_WORKER-Thread-10 INFO:Local step 137000, global step 2195400: loss 0.0111
[2019-04-27 21:44:32,902] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 137000, global step 2195401: learning rate 0.0001
[2019-04-27 21:44:33,577] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:44:33,587] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4901
[2019-04-27 21:44:33,593] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.6518209374643252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 742864.8672195461, 742864.8672195461, 167221.2449869135], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2757600.0000, 
sim time next is 2758200.0000, 
raw observation next is [25.91666666666667, 84.83333333333333, 1.0, 2.0, 0.6449019684677452, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 734975.7019669173, 734975.7019669173, 165973.6906181919], 
processed observation next is [0.0, 0.9565217391304348, 0.5154320987654323, 0.8483333333333333, 1.0, 1.0, 0.5772642481758871, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.26249132213104187, 0.26249132213104187, 0.3191801742657537], 
reward next is 0.6808, 
noisyNet noise sample is [array([-1.5038649], dtype=float32), -1.1592062]. 
=============================================
[2019-04-27 21:44:36,583] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.3623860e-29 1.0000000e+00 1.9602027e-34 1.4987959e-32 4.7734954e-32], sum to 1.0000
[2019-04-27 21:44:36,591] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8955
[2019-04-27 21:44:36,595] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.33333333333333, 94.0, 1.0, 2.0, 0.6307865419994789, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 737477.613795795, 737477.613795795, 164334.7244487175], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2781600.0000, 
sim time next is 2782200.0000, 
raw observation next is [23.16666666666667, 94.0, 1.0, 2.0, 0.614361487083144, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 720705.1578776696, 720705.1578776699, 161524.5760185166], 
processed observation next is [1.0, 0.17391304347826086, 0.4135802469135804, 0.94, 1.0, 1.0, 0.540906532241838, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.25739469924202485, 0.25739469924202496, 0.31062418465099345], 
reward next is 0.6894, 
noisyNet noise sample is [array([1.1137775], dtype=float32), -0.75643086]. 
=============================================
[2019-04-27 21:44:37,732] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:44:37,741] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3573
[2019-04-27 21:44:37,749] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 63.33333333333334, 1.0, 2.0, 0.6235041777034367, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 715176.2322249399, 715176.2322249399, 162395.0555186931], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3100800.0000, 
sim time next is 3101400.0000, 
raw observation next is [29.0, 62.0, 1.0, 2.0, 0.60747021360841, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 700300.4287062406, 700300.4287062406, 159757.2035214588], 
processed observation next is [1.0, 0.9130434782608695, 0.6296296296296297, 0.62, 1.0, 1.0, 0.5327026352481071, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2501072959665145, 0.2501072959665145, 0.3072253913874208], 
reward next is 0.6928, 
noisyNet noise sample is [array([-0.2825317], dtype=float32), 0.5872745]. 
=============================================
[2019-04-27 21:44:38,213] A3C_AGENT_WORKER-Thread-19 INFO:Local step 138500, global step 2197909: loss 0.0666
[2019-04-27 21:44:38,217] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 138500, global step 2197911: learning rate 0.0001
[2019-04-27 21:44:42,666] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-04-27 21:44:42,667] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 21:44:42,668] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 21:44:42,668] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:44:42,668] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 21:44:42,669] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:44:42,669] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 21:44:42,670] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:44:42,671] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:44:42,672] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 21:44:42,673] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:44:42,697] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run89
[2019-04-27 21:44:42,725] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run89
[2019-04-27 21:44:42,754] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run89
[2019-04-27 21:44:42,779] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run89
[2019-04-27 21:44:42,780] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run89
[2019-04-27 21:44:56,217] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.02149324], dtype=float32), -0.014378017]
[2019-04-27 21:44:56,220] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [23.20203618, 65.0853427, 1.0, 2.0, 0.3533659810789893, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 442455.7867274705, 442455.7867274701, 122334.5772240376]
[2019-04-27 21:44:56,221] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 21:44:56,225] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9306322777349547
[2019-04-27 21:45:04,630] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.02149324], dtype=float32), -0.014378017]
[2019-04-27 21:45:04,634] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [22.197061235, 74.801081245, 1.0, 2.0, 0.3864625259301627, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 480479.9127375933, 480479.9127375933, 126763.5645689031]
[2019-04-27 21:45:04,634] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 21:45:04,638] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7647862683346329
[2019-04-27 21:45:08,549] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02149324], dtype=float32), -0.014378017]
[2019-04-27 21:45:08,550] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [24.26988544666667, 59.961720505, 1.0, 2.0, 0.3974943746963105, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 496368.0893125171, 496368.0893125171, 128345.9916088915]
[2019-04-27 21:45:08,552] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 21:45:08,555] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5419130086883758
[2019-04-27 21:45:21,733] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02149324], dtype=float32), -0.014378017]
[2019-04-27 21:45:21,735] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [26.32227153, 64.96194388666666, 1.0, 2.0, 0.5001657521949864, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 597699.6044554795, 597699.6044554795, 142962.3686230781]
[2019-04-27 21:45:21,737] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 21:45:21,740] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5875092712990789
[2019-04-27 21:45:22,307] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.02149324], dtype=float32), -0.014378017]
[2019-04-27 21:45:22,309] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [26.51410294666666, 71.15370623333334, 1.0, 2.0, 0.5762190246197644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 673123.3123432596, 673123.3123432591, 154805.1356518549]
[2019-04-27 21:45:22,310] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 21:45:22,312] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.1772163252295299
[2019-04-27 21:45:22,447] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.02149324], dtype=float32), -0.014378017]
[2019-04-27 21:45:22,448] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [26.0, 79.0, 1.0, 2.0, 0.5925699622012209, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 685760.0321372217, 685760.0321372217, 157307.7171102297]
[2019-04-27 21:45:22,449] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 21:45:22,452] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6395207265848797
[2019-04-27 21:46:00,944] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.02149324], dtype=float32), -0.014378017]
[2019-04-27 21:46:00,944] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [23.11606759333333, 98.16793074, 1.0, 2.0, 0.5699385092777867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 662863.258797904, 662863.258797904, 153616.8906576572]
[2019-04-27 21:46:00,945] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 21:46:00,948] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4910033428274253
[2019-04-27 21:46:01,792] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.02149324], dtype=float32), -0.014378017]
[2019-04-27 21:46:01,793] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [25.0, 69.0, 1.0, 2.0, 0.4640395102992209, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 561804.6262924878, 561804.6262924878, 137645.2524095882]
[2019-04-27 21:46:01,793] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 21:46:01,798] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.12022477924103114
[2019-04-27 21:46:04,049] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02149324], dtype=float32), -0.014378017]
[2019-04-27 21:46:04,050] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [23.17213668, 77.99594009333333, 1.0, 2.0, 0.4428693839226473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 540141.3664122694, 540141.366412269, 134599.7746240846]
[2019-04-27 21:46:04,051] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 21:46:04,054] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.04288316879675769
[2019-04-27 21:46:06,034] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02149324], dtype=float32), -0.014378017]
[2019-04-27 21:46:06,035] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [25.20771604, 72.83034777, 1.0, 2.0, 0.5347027568932468, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 635873.9217687637, 635873.9217687637, 148381.7012957977]
[2019-04-27 21:46:06,036] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 21:46:06,039] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2757675755877129
[2019-04-27 21:46:13,949] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02149324], dtype=float32), -0.014378017]
[2019-04-27 21:46:13,950] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [30.1, 28.0, 1.0, 2.0, 0.9394988873932667, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.388299403626332, 6.9112, 121.9240333591587, 1429581.044892151, 1185267.53375799, 230937.2928264149]
[2019-04-27 21:46:13,952] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:46:13,954] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.5743395e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.311567568092354
[2019-04-27 21:46:13,955] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1429581.044892151 W.
[2019-04-27 21:46:28,249] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8700.1130 2195220806.1349 572.0000
[2019-04-27 21:46:28,271] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8922.6628 2120599135.7723 430.0000
[2019-04-27 21:46:28,283] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8582.8795 2248773970.1662 553.0000
[2019-04-27 21:46:28,299] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8770.2626 2170712315.3188 493.0000
[2019-04-27 21:46:28,540] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8098.4495 2445508074.6415 746.0000
[2019-04-27 21:46:29,558] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 2200000, evaluation results [2200000.0, 8098.4494892610255, 2445508074.64147, 746.0, 8770.262620159681, 2170712315.3187604, 493.0, 8922.662781171732, 2120599135.7722971, 430.0, 8582.879467926026, 2248773970.1662407, 553.0, 8700.112961499688, 2195220806.134913, 572.0]
[2019-04-27 21:46:29,711] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:46:29,717] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3505
[2019-04-27 21:46:29,720] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 74.0, 1.0, 2.0, 0.599064073900767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 690151.516148759, 690151.516148759, 158279.5169470511], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3531600.0000, 
sim time next is 3532200.0000, 
raw observation next is [27.0, 74.83333333333334, 1.0, 2.0, 0.6061410830450965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 696432.3938485297, 696432.3938485297, 159414.8902705814], 
processed observation next is [1.0, 0.9130434782608695, 0.5555555555555556, 0.7483333333333334, 1.0, 1.0, 0.5311203369584482, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.24872585494590346, 0.24872585494590346, 0.306567096674195], 
reward next is 0.6934, 
noisyNet noise sample is [array([0.16471337], dtype=float32), -1.06525]. 
=============================================
[2019-04-27 21:46:33,085] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3360253e-31 1.0000000e+00 3.4811307e-36 1.7634379e-36 1.2691978e-36], sum to 1.0000
[2019-04-27 21:46:33,091] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3362
[2019-04-27 21:46:33,104] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1609333.907523665 W.
[2019-04-27 21:46:33,110] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.66666666666667, 87.33333333333334, 1.0, 2.0, 0.7845739804304909, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.9260426156618, 1609333.907523665, 1609333.907523666, 333189.6919952715], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2971200.0000, 
sim time next is 2971800.0000, 
raw observation next is [27.0, 86.5, 1.0, 2.0, 0.4961686386015489, 1.0, 1.0, 0.4961686386015489, 1.0, 2.0, 0.7899166317381006, 6.9112, 6.9112, 121.94756008, 1697452.750493672, 1697452.750493672, 341712.5336262805], 
processed observation next is [1.0, 0.391304347826087, 0.5555555555555556, 0.865, 1.0, 1.0, 0.40020076023993917, 1.0, 0.5, 0.40020076023993917, 1.0, 1.0, 0.7373957896726258, 0.0, 0.0, 0.8096049824067558, 0.6062331251763114, 0.6062331251763114, 0.6571394877428471], 
reward next is 0.3429, 
noisyNet noise sample is [array([-0.62784106], dtype=float32), -0.7633446]. 
=============================================
[2019-04-27 21:46:33,649] A3C_AGENT_WORKER-Thread-21 INFO:Local step 137500, global step 2202125: loss 0.2449
[2019-04-27 21:46:33,652] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 137500, global step 2202125: learning rate 0.0001
[2019-04-27 21:46:33,816] A3C_AGENT_WORKER-Thread-3 INFO:Local step 138500, global step 2202224: loss 0.4021
[2019-04-27 21:46:33,818] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 138500, global step 2202224: learning rate 0.0001
[2019-04-27 21:46:34,061] A3C_AGENT_WORKER-Thread-15 INFO:Local step 137500, global step 2202374: loss 0.0666
[2019-04-27 21:46:34,064] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 137500, global step 2202377: learning rate 0.0001
[2019-04-27 21:46:34,198] A3C_AGENT_WORKER-Thread-12 INFO:Local step 137500, global step 2202458: loss 0.1302
[2019-04-27 21:46:34,199] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 137500, global step 2202458: learning rate 0.0001
[2019-04-27 21:46:34,352] A3C_AGENT_WORKER-Thread-17 INFO:Local step 137500, global step 2202549: loss 1.5229
[2019-04-27 21:46:34,353] A3C_AGENT_WORKER-Thread-2 INFO:Local step 137500, global step 2202552: loss 0.8577
[2019-04-27 21:46:34,357] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 137500, global step 2202552: learning rate 0.0001
[2019-04-27 21:46:34,358] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 137500, global step 2202553: learning rate 0.0001
[2019-04-27 21:46:34,754] A3C_AGENT_WORKER-Thread-9 INFO:Local step 137500, global step 2202790: loss 0.3108
[2019-04-27 21:46:34,756] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 137500, global step 2202790: learning rate 0.0001
[2019-04-27 21:46:35,041] A3C_AGENT_WORKER-Thread-22 INFO:Local step 137500, global step 2202924: loss 0.4124
[2019-04-27 21:46:35,043] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 137500, global step 2202925: learning rate 0.0001
[2019-04-27 21:46:35,084] A3C_AGENT_WORKER-Thread-20 INFO:Local step 137500, global step 2202947: loss 0.5410
[2019-04-27 21:46:35,088] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 137500, global step 2202947: learning rate 0.0001
[2019-04-27 21:46:35,270] A3C_AGENT_WORKER-Thread-11 INFO:Local step 137500, global step 2203034: loss 0.1731
[2019-04-27 21:46:35,272] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 137500, global step 2203034: learning rate 0.0001
[2019-04-27 21:46:35,308] A3C_AGENT_WORKER-Thread-18 INFO:Local step 137500, global step 2203049: loss 0.0160
[2019-04-27 21:46:35,311] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 137500, global step 2203050: learning rate 0.0001
[2019-04-27 21:46:35,433] A3C_AGENT_WORKER-Thread-16 INFO:Local step 138000, global step 2203107: loss 0.0367
[2019-04-27 21:46:35,437] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 138000, global step 2203108: learning rate 0.0001
[2019-04-27 21:46:35,733] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:46:35,740] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1511
[2019-04-27 21:46:35,745] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.91666666666667, 96.66666666666666, 1.0, 2.0, 0.6232751822900258, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 714885.505488679, 714885.5054886786, 162353.4496920242], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3019800.0000, 
sim time next is 3020400.0000, 
raw observation next is [23.9, 96.0, 1.0, 2.0, 0.6149240732899357, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 706894.3493659113, 706894.3493659113, 160963.3721268478], 
processed observation next is [1.0, 1.0, 0.4407407407407407, 0.96, 1.0, 1.0, 0.5415762777261139, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.25246226763068264, 0.25246226763068264, 0.30954494639778424], 
reward next is 0.6905, 
noisyNet noise sample is [array([-0.7591632], dtype=float32), 0.32933104]. 
=============================================
[2019-04-27 21:46:35,797] A3C_AGENT_WORKER-Thread-13 INFO:Local step 137500, global step 2203277: loss 0.0236
[2019-04-27 21:46:35,803] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 137500, global step 2203277: learning rate 0.0001
[2019-04-27 21:46:35,882] A3C_AGENT_WORKER-Thread-14 INFO:Local step 137500, global step 2203318: loss 0.3122
[2019-04-27 21:46:35,885] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 137500, global step 2203319: learning rate 0.0001
[2019-04-27 21:46:36,131] A3C_AGENT_WORKER-Thread-10 INFO:Local step 137500, global step 2203437: loss 0.1794
[2019-04-27 21:46:36,135] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 137500, global step 2203437: learning rate 0.0001
[2019-04-27 21:46:40,899] A3C_AGENT_WORKER-Thread-19 INFO:Local step 139000, global step 2205684: loss 0.0508
[2019-04-27 21:46:40,902] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 139000, global step 2205684: learning rate 0.0001
[2019-04-27 21:46:41,011] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [8.050445e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-27 21:46:41,018] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2541
[2019-04-27 21:46:41,024] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333334, 47.16666666666666, 1.0, 2.0, 0.4371854902916439, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 534959.9507497991, 534959.9507497987, 133810.7054013289], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3117000.0000, 
sim time next is 3117600.0000, 
raw observation next is [28.4, 45.0, 1.0, 2.0, 0.4220023934246077, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 519176.580620777, 519176.580620777, 131672.4825104056], 
processed observation next is [1.0, 0.08695652173913043, 0.6074074074074074, 0.45, 1.0, 1.0, 0.3119076112197711, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.18542020736456322, 0.18542020736456322, 0.25321631252001076], 
reward next is 0.7468, 
noisyNet noise sample is [array([-1.7423139], dtype=float32), -2.7451584]. 
=============================================
[2019-04-27 21:46:43,154] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.1704172e-31 1.0000000e+00 2.0308200e-36 9.2606350e-38 5.2916685e-38], sum to 1.0000
[2019-04-27 21:46:43,160] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5004
[2019-04-27 21:46:43,168] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1406454.332785745 W.
[2019-04-27 21:46:43,172] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.16666666666666, 50.5, 1.0, 2.0, 0.6080703259016589, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9694041190811269, 6.911199999999999, 6.9112, 121.9260426156618, 1406454.332785745, 1406454.332785746, 296705.2870755692], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3143400.0000, 
sim time next is 3144000.0000, 
raw observation next is [30.33333333333334, 49.0, 1.0, 2.0, 0.4100111655814944, 1.0, 1.0, 0.4100111655814944, 1.0, 2.0, 0.6528046389966659, 6.9112, 6.9112, 121.94756008, 1404825.811264542, 1404825.811264542, 301512.3843829997], 
processed observation next is [1.0, 0.391304347826087, 0.6790123456790126, 0.49, 1.0, 1.0, 0.2976323399779695, 1.0, 0.5, 0.2976323399779695, 1.0, 1.0, 0.5660057987458322, 0.0, 0.0, 0.8096049824067558, 0.5017235040230507, 0.5017235040230507, 0.5798315084288456], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.31671268], dtype=float32), -1.8455623]. 
=============================================
[2019-04-27 21:46:43,187] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[49.688858]
 [49.79034 ]
 [49.898033]
 [50.029312]
 [50.084095]], R is [[48.29740906]
 [47.81443405]
 [47.33628845]
 [46.86292648]
 [46.39429855]].
[2019-04-27 21:46:44,271] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:46:44,277] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3790
[2019-04-27 21:46:44,287] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1959113.065340796 W.
[2019-04-27 21:46:44,292] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [35.0, 32.0, 1.0, 2.0, 0.5725686699862235, 1.0, 2.0, 0.5725686699862235, 1.0, 1.0, 0.9115479698778163, 6.9112, 6.9112, 121.94756008, 1959113.065340796, 1959113.065340796, 380632.2891291056], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3168000.0000, 
sim time next is 3168600.0000, 
raw observation next is [34.95, 32.0, 1.0, 2.0, 1.02, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9856992361953597, 7.099608712519323, 6.9112, 121.9250736002347, 1990413.598135473, 1893932.264343928, 381728.7872622602], 
processed observation next is [1.0, 0.6956521739130435, 0.8500000000000001, 0.32, 1.0, 1.0, 1.0238095238095237, 0.0, 0.5, -0.1904761904761905, 1.0, 1.0, 0.9821240452441995, 0.018840871251932255, 0.0, 0.809455695565405, 0.7108619993340974, 0.6764043801228314, 0.7340938216581927], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.3428353], dtype=float32), -0.6930077]. 
=============================================
[2019-04-27 21:46:45,325] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:46:45,339] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5903
[2019-04-27 21:46:45,346] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1869838.088510446 W.
[2019-04-27 21:46:45,350] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [34.75, 32.0, 1.0, 2.0, 0.8107415750502212, 1.0, 2.0, 0.8107415750502212, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426081371, 1869838.088510446, 1869838.088510446, 349297.7990497912], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3171000.0000, 
sim time next is 3171600.0000, 
raw observation next is [34.7, 32.0, 1.0, 2.0, 0.9114089556671378, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9802048117712304, 6.9112, 6.9112, 121.9260426156595, 1775375.719302257, 1775375.719302257, 356569.2184112534], 
processed observation next is [1.0, 0.7391304347826086, 0.8407407407407409, 0.32, 1.0, 1.0, 0.8945344710323069, 0.0, 0.5, -0.1904761904761905, 1.0, 0.5, 0.9752560147140379, 0.0, 0.0, 0.8094621288201207, 0.6340627568936632, 0.6340627568936632, 0.6857100354062565], 
reward next is 0.3143, 
noisyNet noise sample is [array([0.27772805], dtype=float32), -0.6728325]. 
=============================================
[2019-04-27 21:46:46,425] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:46:46,434] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9654
[2019-04-27 21:46:46,441] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666667, 74.83333333333334, 1.0, 2.0, 0.5796350760077945, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 675639.7203346051, 675639.7203346051, 155320.5163890673], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3196200.0000, 
sim time next is 3196800.0000, 
raw observation next is [26.0, 74.0, 1.0, 2.0, 0.5633336866446553, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 660411.1262160483, 660411.1262160479, 152738.4712860762], 
processed observation next is [0.0, 0.0, 0.5185185185185185, 0.74, 1.0, 1.0, 0.4801591507674468, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.23586111650573152, 0.23586111650573138, 0.2937278293963004], 
reward next is 0.7063, 
noisyNet noise sample is [array([-1.8993254], dtype=float32), -0.9542135]. 
=============================================
[2019-04-27 21:46:50,250] A3C_AGENT_WORKER-Thread-21 INFO:Local step 138000, global step 2210098: loss 0.0087
[2019-04-27 21:46:50,253] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 138000, global step 2210098: learning rate 0.0001
[2019-04-27 21:46:50,485] A3C_AGENT_WORKER-Thread-3 INFO:Local step 139000, global step 2210206: loss 0.0630
[2019-04-27 21:46:50,488] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 139000, global step 2210207: learning rate 0.0001
[2019-04-27 21:46:50,780] A3C_AGENT_WORKER-Thread-15 INFO:Local step 138000, global step 2210344: loss 0.0502
[2019-04-27 21:46:50,781] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 138000, global step 2210344: learning rate 0.0001
[2019-04-27 21:46:50,966] A3C_AGENT_WORKER-Thread-12 INFO:Local step 138000, global step 2210427: loss 0.0123
[2019-04-27 21:46:50,968] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 138000, global step 2210427: learning rate 0.0001
[2019-04-27 21:46:51,152] A3C_AGENT_WORKER-Thread-17 INFO:Local step 138000, global step 2210515: loss 0.0290
[2019-04-27 21:46:51,155] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 138000, global step 2210516: learning rate 0.0001
[2019-04-27 21:46:51,167] A3C_AGENT_WORKER-Thread-2 INFO:Local step 138000, global step 2210521: loss 0.0181
[2019-04-27 21:46:51,170] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 138000, global step 2210522: learning rate 0.0001
[2019-04-27 21:46:51,672] A3C_AGENT_WORKER-Thread-9 INFO:Local step 138000, global step 2210768: loss 0.0484
[2019-04-27 21:46:51,674] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 138000, global step 2210769: learning rate 0.0001
[2019-04-27 21:46:51,838] A3C_AGENT_WORKER-Thread-22 INFO:Local step 138000, global step 2210846: loss 0.0443
[2019-04-27 21:46:51,841] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 138000, global step 2210846: learning rate 0.0001
[2019-04-27 21:46:52,042] A3C_AGENT_WORKER-Thread-20 INFO:Local step 138000, global step 2210947: loss 0.0285
[2019-04-27 21:46:52,044] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 138000, global step 2210948: learning rate 0.0001
[2019-04-27 21:46:52,253] A3C_AGENT_WORKER-Thread-11 INFO:Local step 138000, global step 2211050: loss 0.0540
[2019-04-27 21:46:52,256] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 138000, global step 2211052: learning rate 0.0001
[2019-04-27 21:46:52,320] A3C_AGENT_WORKER-Thread-18 INFO:Local step 138000, global step 2211080: loss 0.0373
[2019-04-27 21:46:52,322] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 138000, global step 2211080: learning rate 0.0001
[2019-04-27 21:46:52,690] A3C_AGENT_WORKER-Thread-16 INFO:Local step 138500, global step 2211259: loss 0.1588
[2019-04-27 21:46:52,693] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 138500, global step 2211259: learning rate 0.0001
[2019-04-27 21:46:52,704] A3C_AGENT_WORKER-Thread-13 INFO:Local step 138000, global step 2211266: loss 0.0332
[2019-04-27 21:46:52,709] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 138000, global step 2211266: learning rate 0.0001
[2019-04-27 21:46:52,810] A3C_AGENT_WORKER-Thread-14 INFO:Local step 138000, global step 2211316: loss 0.0476
[2019-04-27 21:46:52,812] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 138000, global step 2211316: learning rate 0.0001
[2019-04-27 21:46:53,054] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:46:53,066] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6196
[2019-04-27 21:46:53,072] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.15, 76.0, 1.0, 2.0, 0.7691149365758765, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 876618.5008408854, 876618.5008408854, 189645.1020437579], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3958200.0000, 
sim time next is 3958800.0000, 
raw observation next is [29.2, 75.0, 1.0, 2.0, 0.7673327315892439, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 874586.0279146407, 874586.0279146407, 189286.0553163248], 
processed observation next is [0.0, 0.8260869565217391, 0.637037037037037, 0.75, 1.0, 1.0, 0.7230151566538617, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3123521528266574, 0.3123521528266574, 0.36401164483908616], 
reward next is 0.6360, 
noisyNet noise sample is [array([0.06161518], dtype=float32), -0.7269276]. 
=============================================
[2019-04-27 21:46:53,114] A3C_AGENT_WORKER-Thread-10 INFO:Local step 138000, global step 2211463: loss 0.0268
[2019-04-27 21:46:53,116] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 138000, global step 2211463: learning rate 0.0001
[2019-04-27 21:46:53,888] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:46:53,897] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7908
[2019-04-27 21:46:53,904] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666667, 88.16666666666667, 1.0, 2.0, 0.692334984962103, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 789061.5321166121, 789061.5321166121, 174697.4625374427], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3361800.0000, 
sim time next is 3362400.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.6902531025586766, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 786687.5710085721, 786687.5710085721, 174306.2998859342], 
processed observation next is [0.0, 0.9565217391304348, 0.5185185185185185, 0.89, 1.0, 1.0, 0.6312536935222339, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2809598467887758, 0.2809598467887758, 0.33520442285756574], 
reward next is 0.6648, 
noisyNet noise sample is [array([0.17238605], dtype=float32), 0.16478823]. 
=============================================
[2019-04-27 21:46:55,025] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.91271062e-30 1.00000000e+00 9.56205696e-34 2.63144972e-34
 1.30991305e-33], sum to 1.0000
[2019-04-27 21:46:55,037] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2683
[2019-04-27 21:46:55,043] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.45, 94.0, 1.0, 2.0, 0.6720767134192764, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 783767.9220673472, 783767.9220673467, 171783.1671095428], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3385800.0000, 
sim time next is 3386400.0000, 
raw observation next is [23.26666666666667, 94.0, 1.0, 2.0, 0.6378293925796378, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 746638.4551639896, 746638.4551639891, 165640.1876819055], 
processed observation next is [1.0, 0.17391304347826086, 0.41728395061728407, 0.94, 1.0, 1.0, 0.5688445149757593, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2666565911299963, 0.2666565911299961, 0.3185388224652029], 
reward next is 0.6815, 
noisyNet noise sample is [array([1.1395622], dtype=float32), 0.9103201]. 
=============================================
[2019-04-27 21:46:55,752] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.5486388e-30 1.0000000e+00 7.7320189e-34 1.6502586e-35 1.6508737e-30], sum to 1.0000
[2019-04-27 21:46:55,759] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1296
[2019-04-27 21:46:55,764] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.16666666666666, 92.5, 1.0, 2.0, 0.6566356313974391, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 758412.7379660967, 758412.7379660967, 168590.7318271912], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3394200.0000, 
sim time next is 3394800.0000, 
raw observation next is [24.4, 91.0, 1.0, 2.0, 0.6528926096285191, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 753492.5181797235, 753492.5181797231, 167879.703057833], 
processed observation next is [1.0, 0.30434782608695654, 0.4592592592592592, 0.91, 1.0, 1.0, 0.5867769162244275, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2691044707784727, 0.26910447077847255, 0.322845582803525], 
reward next is 0.6772, 
noisyNet noise sample is [array([1.4146872], dtype=float32), -0.82735133]. 
=============================================
[2019-04-27 21:46:58,292] A3C_AGENT_WORKER-Thread-19 INFO:Local step 139500, global step 2213904: loss 9.0261
[2019-04-27 21:46:58,293] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 139500, global step 2213904: learning rate 0.0001
[2019-04-27 21:47:07,110] A3C_AGENT_WORKER-Thread-21 INFO:Local step 138500, global step 2218083: loss 0.0890
[2019-04-27 21:47:07,113] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 138500, global step 2218083: learning rate 0.0001
[2019-04-27 21:47:07,698] A3C_AGENT_WORKER-Thread-3 INFO:Local step 139500, global step 2218356: loss 3.5139
[2019-04-27 21:47:07,701] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 139500, global step 2218357: learning rate 0.0001
[2019-04-27 21:47:07,740] A3C_AGENT_WORKER-Thread-15 INFO:Local step 138500, global step 2218376: loss 0.4234
[2019-04-27 21:47:07,745] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 138500, global step 2218377: learning rate 0.0001
[2019-04-27 21:47:07,828] A3C_AGENT_WORKER-Thread-12 INFO:Local step 138500, global step 2218418: loss 0.1121
[2019-04-27 21:47:07,836] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 138500, global step 2218420: learning rate 0.0001
[2019-04-27 21:47:07,944] A3C_AGENT_WORKER-Thread-17 INFO:Local step 138500, global step 2218473: loss 0.4874
[2019-04-27 21:47:07,949] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 138500, global step 2218473: learning rate 0.0001
[2019-04-27 21:47:08,037] A3C_AGENT_WORKER-Thread-2 INFO:Local step 138500, global step 2218515: loss 0.3270
[2019-04-27 21:47:08,039] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 138500, global step 2218515: learning rate 0.0001
[2019-04-27 21:47:08,703] A3C_AGENT_WORKER-Thread-9 INFO:Local step 138500, global step 2218838: loss 0.4583
[2019-04-27 21:47:08,705] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 138500, global step 2218838: learning rate 0.0001
[2019-04-27 21:47:08,882] A3C_AGENT_WORKER-Thread-22 INFO:Local step 138500, global step 2218921: loss 0.2126
[2019-04-27 21:47:08,884] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 138500, global step 2218923: learning rate 0.0001
[2019-04-27 21:47:09,002] A3C_AGENT_WORKER-Thread-11 INFO:Local step 138500, global step 2218974: loss 0.3405
[2019-04-27 21:47:09,006] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 138500, global step 2218974: learning rate 0.0001
[2019-04-27 21:47:09,087] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:47:09,094] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3609
[2019-04-27 21:47:09,102] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1456181.123414789 W.
[2019-04-27 21:47:09,105] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.1, 81.66666666666667, 1.0, 2.0, 0.6385602321485508, 1.0, 2.0, 0.6385602321485508, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1456181.123414789, 1456181.12341479, 281558.9904312956], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3601200.0000, 
sim time next is 3601800.0000, 
raw observation next is [25.15, 81.0, 1.0, 2.0, 0.6516590586176285, 1.0, 2.0, 0.6516590586176285, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 1486080.841671424, 1486080.841671423, 286268.5453960929], 
processed observation next is [1.0, 0.6956521739130435, 0.487037037037037, 0.81, 1.0, 1.0, 0.5853084031162245, 1.0, 1.0, 0.5853084031162245, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.5307431577397943, 0.530743157739794, 0.5505164334540248], 
reward next is 0.4495, 
noisyNet noise sample is [array([0.54145557], dtype=float32), 1.1038035]. 
=============================================
[2019-04-27 21:47:09,121] A3C_AGENT_WORKER-Thread-20 INFO:Local step 138500, global step 2219031: loss 0.1607
[2019-04-27 21:47:09,124] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 138500, global step 2219032: learning rate 0.0001
[2019-04-27 21:47:09,195] A3C_AGENT_WORKER-Thread-16 INFO:Local step 139000, global step 2219066: loss 0.0820
[2019-04-27 21:47:09,196] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 139000, global step 2219067: learning rate 0.0001
[2019-04-27 21:47:09,302] A3C_AGENT_WORKER-Thread-18 INFO:Local step 138500, global step 2219122: loss 0.1639
[2019-04-27 21:47:09,303] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 138500, global step 2219122: learning rate 0.0001
[2019-04-27 21:47:09,323] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:47:09,332] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9824
[2019-04-27 21:47:09,337] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.66666666666666, 92.66666666666667, 1.0, 2.0, 0.5590435247711133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 652416.70431743, 652416.70431743, 151895.0938447245], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3619200.0000, 
sim time next is 3619800.0000, 
raw observation next is [23.5, 94.5, 1.0, 2.0, 0.5659090101261438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 659498.7813577587, 659498.7813577587, 153000.1087357659], 
processed observation next is [1.0, 0.9130434782608695, 0.42592592592592593, 0.945, 1.0, 1.0, 0.48322501205493307, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2355352790563424, 0.2355352790563424, 0.2942309783380114], 
reward next is 0.7058, 
noisyNet noise sample is [array([0.4568012], dtype=float32), 0.1984801]. 
=============================================
[2019-04-27 21:47:09,619] A3C_AGENT_WORKER-Thread-13 INFO:Local step 138500, global step 2219270: loss 0.3333
[2019-04-27 21:47:09,621] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 138500, global step 2219270: learning rate 0.0001
[2019-04-27 21:47:09,684] A3C_AGENT_WORKER-Thread-14 INFO:Local step 138500, global step 2219300: loss 0.1984
[2019-04-27 21:47:09,688] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 138500, global step 2219301: learning rate 0.0001
[2019-04-27 21:47:10,040] A3C_AGENT_WORKER-Thread-10 INFO:Local step 138500, global step 2219471: loss 0.2023
[2019-04-27 21:47:10,045] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 138500, global step 2219471: learning rate 0.0001
[2019-04-27 21:47:12,688] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.6743982e-27 1.0000000e+00 4.1659038e-30 3.0663196e-33 1.8349752e-26], sum to 1.0000
[2019-04-27 21:47:12,694] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3497
[2019-04-27 21:47:12,700] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1395306.24086685 W.
[2019-04-27 21:47:12,706] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [23.16666666666667, 99.00000000000001, 1.0, 2.0, 0.4079266350747685, 1.0, 1.0, 0.4079266350747685, 1.0, 1.0, 0.6494324882820447, 6.911200000000001, 6.9112, 121.94756008, 1395306.24086685, 1395306.24086685, 300571.8481032739], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3672600.0000, 
sim time next is 3673200.0000, 
raw observation next is [23.33333333333334, 98.0, 1.0, 2.0, 0.4566751560831942, 0.0, 1.0, 0.0, 1.0, 2.0, 0.7270417213559496, 6.9112, 6.9112, 121.9260425494359, 1041126.148906362, 1041126.148906362, 242232.4516088355], 
processed observation next is [1.0, 0.5217391304347826, 0.4197530864197533, 0.98, 1.0, 1.0, 0.3531847096228502, 0.0, 0.5, -0.1904761904761905, 1.0, 1.0, 0.6588021516949368, 0.0, 0.0, 0.8094621283804648, 0.37183076746655785, 0.37183076746655785, 0.46583163770929903], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.4326787], dtype=float32), 0.022765363]. 
=============================================
[2019-04-27 21:47:14,962] A3C_AGENT_WORKER-Thread-19 INFO:Local step 140000, global step 2221800: loss 0.0059
[2019-04-27 21:47:14,964] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 140000, global step 2221800: learning rate 0.0001
[2019-04-27 21:47:17,067] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:47:17,076] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0813
[2019-04-27 21:47:17,085] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2320211.593737478 W.
[2019-04-27 21:47:17,088] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.26666666666667, 61.33333333333333, 1.0, 2.0, 0.7292028638615068, 1.0, 2.0, 0.6779660939071881, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.94756008, 2320211.593737478, 2320211.593737478, 437179.5626312132], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3775200.0000, 
sim time next is 3775800.0000, 
raw observation next is [32.83333333333333, 66.16666666666667, 1.0, 2.0, 0.8752845558364665, 1.0, 2.0, 0.7510069398946678, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.94756008, 2570540.045456795, 2570540.045456795, 479663.2061409868], 
processed observation next is [1.0, 0.6956521739130435, 0.7716049382716048, 0.6616666666666667, 1.0, 1.0, 0.8515292331386506, 1.0, 1.0, 0.703579690350795, 1.0, 1.0, 0.9972168686025908, 0.0, 0.0, 0.8096049824067558, 0.9180500162345696, 0.9180500162345696, 0.9224292425788208], 
reward next is 0.0776, 
noisyNet noise sample is [array([0.88773346], dtype=float32), -1.055112]. 
=============================================
[2019-04-27 21:47:17,560] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:47:17,568] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4252
[2019-04-27 21:47:17,573] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.75, 67.0, 1.0, 2.0, 0.7802597750347094, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 889328.4867677265, 889328.4867677265, 191904.0493726349], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3785400.0000, 
sim time next is 3786000.0000, 
raw observation next is [31.33333333333334, 70.66666666666667, 1.0, 2.0, 0.8065515221866764, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 919313.4027274995, 919313.4027274995, 197307.5699735106], 
processed observation next is [1.0, 0.8260869565217391, 0.7160493827160496, 0.7066666666666667, 1.0, 1.0, 0.7697041930793767, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.32832621525982125, 0.32832621525982125, 0.37943763456444346], 
reward next is 0.6206, 
noisyNet noise sample is [array([-0.32066107], dtype=float32), -0.37689075]. 
=============================================
[2019-04-27 21:47:17,583] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[82.29422 ]
 [81.982056]
 [81.843605]
 [81.64634 ]
 [81.11471 ]], R is [[82.37371826]
 [82.18093109]
 [82.00463867]
 [81.8376236 ]
 [81.67276001]].
[2019-04-27 21:47:19,846] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:47:19,854] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1242
[2019-04-27 21:47:19,860] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.58333333333333, 72.33333333333333, 1.0, 2.0, 0.5731643605778134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 668825.050354991, 668825.050354991, 154257.5633832238], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3809400.0000, 
sim time next is 3810000.0000, 
raw observation next is [26.46666666666667, 72.66666666666667, 1.0, 2.0, 0.5681402905774049, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 663772.2166963534, 663772.2166963534, 153447.2189862411], 
processed observation next is [0.0, 0.08695652173913043, 0.5358024691358025, 0.7266666666666667, 1.0, 1.0, 0.4858812983064344, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.23706150596298337, 0.23706150596298337, 0.2950908057427713], 
reward next is 0.7049, 
noisyNet noise sample is [array([1.7038937], dtype=float32), 1.2932268]. 
=============================================
[2019-04-27 21:47:19,869] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[71.26783 ]
 [71.28667 ]
 [70.88705 ]
 [70.638176]
 [70.31838 ]], R is [[71.49801636]
 [71.48638916]
 [71.47283173]
 [71.45571899]
 [71.43421936]].
[2019-04-27 21:47:21,670] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-27 21:47:21,670] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 21:47:21,672] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 21:47:21,673] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:47:21,673] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 21:47:21,675] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:47:21,674] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 21:47:21,676] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:47:21,674] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 21:47:21,680] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:47:21,680] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:47:21,708] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run90
[2019-04-27 21:47:21,708] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run90
[2019-04-27 21:47:21,759] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run90
[2019-04-27 21:47:21,780] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run90
[2019-04-27 21:47:21,814] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run90
[2019-04-27 21:47:25,571] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0148017], dtype=float32), -0.016894735]
[2019-04-27 21:47:25,573] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [21.53333333333333, 62.66666666666667, 1.0, 2.0, 0.2758124172645286, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 352818.342948971, 352818.3429489706, 112640.5830024893]
[2019-04-27 21:47:25,574] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:47:25,578] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.09210392498245723
[2019-04-27 21:47:29,387] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0148017], dtype=float32), -0.016894735]
[2019-04-27 21:47:29,387] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [25.83333333333334, 46.33333333333333, 1.0, 2.0, 0.8330627348986528, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1050252.506727442, 1050252.506727442, 206667.0511668756]
[2019-04-27 21:47:29,388] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:47:29,390] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.2961767e-29 1.0000000e+00 1.1089266e-32 5.0916986e-33 2.8643266e-31], sampled 0.11568597804524061
[2019-04-27 21:47:40,130] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.0148017], dtype=float32), -0.016894735]
[2019-04-27 21:47:40,133] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [18.33333333333334, 65.33333333333333, 1.0, 2.0, 0.2150426839158594, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 277378.4068001808, 277378.4068001808, 87787.03601969937]
[2019-04-27 21:47:40,134] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 21:47:40,136] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2782945694602229
[2019-04-27 21:47:51,005] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.0148017], dtype=float32), -0.016894735]
[2019-04-27 21:47:51,007] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [18.7, 92.0, 1.0, 2.0, 0.3238162767052502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 409177.3657679023, 409177.3657679023, 118539.7529552253]
[2019-04-27 21:47:51,009] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 21:47:51,012] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.34695370913545864
[2019-04-27 21:47:51,487] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0148017], dtype=float32), -0.016894735]
[2019-04-27 21:47:51,488] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [23.65868315166667, 73.74650335833334, 1.0, 2.0, 0.4283870395176082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 523281.2934074721, 523281.2934074721, 132498.1349321323]
[2019-04-27 21:47:51,490] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 21:47:51,493] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.37051318338120054
[2019-04-27 21:48:00,469] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0148017], dtype=float32), -0.016894735]
[2019-04-27 21:48:00,471] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [22.629446025, 102.61463325, 1.0, 2.0, 0.5776604848917626, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 674860.6680377587, 674860.6680377587, 155052.4474836851]
[2019-04-27 21:48:00,473] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 21:48:00,477] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [7.0581386e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.37104580810871657
[2019-04-27 21:48:01,930] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0148017], dtype=float32), -0.016894735]
[2019-04-27 21:48:01,931] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [27.5, 72.0, 1.0, 2.0, 0.6038000372321792, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 692989.8547709916, 692989.8547709916, 158972.6341720419]
[2019-04-27 21:48:01,931] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 21:48:01,936] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2832999594574286
[2019-04-27 21:49:00,278] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0148017], dtype=float32), -0.016894735]
[2019-04-27 21:49:00,279] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [25.99116620333333, 70.72048673333333, 1.0, 2.0, 0.7500496443879862, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 882965.9786399667, 882965.9786399667, 187191.7553492339]
[2019-04-27 21:49:00,281] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 21:49:00,283] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.427853e-35 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.20320274672288974
[2019-04-27 21:49:03,624] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0148017], dtype=float32), -0.016894735]
[2019-04-27 21:49:03,625] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [26.96666666666667, 64.33333333333334, 1.0, 2.0, 0.508309922728781, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 604120.487263188, 604120.487263188, 144126.275294563]
[2019-04-27 21:49:03,627] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:49:03,629] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6409097002115535
[2019-04-27 21:49:07,245] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0148017], dtype=float32), -0.016894735]
[2019-04-27 21:49:07,247] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [18.71414105833333, 93.36193884333333, 1.0, 2.0, 0.4397595690056677, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 555105.4171803844, 555105.4171803844, 134540.4936082517]
[2019-04-27 21:49:07,247] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 21:49:07,250] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [8.349771e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.3278935488271505
[2019-04-27 21:49:07,847] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8770.2626 2170712315.3188 493.0000
[2019-04-27 21:49:07,907] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8700.8700 2195155380.8105 572.0000
[2019-04-27 21:49:07,992] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8922.6628 2120599135.7723 430.0000
[2019-04-27 21:49:08,012] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8582.8795 2248773970.1662 553.0000
[2019-04-27 21:49:08,040] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8099.2132 2445444095.4335 746.0000
[2019-04-27 21:49:09,059] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 2225000, evaluation results [2225000.0, 8099.213190057684, 2445444095.433475, 746.0, 8770.262620159681, 2170712315.3187604, 493.0, 8922.662781171732, 2120599135.7722971, 430.0, 8582.879467926026, 2248773970.1662407, 553.0, 8700.870031992405, 2195155380.8105125, 572.0]
[2019-04-27 21:49:11,123] A3C_AGENT_WORKER-Thread-21 INFO:Local step 139000, global step 2226093: loss 0.0027
[2019-04-27 21:49:11,124] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 139000, global step 2226094: learning rate 0.0001
[2019-04-27 21:49:11,468] A3C_AGENT_WORKER-Thread-3 INFO:Local step 140000, global step 2226274: loss 0.1036
[2019-04-27 21:49:11,469] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 140000, global step 2226274: learning rate 0.0001
[2019-04-27 21:49:11,626] A3C_AGENT_WORKER-Thread-12 INFO:Local step 139000, global step 2226361: loss 0.0152
[2019-04-27 21:49:11,628] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 139000, global step 2226362: learning rate 0.0001
[2019-04-27 21:49:11,645] A3C_AGENT_WORKER-Thread-15 INFO:Local step 139000, global step 2226370: loss 0.0061
[2019-04-27 21:49:11,647] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 139000, global step 2226370: learning rate 0.0001
[2019-04-27 21:49:11,743] A3C_AGENT_WORKER-Thread-2 INFO:Local step 139000, global step 2226425: loss 0.0059
[2019-04-27 21:49:11,747] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 139000, global step 2226427: learning rate 0.0001
[2019-04-27 21:49:11,783] A3C_AGENT_WORKER-Thread-17 INFO:Local step 139000, global step 2226447: loss 0.0004
[2019-04-27 21:49:11,785] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 139000, global step 2226448: learning rate 0.0001
[2019-04-27 21:49:11,879] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:49:11,887] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4799
[2019-04-27 21:49:11,902] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1466276.671026863 W.
[2019-04-27 21:49:11,908] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.66666666666667, 54.5, 1.0, 2.0, 0.6429830676762853, 1.0, 2.0, 0.6429830676762853, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1466276.671026863, 1466276.671026864, 283142.4039339677], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4186200.0000, 
sim time next is 4186800.0000, 
raw observation next is [29.8, 53.0, 1.0, 2.0, 0.4222741857051723, 1.0, 2.0, 0.4222741857051723, 1.0, 1.0, 0.67239289258128, 6.911199999999999, 6.9112, 121.94756008, 1449144.155630046, 1449144.155630047, 307009.6741997311], 
processed observation next is [1.0, 0.4782608695652174, 0.6592592592592593, 0.53, 1.0, 1.0, 0.3122311734585384, 1.0, 1.0, 0.3122311734585384, 1.0, 0.5, 0.5904911157266, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.5175514841535879, 0.5175514841535882, 0.5904032196148675], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.4211348], dtype=float32), 1.340948]. 
=============================================
[2019-04-27 21:49:12,491] A3C_AGENT_WORKER-Thread-9 INFO:Local step 139000, global step 2226815: loss 0.0046
[2019-04-27 21:49:12,492] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 139000, global step 2226815: learning rate 0.0001
[2019-04-27 21:49:12,735] A3C_AGENT_WORKER-Thread-11 INFO:Local step 139000, global step 2226937: loss 0.0167
[2019-04-27 21:49:12,737] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 139000, global step 2226937: learning rate 0.0001
[2019-04-27 21:49:12,828] A3C_AGENT_WORKER-Thread-22 INFO:Local step 139000, global step 2226987: loss 0.0108
[2019-04-27 21:49:12,832] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 139000, global step 2226988: learning rate 0.0001
[2019-04-27 21:49:12,834] A3C_AGENT_WORKER-Thread-20 INFO:Local step 139000, global step 2226990: loss 0.0045
[2019-04-27 21:49:12,836] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 139000, global step 2226990: learning rate 0.0001
[2019-04-27 21:49:13,025] A3C_AGENT_WORKER-Thread-18 INFO:Local step 139000, global step 2227083: loss 0.0034
[2019-04-27 21:49:13,027] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 139000, global step 2227084: learning rate 0.0001
[2019-04-27 21:49:13,196] A3C_AGENT_WORKER-Thread-13 INFO:Local step 139000, global step 2227172: loss 0.0110
[2019-04-27 21:49:13,200] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 139000, global step 2227172: learning rate 0.0001
[2019-04-27 21:49:13,397] A3C_AGENT_WORKER-Thread-16 INFO:Local step 139500, global step 2227278: loss 5.0581
[2019-04-27 21:49:13,401] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 139500, global step 2227280: learning rate 0.0001
[2019-04-27 21:49:13,472] A3C_AGENT_WORKER-Thread-14 INFO:Local step 139000, global step 2227316: loss 0.0118
[2019-04-27 21:49:13,473] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 139000, global step 2227316: learning rate 0.0001
[2019-04-27 21:49:13,744] A3C_AGENT_WORKER-Thread-10 INFO:Local step 139000, global step 2227458: loss 0.0097
[2019-04-27 21:49:13,749] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 139000, global step 2227458: learning rate 0.0001
[2019-04-27 21:49:16,673] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:49:16,683] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8196
[2019-04-27 21:49:16,691] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1297166.942846925 W.
[2019-04-27 21:49:16,700] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 38.0, 1.0, 2.0, 0.5490980430814363, 0.0, 1.0, 0.0, 1.0, 2.0, 0.8801036419119187, 6.911199999999999, 6.9112, 121.9260426156618, 1297166.942846925, 1297166.942846925, 273586.4171742387], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4278600.0000, 
sim time next is 4279200.0000, 
raw observation next is [32.0, 38.0, 1.0, 2.0, 0.3640431304010513, 1.0, 1.0, 0.3640431304010513, 1.0, 2.0, 0.5814299713831582, 6.911200000000001, 6.9112, 121.94756008, 1274749.963227936, 1274749.963227936, 281712.873318149], 
processed observation next is [1.0, 0.5217391304347826, 0.7407407407407407, 0.38, 1.0, 1.0, 0.24290848857268013, 1.0, 0.5, 0.24290848857268013, 1.0, 1.0, 0.47678746422894774, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.45526784400997716, 0.45526784400997716, 0.541755525611825], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.18312828], dtype=float32), -0.47767672]. 
=============================================
[2019-04-27 21:49:17,055] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.670364e-32 1.000000e+00 3.413238e-37 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-27 21:49:17,061] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7169
[2019-04-27 21:49:17,070] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1576319.584822708 W.
[2019-04-27 21:49:17,073] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.06666666666667, 87.33333333333334, 1.0, 2.0, 0.4607926345656602, 1.0, 2.0, 0.4607926345656602, 1.0, 1.0, 0.7335968811969479, 6.911199999999999, 6.9112, 121.94756008, 1576319.584822708, 1576319.584822708, 324735.5679607608], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4018800.0000, 
sim time next is 4019400.0000, 
raw observation next is [26.1, 86.5, 1.0, 2.0, 0.6418771855105488, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.9260426156618, 1446464.288589561, 1446464.288589562, 306979.5480681436], 
processed observation next is [1.0, 0.5217391304347826, 0.5222222222222223, 0.865, 1.0, 1.0, 0.5736633160839867, 0.0, 0.5, -0.1904761904761905, 1.0, 1.0, 0.9972168686025908, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.516594388781986, 0.5165943887819865, 0.59034528474643], 
reward next is 0.4097, 
noisyNet noise sample is [array([0.06504298], dtype=float32), 1.2986852]. 
=============================================
[2019-04-27 21:49:18,436] A3C_AGENT_WORKER-Thread-19 INFO:Local step 140500, global step 2229918: loss 92.5154
[2019-04-27 21:49:18,438] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 140500, global step 2229919: learning rate 0.0001
[2019-04-27 21:49:22,275] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.6764725e-25 1.0000000e+00 5.6575304e-28 2.0357469e-27 4.1956805e-22], sum to 1.0000
[2019-04-27 21:49:22,285] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1182
[2019-04-27 21:49:22,294] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1406540.802240581 W.
[2019-04-27 21:49:22,299] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.13333333333333, 83.66666666666667, 1.0, 2.0, 0.9784070054464278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.355273408700319, 6.9112, 121.9244651632993, 1406540.802240581, 1179138.468274656, 238702.4461324576], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4090800.0000, 
sim time next is 4091400.0000, 
raw observation next is [23.35, 83.5, 1.0, 2.0, 0.5908915578332543, 1.0, 1.0, 0.5908915578332543, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9257752023137, 1391632.624881865, 1391632.624881865, 266966.5144886479], 
processed observation next is [1.0, 0.34782608695652173, 0.42037037037037045, 0.835, 1.0, 1.0, 0.5129661402776837, 1.0, 0.5, 0.5129661402776837, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094603534735948, 0.4970116517435232, 0.4970116517435232, 0.5133971432473998], 
reward next is 0.4866, 
noisyNet noise sample is [array([0.71561545], dtype=float32), -0.35595125]. 
=============================================
[2019-04-27 21:49:23,237] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:49:23,249] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5689
[2019-04-27 21:49:23,256] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1534286.806345462 W.
[2019-04-27 21:49:23,261] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.33333333333334, 70.0, 1.0, 2.0, 0.4485178309934493, 1.0, 2.0, 0.4485178309934493, 1.0, 2.0, 0.7140549941475446, 6.9112, 6.9112, 121.94756008, 1534286.806345462, 1534286.806345462, 318987.3120393931], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4105200.0000, 
sim time next is 4105800.0000, 
raw observation next is [28.5, 70.0, 1.0, 2.0, 0.6565198642543789, 1.0, 2.0, 0.6565198642543789, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1497176.538647572, 1497176.538647572, 288032.5191335241], 
processed observation next is [1.0, 0.5217391304347826, 0.6111111111111112, 0.7, 1.0, 1.0, 0.5910950764933082, 1.0, 1.0, 0.5910950764933082, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288201359, 0.5347059066598472, 0.5347059066598472, 0.5539086906413925], 
reward next is 0.4461, 
noisyNet noise sample is [array([1.3629586], dtype=float32), -0.9359959]. 
=============================================
[2019-04-27 21:49:27,637] A3C_AGENT_WORKER-Thread-21 INFO:Local step 139500, global step 2234233: loss 2.1623
[2019-04-27 21:49:27,642] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 139500, global step 2234237: learning rate 0.0001
[2019-04-27 21:49:27,762] A3C_AGENT_WORKER-Thread-3 INFO:Local step 140500, global step 2234293: loss -12.9066
[2019-04-27 21:49:27,765] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 140500, global step 2234293: learning rate 0.0001
[2019-04-27 21:49:27,985] A3C_AGENT_WORKER-Thread-12 INFO:Local step 139500, global step 2234393: loss 5.8643
[2019-04-27 21:49:27,988] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 139500, global step 2234394: learning rate 0.0001
[2019-04-27 21:49:28,056] A3C_AGENT_WORKER-Thread-15 INFO:Local step 139500, global step 2234427: loss 3.0488
[2019-04-27 21:49:28,058] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 139500, global step 2234427: learning rate 0.0001
[2019-04-27 21:49:28,077] A3C_AGENT_WORKER-Thread-2 INFO:Local step 139500, global step 2234436: loss 3.2804
[2019-04-27 21:49:28,077] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 139500, global step 2234436: learning rate 0.0001
[2019-04-27 21:49:28,311] A3C_AGENT_WORKER-Thread-17 INFO:Local step 139500, global step 2234541: loss 2.3429
[2019-04-27 21:49:28,312] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 139500, global step 2234541: learning rate 0.0001
[2019-04-27 21:49:28,918] A3C_AGENT_WORKER-Thread-9 INFO:Local step 139500, global step 2234824: loss 1.2570
[2019-04-27 21:49:28,921] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 139500, global step 2234824: learning rate 0.0001
[2019-04-27 21:49:29,230] A3C_AGENT_WORKER-Thread-22 INFO:Local step 139500, global step 2234965: loss 1.7980
[2019-04-27 21:49:29,235] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 139500, global step 2234968: learning rate 0.0001
[2019-04-27 21:49:29,345] A3C_AGENT_WORKER-Thread-11 INFO:Local step 139500, global step 2235020: loss 3.3669
[2019-04-27 21:49:29,348] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 139500, global step 2235021: learning rate 0.0001
[2019-04-27 21:49:29,431] A3C_AGENT_WORKER-Thread-16 INFO:Local step 140000, global step 2235061: loss 0.2209
[2019-04-27 21:49:29,434] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 140000, global step 2235061: learning rate 0.0001
[2019-04-27 21:49:29,454] A3C_AGENT_WORKER-Thread-20 INFO:Local step 139500, global step 2235069: loss 3.8888
[2019-04-27 21:49:29,457] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 139500, global step 2235069: learning rate 0.0001
[2019-04-27 21:49:29,567] A3C_AGENT_WORKER-Thread-18 INFO:Local step 139500, global step 2235124: loss 3.4767
[2019-04-27 21:49:29,568] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 139500, global step 2235124: learning rate 0.0001
[2019-04-27 21:49:29,746] A3C_AGENT_WORKER-Thread-13 INFO:Local step 139500, global step 2235204: loss 2.0848
[2019-04-27 21:49:29,750] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 139500, global step 2235204: learning rate 0.0001
[2019-04-27 21:49:30,083] A3C_AGENT_WORKER-Thread-14 INFO:Local step 139500, global step 2235370: loss 2.7624
[2019-04-27 21:49:30,085] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 139500, global step 2235370: learning rate 0.0001
[2019-04-27 21:49:30,139] A3C_AGENT_WORKER-Thread-10 INFO:Local step 139500, global step 2235392: loss 1.3234
[2019-04-27 21:49:30,141] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 139500, global step 2235394: learning rate 0.0001
[2019-04-27 21:49:30,723] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:49:30,736] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9330
[2019-04-27 21:49:30,740] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 62.0, 1.0, 2.0, 0.5766050708675744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 667303.5507137891, 667303.5507137891, 154590.8972316101], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4222800.0000, 
sim time next is 4223400.0000, 
raw observation next is [28.5, 64.0, 1.0, 2.0, 0.5813987932139973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 672507.1802205545, 672507.1802205545, 155385.8770824972], 
processed observation next is [1.0, 0.9130434782608695, 0.6111111111111112, 0.64, 1.0, 1.0, 0.5016652300166635, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.24018113579305517, 0.24018113579305517, 0.29881899438941767], 
reward next is 0.7012, 
noisyNet noise sample is [array([0.02894751], dtype=float32), -2.263564]. 
=============================================
[2019-04-27 21:49:31,378] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.6610634e-35 1.0000000e+00 2.6542602e-37 3.7377991e-37 0.0000000e+00], sum to 1.0000
[2019-04-27 21:49:31,388] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1084
[2019-04-27 21:49:31,392] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666667, 86.33333333333334, 1.0, 2.0, 0.6276128074669157, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 715262.5636989145, 715262.5636989145, 162891.9298315475], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5041200.0000, 
sim time next is 5041800.0000, 
raw observation next is [25.75, 87.0, 1.0, 2.0, 0.636699845695557, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 725623.5608755096, 725623.5608755096, 164505.9760730887], 
processed observation next is [0.0, 0.34782608695652173, 0.5092592592592593, 0.87, 1.0, 1.0, 0.5674998163042345, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.25915127174125346, 0.25915127174125346, 0.3163576462944014], 
reward next is 0.6836, 
noisyNet noise sample is [array([-0.01528871], dtype=float32), 0.9503933]. 
=============================================
[2019-04-27 21:49:32,227] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.3573016e-29 1.0000000e+00 1.2876103e-31 1.4746252e-32 2.2550728e-31], sum to 1.0000
[2019-04-27 21:49:32,237] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8538
[2019-04-27 21:49:32,242] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 78.5, 1.0, 2.0, 0.4574935152293714, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 563476.7370476205, 563476.73704762, 136932.3586827598], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4257000.0000, 
sim time next is 4257600.0000, 
raw observation next is [23.0, 77.0, 1.0, 2.0, 0.5757666537117578, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 706152.6602177055, 706152.6602177055, 155942.9355476096], 
processed observation next is [1.0, 0.2608695652173913, 0.4074074074074074, 0.77, 1.0, 1.0, 0.49496030203780694, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.25219737864918057, 0.25219737864918057, 0.29989026066848], 
reward next is 0.7001, 
noisyNet noise sample is [array([-1.1254648], dtype=float32), -1.6065929]. 
=============================================
[2019-04-27 21:49:35,055] A3C_AGENT_WORKER-Thread-19 INFO:Local step 141000, global step 2237725: loss 0.7459
[2019-04-27 21:49:35,060] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 141000, global step 2237726: learning rate 0.0001
[2019-04-27 21:49:40,447] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:49:40,454] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4502
[2019-04-27 21:49:40,459] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.83333333333334, 88.66666666666667, 1.0, 2.0, 0.5056453399018505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 605112.9428553772, 605112.9428553772, 143858.5782091677], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4405200.0000, 
sim time next is 4405800.0000, 
raw observation next is [22.41666666666667, 91.33333333333334, 1.0, 2.0, 0.5023813405989893, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 601968.6972215284, 601968.6972215284, 143370.1207648285], 
processed observation next is [1.0, 1.0, 0.38580246913580263, 0.9133333333333334, 1.0, 1.0, 0.40759683404641583, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21498882043626014, 0.21498882043626014, 0.27571177070159325], 
reward next is 0.7243, 
noisyNet noise sample is [array([0.4930736], dtype=float32), 0.3092967]. 
=============================================
[2019-04-27 21:49:44,323] A3C_AGENT_WORKER-Thread-21 INFO:Local step 140000, global step 2242134: loss 0.0619
[2019-04-27 21:49:44,326] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 140000, global step 2242135: learning rate 0.0001
[2019-04-27 21:49:44,452] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:49:44,458] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2464
[2019-04-27 21:49:44,465] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.98333333333333, 93.33333333333334, 1.0, 2.0, 0.5938444598338, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 687333.4916702682, 687333.4916702678, 157531.1470646218], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4503000.0000, 
sim time next is 4503600.0000, 
raw observation next is [24.0, 94.0, 1.0, 2.0, 0.6006509801720175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 693680.4526758089, 693680.4526758089, 158633.6857563321], 
processed observation next is [0.0, 0.13043478260869565, 0.4444444444444444, 0.94, 1.0, 1.0, 0.5245845002047826, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2477430188127889, 0.2477430188127889, 0.30506478030063866], 
reward next is 0.6949, 
noisyNet noise sample is [array([-1.275417], dtype=float32), -0.6604869]. 
=============================================
[2019-04-27 21:49:44,589] A3C_AGENT_WORKER-Thread-3 INFO:Local step 141000, global step 2242258: loss 0.0194
[2019-04-27 21:49:44,591] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 141000, global step 2242259: learning rate 0.0001
[2019-04-27 21:49:44,660] A3C_AGENT_WORKER-Thread-12 INFO:Local step 140000, global step 2242290: loss 0.0376
[2019-04-27 21:49:44,662] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 140000, global step 2242291: learning rate 0.0001
[2019-04-27 21:49:44,787] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:49:44,797] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0425
[2019-04-27 21:49:44,807] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.25, 92.5, 1.0, 2.0, 0.5501047569572993, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 647856.7210228491, 647856.7210228491, 150664.1431056346], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4511400.0000, 
sim time next is 4512000.0000, 
raw observation next is [23.2, 94.0, 1.0, 2.0, 0.5543906944395984, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 651133.363753779, 651133.363753779, 151300.5430179395], 
processed observation next is [0.0, 0.21739130434782608, 0.4148148148148148, 0.94, 1.0, 1.0, 0.4695127314757124, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.23254762991206393, 0.23254762991206393, 0.2909625827268067], 
reward next is 0.7090, 
noisyNet noise sample is [array([0.8016918], dtype=float32), 1.0526097]. 
=============================================
[2019-04-27 21:49:44,819] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[71.446304]
 [71.423836]
 [71.303375]
 [71.25549 ]
 [71.20078 ]], R is [[71.43682098]
 [71.43270874]
 [71.42897034]
 [71.42304993]
 [71.4149704 ]].
[2019-04-27 21:49:44,822] A3C_AGENT_WORKER-Thread-15 INFO:Local step 140000, global step 2242368: loss 0.0225
[2019-04-27 21:49:44,823] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 140000, global step 2242369: learning rate 0.0001
[2019-04-27 21:49:44,917] A3C_AGENT_WORKER-Thread-2 INFO:Local step 140000, global step 2242410: loss 0.0331
[2019-04-27 21:49:44,918] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 140000, global step 2242410: learning rate 0.0001
[2019-04-27 21:49:45,240] A3C_AGENT_WORKER-Thread-17 INFO:Local step 140000, global step 2242561: loss 0.0177
[2019-04-27 21:49:45,240] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 140000, global step 2242562: learning rate 0.0001
[2019-04-27 21:49:45,721] A3C_AGENT_WORKER-Thread-9 INFO:Local step 140000, global step 2242793: loss 0.0240
[2019-04-27 21:49:45,723] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 140000, global step 2242794: learning rate 0.0001
[2019-04-27 21:49:45,946] A3C_AGENT_WORKER-Thread-22 INFO:Local step 140000, global step 2242899: loss 0.0368
[2019-04-27 21:49:45,947] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 140000, global step 2242899: learning rate 0.0001
[2019-04-27 21:49:46,174] A3C_AGENT_WORKER-Thread-20 INFO:Local step 140000, global step 2243007: loss 0.0213
[2019-04-27 21:49:46,176] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 140000, global step 2243007: learning rate 0.0001
[2019-04-27 21:49:46,289] A3C_AGENT_WORKER-Thread-11 INFO:Local step 140000, global step 2243057: loss 0.0109
[2019-04-27 21:49:46,290] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 140000, global step 2243058: learning rate 0.0001
[2019-04-27 21:49:46,412] A3C_AGENT_WORKER-Thread-18 INFO:Local step 140000, global step 2243118: loss 0.0107
[2019-04-27 21:49:46,415] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 140000, global step 2243118: learning rate 0.0001
[2019-04-27 21:49:46,573] A3C_AGENT_WORKER-Thread-16 INFO:Local step 140500, global step 2243200: loss 22.9209
[2019-04-27 21:49:46,575] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 140500, global step 2243200: learning rate 0.0001
[2019-04-27 21:49:46,749] A3C_AGENT_WORKER-Thread-13 INFO:Local step 140000, global step 2243284: loss 0.0129
[2019-04-27 21:49:46,755] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 140000, global step 2243284: learning rate 0.0001
[2019-04-27 21:49:46,860] A3C_AGENT_WORKER-Thread-14 INFO:Local step 140000, global step 2243343: loss 0.0101
[2019-04-27 21:49:46,869] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 140000, global step 2243343: learning rate 0.0001
[2019-04-27 21:49:47,006] A3C_AGENT_WORKER-Thread-10 INFO:Local step 140000, global step 2243411: loss 0.0161
[2019-04-27 21:49:47,007] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 140000, global step 2243411: learning rate 0.0001
[2019-04-27 21:49:49,573] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4063641e-29 1.0000000e+00 4.1176303e-33 2.9938133e-34 1.8333333e-31], sum to 1.0000
[2019-04-27 21:49:49,580] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4512
[2019-04-27 21:49:49,585] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.75, 87.83333333333334, 1.0, 2.0, 0.8309634861111234, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 959930.3267797282, 959930.3267797282, 203087.8627495842], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5367000.0000, 
sim time next is 5367600.0000, 
raw observation next is [24.7, 88.0, 1.0, 2.0, 0.7499357228364651, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 866873.6582520196, 866873.6582520196, 186425.5488268975], 
processed observation next is [1.0, 0.13043478260869565, 0.4703703703703703, 0.88, 1.0, 1.0, 0.7023044319481728, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.309597735090007, 0.309597735090007, 0.3585106708209567], 
reward next is 0.6415, 
noisyNet noise sample is [array([0.06589647], dtype=float32), -0.37195873]. 
=============================================
[2019-04-27 21:49:50,188] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2507276e-28 1.0000000e+00 1.1892146e-32 2.7376202e-35 9.3813224e-36], sum to 1.0000
[2019-04-27 21:49:50,197] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1140
[2019-04-27 21:49:50,205] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2050284.446416964 W.
[2019-04-27 21:49:50,208] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.08333333333333, 89.83333333333333, 1.0, 2.0, 0.5991837906870094, 1.0, 2.0, 0.5991837906870094, 1.0, 2.0, 0.9539201088274346, 6.9112, 6.9112, 121.94756008, 2050284.446416964, 2050284.446416964, 394931.0609331644], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4873800.0000, 
sim time next is 4874400.0000, 
raw observation next is [27.3, 89.0, 1.0, 2.0, 0.913704901403516, 1.0, 2.0, 0.913704901403516, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 2084381.09873513, 2084381.09873513, 392980.2471368338], 
processed observation next is [1.0, 0.43478260869565216, 0.5666666666666667, 0.89, 1.0, 1.0, 0.8972677397660905, 1.0, 1.0, 0.8972677397660905, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.7444218209768321, 0.7444218209768321, 0.7557312444939112], 
reward next is 0.2443, 
noisyNet noise sample is [array([-1.3202763], dtype=float32), -1.5408937]. 
=============================================
[2019-04-27 21:49:52,111] A3C_AGENT_WORKER-Thread-19 INFO:Local step 141500, global step 2245848: loss -19.7165
[2019-04-27 21:49:52,118] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 141500, global step 2245849: learning rate 0.0001
[2019-04-27 21:50:00,533] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:50:00,542] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7642
[2019-04-27 21:50:00,546] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.3, 72.0, 1.0, 2.0, 0.7710655770878831, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 878843.0681999413, 878843.0681999413, 190040.5160945226], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5059200.0000, 
sim time next is 5059800.0000, 
raw observation next is [30.45, 70.5, 1.0, 2.0, 0.7760533694795886, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 884531.321550477, 884531.321550477, 191047.5375970332], 
processed observation next is [0.0, 0.5652173913043478, 0.6833333333333333, 0.705, 1.0, 1.0, 0.7333968684280816, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3159040434108846, 0.3159040434108846, 0.3673991107635254], 
reward next is 0.6326, 
noisyNet noise sample is [array([-0.46217254], dtype=float32), 0.46415985]. 
=============================================
[2019-04-27 21:50:00,919] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-27 21:50:00,923] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 21:50:00,924] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:50:00,924] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 21:50:00,925] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:50:00,925] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 21:50:00,926] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:50:00,926] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 21:50:00,927] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 21:50:00,927] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:50:00,930] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:50:00,957] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run91
[2019-04-27 21:50:00,981] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run91
[2019-04-27 21:50:01,008] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run91
[2019-04-27 21:50:01,009] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run91
[2019-04-27 21:50:01,059] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run91
[2019-04-27 21:50:09,370] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.015007], dtype=float32), -0.015168885]
[2019-04-27 21:50:09,373] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [20.68333333333334, 71.0, 1.0, 2.0, 0.3687110962753458, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 469457.7951146447, 469457.7951146447, 124482.6473146356]
[2019-04-27 21:50:09,374] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:50:09,377] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9936630095492718
[2019-04-27 21:50:10,182] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.015007], dtype=float32), -0.015168885]
[2019-04-27 21:50:10,183] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [31.33333333333333, 21.66666666666667, 1.0, 2.0, 0.6273255478916044, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 799842.9447617967, 799842.9447617967, 165576.2145974521]
[2019-04-27 21:50:10,183] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 21:50:10,185] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.41903004982164516
[2019-04-27 21:50:32,425] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.015007], dtype=float32), -0.015168885]
[2019-04-27 21:50:32,426] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [24.06666666666667, 89.16666666666667, 1.0, 2.0, 0.7163315690006807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 836468.6588087745, 836468.6588087745, 180236.2117938403]
[2019-04-27 21:50:32,428] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:50:32,430] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [5.314893e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.38573715243815054
[2019-04-27 21:50:55,223] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.015007], dtype=float32), -0.015168885]
[2019-04-27 21:50:55,226] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [25.0, 83.0, 1.0, 2.0, 0.5649954214139961, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 658890.330210806, 658890.330210806, 152867.2463998357]
[2019-04-27 21:50:55,228] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 21:50:55,233] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.37354551472951736
[2019-04-27 21:51:02,904] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.015007], dtype=float32), -0.015168885]
[2019-04-27 21:51:02,905] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [28.91666666666667, 63.16666666666667, 1.0, 2.0, 0.5986512553852936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 689498.7917326109, 689498.7917326109, 158199.8167376553]
[2019-04-27 21:51:02,906] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:51:02,909] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6326312630680285
[2019-04-27 21:51:07,575] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.015007], dtype=float32), -0.015168885]
[2019-04-27 21:51:07,577] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [27.95, 61.16666666666667, 1.0, 2.0, 0.5409940613515035, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 637689.2874746883, 637689.2874746883, 149188.3739362736]
[2019-04-27 21:51:07,577] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 21:51:07,579] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5423267341771396
[2019-04-27 21:51:21,635] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.015007], dtype=float32), -0.015168885]
[2019-04-27 21:51:21,638] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [24.36089285, 59.51669691, 1.0, 2.0, 0.3874198465563942, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 483593.3175793177, 483593.3175793177, 126934.5600121249]
[2019-04-27 21:51:21,639] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 21:51:21,642] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5272229631475347
[2019-04-27 21:51:22,327] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.015007], dtype=float32), -0.015168885]
[2019-04-27 21:51:22,329] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [21.4, 81.66666666666667, 1.0, 2.0, 0.3943092881691931, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 489404.2004775843, 489404.2004775843, 127838.8144660111]
[2019-04-27 21:51:22,330] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:51:22,333] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.04740916580221666
[2019-04-27 21:51:46,658] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8770.2626 2170712315.3188 493.0000
[2019-04-27 21:51:47,232] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8582.8795 2248773970.1662 553.0000
[2019-04-27 21:51:47,305] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8700.1130 2195220806.1349 572.0000
[2019-04-27 21:51:47,359] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8922.6628 2120599135.7723 430.0000
[2019-04-27 21:51:47,401] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8098.4495 2445508074.6415 746.0000
[2019-04-27 21:51:48,418] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 2250000, evaluation results [2250000.0, 8098.4494892610255, 2445508074.64147, 746.0, 8770.262620159681, 2170712315.3187604, 493.0, 8922.662781171732, 2120599135.7722971, 430.0, 8582.879467926026, 2248773970.1662407, 553.0, 8700.112961499688, 2195220806.134913, 572.0]
[2019-04-27 21:51:48,876] A3C_AGENT_WORKER-Thread-21 INFO:Local step 140500, global step 2250240: loss 3.5184
[2019-04-27 21:51:48,878] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 140500, global step 2250240: learning rate 0.0001
[2019-04-27 21:51:48,895] A3C_AGENT_WORKER-Thread-3 INFO:Local step 141500, global step 2250251: loss -25.7469
[2019-04-27 21:51:48,896] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 141500, global step 2250251: learning rate 0.0001
[2019-04-27 21:51:48,996] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:51:49,004] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9442
[2019-04-27 21:51:49,009] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1939402.539456112 W.
[2019-04-27 21:51:49,013] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.08333333333333, 88.33333333333334, 1.0, 2.0, 0.5668143358756437, 1.0, 2.0, 0.5668143358756437, 1.0, 2.0, 0.9023868825681947, 6.9112, 6.9112, 121.94756008, 1939402.539456112, 1939402.539456112, 377591.035129672], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4805400.0000, 
sim time next is 4806000.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.7260446986970712, 1.0, 2.0, 0.7260446986970712, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.926042615639, 1655886.106412724, 1655886.106412724, 314143.403331694], 
processed observation next is [1.0, 0.6521739130434783, 0.5555555555555556, 0.89, 1.0, 1.0, 0.6738627365441324, 1.0, 1.0, 0.6738627365441324, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288199846, 0.5913878951474014, 0.5913878951474014, 0.604121929484027], 
reward next is 0.3959, 
noisyNet noise sample is [array([0.20353524], dtype=float32), -1.8877392]. 
=============================================
[2019-04-27 21:51:49,033] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[60.199455]
 [59.935894]
 [59.30358 ]
 [59.077858]
 [58.95822 ]], R is [[60.86283112]
 [60.52806854]
 [60.18541718]
 [59.58356476]
 [58.98772812]].
[2019-04-27 21:51:49,207] A3C_AGENT_WORKER-Thread-12 INFO:Local step 140500, global step 2250412: loss -25.4251
[2019-04-27 21:51:49,210] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 140500, global step 2250413: learning rate 0.0001
[2019-04-27 21:51:49,220] A3C_AGENT_WORKER-Thread-15 INFO:Local step 140500, global step 2250419: loss -40.6419
[2019-04-27 21:51:49,222] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 140500, global step 2250420: learning rate 0.0001
[2019-04-27 21:51:49,361] A3C_AGENT_WORKER-Thread-2 INFO:Local step 140500, global step 2250492: loss -14.4964
[2019-04-27 21:51:49,366] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 140500, global step 2250492: learning rate 0.0001
[2019-04-27 21:51:49,559] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.2963991e-30 1.0000000e+00 1.2722972e-33 9.9325685e-36 2.0678520e-35], sum to 1.0000
[2019-04-27 21:51:49,563] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2424
[2019-04-27 21:51:49,571] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1313090.136440262 W.
[2019-04-27 21:51:49,575] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [23.98333333333333, 94.0, 1.0, 2.0, 1.003626044138587, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.221317770539535, 6.9112, 121.924730372947, 1313090.136440262, 1154283.829101315, 242171.9621298229], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4787400.0000, 
sim time next is 4788000.0000, 
raw observation next is [24.0, 94.0, 1.0, 2.0, 0.564967281567039, 0.0, 2.0, 0.0, 1.0, 1.0, 0.8994463119545368, 6.9112, 6.9112, 121.925855868211, 1288217.661464087, 1288217.661464087, 280476.2228264604], 
processed observation next is [1.0, 0.43478260869565216, 0.4444444444444444, 0.94, 1.0, 1.0, 0.48210390662742736, 0.0, 1.0, -0.1904761904761905, 1.0, 0.5, 0.8743078899431711, 0.0, 0.0, 0.8094608890112663, 0.4600777362371739, 0.4600777362371739, 0.5393773515893469], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7368818], dtype=float32), 1.6045021]. 
=============================================
[2019-04-27 21:51:49,578] A3C_AGENT_WORKER-Thread-17 INFO:Local step 140500, global step 2250600: loss -18.0051
[2019-04-27 21:51:49,581] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 140500, global step 2250601: learning rate 0.0001
[2019-04-27 21:51:49,588] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[48.978157]
 [48.705334]
 [48.58652 ]
 [47.78532 ]
 [47.94548 ]], R is [[49.24422836]
 [48.75178528]
 [48.26426697]
 [47.78162384]
 [47.3038063 ]].
[2019-04-27 21:51:49,995] A3C_AGENT_WORKER-Thread-9 INFO:Local step 140500, global step 2250813: loss 7.6271
[2019-04-27 21:51:49,997] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 140500, global step 2250813: learning rate 0.0001
[2019-04-27 21:51:50,129] A3C_AGENT_WORKER-Thread-22 INFO:Local step 140500, global step 2250883: loss -51.2217
[2019-04-27 21:51:50,131] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 140500, global step 2250884: learning rate 0.0001
[2019-04-27 21:51:50,297] A3C_AGENT_WORKER-Thread-11 INFO:Local step 140500, global step 2250966: loss -24.5011
[2019-04-27 21:51:50,299] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 140500, global step 2250966: learning rate 0.0001
[2019-04-27 21:51:50,447] A3C_AGENT_WORKER-Thread-20 INFO:Local step 140500, global step 2251053: loss -20.0981
[2019-04-27 21:51:50,454] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 140500, global step 2251054: learning rate 0.0001
[2019-04-27 21:51:50,508] A3C_AGENT_WORKER-Thread-16 INFO:Local step 141000, global step 2251082: loss 1.4846
[2019-04-27 21:51:50,512] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 141000, global step 2251084: learning rate 0.0001
[2019-04-27 21:51:50,735] A3C_AGENT_WORKER-Thread-18 INFO:Local step 140500, global step 2251200: loss 69.3933
[2019-04-27 21:51:50,738] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 140500, global step 2251200: learning rate 0.0001
[2019-04-27 21:51:50,811] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.2271456e-27 1.0000000e+00 4.5211247e-33 3.0743444e-32 1.0685023e-29], sum to 1.0000
[2019-04-27 21:51:50,834] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9871
[2019-04-27 21:51:50,840] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 94.0, 1.0, 2.0, 0.7288251207443234, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 830672.2631557682, 830672.2631557682, 181669.4464571393], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4852800.0000, 
sim time next is 4853400.0000, 
raw observation next is [25.0, 94.33333333333334, 1.0, 2.0, 0.7509595276384351, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 855913.8524398622, 855913.8524398617, 186012.2817266248], 
processed observation next is [1.0, 0.17391304347826086, 0.48148148148148145, 0.9433333333333335, 1.0, 1.0, 0.7035232471886133, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.30568351872852223, 0.30568351872852206, 0.3577159263973554], 
reward next is 0.6423, 
noisyNet noise sample is [array([-0.8959943], dtype=float32), 0.18895938]. 
=============================================
[2019-04-27 21:51:50,860] A3C_AGENT_WORKER-Thread-13 INFO:Local step 140500, global step 2251263: loss -26.4883
[2019-04-27 21:51:50,863] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 140500, global step 2251263: learning rate 0.0001
[2019-04-27 21:51:50,953] A3C_AGENT_WORKER-Thread-14 INFO:Local step 140500, global step 2251308: loss -35.7088
[2019-04-27 21:51:50,957] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 140500, global step 2251309: learning rate 0.0001
[2019-04-27 21:51:51,187] A3C_AGENT_WORKER-Thread-10 INFO:Local step 140500, global step 2251424: loss -31.3043
[2019-04-27 21:51:51,190] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 140500, global step 2251425: learning rate 0.0001
[2019-04-27 21:51:51,496] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:51:51,510] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4013
[2019-04-27 21:51:51,513] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.73333333333333, 94.66666666666667, 1.0, 2.0, 0.7100553239145743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 809268.2522863428, 809268.2522863428, 178057.5754630123], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4836000.0000, 
sim time next is 4836600.0000, 
raw observation next is [25.55, 96.0, 1.0, 2.0, 0.709096297140216, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 808174.6486404609, 808174.6486404605, 177874.4124614904], 
processed observation next is [1.0, 1.0, 0.5018518518518519, 0.96, 1.0, 1.0, 0.6536860680240667, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2886338030858789, 0.2886338030858787, 0.3420661778105585], 
reward next is 0.6579, 
noisyNet noise sample is [array([-1.5803568], dtype=float32), 1.0941133]. 
=============================================
[2019-04-27 21:51:55,208] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4655914e-31 1.0000000e+00 4.2147079e-36 0.0000000e+00 4.4386786e-37], sum to 1.0000
[2019-04-27 21:51:55,215] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0727
[2019-04-27 21:51:55,223] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2873648.979857281 W.
[2019-04-27 21:51:55,226] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.8, 77.5, 1.0, 2.0, 1.02, 1.0, 2.0, 0.8394206635749414, 1.0, 1.0, 0.9977734948820727, 6.982744754774293, 6.9112, 121.94756008, 2873648.979857281, 2837005.205076043, 529312.6342662033], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4897800.0000, 
sim time next is 4898400.0000, 
raw observation next is [31.06666666666667, 73.66666666666667, 1.0, 2.0, 0.8941576499731605, 1.0, 2.0, 0.760443486963015, 1.0, 2.0, 0.9977734948820727, 6.911200000000001, 6.9112, 121.94756008, 2602886.47067742, 2602886.47067742, 485488.4244246531], 
processed observation next is [1.0, 0.6956521739130435, 0.7061728395061729, 0.7366666666666667, 1.0, 1.0, 0.8739972023490006, 1.0, 1.0, 0.7148136749559703, 1.0, 1.0, 0.9972168686025908, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.9296023109562215, 0.9296023109562215, 0.9336315854320252], 
reward next is 0.0664, 
noisyNet noise sample is [array([1.7411907], dtype=float32), -0.0346403]. 
=============================================
[2019-04-27 21:51:55,478] A3C_AGENT_WORKER-Thread-19 INFO:Local step 142000, global step 2253655: loss 0.2207
[2019-04-27 21:51:55,480] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 142000, global step 2253655: learning rate 0.0001
[2019-04-27 21:52:04,333] A3C_AGENT_WORKER-Thread-21 INFO:Local step 141000, global step 2258084: loss 0.0146
[2019-04-27 21:52:04,335] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 141000, global step 2258084: learning rate 0.0001
[2019-04-27 21:52:04,439] A3C_AGENT_WORKER-Thread-3 INFO:Local step 142000, global step 2258133: loss 0.1781
[2019-04-27 21:52:04,444] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 142000, global step 2258134: learning rate 0.0001
[2019-04-27 21:52:04,951] A3C_AGENT_WORKER-Thread-12 INFO:Local step 141000, global step 2258376: loss 0.0140
[2019-04-27 21:52:04,953] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 141000, global step 2258376: learning rate 0.0001
[2019-04-27 21:52:05,005] A3C_AGENT_WORKER-Thread-15 INFO:Local step 141000, global step 2258401: loss 0.0262
[2019-04-27 21:52:05,008] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 141000, global step 2258402: learning rate 0.0001
[2019-04-27 21:52:05,289] A3C_AGENT_WORKER-Thread-2 INFO:Local step 141000, global step 2258535: loss 0.0805
[2019-04-27 21:52:05,293] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 141000, global step 2258535: learning rate 0.0001
[2019-04-27 21:52:05,345] A3C_AGENT_WORKER-Thread-17 INFO:Local step 141000, global step 2258565: loss 0.0294
[2019-04-27 21:52:05,347] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 141000, global step 2258565: learning rate 0.0001
[2019-04-27 21:52:05,925] A3C_AGENT_WORKER-Thread-22 INFO:Local step 141000, global step 2258833: loss 0.0687
[2019-04-27 21:52:05,928] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 141000, global step 2258833: learning rate 0.0001
[2019-04-27 21:52:06,043] A3C_AGENT_WORKER-Thread-11 INFO:Local step 141000, global step 2258889: loss 0.0454
[2019-04-27 21:52:06,045] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 141000, global step 2258889: learning rate 0.0001
[2019-04-27 21:52:06,067] A3C_AGENT_WORKER-Thread-9 INFO:Local step 141000, global step 2258900: loss 0.0506
[2019-04-27 21:52:06,069] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 141000, global step 2258900: learning rate 0.0001
[2019-04-27 21:52:06,423] A3C_AGENT_WORKER-Thread-20 INFO:Local step 141000, global step 2259068: loss 0.0110
[2019-04-27 21:52:06,425] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 141000, global step 2259068: learning rate 0.0001
[2019-04-27 21:52:06,622] A3C_AGENT_WORKER-Thread-18 INFO:Local step 141000, global step 2259179: loss 0.0242
[2019-04-27 21:52:06,630] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 141000, global step 2259181: learning rate 0.0001
[2019-04-27 21:52:06,763] A3C_AGENT_WORKER-Thread-16 INFO:Local step 141500, global step 2259249: loss -56.1189
[2019-04-27 21:52:06,768] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 141500, global step 2259250: learning rate 0.0001
[2019-04-27 21:52:06,857] A3C_AGENT_WORKER-Thread-13 INFO:Local step 141000, global step 2259294: loss 0.0982
[2019-04-27 21:52:06,860] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 141000, global step 2259294: learning rate 0.0001
[2019-04-27 21:52:06,900] A3C_AGENT_WORKER-Thread-14 INFO:Local step 141000, global step 2259311: loss 0.1618
[2019-04-27 21:52:06,902] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 141000, global step 2259311: learning rate 0.0001
[2019-04-27 21:52:07,164] A3C_AGENT_WORKER-Thread-10 INFO:Local step 141000, global step 2259437: loss 0.1186
[2019-04-27 21:52:07,169] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 141000, global step 2259437: learning rate 0.0001
[2019-04-27 21:52:11,774] A3C_AGENT_WORKER-Thread-19 INFO:Local step 142500, global step 2261631: loss 80.5708
[2019-04-27 21:52:11,776] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 142500, global step 2261631: learning rate 0.0001
[2019-04-27 21:52:14,744] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.3100204e-32 1.0000000e+00 4.2290878e-37 2.1542137e-37 0.0000000e+00], sum to 1.0000
[2019-04-27 21:52:14,756] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4356
[2019-04-27 21:52:14,759] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.36666666666667, 75.66666666666667, 1.0, 2.0, 0.3671478209498828, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 460823.7211325434, 460823.7211325434, 124200.1938716588], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5901600.0000, 
sim time next is 5902200.0000, 
raw observation next is [21.55, 75.0, 1.0, 2.0, 0.3709635900202826, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 465120.4388616798, 465120.4388616793, 124710.0991545117], 
processed observation next is [1.0, 0.30434782608695654, 0.35370370370370374, 0.75, 1.0, 1.0, 0.2511471309765269, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.16611444245059992, 0.16611444245059975, 0.23982711375867635], 
reward next is 0.7602, 
noisyNet noise sample is [array([1.1027561], dtype=float32), -0.31268656]. 
=============================================
[2019-04-27 21:52:21,058] A3C_AGENT_WORKER-Thread-3 INFO:Local step 142500, global step 2265981: loss 38.6505
[2019-04-27 21:52:21,061] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 142500, global step 2265981: learning rate 0.0001
[2019-04-27 21:52:21,551] A3C_AGENT_WORKER-Thread-21 INFO:Local step 141500, global step 2266219: loss -122.0597
[2019-04-27 21:52:21,554] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 141500, global step 2266219: learning rate 0.0001
[2019-04-27 21:52:21,981] A3C_AGENT_WORKER-Thread-12 INFO:Local step 141500, global step 2266428: loss 2.6808
[2019-04-27 21:52:21,986] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 141500, global step 2266428: learning rate 0.0001
[2019-04-27 21:52:22,142] A3C_AGENT_WORKER-Thread-15 INFO:Local step 141500, global step 2266502: loss -46.9204
[2019-04-27 21:52:22,144] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 141500, global step 2266503: learning rate 0.0001
[2019-04-27 21:52:22,266] A3C_AGENT_WORKER-Thread-2 INFO:Local step 141500, global step 2266560: loss -66.3001
[2019-04-27 21:52:22,268] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 141500, global step 2266560: learning rate 0.0001
[2019-04-27 21:52:22,310] A3C_AGENT_WORKER-Thread-17 INFO:Local step 141500, global step 2266581: loss -59.5786
[2019-04-27 21:52:22,312] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 141500, global step 2266581: learning rate 0.0001
[2019-04-27 21:52:22,884] A3C_AGENT_WORKER-Thread-11 INFO:Local step 141500, global step 2266867: loss -56.5284
[2019-04-27 21:52:22,886] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 141500, global step 2266867: learning rate 0.0001
[2019-04-27 21:52:22,917] A3C_AGENT_WORKER-Thread-22 INFO:Local step 141500, global step 2266882: loss -20.8405
[2019-04-27 21:52:22,920] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 141500, global step 2266883: learning rate 0.0001
[2019-04-27 21:52:22,989] A3C_AGENT_WORKER-Thread-9 INFO:Local step 141500, global step 2266912: loss -3.2682
[2019-04-27 21:52:22,990] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 141500, global step 2266912: learning rate 0.0001
[2019-04-27 21:52:23,496] A3C_AGENT_WORKER-Thread-20 INFO:Local step 141500, global step 2267141: loss -26.9678
[2019-04-27 21:52:23,498] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 141500, global step 2267141: learning rate 0.0001
[2019-04-27 21:52:23,557] A3C_AGENT_WORKER-Thread-16 INFO:Local step 142000, global step 2267172: loss 0.4691
[2019-04-27 21:52:23,561] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 142000, global step 2267175: learning rate 0.0001
[2019-04-27 21:52:23,570] A3C_AGENT_WORKER-Thread-18 INFO:Local step 141500, global step 2267179: loss -25.6337
[2019-04-27 21:52:23,573] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 141500, global step 2267179: learning rate 0.0001
[2019-04-27 21:52:23,731] A3C_AGENT_WORKER-Thread-13 INFO:Local step 141500, global step 2267261: loss -35.8353
[2019-04-27 21:52:23,733] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 141500, global step 2267261: learning rate 0.0001
[2019-04-27 21:52:23,828] A3C_AGENT_WORKER-Thread-14 INFO:Local step 141500, global step 2267303: loss -13.5277
[2019-04-27 21:52:23,832] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 141500, global step 2267304: learning rate 0.0001
[2019-04-27 21:52:24,041] A3C_AGENT_WORKER-Thread-10 INFO:Local step 141500, global step 2267407: loss 15.4815
[2019-04-27 21:52:24,043] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 141500, global step 2267407: learning rate 0.0001
[2019-04-27 21:52:25,315] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:52:25,321] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5264
[2019-04-27 21:52:25,327] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.81666666666667, 86.5, 1.0, 2.0, 0.7771161954915925, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 885743.4110613028, 885743.4110613023, 191262.7352368004], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5436600.0000, 
sim time next is 5437200.0000, 
raw observation next is [27.73333333333333, 87.0, 1.0, 2.0, 0.7741428287857173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000002, 6.9112, 121.9260426156618, 882352.4689576317, 882352.4689576308, 190660.9159756245], 
processed observation next is [1.0, 0.9565217391304348, 0.5827160493827159, 0.87, 1.0, 1.0, 0.731122415221092, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 1.7763568394002506e-16, 0.0, 0.8094621288201359, 0.31512588177058276, 0.31512588177058243, 0.3666556076454317], 
reward next is 0.6333, 
noisyNet noise sample is [array([-2.6320035], dtype=float32), 1.0929842]. 
=============================================
[2019-04-27 21:52:25,677] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:52:25,687] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8291
[2019-04-27 21:52:25,692] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.48333333333333, 88.5, 1.0, 2.0, 0.7727105435631425, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 880719.0409865915, 880719.040986591, 190371.412226099], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5439000.0000, 
sim time next is 5439600.0000, 
raw observation next is [27.4, 89.0, 1.0, 2.0, 0.7709230452844248, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000002, 6.9112, 121.9260426156618, 878680.5205535698, 878680.5205535688, 190010.7659133137], 
processed observation next is [1.0, 1.0, 0.5703703703703703, 0.89, 1.0, 1.0, 0.7272893396243152, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 1.7763568394002506e-16, 0.0, 0.8094621288201359, 0.3138144716262749, 0.31381447162627457, 0.3654053190640648], 
reward next is 0.6346, 
noisyNet noise sample is [array([-1.5173235], dtype=float32), -0.7589121]. 
=============================================
[2019-04-27 21:52:28,444] A3C_AGENT_WORKER-Thread-19 INFO:Local step 143000, global step 2269465: loss 0.2235
[2019-04-27 21:52:28,446] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 143000, global step 2269466: learning rate 0.0001
[2019-04-27 21:52:32,047] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.0045448e-30 1.0000000e+00 1.1032089e-33 3.4653410e-34 8.9653779e-31], sum to 1.0000
[2019-04-27 21:52:32,055] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8219
[2019-04-27 21:52:32,061] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.38333333333333, 87.5, 1.0, 2.0, 0.6444664657036133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 735153.4002993043, 735153.4002993043, 165929.0557760294], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5554200.0000, 
sim time next is 5554800.0000, 
raw observation next is [25.4, 87.0, 1.0, 2.0, 0.6539071079498567, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 746813.552183487, 746813.5521834865, 167678.3022111263], 
processed observation next is [1.0, 0.30434782608695654, 0.49629629629629624, 0.87, 1.0, 1.0, 0.587984652321258, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.26671912577981677, 0.2667191257798166, 0.3224582734829352], 
reward next is 0.6775, 
noisyNet noise sample is [array([1.1080604], dtype=float32), -0.79457027]. 
=============================================
[2019-04-27 21:52:32,562] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:52:32,573] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9162
[2019-04-27 21:52:32,578] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.2, 91.0, 1.0, 2.0, 0.6112221192546309, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 696574.334125584, 696574.3341255845, 160019.1780146024], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5593200.0000, 
sim time next is 5593800.0000, 
raw observation next is [25.25, 91.0, 1.0, 2.0, 0.6239788349830097, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 711119.1649929314, 711119.1649929314, 162251.90983531], 
processed observation next is [1.0, 0.7391304347826086, 0.49074074074074076, 0.91, 1.0, 1.0, 0.5523557559321544, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2539711303546184, 0.2539711303546184, 0.31202290352944234], 
reward next is 0.6880, 
noisyNet noise sample is [array([0.2633351], dtype=float32), 0.4024958]. 
=============================================
[2019-04-27 21:52:37,784] A3C_AGENT_WORKER-Thread-3 INFO:Local step 143000, global step 2273905: loss 0.0108
[2019-04-27 21:52:37,787] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 143000, global step 2273905: learning rate 0.0001
[2019-04-27 21:52:38,234] A3C_AGENT_WORKER-Thread-21 INFO:Local step 142000, global step 2274111: loss 0.1974
[2019-04-27 21:52:38,239] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 142000, global step 2274111: learning rate 0.0001
[2019-04-27 21:52:38,691] A3C_AGENT_WORKER-Thread-12 INFO:Local step 142000, global step 2274328: loss 0.8725
[2019-04-27 21:52:38,694] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 142000, global step 2274328: learning rate 0.0001
[2019-04-27 21:52:39,005] A3C_AGENT_WORKER-Thread-2 INFO:Local step 142000, global step 2274486: loss 0.4165
[2019-04-27 21:52:39,007] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 142000, global step 2274487: learning rate 0.0001
[2019-04-27 21:52:39,039] A3C_AGENT_WORKER-Thread-15 INFO:Local step 142000, global step 2274498: loss 0.3324
[2019-04-27 21:52:39,041] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 142000, global step 2274500: learning rate 0.0001
[2019-04-27 21:52:39,322] A3C_AGENT_WORKER-Thread-17 INFO:Local step 142000, global step 2274638: loss 0.2680
[2019-04-27 21:52:39,326] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 142000, global step 2274639: learning rate 0.0001
[2019-04-27 21:52:39,806] A3C_AGENT_WORKER-Thread-11 INFO:Local step 142000, global step 2274871: loss 0.1976
[2019-04-27 21:52:39,808] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 142000, global step 2274872: learning rate 0.0001
[2019-04-27 21:52:39,833] A3C_AGENT_WORKER-Thread-22 INFO:Local step 142000, global step 2274884: loss 0.1351
[2019-04-27 21:52:39,837] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 142000, global step 2274885: learning rate 0.0001
[2019-04-27 21:52:39,958] A3C_AGENT_WORKER-Thread-9 INFO:Local step 142000, global step 2274943: loss 0.1197
[2019-04-27 21:52:39,959] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 142000, global step 2274943: learning rate 0.0001
[2019-04-27 21:52:40,078] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-27 21:52:40,079] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 21:52:40,080] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 21:52:40,080] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 21:52:40,084] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 21:52:40,087] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:52:40,085] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:52:40,081] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 21:52:40,089] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:52:40,091] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:52:40,092] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:52:40,116] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run92
[2019-04-27 21:52:40,143] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run92
[2019-04-27 21:52:40,143] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run92
[2019-04-27 21:52:40,167] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run92
[2019-04-27 21:52:40,226] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run92
[2019-04-27 21:52:46,429] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.01713451], dtype=float32), -0.014477464]
[2019-04-27 21:52:46,430] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [26.2, 35.0, 1.0, 2.0, 0.7052131157444003, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 905575.2641721091, 905575.2641721091, 180295.3343873203]
[2019-04-27 21:52:46,433] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:52:46,438] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [2.1335158e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.24796062450705125
[2019-04-27 21:53:24,264] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.01713451], dtype=float32), -0.014477464]
[2019-04-27 21:53:24,265] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [22.0, 100.0, 1.0, 2.0, 0.5552152736031113, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 658869.4793712854, 658869.4793712851, 151711.2654477615]
[2019-04-27 21:53:24,266] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:53:24,270] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.16563506172679543
[2019-04-27 21:53:35,617] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.01713451], dtype=float32), -0.014477464]
[2019-04-27 21:53:35,618] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [29.33333333333334, 77.33333333333334, 1.0, 2.0, 0.7799672210268516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 888994.8446642434, 888994.8446642434, 191841.659051907]
[2019-04-27 21:53:35,621] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 21:53:35,625] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.33984245005100777
[2019-04-27 21:54:07,526] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01713451], dtype=float32), -0.014477464]
[2019-04-27 21:54:07,527] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [23.06666666666667, 88.33333333333334, 1.0, 2.0, 0.4933357367622256, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 588570.6304562942, 588570.6304562942, 141858.0091394351]
[2019-04-27 21:54:07,528] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 21:54:07,529] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.14418756692656165
[2019-04-27 21:54:16,366] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01713451], dtype=float32), -0.014477464]
[2019-04-27 21:54:16,367] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [28.56666666666667, 79.0, 1.0, 2.0, 0.7430096001454002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 846847.8348523785, 846847.8348523785, 184449.8083466775]
[2019-04-27 21:54:16,369] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 21:54:16,372] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.14577180497669318
[2019-04-27 21:54:20,877] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.01713451], dtype=float32), -0.014477464]
[2019-04-27 21:54:20,878] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [22.01447260666666, 95.23834624, 1.0, 2.0, 0.5776242742615056, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 692585.7511166417, 692585.7511166417, 155755.8712168844]
[2019-04-27 21:54:20,878] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 21:54:20,880] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.18975483017500994
[2019-04-27 21:54:26,463] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8770.2626 2170712315.3188 493.0000
[2019-04-27 21:54:26,714] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8582.8795 2248773970.1662 553.0000
[2019-04-27 21:54:26,885] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8700.1130 2195220806.1349 572.0000
[2019-04-27 21:54:26,947] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8099.2132 2445444095.4335 746.0000
[2019-04-27 21:54:27,003] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8922.6628 2120599135.7723 430.0000
[2019-04-27 21:54:28,019] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 2275000, evaluation results [2275000.0, 8099.213190057684, 2445444095.433475, 746.0, 8770.262620159681, 2170712315.3187604, 493.0, 8922.662781171732, 2120599135.7722971, 430.0, 8582.879467926026, 2248773970.1662407, 553.0, 8700.112961499688, 2195220806.134913, 572.0]
[2019-04-27 21:54:28,074] A3C_AGENT_WORKER-Thread-16 INFO:Local step 142500, global step 2275032: loss 91.7017
[2019-04-27 21:54:28,076] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 142500, global step 2275032: learning rate 0.0001
[2019-04-27 21:54:28,397] A3C_AGENT_WORKER-Thread-20 INFO:Local step 142000, global step 2275200: loss 0.1067
[2019-04-27 21:54:28,399] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 142000, global step 2275200: learning rate 0.0001
[2019-04-27 21:54:28,415] A3C_AGENT_WORKER-Thread-18 INFO:Local step 142000, global step 2275207: loss 0.1468
[2019-04-27 21:54:28,416] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 142000, global step 2275207: learning rate 0.0001
[2019-04-27 21:54:28,679] A3C_AGENT_WORKER-Thread-14 INFO:Local step 142000, global step 2275349: loss 0.0970
[2019-04-27 21:54:28,682] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 142000, global step 2275350: learning rate 0.0001
[2019-04-27 21:54:28,834] A3C_AGENT_WORKER-Thread-13 INFO:Local step 142000, global step 2275427: loss 0.2085
[2019-04-27 21:54:28,841] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 142000, global step 2275429: learning rate 0.0001
[2019-04-27 21:54:28,960] A3C_AGENT_WORKER-Thread-10 INFO:Local step 142000, global step 2275490: loss 0.0828
[2019-04-27 21:54:28,963] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 142000, global step 2275491: learning rate 0.0001
[2019-04-27 21:54:30,593] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.6682876e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 21:54:30,600] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8688
[2019-04-27 21:54:30,605] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.6, 89.66666666666666, 1.0, 2.0, 0.5614489590967575, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 673154.6223671817, 673154.6223671817, 153010.3815619303], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6057600.0000, 
sim time next is 6058200.0000, 
raw observation next is [22.65, 89.33333333333333, 1.0, 2.0, 0.5472787395310141, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 656129.3724124992, 656129.3724124992, 150643.4939074561], 
processed observation next is [1.0, 0.08695652173913043, 0.3944444444444444, 0.8933333333333333, 1.0, 1.0, 0.46104611848930255, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2343319187187497, 0.2343319187187497, 0.2896990267451079], 
reward next is 0.7103, 
noisyNet noise sample is [array([-0.29776385], dtype=float32), -0.9558237]. 
=============================================
[2019-04-27 21:54:31,401] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.1791718e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 21:54:31,415] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1129
[2019-04-27 21:54:31,420] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.1, 87.0, 1.0, 2.0, 0.6382600076096098, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 775884.5248766068, 775884.5248766068, 166814.1259252875], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5797200.0000, 
sim time next is 5797800.0000, 
raw observation next is [22.05, 87.5, 1.0, 2.0, 0.558578490899124, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 678957.295517853, 678957.295517853, 152837.620687582], 
processed observation next is [1.0, 0.08695652173913043, 0.37222222222222223, 0.875, 1.0, 1.0, 0.47449820345133814, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2424847483992332, 0.2424847483992332, 0.2939185013222731], 
reward next is 0.7061, 
noisyNet noise sample is [array([-1.5786164], dtype=float32), 0.6149479]. 
=============================================
[2019-04-27 21:54:32,955] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4882917e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 21:54:32,969] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2247
[2019-04-27 21:54:32,974] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.23333333333333, 79.66666666666667, 1.0, 2.0, 0.4782529187586193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 580352.3454525685, 580352.345452568, 139857.8288473186], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5816400.0000, 
sim time next is 5817000.0000, 
raw observation next is [23.36666666666667, 78.83333333333333, 1.0, 2.0, 0.4834153855524796, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 586468.9733059745, 586468.9733059745, 140650.8318424452], 
processed observation next is [1.0, 0.30434782608695654, 0.4209876543209878, 0.7883333333333333, 1.0, 1.0, 0.3850183161339043, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.20945320475213375, 0.20945320475213375, 0.2704823689277792], 
reward next is 0.7295, 
noisyNet noise sample is [array([1.9321492], dtype=float32), 0.8347685]. 
=============================================
[2019-04-27 21:54:32,981] A3C_AGENT_WORKER-Thread-19 INFO:Local step 143500, global step 2277598: loss -20.5363
[2019-04-27 21:54:32,982] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 143500, global step 2277599: learning rate 0.0001
[2019-04-27 21:54:32,987] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[69.66854]
 [69.62776]
 [69.52765]
 [69.40757]
 [69.66129]], R is [[69.69438171]
 [69.72847748]
 [69.76393127]
 [69.79402161]
 [69.80280304]].
[2019-04-27 21:54:35,287] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8841103e-33 1.0000000e+00 2.1058462e-36 0.0000000e+00 9.7546275e-35], sum to 1.0000
[2019-04-27 21:54:35,294] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6154
[2019-04-27 21:54:35,304] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1442462.774549821 W.
[2019-04-27 21:54:35,309] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.83333333333333, 45.66666666666667, 1.0, 2.0, 0.5970995663307853, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9564981768050578, 6.911200000000001, 6.9112, 121.9256893416169, 1442462.774549821, 1442462.77454982, 287427.6268929681], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5841600.0000, 
sim time next is 5842200.0000, 
raw observation next is [27.86666666666667, 45.33333333333333, 1.0, 2.0, 0.5793526093127144, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9441036997912834, 6.9112, 6.9112, 121.9260425079378, 1410186.774213728, 1410186.774213728, 283130.3241955221], 
processed observation next is [1.0, 0.6086956521739131, 0.5876543209876545, 0.4533333333333333, 1.0, 1.0, 0.49922929680085043, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.9301296247391041, 0.0, 0.0, 0.8094621281049605, 0.50363813364776, 0.50363813364776, 0.5444813926836963], 
reward next is 0.4555, 
noisyNet noise sample is [array([0.28489786], dtype=float32), 0.14735603]. 
=============================================
[2019-04-27 21:54:37,833] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:54:37,839] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3239
[2019-04-27 21:54:37,849] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1521779.702848441 W.
[2019-04-27 21:54:37,853] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.45, 43.5, 1.0, 2.0, 0.9993393415541073, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.520460694529604, 6.9112, 121.9235153625226, 1521779.702848441, 1209790.239121543, 244017.0617482657], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5927400.0000, 
sim time next is 5928000.0000, 
raw observation next is [29.4, 44.0, 1.0, 2.0, 0.3928058737320132, 1.0, 1.0, 0.3928058737320132, 1.0, 1.0, 0.6298220562431062, 6.9112, 6.9112, 121.94756008, 1393374.654004989, 1393374.654004989, 293841.6304988527], 
processed observation next is [1.0, 0.6086956521739131, 0.6444444444444444, 0.44, 1.0, 1.0, 0.2771498496809681, 1.0, 0.5, 0.2771498496809681, 1.0, 0.5, 0.5372775703038827, 0.0, 0.0, 0.8096049824067558, 0.4976338050017818, 0.4976338050017818, 0.5650800586516398], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.48747545], dtype=float32), -1.0761182]. 
=============================================
[2019-04-27 21:54:37,872] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[71.863594]
 [71.41323 ]
 [71.43473 ]
 [70.64583 ]
 [70.39917 ]], R is [[71.42279816]
 [70.70857239]
 [70.4644165 ]
 [70.23512268]
 [69.97027588]].
[2019-04-27 21:54:41,271] A3C_AGENT_WORKER-Thread-3 INFO:Local step 143500, global step 2281984: loss -33.2175
[2019-04-27 21:54:41,274] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 143500, global step 2281985: learning rate 0.0001
[2019-04-27 21:54:41,640] A3C_AGENT_WORKER-Thread-21 INFO:Local step 142500, global step 2282161: loss -27.7758
[2019-04-27 21:54:41,642] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 142500, global step 2282162: learning rate 0.0001
[2019-04-27 21:54:42,139] A3C_AGENT_WORKER-Thread-12 INFO:Local step 142500, global step 2282401: loss 45.0930
[2019-04-27 21:54:42,142] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 142500, global step 2282401: learning rate 0.0001
[2019-04-27 21:54:42,392] A3C_AGENT_WORKER-Thread-15 INFO:Local step 142500, global step 2282519: loss 54.8576
[2019-04-27 21:54:42,395] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 142500, global step 2282519: learning rate 0.0001
[2019-04-27 21:54:42,419] A3C_AGENT_WORKER-Thread-2 INFO:Local step 142500, global step 2282530: loss 34.8006
[2019-04-27 21:54:42,421] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 142500, global step 2282530: learning rate 0.0001
[2019-04-27 21:54:42,591] A3C_AGENT_WORKER-Thread-17 INFO:Local step 142500, global step 2282609: loss 58.0917
[2019-04-27 21:54:42,593] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 142500, global step 2282609: learning rate 0.0001
[2019-04-27 21:54:43,029] A3C_AGENT_WORKER-Thread-11 INFO:Local step 142500, global step 2282813: loss 20.8514
[2019-04-27 21:54:43,030] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 142500, global step 2282813: learning rate 0.0001
[2019-04-27 21:54:43,130] A3C_AGENT_WORKER-Thread-9 INFO:Local step 142500, global step 2282862: loss 83.2439
[2019-04-27 21:54:43,133] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 142500, global step 2282862: learning rate 0.0001
[2019-04-27 21:54:43,269] A3C_AGENT_WORKER-Thread-22 INFO:Local step 142500, global step 2282925: loss -10.8255
[2019-04-27 21:54:43,272] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 142500, global step 2282925: learning rate 0.0001
[2019-04-27 21:54:43,742] A3C_AGENT_WORKER-Thread-20 INFO:Local step 142500, global step 2283146: loss 68.7036
[2019-04-27 21:54:43,749] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 142500, global step 2283146: learning rate 0.0001
[2019-04-27 21:54:43,793] A3C_AGENT_WORKER-Thread-16 INFO:Local step 143000, global step 2283175: loss 0.0290
[2019-04-27 21:54:43,796] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 143000, global step 2283177: learning rate 0.0001
[2019-04-27 21:54:43,840] A3C_AGENT_WORKER-Thread-18 INFO:Local step 142500, global step 2283198: loss 15.0103
[2019-04-27 21:54:43,844] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 142500, global step 2283198: learning rate 0.0001
[2019-04-27 21:54:44,054] A3C_AGENT_WORKER-Thread-13 INFO:Local step 142500, global step 2283293: loss 82.1345
[2019-04-27 21:54:44,055] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 142500, global step 2283293: learning rate 0.0001
[2019-04-27 21:54:44,105] A3C_AGENT_WORKER-Thread-14 INFO:Local step 142500, global step 2283316: loss 6.5485
[2019-04-27 21:54:44,110] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 142500, global step 2283316: learning rate 0.0001
[2019-04-27 21:54:44,222] A3C_AGENT_WORKER-Thread-10 INFO:Local step 142500, global step 2283373: loss -15.2364
[2019-04-27 21:54:44,223] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 142500, global step 2283373: learning rate 0.0001
[2019-04-27 21:54:48,511] A3C_AGENT_WORKER-Thread-19 INFO:Local step 144000, global step 2285392: loss 9.8899
[2019-04-27 21:54:48,514] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 144000, global step 2285393: learning rate 0.0001
[2019-04-27 21:54:49,317] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:54:49,329] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3548
[2019-04-27 21:54:49,336] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.35, 61.5, 1.0, 2.0, 0.546928765784739, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 639820.8303352107, 639820.8303352103, 149959.6585461556], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6115800.0000, 
sim time next is 6116400.0000, 
raw observation next is [28.2, 62.0, 1.0, 2.0, 0.5466682988667249, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 640149.1109863515, 640149.1109863515, 149943.9168529086], 
processed observation next is [1.0, 0.8260869565217391, 0.6, 0.62, 1.0, 1.0, 0.46031940341276767, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.22862468249512555, 0.22862468249512555, 0.28835368625559343], 
reward next is 0.7116, 
noisyNet noise sample is [array([-0.7948633], dtype=float32), -0.15670504]. 
=============================================
[2019-04-27 21:54:49,857] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.6397985e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 21:54:49,866] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6792
[2019-04-27 21:54:49,874] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.06666666666667, 90.0, 1.0, 2.0, 0.5831364577220224, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 692463.2088546848, 692463.2088546848, 156447.9371331398], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6146400.0000, 
sim time next is 6147000.0000, 
raw observation next is [23.0, 90.0, 1.0, 2.0, 0.5656377486597328, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 672575.8109836952, 672575.8109836952, 153508.1578516165], 
processed observation next is [1.0, 0.13043478260869565, 0.4074074074074074, 0.9, 1.0, 1.0, 0.48290208173777716, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.24020564677989115, 0.24020564677989115, 0.2952079958684933], 
reward next is 0.7048, 
noisyNet noise sample is [array([1.3116693], dtype=float32), -0.18475658]. 
=============================================
[2019-04-27 21:54:49,897] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[62.9358  ]
 [62.997684]
 [62.42006 ]
 [62.692947]
 [62.689976]], R is [[63.28462601]
 [63.35091782]
 [63.39374161]
 [63.41970825]
 [63.45069122]].
[2019-04-27 21:54:51,676] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4121396e-34 1.0000000e+00 4.9346408e-36 1.0306231e-36 1.4715139e-37], sum to 1.0000
[2019-04-27 21:54:51,684] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0078
[2019-04-27 21:54:51,691] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.66666666666667, 90.33333333333334, 1.0, 2.0, 0.6351682685217477, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 759197.9662186047, 759197.9662186047, 165801.1814695125], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6150000.0000, 
sim time next is 6150600.0000, 
raw observation next is [22.6, 90.5, 1.0, 2.0, 0.6218824695993214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 743936.9152875405, 743936.9152875402, 163434.8694727962], 
processed observation next is [1.0, 0.17391304347826086, 0.39259259259259266, 0.905, 1.0, 1.0, 0.5498600828563349, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2656917554598359, 0.26569175545983575, 0.3142978259092235], 
reward next is 0.6857, 
noisyNet noise sample is [array([1.0963416], dtype=float32), 0.6337943]. 
=============================================
[2019-04-27 21:54:53,439] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0729255e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 21:54:53,458] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6976
[2019-04-27 21:54:53,462] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666667, 79.0, 1.0, 2.0, 0.6665151221333827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 759619.7789227375, 759619.778922737, 169900.5803052402], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6484800.0000, 
sim time next is 6485400.0000, 
raw observation next is [27.1, 79.5, 1.0, 2.0, 0.6671117182751779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 760300.0500358714, 760300.0500358714, 170010.1264338021], 
processed observation next is [1.0, 0.043478260869565216, 0.5592592592592593, 0.795, 1.0, 1.0, 0.6037044265180689, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2715357321556684, 0.2715357321556684, 0.3269425508342348], 
reward next is 0.6731, 
noisyNet noise sample is [array([0.7026479], dtype=float32), 0.064211704]. 
=============================================
[2019-04-27 21:54:57,418] A3C_AGENT_WORKER-Thread-3 INFO:Local step 144000, global step 2289609: loss 13.6972
[2019-04-27 21:54:57,421] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 144000, global step 2289609: learning rate 0.0001
[2019-04-27 21:54:58,451] A3C_AGENT_WORKER-Thread-21 INFO:Local step 143000, global step 2290104: loss 0.1274
[2019-04-27 21:54:58,456] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 143000, global step 2290106: learning rate 0.0001
[2019-04-27 21:54:59,194] A3C_AGENT_WORKER-Thread-12 INFO:Local step 143000, global step 2290459: loss 0.0139
[2019-04-27 21:54:59,196] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 143000, global step 2290459: learning rate 0.0001
[2019-04-27 21:54:59,314] A3C_AGENT_WORKER-Thread-2 INFO:Local step 143000, global step 2290512: loss 0.0849
[2019-04-27 21:54:59,316] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 143000, global step 2290513: learning rate 0.0001
[2019-04-27 21:54:59,393] A3C_AGENT_WORKER-Thread-15 INFO:Local step 143000, global step 2290551: loss 0.0606
[2019-04-27 21:54:59,395] A3C_AGENT_WORKER-Thread-17 INFO:Local step 143000, global step 2290551: loss 0.0196
[2019-04-27 21:54:59,395] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 143000, global step 2290551: learning rate 0.0001
[2019-04-27 21:54:59,399] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 143000, global step 2290552: learning rate 0.0001
[2019-04-27 21:54:59,861] A3C_AGENT_WORKER-Thread-11 INFO:Local step 143000, global step 2290770: loss 0.0192
[2019-04-27 21:54:59,862] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 143000, global step 2290770: learning rate 0.0001
[2019-04-27 21:55:00,163] A3C_AGENT_WORKER-Thread-9 INFO:Local step 143000, global step 2290913: loss 0.0372
[2019-04-27 21:55:00,166] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 143000, global step 2290913: learning rate 0.0001
[2019-04-27 21:55:00,389] A3C_AGENT_WORKER-Thread-22 INFO:Local step 143000, global step 2291019: loss 0.0036
[2019-04-27 21:55:00,393] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 143000, global step 2291021: learning rate 0.0001
[2019-04-27 21:55:00,792] A3C_AGENT_WORKER-Thread-18 INFO:Local step 143000, global step 2291207: loss 0.0172
[2019-04-27 21:55:00,795] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 143000, global step 2291207: learning rate 0.0001
[2019-04-27 21:55:00,812] A3C_AGENT_WORKER-Thread-20 INFO:Local step 143000, global step 2291219: loss 0.0021
[2019-04-27 21:55:00,814] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 143000, global step 2291219: learning rate 0.0001
[2019-04-27 21:55:00,815] A3C_AGENT_WORKER-Thread-16 INFO:Local step 143500, global step 2291219: loss -49.6372
[2019-04-27 21:55:00,819] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 143500, global step 2291220: learning rate 0.0001
[2019-04-27 21:55:01,151] A3C_AGENT_WORKER-Thread-14 INFO:Local step 143000, global step 2291376: loss 0.1002
[2019-04-27 21:55:01,153] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 143000, global step 2291377: learning rate 0.0001
[2019-04-27 21:55:01,198] A3C_AGENT_WORKER-Thread-13 INFO:Local step 143000, global step 2291396: loss 0.1085
[2019-04-27 21:55:01,203] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 143000, global step 2291397: learning rate 0.0001
[2019-04-27 21:55:01,500] A3C_AGENT_WORKER-Thread-10 INFO:Local step 143000, global step 2291543: loss 0.0676
[2019-04-27 21:55:01,501] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 143000, global step 2291543: learning rate 0.0001
[2019-04-27 21:55:05,323] A3C_AGENT_WORKER-Thread-19 INFO:Local step 144500, global step 2293345: loss 0.2453
[2019-04-27 21:55:05,326] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 144500, global step 2293345: learning rate 0.0001
[2019-04-27 21:55:05,607] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.3934337e-28 1.0000000e+00 1.5327710e-32 3.8620979e-32 1.1790419e-25], sum to 1.0000
[2019-04-27 21:55:05,615] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5236
[2019-04-27 21:55:05,623] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.1, 91.5, 1.0, 2.0, 0.9456202766384331, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1077936.489920036, 1077936.489920037, 227858.0688825696], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6420600.0000, 
sim time next is 6421200.0000, 
raw observation next is [25.13333333333333, 91.33333333333334, 1.0, 2.0, 0.8325613906062939, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 948977.9992499042, 948977.9992499042, 202754.1076108355], 
processed observation next is [1.0, 0.30434782608695654, 0.4864197530864196, 0.9133333333333334, 1.0, 1.0, 0.8006683221503498, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3389207140178229, 0.3389207140178229, 0.38991174540545287], 
reward next is 0.6101, 
noisyNet noise sample is [array([-0.22020936], dtype=float32), -0.8762175]. 
=============================================
[2019-04-27 21:55:08,232] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:55:08,242] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7790
[2019-04-27 21:55:08,253] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2119276.6174858 W.
[2019-04-27 21:55:08,258] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.6, 55.0, 1.0, 2.0, 0.9289834962122793, 1.0, 2.0, 0.9289834962122793, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 2119276.6174858, 2119276.617485801, 399934.6637200892], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6452400.0000, 
sim time next is 6453000.0000, 
raw observation next is [31.55, 55.0, 1.0, 2.0, 0.599466798251953, 1.0, 2.0, 0.599466798251953, 1.0, 1.0, 0.9543706660877379, 6.911199999999999, 6.9112, 121.94756008, 2051253.952503097, 2051253.952503098, 395085.1553120578], 
processed observation next is [1.0, 0.6956521739130435, 0.7240740740740741, 0.55, 1.0, 1.0, 0.5231747598237536, 1.0, 1.0, 0.5231747598237536, 1.0, 0.5, 0.9429633326096722, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.7325906973225347, 0.732590697322535, 0.7597791448308804], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.02947113], dtype=float32), 1.015046]. 
=============================================
[2019-04-27 21:55:08,268] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[65.20653 ]
 [64.62904 ]
 [64.7325  ]
 [63.74918 ]
 [63.588013]], R is [[64.98256683]
 [64.33274078]
 [63.68941498]
 [63.30863571]
 [62.9226532 ]].
[2019-04-27 21:55:13,901] A3C_AGENT_WORKER-Thread-3 INFO:Local step 144500, global step 2297402: loss 0.3480
[2019-04-27 21:55:13,909] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 144500, global step 2297406: learning rate 0.0001
[2019-04-27 21:55:15,600] A3C_AGENT_WORKER-Thread-21 INFO:Local step 143500, global step 2298207: loss -2.5527
[2019-04-27 21:55:15,602] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 143500, global step 2298207: learning rate 0.0001
[2019-04-27 21:55:16,228] A3C_AGENT_WORKER-Thread-2 INFO:Local step 143500, global step 2298498: loss -17.9393
[2019-04-27 21:55:16,232] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 143500, global step 2298500: learning rate 0.0001
[2019-04-27 21:55:16,241] A3C_AGENT_WORKER-Thread-12 INFO:Local step 143500, global step 2298502: loss -15.2588
[2019-04-27 21:55:16,247] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 143500, global step 2298502: learning rate 0.0001
[2019-04-27 21:55:16,459] A3C_AGENT_WORKER-Thread-15 INFO:Local step 143500, global step 2298602: loss -8.4812
[2019-04-27 21:55:16,461] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 143500, global step 2298603: learning rate 0.0001
[2019-04-27 21:55:16,493] A3C_AGENT_WORKER-Thread-17 INFO:Local step 143500, global step 2298618: loss -7.3595
[2019-04-27 21:55:16,496] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 143500, global step 2298618: learning rate 0.0001
[2019-04-27 21:55:16,866] A3C_AGENT_WORKER-Thread-11 INFO:Local step 143500, global step 2298804: loss -5.2354
[2019-04-27 21:55:16,873] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 143500, global step 2298804: learning rate 0.0001
[2019-04-27 21:55:17,302] A3C_AGENT_WORKER-Thread-9 INFO:Local step 143500, global step 2299013: loss 26.5973
[2019-04-27 21:55:17,304] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 143500, global step 2299013: learning rate 0.0001
[2019-04-27 21:55:17,313] A3C_AGENT_WORKER-Thread-16 INFO:Local step 144000, global step 2299018: loss 23.6677
[2019-04-27 21:55:17,316] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 144000, global step 2299018: learning rate 0.0001
[2019-04-27 21:55:17,375] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:55:17,381] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8356
[2019-04-27 21:55:17,387] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 37.0, 1.0, 2.0, 0.3361141868033221, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 422511.221575034, 422511.221575034, 120095.8103835829], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6630000.0000, 
sim time next is 6630600.0000, 
raw observation next is [28.3, 39.5, 1.0, 2.0, 0.3385104046328595, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 424090.0044470103, 424090.0044470099, 120384.9166264826], 
processed observation next is [1.0, 0.7391304347826086, 0.6037037037037037, 0.395, 1.0, 1.0, 0.21251238646768986, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.15146071587393226, 0.1514607158739321, 0.2315094550509281], 
reward next is 0.7685, 
noisyNet noise sample is [array([0.76449156], dtype=float32), 2.2542045]. 
=============================================
[2019-04-27 21:55:17,393] A3C_AGENT_WORKER-Thread-22 INFO:Local step 143500, global step 2299050: loss 54.1029
[2019-04-27 21:55:17,399] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 143500, global step 2299050: learning rate 0.0001
[2019-04-27 21:55:17,715] A3C_AGENT_WORKER-Thread-20 INFO:Local step 143500, global step 2299204: loss -2.8277
[2019-04-27 21:55:17,719] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 143500, global step 2299204: learning rate 0.0001
[2019-04-27 21:55:17,794] A3C_AGENT_WORKER-Thread-18 INFO:Local step 143500, global step 2299241: loss 18.0577
[2019-04-27 21:55:17,800] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 143500, global step 2299244: learning rate 0.0001
[2019-04-27 21:55:18,140] A3C_AGENT_WORKER-Thread-14 INFO:Local step 143500, global step 2299406: loss 56.6146
[2019-04-27 21:55:18,143] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 143500, global step 2299407: learning rate 0.0001
[2019-04-27 21:55:18,225] A3C_AGENT_WORKER-Thread-13 INFO:Local step 143500, global step 2299444: loss 24.7696
[2019-04-27 21:55:18,226] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 143500, global step 2299444: learning rate 0.0001
[2019-04-27 21:55:18,541] A3C_AGENT_WORKER-Thread-10 INFO:Local step 143500, global step 2299590: loss 1.4068
[2019-04-27 21:55:18,543] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 143500, global step 2299590: learning rate 0.0001
[2019-04-27 21:55:19,409] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-04-27 21:55:19,412] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 21:55:19,413] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 21:55:19,413] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:55:19,414] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 21:55:19,415] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:55:19,414] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 21:55:19,415] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 21:55:19,417] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:55:19,419] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:55:19,419] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:55:19,441] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run93
[2019-04-27 21:55:19,442] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run93
[2019-04-27 21:55:19,466] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run93
[2019-04-27 21:55:19,518] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run93
[2019-04-27 21:55:19,518] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run93
[2019-04-27 21:56:05,517] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.01652399], dtype=float32), -0.013213804]
[2019-04-27 21:56:05,518] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [33.04039134, 23.68835602, 1.0, 2.0, 0.3743193559099317, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 469361.0073061659, 469361.0073061659, 125168.0454026889]
[2019-04-27 21:56:05,519] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 21:56:05,522] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3108727734742738
[2019-04-27 21:56:18,626] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.01652399], dtype=float32), -0.013213804]
[2019-04-27 21:56:18,628] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [28.82625786833333, 66.99788531, 1.0, 2.0, 0.7961537080884918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 907454.8804602958, 907454.8804602958, 195140.9002149047]
[2019-04-27 21:56:18,630] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 21:56:18,632] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8413668612170604
[2019-04-27 21:56:19,745] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01652399], dtype=float32), -0.013213804]
[2019-04-27 21:56:19,749] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [29.0, 79.0, 1.0, 2.0, 0.7591329878491982, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 865234.896305698, 865234.896305698, 187646.6912763609]
[2019-04-27 21:56:19,750] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 21:56:19,754] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3868264274990909
[2019-04-27 21:56:26,661] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01652399], dtype=float32), -0.013213804]
[2019-04-27 21:56:26,663] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [29.94321793, 66.06159390333333, 1.0, 2.0, 0.690700531618618, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.9260426156618, 1502187.233939883, 1502187.233939884, 315551.4553607589]
[2019-04-27 21:56:26,664] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 21:56:26,666] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5552803196806029
[2019-04-27 21:56:26,668] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1502187.233939883 W.
[2019-04-27 21:56:44,306] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.01652399], dtype=float32), -0.013213804]
[2019-04-27 21:56:44,307] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [30.36666666666667, 64.66666666666667, 1.0, 2.0, 0.664838725270188, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 757708.2639927886, 757708.2639927882, 169596.4577354836]
[2019-04-27 21:56:44,308] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 21:56:44,310] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.11800217257781465
[2019-04-27 21:56:48,890] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.01652399], dtype=float32), -0.013213804]
[2019-04-27 21:56:48,891] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [23.95, 88.33333333333334, 1.0, 2.0, 0.5412943967742718, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 634619.7616834126, 634619.7616834126, 149095.574471768]
[2019-04-27 21:56:48,893] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 21:56:48,896] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8561602462431891
[2019-04-27 21:57:06,000] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8923.4199 2120533710.1715 430.0000
[2019-04-27 21:57:06,469] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8700.8700 2195155380.8105 572.0000
[2019-04-27 21:57:06,551] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8582.8795 2248773970.1662 553.0000
[2019-04-27 21:57:06,612] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8099.2132 2445444095.4335 746.0000
[2019-04-27 21:57:06,636] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8770.2626 2170712315.3188 493.0000
[2019-04-27 21:57:07,654] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 2300000, evaluation results [2300000.0, 8099.213190057684, 2445444095.433475, 746.0, 8770.262620159681, 2170712315.3187604, 493.0, 8923.419851553375, 2120533710.171511, 430.0, 8582.879467926026, 2248773970.1662407, 553.0, 8700.870031992405, 2195155380.8105125, 572.0]
[2019-04-27 21:57:09,652] A3C_AGENT_WORKER-Thread-19 INFO:Local step 145000, global step 2301034: loss 0.0056
[2019-04-27 21:57:09,658] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 145000, global step 2301034: learning rate 0.0001
[2019-04-27 21:57:15,220] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:57:15,228] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8274
[2019-04-27 21:57:15,232] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.26666666666667, 79.33333333333334, 1.0, 2.0, 0.4609191777695189, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 559160.787415791, 559160.787415791, 137209.1719006291], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6825000.0000, 
sim time next is 6825600.0000, 
raw observation next is [23.2, 79.0, 1.0, 2.0, 0.4562303080356091, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 554464.4054307624, 554464.4054307624, 136533.7494625571], 
processed observation next is [0.0, 0.0, 0.4148148148148148, 0.79, 1.0, 1.0, 0.3526551286138204, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.19802300193955802, 0.19802300193955802, 0.26256490281260986], 
reward next is 0.7374, 
noisyNet noise sample is [array([0.45225117], dtype=float32), 0.8655314]. 
=============================================
[2019-04-27 21:57:15,923] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:57:15,931] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2294
[2019-04-27 21:57:15,939] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.46666666666667, 80.33333333333334, 1.0, 2.0, 0.4758639726752861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 574120.2927528089, 574120.2927528085, 139383.2278296803], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6823200.0000, 
sim time next is 6823800.0000, 
raw observation next is [23.4, 80.0, 1.0, 2.0, 0.4706292886299443, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 568860.274247843, 568860.274247843, 138617.3065797711], 
processed observation next is [1.0, 1.0, 0.42222222222222217, 0.8, 1.0, 1.0, 0.36979677217850515, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2031643836599439, 0.2031643836599439, 0.26657174342263673], 
reward next is 0.7334, 
noisyNet noise sample is [array([-0.30557728], dtype=float32), -1.1622784]. 
=============================================
[2019-04-27 21:57:17,712] A3C_AGENT_WORKER-Thread-3 INFO:Local step 145000, global step 2305252: loss 0.0534
[2019-04-27 21:57:17,713] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 145000, global step 2305252: learning rate 0.0001
[2019-04-27 21:57:19,660] A3C_AGENT_WORKER-Thread-21 INFO:Local step 144000, global step 2306205: loss 18.1692
[2019-04-27 21:57:19,662] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 144000, global step 2306206: learning rate 0.0001
[2019-04-27 21:57:20,202] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:57:20,209] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4000
[2019-04-27 21:57:20,213] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 69.0, 1.0, 2.0, 0.4046163202326042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 500548.7658228268, 500548.7658228268, 129253.5274702984], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6909600.0000, 
sim time next is 6910200.0000, 
raw observation next is [23.45, 69.5, 1.0, 2.0, 0.4054318646218649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 501373.0894758395, 501373.0894758395, 129364.8192175685], 
processed observation next is [0.0, 1.0, 0.42407407407407405, 0.695, 1.0, 1.0, 0.29218079121650586, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1790618176699427, 0.1790618176699427, 0.24877849849532402], 
reward next is 0.7512, 
noisyNet noise sample is [array([0.17075793], dtype=float32), 0.23028187]. 
=============================================
[2019-04-27 21:57:20,300] A3C_AGENT_WORKER-Thread-2 INFO:Local step 144000, global step 2306537: loss 21.1390
[2019-04-27 21:57:20,304] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 144000, global step 2306540: learning rate 0.0001
[2019-04-27 21:57:20,310] A3C_AGENT_WORKER-Thread-12 INFO:Local step 144000, global step 2306544: loss 22.6176
[2019-04-27 21:57:20,313] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 144000, global step 2306545: learning rate 0.0001
[2019-04-27 21:57:20,450] A3C_AGENT_WORKER-Thread-15 INFO:Local step 144000, global step 2306615: loss 24.9323
[2019-04-27 21:57:20,454] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 144000, global step 2306615: learning rate 0.0001
[2019-04-27 21:57:20,564] A3C_AGENT_WORKER-Thread-17 INFO:Local step 144000, global step 2306667: loss 22.4596
[2019-04-27 21:57:20,568] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 144000, global step 2306668: learning rate 0.0001
[2019-04-27 21:57:20,579] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:57:20,580] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9877
[2019-04-27 21:57:20,586] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.3, 71.0, 1.0, 2.0, 0.4089761553230605, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 505205.383233199, 505205.383233199, 129855.4867964328], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6912000.0000, 
sim time next is 6912600.0000, 
raw observation next is [23.26666666666667, 71.33333333333334, 1.0, 2.0, 0.4096587835948393, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 505910.620409999, 505910.6204099986, 129949.4189490843], 
processed observation next is [0.0, 0.0, 0.41728395061728407, 0.7133333333333334, 1.0, 1.0, 0.29721283761290396, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1806823644321425, 0.18068236443214236, 0.24990272874823904], 
reward next is 0.7501, 
noisyNet noise sample is [array([0.16994275], dtype=float32), 1.4888924]. 
=============================================
[2019-04-27 21:57:20,767] A3C_AGENT_WORKER-Thread-11 INFO:Local step 144000, global step 2306791: loss 29.1545
[2019-04-27 21:57:20,769] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:57:20,771] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 144000, global step 2306792: learning rate 0.0001
[2019-04-27 21:57:20,779] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3628
[2019-04-27 21:57:20,783] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 78.0, 1.0, 2.0, 0.4181834921441442, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 514893.965752678, 514893.9657526775, 131132.5835919751], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6921000.0000, 
sim time next is 6921600.0000, 
raw observation next is [22.4, 78.66666666666666, 1.0, 2.0, 0.418301990992712, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 515085.9484009991, 515085.9484009991, 131150.8046902682], 
processed observation next is [0.0, 0.08695652173913043, 0.38518518518518513, 0.7866666666666666, 1.0, 1.0, 0.30750237022941906, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.18395926728607112, 0.18395926728607112, 0.2522130859428235], 
reward next is 0.7478, 
noisyNet noise sample is [array([2.2805097], dtype=float32), 1.1543566]. 
=============================================
[2019-04-27 21:57:20,865] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.7019684e-30 1.0000000e+00 1.4539516e-33 2.6843128e-34 1.7511451e-27], sum to 1.0000
[2019-04-27 21:57:20,872] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0446
[2019-04-27 21:57:20,877] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.36666666666667, 65.66666666666667, 1.0, 2.0, 0.9088838366539338, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.946146455212061, 6.9112, 121.9258300999576, 1121121.476467588, 1103225.799112305, 222603.610592304], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7728000.0000, 
sim time next is 7728600.0000, 
raw observation next is [25.73333333333333, 64.33333333333333, 1.0, 2.0, 0.9267813153159952, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.050831644067149, 6.9112, 121.9253857520995, 1194153.677009001, 1122650.180331436, 226666.1772000651], 
processed observation next is [1.0, 0.43478260869565216, 0.5086419753086419, 0.6433333333333333, 1.0, 1.0, 0.9128348991857085, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.013963164406714856, 0.0, 0.8094577679291722, 0.42648345607464316, 0.4009464929755128, 0.43589649461550983], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.89222574], dtype=float32), 0.8284582]. 
=============================================
[2019-04-27 21:57:21,191] A3C_AGENT_WORKER-Thread-9 INFO:Local step 144000, global step 2307039: loss 30.0044
[2019-04-27 21:57:21,194] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 144000, global step 2307040: learning rate 0.0001
[2019-04-27 21:57:21,214] A3C_AGENT_WORKER-Thread-16 INFO:Local step 144500, global step 2307049: loss 0.0308
[2019-04-27 21:57:21,216] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 144500, global step 2307052: learning rate 0.0001
[2019-04-27 21:57:21,220] A3C_AGENT_WORKER-Thread-22 INFO:Local step 144000, global step 2307059: loss 29.0668
[2019-04-27 21:57:21,223] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 144000, global step 2307059: learning rate 0.0001
[2019-04-27 21:57:21,447] A3C_AGENT_WORKER-Thread-20 INFO:Local step 144000, global step 2307192: loss 30.0334
[2019-04-27 21:57:21,448] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 144000, global step 2307192: learning rate 0.0001
[2019-04-27 21:57:21,575] A3C_AGENT_WORKER-Thread-18 INFO:Local step 144000, global step 2307278: loss 32.5529
[2019-04-27 21:57:21,577] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 144000, global step 2307278: learning rate 0.0001
[2019-04-27 21:57:21,732] A3C_AGENT_WORKER-Thread-14 INFO:Local step 144000, global step 2307377: loss 34.8469
[2019-04-27 21:57:21,733] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 144000, global step 2307377: learning rate 0.0001
[2019-04-27 21:57:21,858] A3C_AGENT_WORKER-Thread-13 INFO:Local step 144000, global step 2307449: loss 34.8252
[2019-04-27 21:57:21,861] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 144000, global step 2307449: learning rate 0.0001
[2019-04-27 21:57:22,184] A3C_AGENT_WORKER-Thread-10 INFO:Local step 144000, global step 2307614: loss 29.3685
[2019-04-27 21:57:22,187] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 144000, global step 2307614: learning rate 0.0001
[2019-04-27 21:57:23,525] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:57:23,537] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4662
[2019-04-27 21:57:23,542] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.63333333333333, 61.33333333333334, 1.0, 2.0, 0.4427628508542708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 541340.9370717123, 541340.9370717123, 134621.8796931429], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6987000.0000, 
sim time next is 6987600.0000, 
raw observation next is [25.5, 62.0, 1.0, 2.0, 0.4416815814075381, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 540128.8008461375, 540128.8008461375, 134464.983013418], 
processed observation next is [0.0, 0.9130434782608695, 0.5, 0.62, 1.0, 1.0, 0.33533521596135485, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1929031431593348, 0.1929031431593348, 0.2585865057950346], 
reward next is 0.7414, 
noisyNet noise sample is [array([-0.21360748], dtype=float32), 1.4916425]. 
=============================================
[2019-04-27 21:57:23,703] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:57:23,711] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1098
[2019-04-27 21:57:23,716] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.26666666666667, 91.33333333333334, 1.0, 2.0, 0.3799843583735951, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 471352.2854934214, 471352.2854934214, 125846.917540235], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7627200.0000, 
sim time next is 7627800.0000, 
raw observation next is [20.33333333333333, 91.16666666666667, 1.0, 2.0, 0.380186962848824, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 471300.1216024373, 471300.1216024373, 125868.0480642961], 
processed observation next is [1.0, 0.2608695652173913, 0.3086419753086418, 0.9116666666666667, 1.0, 1.0, 0.26212733672479044, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16832147200087047, 0.16832147200087047, 0.2420539385851848], 
reward next is 0.7579, 
noisyNet noise sample is [array([1.0994112], dtype=float32), 0.17193355]. 
=============================================
[2019-04-27 21:57:25,336] A3C_AGENT_WORKER-Thread-19 INFO:Local step 145500, global step 2309105: loss 0.3498
[2019-04-27 21:57:25,337] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 145500, global step 2309105: learning rate 0.0001
[2019-04-27 21:57:26,042] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.6977105e-32 1.0000000e+00 1.8736073e-36 1.0866631e-36 5.1829469e-31], sum to 1.0000
[2019-04-27 21:57:26,050] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4218
[2019-04-27 21:57:26,055] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.2, 80.0, 1.0, 2.0, 0.9013134368917247, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9259959713014, 1092843.41677925, 1092843.41677925, 220823.0624745169], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7030800.0000, 
sim time next is 7031400.0000, 
raw observation next is [23.36666666666667, 79.33333333333334, 1.0, 2.0, 0.9677936542147358, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.317590876277044, 6.9112, 121.9244895575976, 1380252.679291987, 1172146.878658025, 236370.1279675151], 
processed observation next is [1.0, 0.391304347826087, 0.4209876543209878, 0.7933333333333334, 1.0, 1.0, 0.9616591121603998, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.040639087627704386, 0.0, 0.8094518181296562, 0.49294738546142397, 0.41862388523500893, 0.4545579383990675], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.30744293], dtype=float32), -1.8055786]. 
=============================================
[2019-04-27 21:57:28,061] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.5555117e-31 1.0000000e+00 2.3401204e-34 9.4435818e-36 1.3040584e-35], sum to 1.0000
[2019-04-27 21:57:28,072] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1691
[2019-04-27 21:57:28,081] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.01666666666667, 80.66666666666667, 1.0, 2.0, 0.8686843987365739, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1054787.728149049, 1054787.728149049, 213504.8998487096], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7030200.0000, 
sim time next is 7030800.0000, 
raw observation next is [23.2, 80.0, 1.0, 2.0, 0.9013134368917247, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9259959713014, 1092843.41677925, 1092843.41677925, 220823.0624745169], 
processed observation next is [1.0, 0.391304347826087, 0.4148148148148148, 0.8, 1.0, 1.0, 0.8825159962996723, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094618191500896, 0.39030122027830355, 0.39030122027830355, 0.4246597355279171], 
reward next is 0.5753, 
noisyNet noise sample is [array([0.05148597], dtype=float32), 0.7543389]. 
=============================================
[2019-04-27 21:57:34,096] A3C_AGENT_WORKER-Thread-3 INFO:Local step 145500, global step 2313242: loss 0.2249
[2019-04-27 21:57:34,102] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 145500, global step 2313245: learning rate 0.0001
[2019-04-27 21:57:34,250] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:57:34,251] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:57:34,324] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res14/Eplus-env-sub_run12
[2019-04-27 21:57:35,801] A3C_AGENT_WORKER-Thread-21 INFO:Local step 144500, global step 2314154: loss 0.1182
[2019-04-27 21:57:35,806] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 144500, global step 2314156: learning rate 0.0001
[2019-04-27 21:57:36,396] A3C_AGENT_WORKER-Thread-12 INFO:Local step 144500, global step 2314435: loss 0.0195
[2019-04-27 21:57:36,400] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 144500, global step 2314435: learning rate 0.0001
[2019-04-27 21:57:36,407] A3C_AGENT_WORKER-Thread-2 INFO:Local step 144500, global step 2314439: loss 0.0286
[2019-04-27 21:57:36,410] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 144500, global step 2314440: learning rate 0.0001
[2019-04-27 21:57:36,681] A3C_AGENT_WORKER-Thread-15 INFO:Local step 144500, global step 2314568: loss 0.0564
[2019-04-27 21:57:36,682] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 144500, global step 2314568: learning rate 0.0001
[2019-04-27 21:57:36,682] A3C_AGENT_WORKER-Thread-17 INFO:Local step 144500, global step 2314569: loss 0.0499
[2019-04-27 21:57:36,686] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 144500, global step 2314570: learning rate 0.0001
[2019-04-27 21:57:37,013] A3C_AGENT_WORKER-Thread-11 INFO:Local step 144500, global step 2314729: loss 0.0690
[2019-04-27 21:57:37,016] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 144500, global step 2314730: learning rate 0.0001
[2019-04-27 21:57:37,446] A3C_AGENT_WORKER-Thread-9 INFO:Local step 144500, global step 2314941: loss 0.0231
[2019-04-27 21:57:37,450] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 144500, global step 2314941: learning rate 0.0001
[2019-04-27 21:57:37,474] A3C_AGENT_WORKER-Thread-22 INFO:Local step 144500, global step 2314955: loss 0.0497
[2019-04-27 21:57:37,480] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 144500, global step 2314957: learning rate 0.0001
[2019-04-27 21:57:37,490] A3C_AGENT_WORKER-Thread-16 INFO:Local step 145000, global step 2314960: loss 0.0193
[2019-04-27 21:57:37,492] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 145000, global step 2314961: learning rate 0.0001
[2019-04-27 21:57:37,756] A3C_AGENT_WORKER-Thread-20 INFO:Local step 144500, global step 2315087: loss 0.0831
[2019-04-27 21:57:37,757] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 144500, global step 2315087: learning rate 0.0001
[2019-04-27 21:57:38,097] A3C_AGENT_WORKER-Thread-18 INFO:Local step 144500, global step 2315251: loss 0.0635
[2019-04-27 21:57:38,100] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 144500, global step 2315252: learning rate 0.0001
[2019-04-27 21:57:38,181] A3C_AGENT_WORKER-Thread-14 INFO:Local step 144500, global step 2315287: loss 0.0396
[2019-04-27 21:57:38,183] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 144500, global step 2315288: learning rate 0.0001
[2019-04-27 21:57:38,256] A3C_AGENT_WORKER-Thread-13 INFO:Local step 144500, global step 2315332: loss 0.0269
[2019-04-27 21:57:38,259] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 144500, global step 2315333: learning rate 0.0001
[2019-04-27 21:57:38,574] A3C_AGENT_WORKER-Thread-10 INFO:Local step 144500, global step 2315483: loss 0.0407
[2019-04-27 21:57:38,577] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 144500, global step 2315485: learning rate 0.0001
[2019-04-27 21:57:40,304] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:57:40,314] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6854
[2019-04-27 21:57:40,317] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.33333333333334, 89.16666666666667, 1.0, 2.0, 0.3988241170447791, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 496025.2926532617, 496025.2926532612, 128493.91922685], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7276200.0000, 
sim time next is 7276800.0000, 
raw observation next is [20.36666666666667, 89.33333333333334, 1.0, 2.0, 0.3816860159993338, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 474369.2208568085, 474369.2208568085, 126100.3502303049], 
processed observation next is [1.0, 0.21739130434782608, 0.3098765432098767, 0.8933333333333334, 1.0, 1.0, 0.26391192380873074, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16941757887743158, 0.16941757887743158, 0.24250067351981713], 
reward next is 0.7575, 
noisyNet noise sample is [array([-2.3157885], dtype=float32), 1.6463671]. 
=============================================
[2019-04-27 21:57:40,924] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:57:40,931] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4081
[2019-04-27 21:57:40,935] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.23333333333333, 89.33333333333334, 1.0, 2.0, 0.5595378203740596, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 686600.3761833212, 686600.3761833212, 153188.3629653082], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7287600.0000, 
sim time next is 7288200.0000, 
raw observation next is [21.35, 88.5, 1.0, 2.0, 0.6479486298418465, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 794697.7834808667, 794697.7834808667, 168801.9979568262], 
processed observation next is [1.0, 0.34782608695652173, 0.3462962962962963, 0.885, 1.0, 1.0, 0.5808912260021982, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2838206369574524, 0.2838206369574524, 0.3246192268400504], 
reward next is 0.6754, 
noisyNet noise sample is [array([1.0931342], dtype=float32), 0.5651419]. 
=============================================
[2019-04-27 21:57:42,829] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:57:42,829] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:57:42,903] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res3/Eplus-env-sub_run12
[2019-04-27 21:57:44,102] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:57:44,110] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1202
[2019-04-27 21:57:44,115] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.6, 83.0, 1.0, 2.0, 0.3651153632972859, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 456880.9915437361, 456880.9915437361, 123902.4404790275], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7351200.0000, 
sim time next is 7351800.0000, 
raw observation next is [20.53333333333333, 83.83333333333334, 1.0, 2.0, 0.514561706555936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 643694.4432493184, 643694.4432493184, 146050.2461386307], 
processed observation next is [1.0, 0.08695652173913043, 0.3160493827160493, 0.8383333333333334, 1.0, 1.0, 0.4220972697094476, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.22989087258904228, 0.22989087258904228, 0.2808658579589052], 
reward next is 0.7191, 
noisyNet noise sample is [array([0.5613409], dtype=float32), 0.8706084]. 
=============================================
[2019-04-27 21:57:49,101] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:57:49,110] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7613
[2019-04-27 21:57:49,114] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.03333333333333, 27.66666666666667, 1.0, 2.0, 0.3457217019770887, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 442102.1855964321, 442102.1855964321, 121413.1074774728], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 246000.0000, 
sim time next is 246600.0000, 
raw observation next is [28.7, 28.5, 1.0, 2.0, 0.3423598016718446, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 438029.5142467616, 438029.5142467616, 120971.3630877984], 
processed observation next is [0.0, 0.8695652173913043, 0.6185185185185185, 0.285, 1.0, 1.0, 0.2170950019902912, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1564391122309863, 0.1564391122309863, 0.2326372367073046], 
reward next is 0.7674, 
noisyNet noise sample is [array([1.1832353], dtype=float32), 1.0027192]. 
=============================================
[2019-04-27 21:57:49,616] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:57:49,627] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0090
[2019-04-27 21:57:49,635] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.9, 92.5, 1.0, 2.0, 0.3803009537742372, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 473095.6242887283, 473095.6242887279, 125919.0130290878], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7435800.0000, 
sim time next is 7436400.0000, 
raw observation next is [19.86666666666667, 92.66666666666667, 1.0, 2.0, 0.3797144451492913, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 472454.093893419, 472454.093893419, 125840.2092169544], 
processed observation next is [0.0, 0.043478260869565216, 0.2913580246913582, 0.9266666666666667, 1.0, 1.0, 0.26156481565391826, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16873360496193537, 0.16873360496193537, 0.24200040234029693], 
reward next is 0.7580, 
noisyNet noise sample is [array([0.20810951], dtype=float32), 0.13112661]. 
=============================================
[2019-04-27 21:57:50,479] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:57:50,488] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2877
[2019-04-27 21:57:50,493] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.33333333333333, 51.16666666666666, 1.0, 2.0, 0.2545756168311171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 328381.9062604767, 328381.9062604767, 93008.35270226706], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 270600.0000, 
sim time next is 271200.0000, 
raw observation next is [20.26666666666667, 51.33333333333334, 1.0, 2.0, 0.2532964030626842, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 326731.4727316831, 326731.4727316831, 92626.55987684756], 
processed observation next is [0.0, 0.13043478260869565, 0.3061728395061729, 0.5133333333333334, 1.0, 1.0, 0.11106714650319546, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.11668981168988683, 0.11668981168988683, 0.1781279997631684], 
reward next is 0.8219, 
noisyNet noise sample is [array([-1.8017671], dtype=float32), 0.7732793]. 
=============================================
[2019-04-27 21:57:52,142] A3C_AGENT_WORKER-Thread-21 INFO:Local step 145000, global step 2322046: loss 0.0159
[2019-04-27 21:57:52,144] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 145000, global step 2322046: learning rate 0.0001
[2019-04-27 21:57:52,707] A3C_AGENT_WORKER-Thread-12 INFO:Local step 145000, global step 2322312: loss 0.0033
[2019-04-27 21:57:52,712] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 145000, global step 2322314: learning rate 0.0001
[2019-04-27 21:57:52,755] A3C_AGENT_WORKER-Thread-2 INFO:Local step 145000, global step 2322337: loss 0.0023
[2019-04-27 21:57:52,760] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 145000, global step 2322337: learning rate 0.0001
[2019-04-27 21:57:52,946] A3C_AGENT_WORKER-Thread-17 INFO:Local step 145000, global step 2322423: loss 0.0046
[2019-04-27 21:57:52,949] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 145000, global step 2322426: learning rate 0.0001
[2019-04-27 21:57:53,074] A3C_AGENT_WORKER-Thread-15 INFO:Local step 145000, global step 2322482: loss 0.0058
[2019-04-27 21:57:53,076] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 145000, global step 2322482: learning rate 0.0001
[2019-04-27 21:57:53,256] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:57:53,266] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7759
[2019-04-27 21:57:53,272] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 93.66666666666667, 1.0, 2.0, 0.478144122133895, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 574402.9506259514, 574402.9506259514, 139650.0789666493], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7510800.0000, 
sim time next is 7511400.0000, 
raw observation next is [21.95, 94.0, 1.0, 2.0, 0.4774217872444759, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 573627.2464989118, 573627.2464989118, 139542.3522831036], 
processed observation next is [0.0, 0.9565217391304348, 0.36851851851851847, 0.94, 1.0, 1.0, 0.3778830800529475, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.20486687374961135, 0.20486687374961135, 0.2683506774675069], 
reward next is 0.7316, 
noisyNet noise sample is [array([-0.5443181], dtype=float32), -1.3157762]. 
=============================================
[2019-04-27 21:57:53,300] A3C_AGENT_WORKER-Thread-11 INFO:Local step 145000, global step 2322593: loss 0.0019
[2019-04-27 21:57:53,302] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 145000, global step 2322593: learning rate 0.0001
[2019-04-27 21:57:53,866] A3C_AGENT_WORKER-Thread-9 INFO:Local step 145000, global step 2322863: loss 0.0289
[2019-04-27 21:57:53,867] A3C_AGENT_WORKER-Thread-22 INFO:Local step 145000, global step 2322864: loss 0.0320
[2019-04-27 21:57:53,870] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 145000, global step 2322864: learning rate 0.0001
[2019-04-27 21:57:53,871] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 145000, global step 2322864: learning rate 0.0001
[2019-04-27 21:57:53,978] A3C_AGENT_WORKER-Thread-16 INFO:Local step 145500, global step 2322915: loss 0.0733
[2019-04-27 21:57:53,980] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 145500, global step 2322916: learning rate 0.0001
[2019-04-27 21:57:54,110] A3C_AGENT_WORKER-Thread-20 INFO:Local step 145000, global step 2322977: loss 0.0064
[2019-04-27 21:57:54,112] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 145000, global step 2322978: learning rate 0.0001
[2019-04-27 21:57:54,290] A3C_AGENT_WORKER-Thread-18 INFO:Local step 145000, global step 2323068: loss 0.0044
[2019-04-27 21:57:54,291] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 145000, global step 2323069: learning rate 0.0001
[2019-04-27 21:57:54,495] A3C_AGENT_WORKER-Thread-13 INFO:Local step 145000, global step 2323171: loss 0.0018
[2019-04-27 21:57:54,502] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 145000, global step 2323171: learning rate 0.0001
[2019-04-27 21:57:54,655] A3C_AGENT_WORKER-Thread-14 INFO:Local step 145000, global step 2323246: loss 0.0047
[2019-04-27 21:57:54,660] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 145000, global step 2323246: learning rate 0.0001
[2019-04-27 21:57:54,980] A3C_AGENT_WORKER-Thread-10 INFO:Local step 145000, global step 2323398: loss 0.0018
[2019-04-27 21:57:54,984] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 145000, global step 2323398: learning rate 0.0001
[2019-04-27 21:57:58,346] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-27 21:57:58,350] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 21:57:58,353] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:57:58,355] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 21:57:58,356] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 21:57:58,356] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:57:58,358] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 21:57:58,359] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 21:57:58,361] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:57:58,361] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:57:58,359] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:57:58,390] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run94
[2019-04-27 21:57:58,391] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run94
[2019-04-27 21:57:58,439] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run94
[2019-04-27 21:57:58,475] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run94
[2019-04-27 21:57:58,510] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run94
[2019-04-27 21:58:09,244] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.01121564], dtype=float32), -0.013136145]
[2019-04-27 21:58:09,247] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [19.33333333333333, 66.0, 1.0, 2.0, 0.2383586758130327, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 307459.1742088404, 307459.1742088404, 99875.52145847981]
[2019-04-27 21:58:09,247] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 21:58:09,250] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.19582737640431203
[2019-04-27 21:58:10,194] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.01121564], dtype=float32), -0.013136145]
[2019-04-27 21:58:10,194] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [28.8, 30.5, 1.0, 2.0, 0.3393210097314104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 432080.0822960175, 432080.082296017, 120568.3790920345]
[2019-04-27 21:58:10,197] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 21:58:10,202] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5974864384139221
[2019-04-27 21:58:11,880] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.01121564], dtype=float32), -0.013136145]
[2019-04-27 21:58:11,881] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [24.91666666666666, 60.33333333333334, 1.0, 2.0, 0.4002394264663172, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 495377.328778485, 495377.328778485, 128640.8769630595]
[2019-04-27 21:58:11,882] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:58:11,887] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8708148085449857
[2019-04-27 21:58:20,128] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01121564], dtype=float32), -0.013136145]
[2019-04-27 21:58:20,129] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [29.88308935333333, 27.8052831, 1.0, 2.0, 0.3466871618525541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 441924.4043482147, 441924.4043482147, 121537.9494850223]
[2019-04-27 21:58:20,131] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 21:58:20,135] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.20171453706980091
[2019-04-27 21:58:32,541] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01121564], dtype=float32), -0.013136145]
[2019-04-27 21:58:32,544] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [21.37196421333334, 104.0895107333333, 1.0, 2.0, 0.4933550004529496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 592714.4109318397, 592714.4109318393, 142008.7454658837]
[2019-04-27 21:58:32,545] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 21:58:32,550] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6727791296292269
[2019-04-27 21:58:45,268] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.01121564], dtype=float32), -0.013136145]
[2019-04-27 21:58:45,268] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [29.09097644, 32.00264200333334, 1.0, 2.0, 0.3368206642089795, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 426687.6238705241, 426687.6238705241, 120227.0329003184]
[2019-04-27 21:58:45,270] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 21:58:45,274] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.36520611951882453
[2019-04-27 21:59:06,220] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.01121564], dtype=float32), -0.013136145]
[2019-04-27 21:59:06,220] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [24.75963047, 82.16687784, 1.0, 2.0, 0.5479636830445473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 644393.7903187429, 644393.7903187429, 150272.0327340411]
[2019-04-27 21:59:06,221] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 21:59:06,226] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5124710248965448
[2019-04-27 21:59:24,586] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.01121564], dtype=float32), -0.013136145]
[2019-04-27 21:59:24,588] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [26.13333333333333, 73.0, 1.0, 2.0, 0.6337428558428773, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9865424209606017, 6.9112, 6.9112, 121.9260426156618, 1446233.107752339, 1446233.107752339, 303555.8520410347]
[2019-04-27 21:59:24,588] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:59:24,592] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.37560519604234555
[2019-04-27 21:59:24,594] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1446233.107752339 W.
[2019-04-27 21:59:45,398] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.01121564], dtype=float32), -0.013136145]
[2019-04-27 21:59:45,400] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [24.56666666666667, 66.33333333333333, 1.0, 2.0, 0.4285047899572014, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 525404.3844410321, 525404.3844410321, 132570.1177782814]
[2019-04-27 21:59:45,400] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 21:59:45,403] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.1659034591325318
[2019-04-27 21:59:45,654] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8770.2626 2170712315.3188 493.0000
[2019-04-27 21:59:45,898] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.01121564], dtype=float32), -0.013136145]
[2019-04-27 21:59:45,899] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [23.1, 72.5, 1.0, 2.0, 0.3971561894368569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 490608.1941069754, 490608.1941069749, 128185.023829496]
[2019-04-27 21:59:45,899] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 21:59:45,902] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4829403718617048
[2019-04-27 21:59:46,024] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8582.8795 2248773970.1662 553.0000
[2019-04-27 21:59:46,204] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8923.4199 2120533710.1715 430.0000
[2019-04-27 21:59:46,240] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8099.2132 2445444095.4335 746.0000
[2019-04-27 21:59:46,253] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8700.1130 2195220806.1349 572.0000
[2019-04-27 21:59:47,268] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 2325000, evaluation results [2325000.0, 8099.213190057684, 2445444095.433475, 746.0, 8770.262620159681, 2170712315.3187604, 493.0, 8923.419851553375, 2120533710.171511, 430.0, 8582.879467926026, 2248773970.1662407, 553.0, 8700.112961499688, 2195220806.134913, 572.0]
[2019-04-27 21:59:47,828] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:59:47,834] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2264
[2019-04-27 21:59:47,838] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.3, 82.0, 1.0, 2.0, 0.5179953400593988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 613869.100306277, 613869.100306277, 145600.6741963902], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7588800.0000, 
sim time next is 7589400.0000, 
raw observation next is [24.23333333333333, 82.33333333333334, 1.0, 2.0, 0.5185570612384228, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 614689.676513498, 614689.6765134975, 145696.5626484852], 
processed observation next is [0.0, 0.8695652173913043, 0.45308641975308633, 0.8233333333333335, 1.0, 1.0, 0.42685364433145573, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.21953202732624927, 0.2195320273262491, 0.2801856974009331], 
reward next is 0.7198, 
noisyNet noise sample is [array([0.8182693], dtype=float32), 0.006384468]. 
=============================================
[2019-04-27 21:59:51,575] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 21:59:51,575] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 21:59:51,627] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res11/Eplus-env-sub_run12
[2019-04-27 21:59:56,526] A3C_AGENT_WORKER-Thread-21 INFO:Local step 145500, global step 2329941: loss 0.0376
[2019-04-27 21:59:56,530] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 145500, global step 2329942: learning rate 0.0001
[2019-04-27 21:59:56,721] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 21:59:56,730] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7179
[2019-04-27 21:59:56,735] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.76666666666667, 55.0, 1.0, 2.0, 0.3208292473664171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 406506.3521774015, 406506.3521774015, 118170.3298211306], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7800000.0000, 
sim time next is 7800600.0000, 
raw observation next is [23.98333333333333, 54.0, 1.0, 2.0, 0.3204576427383531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 405901.3459390014, 405901.3459390014, 118121.8264731827], 
processed observation next is [1.0, 0.2608695652173913, 0.4438271604938271, 0.54, 1.0, 1.0, 0.19102100325994414, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1449647664067862, 0.1449647664067862, 0.2271573586022744], 
reward next is 0.7728, 
noisyNet noise sample is [array([-0.00286022], dtype=float32), 0.3900021]. 
=============================================
[2019-04-27 21:59:57,071] A3C_AGENT_WORKER-Thread-12 INFO:Local step 145500, global step 2330220: loss 0.3746
[2019-04-27 21:59:57,074] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 145500, global step 2330220: learning rate 0.0001
[2019-04-27 21:59:57,304] A3C_AGENT_WORKER-Thread-17 INFO:Local step 145500, global step 2330349: loss 0.0662
[2019-04-27 21:59:57,309] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 145500, global step 2330349: learning rate 0.0001
[2019-04-27 21:59:57,330] A3C_AGENT_WORKER-Thread-2 INFO:Local step 145500, global step 2330359: loss 0.0808
[2019-04-27 21:59:57,330] A3C_AGENT_WORKER-Thread-15 INFO:Local step 145500, global step 2330359: loss 0.1125
[2019-04-27 21:59:57,333] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 145500, global step 2330359: learning rate 0.0001
[2019-04-27 21:59:57,334] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 145500, global step 2330359: learning rate 0.0001
[2019-04-27 21:59:57,596] A3C_AGENT_WORKER-Thread-11 INFO:Local step 145500, global step 2330494: loss 0.1017
[2019-04-27 21:59:57,599] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 145500, global step 2330496: learning rate 0.0001
[2019-04-27 21:59:57,979] A3C_AGENT_WORKER-Thread-9 INFO:Local step 145500, global step 2330689: loss 0.0459
[2019-04-27 21:59:57,981] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 145500, global step 2330689: learning rate 0.0001
[2019-04-27 21:59:58,206] A3C_AGENT_WORKER-Thread-22 INFO:Local step 145500, global step 2330805: loss 0.0377
[2019-04-27 21:59:58,210] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 145500, global step 2330807: learning rate 0.0001
[2019-04-27 21:59:58,265] A3C_AGENT_WORKER-Thread-20 INFO:Local step 145500, global step 2330836: loss 0.0390
[2019-04-27 21:59:58,267] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 145500, global step 2330836: learning rate 0.0001
[2019-04-27 21:59:58,357] A3C_AGENT_WORKER-Thread-18 INFO:Local step 145500, global step 2330882: loss 0.0773
[2019-04-27 21:59:58,358] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 145500, global step 2330882: learning rate 0.0001
[2019-04-27 21:59:58,773] A3C_AGENT_WORKER-Thread-13 INFO:Local step 145500, global step 2331097: loss 0.0765
[2019-04-27 21:59:58,778] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 145500, global step 2331098: learning rate 0.0001
[2019-04-27 21:59:58,823] A3C_AGENT_WORKER-Thread-14 INFO:Local step 145500, global step 2331121: loss 0.0127
[2019-04-27 21:59:58,825] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 145500, global step 2331121: learning rate 0.0001
[2019-04-27 21:59:59,090] A3C_AGENT_WORKER-Thread-10 INFO:Local step 145500, global step 2331263: loss 0.1330
[2019-04-27 21:59:59,092] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 145500, global step 2331263: learning rate 0.0001
[2019-04-27 22:00:04,405] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 22:00:04,405] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:00:04,456] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res16/Eplus-env-sub_run12
[2019-04-27 22:00:04,988] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 22:00:04,989] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:00:05,013] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res7/Eplus-env-sub_run12
[2019-04-27 22:00:05,161] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 22:00:05,162] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:00:05,187] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res10/Eplus-env-sub_run12
[2019-04-27 22:00:05,207] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 22:00:05,207] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:00:05,228] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res12/Eplus-env-sub_run12
[2019-04-27 22:00:05,252] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 22:00:05,256] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:00:05,261] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res2/Eplus-env-sub_run12
[2019-04-27 22:00:05,336] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 22:00:05,337] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:00:05,349] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res6/Eplus-env-sub_run12
[2019-04-27 22:00:05,485] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:00:05,485] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2039
[2019-04-27 22:00:05,488] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.75, 31.5, 1.0, 2.0, 0.3309622589687031, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 423641.5986202284, 423641.5986202284, 119488.4303272787], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 311400.0000, 
sim time next is 312000.0000, 
raw observation next is [27.76666666666667, 31.33333333333333, 1.0, 2.0, 0.3309660285358814, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 423735.6798993684, 423735.6798993684, 119488.8750816609], 
processed observation next is [0.0, 0.6086956521739131, 0.5839506172839507, 0.3133333333333333, 1.0, 1.0, 0.2035309863522398, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.15133417139263156, 0.15133417139263156, 0.22978629823396327], 
reward next is 0.7702, 
noisyNet noise sample is [array([-0.28394946], dtype=float32), 0.05381544]. 
=============================================
[2019-04-27 22:00:05,498] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[78.97129 ]
 [78.922844]
 [78.88336 ]
 [78.83851 ]
 [78.716965]], R is [[79.01056671]
 [78.99067688]
 [78.97096252]
 [78.95139313]
 [78.93202209]].
[2019-04-27 22:00:05,587] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 22:00:05,587] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:00:05,598] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res4/Eplus-env-sub_run12
[2019-04-27 22:00:05,689] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 22:00:05,690] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:00:05,695] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res15/Eplus-env-sub_run12
[2019-04-27 22:00:05,724] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 22:00:05,725] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:00:05,732] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res17/Eplus-env-sub_run12
[2019-04-27 22:00:05,767] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 22:00:05,767] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:00:05,772] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res13/Eplus-env-sub_run12
[2019-04-27 22:00:05,937] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 22:00:05,937] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:00:05,941] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res9/Eplus-env-sub_run12
[2019-04-27 22:00:05,969] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 22:00:05,970] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:00:05,974] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res8/Eplus-env-sub_run12
[2019-04-27 22:00:06,002] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 22:00:06,003] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:00:06,016] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res5/Eplus-env-sub_run12
[2019-04-27 22:00:09,455] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1315946e-28 1.0000000e+00 3.6879838e-31 1.9935814e-32 8.7827809e-25], sum to 1.0000
[2019-04-27 22:00:09,463] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9512
[2019-04-27 22:00:09,472] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1367127.137399956 W.
[2019-04-27 22:00:09,478] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.0, 39.0, 1.0, 2.0, 0.9309563266903258, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.298775462877718, 6.9112, 121.9242379877352, 1367127.137399956, 1168656.796908697, 228818.0420825308], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 43200.0000, 
sim time next is 43800.0000, 
raw observation next is [28.18333333333334, 38.83333333333334, 1.0, 2.0, 0.3372115096163277, 1.0, 1.0, 0.3372115096163277, 1.0, 1.0, 0.5528781573313734, 6.9112, 6.9112, 121.94756008, 1239901.352888925, 1239901.352888925, 269250.8607499151], 
processed observation next is [1.0, 0.5217391304347826, 0.599382716049383, 0.3883333333333334, 1.0, 1.0, 0.21096608287658056, 1.0, 0.5, 0.21096608287658056, 1.0, 0.5, 0.44109769666421667, 0.0, 0.0, 0.8096049824067558, 0.4428219117460446, 0.4428219117460446, 0.5177901168267599], 
reward next is 0.4822, 
noisyNet noise sample is [array([0.40616497], dtype=float32), 0.052114855]. 
=============================================
[2019-04-27 22:00:13,302] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:00:13,309] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3096
[2019-04-27 22:00:13,316] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1428673.730488011 W.
[2019-04-27 22:00:13,320] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.25, 49.5, 1.0, 2.0, 0.6064295556526414, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9710444561905883, 6.9112, 6.9112, 121.9260426156618, 1428673.730488011, 1428673.730488011, 295434.2806920304], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 124200.0000, 
sim time next is 124800.0000, 
raw observation next is [29.66666666666666, 48.33333333333333, 1.0, 2.0, 0.6115983116600867, 1.0, 1.0, 0.6115983116600867, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1432755.126343973, 1432755.126343974, 273850.4420910047], 
processed observation next is [1.0, 0.43478260869565216, 0.6543209876543208, 0.4833333333333333, 1.0, 1.0, 0.5376170376905794, 1.0, 0.5, 0.5376170376905794, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.5116982594085617, 0.5116982594085621, 0.5266354655596244], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.08274014], dtype=float32), 1.369703]. 
=============================================
[2019-04-27 22:00:22,579] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:00:22,586] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5505
[2019-04-27 22:00:22,592] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.81666666666667, 50.16666666666667, 1.0, 2.0, 0.2614181102133714, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 337210.1083365334, 337210.1083365334, 95667.19530007259], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 267000.0000, 
sim time next is 267600.0000, 
raw observation next is [20.73333333333333, 50.33333333333334, 1.0, 2.0, 0.2595616225973525, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 334814.8533255542, 334814.8533255538, 95089.8762160176], 
processed observation next is [0.0, 0.08695652173913043, 0.3234567901234567, 0.5033333333333334, 1.0, 1.0, 0.1185257411873244, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.11957673333055509, 0.11957673333055494, 0.1828651465692646], 
reward next is 0.8171, 
noisyNet noise sample is [array([0.9313485], dtype=float32), -0.98402923]. 
=============================================
[2019-04-27 22:00:26,746] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:00:26,755] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8036
[2019-04-27 22:00:26,761] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.7, 48.0, 1.0, 2.0, 0.278167936187274, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 358821.2042698291, 358821.2042698291, 110458.5390408487], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 345600.0000, 
sim time next is 346200.0000, 
raw observation next is [22.55, 48.5, 1.0, 2.0, 0.2764055229370281, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 356547.2607807575, 356547.260780758, 109549.1004118703], 
processed observation next is [1.0, 0.0, 0.3907407407407408, 0.485, 1.0, 1.0, 0.13857800349646202, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1273383074216991, 0.12733830742169927, 0.21067134694590442], 
reward next is 0.7893, 
noisyNet noise sample is [array([-0.6488703], dtype=float32), -0.9527907]. 
=============================================
[2019-04-27 22:00:30,848] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:00:30,855] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1829
[2019-04-27 22:00:30,861] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 35.33333333333334, 1.0, 2.0, 0.3251118444090932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 413705.5180874131, 413705.5180874131, 118729.7062575924], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 420000.0000, 
sim time next is 420600.0000, 
raw observation next is [27.35, 35.66666666666666, 1.0, 2.0, 0.3247005917244273, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 413388.7260886453, 413388.7260886453, 118678.1556789142], 
processed observation next is [1.0, 0.8695652173913043, 0.5685185185185185, 0.3566666666666666, 1.0, 1.0, 0.19607213300527057, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.14763883074594475, 0.14763883074594475, 0.2282272224594504], 
reward next is 0.7718, 
noisyNet noise sample is [array([-0.01853492], dtype=float32), 2.4641433]. 
=============================================
[2019-04-27 22:00:33,306] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.9920886e-27 1.0000000e+00 3.2749487e-30 7.6409291e-31 7.6101514e-24], sum to 1.0000
[2019-04-27 22:00:33,314] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5874
[2019-04-27 22:00:33,322] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1492275.749594483 W.
[2019-04-27 22:00:33,329] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.16666666666666, 34.33333333333334, 1.0, 2.0, 0.633873482224171, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9557457626110882, 6.911200000000001, 6.9112, 121.9257237761804, 1492275.749594483, 1492275.749594482, 292423.1200848636], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 475800.0000, 
sim time next is 476400.0000, 
raw observation next is [30.33333333333334, 33.66666666666667, 1.0, 2.0, 0.324895828416469, 1.0, 1.0, 0.324895828416469, 1.0, 2.0, 0.5272699253830477, 6.911200000000001, 6.9112, 121.94756008, 1179425.997253682, 1179425.997253681, 264916.5367799607], 
processed observation next is [1.0, 0.5217391304347826, 0.6790123456790126, 0.3366666666666667, 1.0, 1.0, 0.1963045576386536, 1.0, 0.5, 0.1963045576386536, 1.0, 1.0, 0.4090874067288096, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.42122357044774356, 0.4212235704477432, 0.5094548784230013], 
reward next is 0.4905, 
noisyNet noise sample is [array([0.49957073], dtype=float32), 0.76777583]. 
=============================================
[2019-04-27 22:00:38,620] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-27 22:00:38,622] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 22:00:38,623] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 22:00:38,623] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:00:38,625] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:00:38,624] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 22:00:38,626] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 22:00:38,625] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 22:00:38,630] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:00:38,630] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:00:38,630] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:00:38,656] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run95
[2019-04-27 22:00:38,685] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run95
[2019-04-27 22:00:38,710] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run95
[2019-04-27 22:00:38,732] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run95
[2019-04-27 22:00:38,760] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run95
[2019-04-27 22:00:58,010] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00666872], dtype=float32), -0.008610436]
[2019-04-27 22:00:58,011] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [26.5, 43.0, 1.0, 2.0, 0.3258848189709423, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 411722.8430409644, 411722.8430409644, 118803.6329596003]
[2019-04-27 22:00:58,013] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:00:58,018] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6774334110147737
[2019-04-27 22:01:02,745] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00666872], dtype=float32), -0.008610436]
[2019-04-27 22:01:02,746] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [25.74329851, 47.01230815833333, 1.0, 2.0, 0.3396380582256643, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 428102.7561797771, 428102.7561797771, 120569.8664010348]
[2019-04-27 22:01:02,747] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:01:02,748] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5977384859376665
[2019-04-27 22:01:04,013] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00666872], dtype=float32), -0.008610436]
[2019-04-27 22:01:04,016] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [22.43119080166667, 74.30107782333334, 1.0, 2.0, 0.3886088220539092, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 482679.4377065807, 482679.4377065807, 127051.8014216372]
[2019-04-27 22:01:04,016] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:01:04,019] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3911243966515582
[2019-04-27 22:01:22,224] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00666872], dtype=float32), -0.008610436]
[2019-04-27 22:01:22,225] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [25.0, 98.0, 1.0, 2.0, 0.7724411847718551, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 880411.8552250122, 880411.8552250122, 190310.3286549824]
[2019-04-27 22:01:22,227] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 22:01:22,229] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [6.1530724e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.026307480841730402
[2019-04-27 22:01:58,333] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00666872], dtype=float32), -0.008610436]
[2019-04-27 22:01:58,336] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [24.38333333333333, 88.83333333333334, 1.0, 2.0, 0.584739645765837, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 678059.984439933, 678059.9844399334, 156031.0022799278]
[2019-04-27 22:01:58,337] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:01:58,339] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5946032982742167
[2019-04-27 22:02:00,163] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00666872], dtype=float32), -0.008610436]
[2019-04-27 22:02:00,164] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [26.08589714666667, 70.03124871333333, 1.0, 2.0, 0.5004853019313803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 595918.4791657628, 595918.4791657628, 142934.5822544039]
[2019-04-27 22:02:00,166] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 22:02:00,169] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2537427464649703
[2019-04-27 22:02:02,942] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00666872], dtype=float32), -0.008610436]
[2019-04-27 22:02:02,942] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [26.11666666666667, 70.5, 1.0, 2.0, 0.5278099426505795, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 624635.3122841743, 624635.3122841743, 147145.0353767719]
[2019-04-27 22:02:02,943] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 22:02:02,945] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9118466580785404
[2019-04-27 22:02:04,310] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00666872], dtype=float32), -0.008610436]
[2019-04-27 22:02:04,312] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [30.35, 66.5, 1.0, 2.0, 1.02, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 8.408096195028342, 6.9112, 121.9199974096278, 1929839.066062093, 1163332.429136676, 245587.7303501591]
[2019-04-27 22:02:04,314] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:02:04,316] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.663048683227998
[2019-04-27 22:02:04,317] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1929839.066062093 W.
[2019-04-27 22:02:25,592] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00666872], dtype=float32), -0.008610436]
[2019-04-27 22:02:25,593] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [21.94757287, 73.33645631333333, 1.0, 2.0, 0.6457458132514518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 804060.8972627269, 804060.8972627264, 168695.4127904465]
[2019-04-27 22:02:25,594] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 22:02:25,597] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [2.2085323e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.04597118486775709
[2019-04-27 22:02:25,950] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8099.2132 2445444095.4335 746.0000
[2019-04-27 22:02:26,275] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8923.4199 2120533710.1715 430.0000
[2019-04-27 22:02:26,462] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8770.2626 2170712315.3188 493.0000
[2019-04-27 22:02:26,508] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8700.1130 2195220806.1349 572.0000
[2019-04-27 22:02:26,514] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8582.8795 2248773970.1662 553.0000
[2019-04-27 22:02:27,530] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 2350000, evaluation results [2350000.0, 8099.213190057684, 2445444095.433475, 746.0, 8770.262620159681, 2170712315.3187604, 493.0, 8923.419851553375, 2120533710.171511, 430.0, 8582.879467926026, 2248773970.1662407, 553.0, 8700.112961499688, 2195220806.134913, 572.0]
[2019-04-27 22:02:29,560] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:02:29,567] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7763
[2019-04-27 22:02:29,570] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.65, 36.66666666666667, 1.0, 2.0, 0.3811126161718955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 473888.846009501, 473888.846009501, 126026.1690283345], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 589800.0000, 
sim time next is 590400.0000, 
raw observation next is [29.5, 37.0, 1.0, 2.0, 0.3799697881595387, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 472760.0555131718, 472760.0555131723, 125875.0243174322], 
processed observation next is [1.0, 0.8695652173913043, 0.6481481481481481, 0.37, 1.0, 1.0, 0.26186879542802227, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.16884287696898995, 0.1688428769689901, 0.24206735445660038], 
reward next is 0.7579, 
noisyNet noise sample is [array([-1.376122], dtype=float32), 0.0945725]. 
=============================================
[2019-04-27 22:02:31,859] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.7985874e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 22:02:31,866] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4208
[2019-04-27 22:02:31,875] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1413426.944026374 W.
[2019-04-27 22:02:31,880] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.13333333333334, 34.0, 1.0, 2.0, 0.5941744234733524, 1.0, 2.0, 0.5941744234733524, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 1413426.944026374, 1413426.944026374, 268670.8314482183], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 645600.0000, 
sim time next is 646200.0000, 
raw observation next is [32.35, 33.0, 1.0, 2.0, 0.5682262793564518, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9169782353813948, 6.911199999999999, 6.9112, 121.9260426156618, 1362794.976606973, 1362794.976606973, 279929.3708515227], 
processed observation next is [1.0, 0.4782608695652174, 0.7537037037037038, 0.33, 1.0, 1.0, 0.48598366590053776, 0.0, 0.5, -0.1904761904761905, 1.0, 0.5, 0.8962227942267433, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.48671249164534747, 0.48671249164534747, 0.5383257131760052], 
reward next is 0.4617, 
noisyNet noise sample is [array([1.0008649], dtype=float32), -0.075021155]. 
=============================================
[2019-04-27 22:02:40,577] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:02:40,580] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2655
[2019-04-27 22:02:40,584] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.16666666666667, 79.0, 1.0, 2.0, 0.4085984947186581, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 504372.9057820008, 504372.9057820003, 129792.8448064123], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1720200.0000, 
sim time next is 1720800.0000, 
raw observation next is [22.1, 79.0, 1.0, 2.0, 0.4066329579763808, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 502409.6434119284, 502409.6434119279, 129524.5902848779], 
processed observation next is [1.0, 0.9565217391304348, 0.3740740740740741, 0.79, 1.0, 1.0, 0.29361066425759624, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.17943201550426013, 0.17943201550425997, 0.2490857505478421], 
reward next is 0.7509, 
noisyNet noise sample is [array([-0.05027324], dtype=float32), 0.54813635]. 
=============================================
[2019-04-27 22:02:40,865] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:02:40,877] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2752
[2019-04-27 22:02:40,881] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 36.0, 1.0, 2.0, 0.4504749478692116, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 546567.4251511313, 546567.4251511313, 135644.6369051602], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 835200.0000, 
sim time next is 835800.0000, 
raw observation next is [31.9, 36.16666666666666, 1.0, 2.0, 0.4537412715369987, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 550720.5457848082, 550720.5457848082, 136138.4788622179], 
processed observation next is [0.0, 0.6956521739130435, 0.7370370370370369, 0.3616666666666666, 1.0, 1.0, 0.34969198992499845, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1966859092088601, 0.1966859092088601, 0.2618047670427267], 
reward next is 0.7382, 
noisyNet noise sample is [array([-1.6636525], dtype=float32), -1.4343388]. 
=============================================
[2019-04-27 22:02:43,548] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:02:43,554] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7388
[2019-04-27 22:02:43,558] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.63333333333334, 50.66666666666667, 1.0, 2.0, 0.4328400703884011, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 529888.6139624728, 529888.6139624728, 133180.5529042904], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 912000.0000, 
sim time next is 912600.0000, 
raw observation next is [27.85, 50.0, 1.0, 2.0, 0.4346263846240873, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 531559.5881168756, 531559.5881168752, 133427.913070681], 
processed observation next is [0.0, 0.5652173913043478, 0.5870370370370371, 0.5, 1.0, 1.0, 0.3269361721715325, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1898427100417413, 0.18984271004174114, 0.2565921405205404], 
reward next is 0.7434, 
noisyNet noise sample is [array([-1.3790629], dtype=float32), -1.1872472]. 
=============================================
[2019-04-27 22:02:45,523] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:02:45,534] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6800
[2019-04-27 22:02:45,539] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 45.0, 1.0, 2.0, 0.3802350873338945, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 474163.3835065859, 474163.3835065859, 125932.7723155608], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 928800.0000, 
sim time next is 929400.0000, 
raw observation next is [27.13333333333333, 45.66666666666666, 1.0, 2.0, 0.381363132398239, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 475704.5825204935, 475704.5825204935, 126090.630988215], 
processed observation next is [0.0, 0.782608695652174, 0.5604938271604937, 0.45666666666666655, 1.0, 1.0, 0.26352753856933214, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1698944937573191, 0.1698944937573191, 0.2424819826696442], 
reward next is 0.7575, 
noisyNet noise sample is [array([-0.03349755], dtype=float32), -0.052357957]. 
=============================================
[2019-04-27 22:02:45,795] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:02:45,796] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3999
[2019-04-27 22:02:45,802] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.2, 56.66666666666667, 1.0, 2.0, 0.3722443543280006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 480212.5606241075, 480212.560624107, 124223.4070912574], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 958800.0000, 
sim time next is 959400.0000, 
raw observation next is [21.15, 57.0, 1.0, 2.0, 0.3105803183131536, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 400642.3004280113, 400642.3004280113, 115576.0663321558], 
processed observation next is [1.0, 0.08695652173913043, 0.33888888888888885, 0.57, 1.0, 1.0, 0.17926228370613523, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1430865358671469, 0.1430865358671469, 0.22226166602337652], 
reward next is 0.7777, 
noisyNet noise sample is [array([0.7343678], dtype=float32), 2.2354414]. 
=============================================
[2019-04-27 22:02:47,481] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:02:47,490] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3228
[2019-04-27 22:02:47,498] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 54.0, 1.0, 2.0, 0.270883156653474, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 348855.4065712512, 348855.4065712512, 112045.2427248712], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 954000.0000, 
sim time next is 954600.0000, 
raw observation next is [21.88333333333334, 54.33333333333333, 1.0, 2.0, 0.2692982859587248, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 346978.9297327605, 346978.9297327605, 111856.4272629489], 
processed observation next is [1.0, 0.043478260869565216, 0.36604938271604964, 0.5433333333333333, 1.0, 1.0, 0.13011700709371998, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.12392104633312874, 0.12392104633312874, 0.21510851396720942], 
reward next is 0.7849, 
noisyNet noise sample is [array([-1.2867428], dtype=float32), -0.33654854]. 
=============================================
[2019-04-27 22:02:54,821] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:02:54,831] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5914
[2019-04-27 22:02:54,835] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.3, 64.33333333333333, 1.0, 2.0, 0.3372723778732112, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 426681.3902147502, 426681.3902147507, 120279.9532595161], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1107600.0000, 
sim time next is 1108200.0000, 
raw observation next is [22.05, 65.66666666666667, 1.0, 2.0, 0.334087102887738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 422823.4142250275, 422823.414225028, 119868.0633158983], 
processed observation next is [1.0, 0.8260869565217391, 0.37222222222222223, 0.6566666666666667, 1.0, 1.0, 0.20724655105683099, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1510083622232241, 0.15100836222322428, 0.2305155063767275], 
reward next is 0.7695, 
noisyNet noise sample is [array([-1.157501], dtype=float32), -0.25614965]. 
=============================================
[2019-04-27 22:02:56,362] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:02:56,375] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1777
[2019-04-27 22:02:56,381] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.05, 60.33333333333334, 1.0, 2.0, 0.3334166682412335, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 421406.0977209226, 421406.0977209221, 119775.302584283], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1105800.0000, 
sim time next is 1106400.0000, 
raw observation next is [22.8, 61.66666666666667, 1.0, 2.0, 0.3333484464454003, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 421435.8772516353, 421435.8772516348, 119767.7283526718], 
processed observation next is [1.0, 0.8260869565217391, 0.4, 0.6166666666666667, 1.0, 1.0, 0.20636719814928606, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.15051281330415547, 0.15051281330415528, 0.23032255452436884], 
reward next is 0.7697, 
noisyNet noise sample is [array([-0.05574524], dtype=float32), -1.467786]. 
=============================================
[2019-04-27 22:02:58,165] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:02:58,172] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9321
[2019-04-27 22:02:58,181] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.76666666666667, 69.0, 1.0, 2.0, 0.6780879017283381, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 855400.0516375661, 855400.0516375656, 174958.7855956457], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1178400.0000, 
sim time next is 1179000.0000, 
raw observation next is [21.7, 69.5, 1.0, 2.0, 0.6467384935040523, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 815809.688081679, 815809.688081679, 169062.7219499009], 
processed observation next is [1.0, 0.6521739130434783, 0.3592592592592592, 0.695, 1.0, 1.0, 0.5794505875048241, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.29136060288631394, 0.29136060288631394, 0.3251206191344248], 
reward next is 0.6749, 
noisyNet noise sample is [array([1.1054844], dtype=float32), 0.10972514]. 
=============================================
[2019-04-27 22:02:58,199] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[87.05002 ]
 [86.929245]
 [86.65671 ]
 [86.23718 ]
 [86.05299 ]], R is [[87.10710907]
 [86.89958191]
 [86.70083618]
 [86.47666931]
 [86.25868988]].
[2019-04-27 22:02:59,532] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:02:59,539] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8102
[2019-04-27 22:02:59,543] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.35, 67.16666666666667, 1.0, 2.0, 0.4811794776246949, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 617188.4856090974, 617188.4856090974, 140892.7045175739], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1155000.0000, 
sim time next is 1155600.0000, 
raw observation next is [20.4, 67.0, 1.0, 2.0, 0.4918452642739484, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 630711.6436112695, 630711.6436112695, 142565.9837819485], 
processed observation next is [1.0, 0.391304347826087, 0.31111111111111106, 0.67, 1.0, 1.0, 0.39505388604041486, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.22525415843259627, 0.22525415843259627, 0.2741653534268241], 
reward next is 0.7258, 
noisyNet noise sample is [array([-0.30385792], dtype=float32), -0.4732066]. 
=============================================
[2019-04-27 22:03:03,649] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:03:03,657] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1801
[2019-04-27 22:03:03,660] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.86666666666667, 66.0, 1.0, 2.0, 0.5615480891982695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 653210.5768685131, 653210.5768685131, 152218.2177885972], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1966200.0000, 
sim time next is 1966800.0000, 
raw observation next is [27.73333333333333, 67.0, 1.0, 2.0, 0.5692182824213593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 661537.5250217651, 661537.5250217647, 153474.1724292014], 
processed observation next is [1.0, 0.782608695652174, 0.5827160493827159, 0.67, 1.0, 1.0, 0.4871646219301896, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.23626340179348757, 0.2362634017934874, 0.29514263928692575], 
reward next is 0.7049, 
noisyNet noise sample is [array([-0.0822752], dtype=float32), -1.4519757]. 
=============================================
[2019-04-27 22:03:07,751] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:03:07,763] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4514
[2019-04-27 22:03:07,770] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.43333333333333, 86.33333333333334, 1.0, 2.0, 0.4261812072597121, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 538146.7482948789, 538146.7482948793, 132544.0164826782], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1303800.0000, 
sim time next is 1304400.0000, 
raw observation next is [19.36666666666667, 86.66666666666667, 1.0, 2.0, 0.3705950547582842, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 468066.5162850841, 468066.5162850841, 124707.1263505522], 
processed observation next is [1.0, 0.08695652173913043, 0.27283950617283964, 0.8666666666666667, 1.0, 1.0, 0.2507083985217669, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1671666129589586, 0.1671666129589586, 0.23982139682798498], 
reward next is 0.7602, 
noisyNet noise sample is [array([-0.49885973], dtype=float32), -1.7283238]. 
=============================================
[2019-04-27 22:03:14,659] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:03:14,668] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4807
[2019-04-27 22:03:14,672] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.7, 25.0, 1.0, 2.0, 0.3829381785031569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 479411.8104910203, 479411.8104910203, 126339.2833141712], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1447200.0000, 
sim time next is 1447800.0000, 
raw observation next is [32.55, 25.33333333333334, 1.0, 2.0, 0.3871524511423226, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 484795.6218310964, 484795.6218310964, 126924.6857038722], 
processed observation next is [0.0, 0.782608695652174, 0.761111111111111, 0.2533333333333334, 1.0, 1.0, 0.2704195846932412, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.17314129351110585, 0.17314129351110585, 0.24408593404590806], 
reward next is 0.7559, 
noisyNet noise sample is [array([0.54276913], dtype=float32), 0.09747909]. 
=============================================
[2019-04-27 22:03:14,717] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:03:14,724] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7729
[2019-04-27 22:03:14,730] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.75, 21.5, 1.0, 2.0, 0.3881813738735173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 487186.3921748362, 487186.3921748358, 127085.5423480675], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1432200.0000, 
sim time next is 1432800.0000, 
raw observation next is [33.9, 21.0, 1.0, 2.0, 0.3894955942341312, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 489061.5905530727, 489061.5905530727, 127271.9646288523], 
processed observation next is [0.0, 0.6086956521739131, 0.811111111111111, 0.21, 1.0, 1.0, 0.27320904075491814, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.17466485376895452, 0.17466485376895452, 0.24475377813240828], 
reward next is 0.7552, 
noisyNet noise sample is [array([0.43801808], dtype=float32), -0.73258615]. 
=============================================
[2019-04-27 22:03:14,945] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:03:14,956] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2810
[2019-04-27 22:03:14,963] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.1, 89.0, 1.0, 2.0, 0.6137767161400187, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 716503.1697562041, 716503.1697562041, 161268.3274942555], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2170800.0000, 
sim time next is 2171400.0000, 
raw observation next is [24.06666666666667, 89.16666666666667, 1.0, 2.0, 0.7163315690006807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 836468.6588087745, 836468.6588087745, 180236.2117938403], 
processed observation next is [1.0, 0.13043478260869565, 0.4469135802469137, 0.8916666666666667, 1.0, 1.0, 0.6622994869055723, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2987388067174195, 0.2987388067174195, 0.34660809960353905], 
reward next is 0.6534, 
noisyNet noise sample is [array([0.7436492], dtype=float32), 1.573049]. 
=============================================
[2019-04-27 22:03:15,351] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:03:15,361] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4173
[2019-04-27 22:03:15,367] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.6, 33.0, 1.0, 2.0, 0.3552698998357868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 446719.8110126802, 446719.8110126797, 122617.2055196914], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1459200.0000, 
sim time next is 1459800.0000, 
raw observation next is [29.45, 33.5, 1.0, 2.0, 0.3533547000383029, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 444335.6878607122, 444335.6878607118, 122362.6687656888], 
processed observation next is [0.0, 0.9130434782608695, 0.6462962962962963, 0.335, 1.0, 1.0, 0.23018416671226535, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1586913170931115, 0.15869131709311138, 0.23531282454940153], 
reward next is 0.7647, 
noisyNet noise sample is [array([-0.46308783], dtype=float32), -1.6820561]. 
=============================================
[2019-04-27 22:03:15,739] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:03:15,748] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0899
[2019-04-27 22:03:15,754] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.25, 69.5, 1.0, 2.0, 0.361570629305992, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 453988.197053726, 453988.197053726, 123450.7006334623], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1488600.0000, 
sim time next is 1489200.0000, 
raw observation next is [22.56666666666667, 68.0, 1.0, 2.0, 0.3640569074943374, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 456639.400630095, 456639.4006300955, 123777.9236876952], 
processed observation next is [0.0, 0.21739130434782608, 0.39135802469135816, 0.68, 1.0, 1.0, 0.24292488987421118, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.16308550022503393, 0.1630855002250341, 0.2380344686301831], 
reward next is 0.7620, 
noisyNet noise sample is [array([0.6175448], dtype=float32), -1.1320647]. 
=============================================
[2019-04-27 22:03:15,984] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:03:15,991] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1320
[2019-04-27 22:03:15,996] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.73333333333333, 47.0, 1.0, 2.0, 0.3442633634765695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 434446.66292571, 434446.6629257095, 121182.0482586733], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1470000.0000, 
sim time next is 1470600.0000, 
raw observation next is [25.45, 48.0, 1.0, 2.0, 0.3434382786192593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 433708.5133276213, 433708.5133276218, 121077.3027865631], 
processed observation next is [0.0, 0.0, 0.4981481481481481, 0.48, 1.0, 1.0, 0.21837890311816582, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1548958976170076, 0.15489589761700778, 0.23284096689723674], 
reward next is 0.7672, 
noisyNet noise sample is [array([1.6653645], dtype=float32), 0.24104023]. 
=============================================
[2019-04-27 22:03:18,139] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-27 22:03:18,141] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 22:03:18,141] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:03:18,142] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 22:03:18,143] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:03:18,143] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 22:03:18,144] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:03:18,144] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 22:03:18,144] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 22:03:18,146] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:03:18,147] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:03:18,174] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run96
[2019-04-27 22:03:18,199] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run96
[2019-04-27 22:03:18,222] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run96
[2019-04-27 22:03:18,248] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run96
[2019-04-27 22:03:18,271] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run96
[2019-04-27 22:03:28,338] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00596161], dtype=float32), -0.010148731]
[2019-04-27 22:03:28,339] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [33.68333333333334, 18.66666666666667, 1.0, 2.0, 0.5687232742787317, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 720242.2956310498, 720242.2956310493, 155184.3731915202]
[2019-04-27 22:03:28,340] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:03:28,344] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7541372374075165
[2019-04-27 22:03:36,478] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00596161], dtype=float32), -0.010148731]
[2019-04-27 22:03:36,479] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [32.53333333333334, 26.83333333333334, 1.0, 2.0, 0.5256890811448661, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 654689.4552928775, 654689.4552928775, 147802.6910793094]
[2019-04-27 22:03:36,481] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:03:36,482] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3103761427848002
[2019-04-27 22:03:46,379] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00596161], dtype=float32), -0.010148731]
[2019-04-27 22:03:46,379] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [18.93333333333333, 94.33333333333334, 1.0, 2.0, 0.3387718694604485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 425698.7475823999, 425698.7475823999, 120438.9960461192]
[2019-04-27 22:03:46,380] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:03:46,383] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9459371050916344
[2019-04-27 22:04:29,931] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00596161], dtype=float32), -0.010148731]
[2019-04-27 22:04:29,934] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [24.15693364333333, 83.26866123833334, 1.0, 2.0, 0.5477409990920131, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 646118.8998884856, 646118.8998884851, 150315.6008685465]
[2019-04-27 22:04:29,935] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 22:04:29,937] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.10199268571751408
[2019-04-27 22:05:03,318] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8770.2626 2170712315.3188 493.0000
[2019-04-27 22:05:03,899] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8700.8700 2195155380.8105 572.0000
[2019-04-27 22:05:04,069] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8923.4199 2120533710.1715 430.0000
[2019-04-27 22:05:04,089] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8582.8795 2248773970.1662 553.0000
[2019-04-27 22:05:04,127] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8099.2132 2445444095.4335 746.0000
[2019-04-27 22:05:05,146] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 2375000, evaluation results [2375000.0, 8099.213190057684, 2445444095.433475, 746.0, 8770.262620159681, 2170712315.3187604, 493.0, 8923.419851553375, 2120533710.171511, 430.0, 8582.879467926026, 2248773970.1662407, 553.0, 8700.870031992405, 2195155380.8105125, 572.0]
[2019-04-27 22:05:05,272] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:05:05,277] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2522
[2019-04-27 22:05:05,280] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.1, 51.0, 1.0, 2.0, 0.4130585140822543, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 508856.925388223, 508856.925388223, 130404.2456297154], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1497600.0000, 
sim time next is 1498200.0000, 
raw observation next is [27.45, 49.66666666666666, 1.0, 2.0, 0.4155880335262342, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 511525.8802364311, 511525.8802364311, 130755.3122255705], 
processed observation next is [0.0, 0.34782608695652173, 0.5722222222222222, 0.4966666666666666, 1.0, 1.0, 0.3042714684836121, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.18268781437015397, 0.18268781437015397, 0.2514525235107125], 
reward next is 0.7485, 
noisyNet noise sample is [array([0.4380228], dtype=float32), -0.62495244]. 
=============================================
[2019-04-27 22:05:05,478] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:05:05,484] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0174
[2019-04-27 22:05:05,489] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.86666666666667, 94.83333333333334, 1.0, 2.0, 0.5422302701012204, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 639093.1528455222, 639093.1528455217, 149388.8782234844], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2229000.0000, 
sim time next is 2229600.0000, 
raw observation next is [22.83333333333334, 94.66666666666667, 1.0, 2.0, 0.539683005634972, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 636764.217115572, 636764.217115572, 148999.2370141607], 
processed observation next is [1.0, 0.8260869565217391, 0.4012345679012348, 0.9466666666666668, 1.0, 1.0, 0.45200357813687136, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.22741579182698998, 0.22741579182698998, 0.28653699425800133], 
reward next is 0.7135, 
noisyNet noise sample is [array([-1.6314836], dtype=float32), 0.80441034]. 
=============================================
[2019-04-27 22:05:07,150] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:05:07,158] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9339
[2019-04-27 22:05:07,161] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.58333333333334, 69.16666666666667, 1.0, 2.0, 0.4241532756847238, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 523594.0053470071, 523594.0053470066, 132028.2361733138], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1548600.0000, 
sim time next is 1549200.0000, 
raw observation next is [23.56666666666667, 68.33333333333334, 1.0, 2.0, 0.421373665006826, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 521178.0849802533, 521178.0849802533, 131649.7942888013], 
processed observation next is [0.0, 0.9565217391304348, 0.4283950617283952, 0.6833333333333335, 1.0, 1.0, 0.3111591250081262, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.18613503035009046, 0.18613503035009046, 0.25317268132461784], 
reward next is 0.7468, 
noisyNet noise sample is [array([-0.13000087], dtype=float32), -0.7704354]. 
=============================================
[2019-04-27 22:05:08,710] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:05:08,718] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8244
[2019-04-27 22:05:08,723] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 67.5, 1.0, 2.0, 0.3879332304354081, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 484287.453889607, 484287.453889607, 127006.7274033654], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1554600.0000, 
sim time next is 1555200.0000, 
raw observation next is [22.9, 68.0, 1.0, 2.0, 0.3869189911325559, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 483148.8994580892, 483148.8994580887, 126868.282827817], 
processed observation next is [1.0, 0.0, 0.4037037037037037, 0.68, 1.0, 1.0, 0.2701416561101856, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.172553178377889, 0.1725531783778888, 0.24397746697657116], 
reward next is 0.7560, 
noisyNet noise sample is [array([-1.640248], dtype=float32), -0.2479822]. 
=============================================
[2019-04-27 22:05:14,088] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:05:14,097] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2443
[2019-04-27 22:05:14,100] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.3, 88.33333333333334, 1.0, 2.0, 0.3198285374633505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 407892.3619867942, 407892.3619867942, 118059.9253192836], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1665600.0000, 
sim time next is 1666200.0000, 
raw observation next is [18.35, 88.16666666666667, 1.0, 2.0, 0.3255112126296354, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 415000.7290615857, 415000.7290615857, 118784.5599321794], 
processed observation next is [1.0, 0.2608695652173913, 0.23518518518518525, 0.8816666666666667, 1.0, 1.0, 0.1970371578924231, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.14821454609342347, 0.14821454609342347, 0.22843184602342193], 
reward next is 0.7716, 
noisyNet noise sample is [array([-0.6276113], dtype=float32), -0.62526107]. 
=============================================
[2019-04-27 22:05:19,656] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:05:19,665] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4773
[2019-04-27 22:05:19,672] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.75, 83.66666666666667, 1.0, 2.0, 0.3131546110102651, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 399414.519066222, 399414.519066222, 117215.4724452402], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1799400.0000, 
sim time next is 1800000.0000, 
raw observation next is [18.3, 85.0, 1.0, 2.0, 0.303496268257782, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 388187.6273831071, 388187.6273831076, 116009.7810975928], 
processed observation next is [1.0, 0.8695652173913043, 0.23333333333333336, 0.85, 1.0, 1.0, 0.17082889078307378, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.13863843835110967, 0.13863843835110987, 0.22309573287998616], 
reward next is 0.7769, 
noisyNet noise sample is [array([-0.26668948], dtype=float32), -0.53562486]. 
=============================================
[2019-04-27 22:05:19,692] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[74.87366 ]
 [74.984116]
 [75.27131 ]
 [75.44111 ]
 [75.50861 ]], R is [[75.08551025]
 [75.10923767]
 [75.13043213]
 [75.14916992]
 [75.16547394]].
[2019-04-27 22:05:25,906] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:05:25,915] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4400
[2019-04-27 22:05:25,921] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.86666666666667, 93.83333333333334, 1.0, 2.0, 0.4792956759790421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 577379.6385442168, 577379.6385442168, 139881.5812359693], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2623800.0000, 
sim time next is 2624400.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.4869563442388586, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 585031.455666031, 585031.4556660305, 141012.1254792261], 
processed observation next is [0.0, 0.391304347826087, 0.37037037037037035, 0.94, 1.0, 1.0, 0.3892337431414983, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.20893980559501107, 0.2089398055950109, 0.2711771643831271], 
reward next is 0.7288, 
noisyNet noise sample is [array([-0.5179283], dtype=float32), 1.7842889]. 
=============================================
[2019-04-27 22:05:27,880] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.496214e-33 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-27 22:05:27,891] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0559
[2019-04-27 22:05:27,895] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.01666666666667, 86.83333333333333, 1.0, 2.0, 0.8464599239068588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1030797.334162651, 1030797.334162651, 208692.7078840011], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1937400.0000, 
sim time next is 1938000.0000, 
raw observation next is [22.13333333333333, 86.66666666666667, 1.0, 2.0, 0.7729893771877698, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 939902.2312866347, 939902.2312866347, 193006.4634842947], 
processed observation next is [1.0, 0.43478260869565216, 0.3753086419753085, 0.8666666666666667, 1.0, 1.0, 0.7297492585568688, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.33567936831665524, 0.33567936831665524, 0.371166275931336], 
reward next is 0.6288, 
noisyNet noise sample is [array([1.8744531], dtype=float32), 0.908794]. 
=============================================
[2019-04-27 22:05:27,919] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[58.615734]
 [58.781788]
 [58.675415]
 [58.752083]
 [58.961628]], R is [[58.79650116]
 [58.8072052 ]
 [58.83785629]
 [58.87227631]
 [58.91664124]].
[2019-04-27 22:05:30,746] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:05:30,758] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1434
[2019-04-27 22:05:30,764] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.58333333333334, 90.16666666666667, 1.0, 2.0, 0.3680160263245321, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 461361.3256474615, 461361.3256474615, 124308.8880099882], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1984200.0000, 
sim time next is 1984800.0000, 
raw observation next is [19.56666666666667, 90.33333333333334, 1.0, 2.0, 0.3656511795750749, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 458411.7032661043, 458411.7032661043, 123989.2786729146], 
processed observation next is [1.0, 1.0, 0.28024691358024706, 0.9033333333333334, 1.0, 1.0, 0.24482283282747017, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1637184654521801, 0.1637184654521801, 0.2384409205248358], 
reward next is 0.7616, 
noisyNet noise sample is [array([0.37557626], dtype=float32), -0.84213454]. 
=============================================
[2019-04-27 22:05:42,859] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:05:42,865] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2092
[2019-04-27 22:05:42,869] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.7, 97.33333333333333, 1.0, 2.0, 0.544921666062068, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 640818.9780656846, 640818.9780656846, 149771.3417012416], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2241600.0000, 
sim time next is 2242200.0000, 
raw observation next is [22.7, 97.66666666666667, 1.0, 2.0, 0.5480842333623055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 644013.7464106596, 644013.7464106596, 150269.9259822002], 
processed observation next is [1.0, 0.9565217391304348, 0.39629629629629626, 0.9766666666666667, 1.0, 1.0, 0.46200503971703033, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.23000490943237845, 0.23000490943237845, 0.2889806268888465], 
reward next is 0.7110, 
noisyNet noise sample is [array([-1.4887626], dtype=float32), -0.37032893]. 
=============================================
[2019-04-27 22:05:53,165] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:05:53,172] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3039
[2019-04-27 22:05:53,175] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 79.83333333333334, 1.0, 2.0, 0.6529110261608805, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 744107.8184596474, 744107.8184596469, 167419.5788945851], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2754600.0000, 
sim time next is 2755200.0000, 
raw observation next is [26.66666666666667, 80.66666666666667, 1.0, 2.0, 0.6567387043024492, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 748472.2657834747, 748472.2657834747, 168113.9413824977], 
processed observation next is [0.0, 0.9130434782608695, 0.5432098765432101, 0.8066666666666668, 1.0, 1.0, 0.5913556003600585, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2673115234940981, 0.2673115234940981, 0.3232960411201879], 
reward next is 0.6767, 
noisyNet noise sample is [array([1.4984239], dtype=float32), -1.0418408]. 
=============================================
[2019-04-27 22:05:55,656] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:05:55,667] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6873
[2019-04-27 22:05:55,674] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1326316.825538049 W.
[2019-04-27 22:05:55,677] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.18333333333333, 23.16666666666667, 1.0, 2.0, 0.9368721660568445, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.240277193136862, 6.9112, 121.92463189464, 1326316.825538049, 1157801.839857889, 229781.1338226268], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2465400.0000, 
sim time next is 2466000.0000, 
raw observation next is [34.3, 23.0, 1.0, 2.0, 0.510808986430538, 1.0, 1.0, 0.510808986430538, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.925844451163, 1246593.666140397, 1246593.666140398, 242043.191012376], 
processed observation next is [1.0, 0.5652173913043478, 0.8259259259259258, 0.23, 1.0, 1.0, 0.4176297457506404, 1.0, 0.5, 0.4176297457506404, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094608132139405, 0.44521202362157036, 0.4452120236215707, 0.4654676750238], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.242299], dtype=float32), 1.0628234]. 
=============================================
[2019-04-27 22:05:55,682] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-27 22:05:55,684] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 22:05:55,685] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 22:05:55,687] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 22:05:55,687] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 22:05:55,689] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 22:05:55,689] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:05:55,690] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:05:55,690] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:05:55,690] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:05:55,687] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:05:55,720] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run97
[2019-04-27 22:05:55,747] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run97
[2019-04-27 22:05:55,775] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run97
[2019-04-27 22:05:55,795] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run97
[2019-04-27 22:05:55,818] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run97
[2019-04-27 22:06:46,338] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00653954], dtype=float32), -0.0043878187]
[2019-04-27 22:06:46,339] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [26.69273489, 68.48998190666667, 1.0, 2.0, 0.5349478921907131, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 630313.3844500013, 630313.3844500013, 148191.7659493382]
[2019-04-27 22:06:46,340] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:06:46,344] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.21825100676315057
[2019-04-27 22:06:47,718] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00653954], dtype=float32), -0.0043878187]
[2019-04-27 22:06:47,721] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [22.957037545, 109.8799354, 1.0, 2.0, 0.6396318678118362, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 741249.4075461413, 741249.4075461413, 165625.7125803704]
[2019-04-27 22:06:47,723] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:06:47,725] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6102453300250377
[2019-04-27 22:07:31,506] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00653954], dtype=float32), -0.0043878187]
[2019-04-27 22:07:31,507] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [22.5, 82.0, 1.0, 2.0, 0.4382819179523227, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 535399.322390278, 535399.322390278, 133947.2526159368]
[2019-04-27 22:07:31,511] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 22:07:31,514] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.09196193287583454
[2019-04-27 22:07:43,770] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8099.2132 2445444095.4335 746.0000
[2019-04-27 22:07:44,099] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8923.4199 2120533710.1715 430.0000
[2019-04-27 22:07:44,113] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8770.2626 2170712315.3188 493.0000
[2019-04-27 22:07:44,168] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8582.8795 2248773970.1662 553.0000
[2019-04-27 22:07:44,171] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8700.8700 2195155380.8105 572.0000
[2019-04-27 22:07:45,189] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 2400000, evaluation results [2400000.0, 8099.213190057684, 2445444095.433475, 746.0, 8770.262620159681, 2170712315.3187604, 493.0, 8923.419851553375, 2120533710.171511, 430.0, 8582.879467926026, 2248773970.1662407, 553.0, 8700.870031992405, 2195155380.8105125, 572.0]
[2019-04-27 22:07:45,198] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[60.47162 ]
 [60.34381 ]
 [59.71443 ]
 [59.68615 ]
 [59.653248]], R is [[60.35840607]
 [59.75482178]
 [59.68747711]
 [59.55860138]
 [59.41604996]].
[2019-04-27 22:07:45,264] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0467274e-32 1.0000000e+00 2.3271821e-37 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 22:07:45,274] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7614
[2019-04-27 22:07:45,282] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.55, 36.16666666666666, 1.0, 2.0, 0.3542074440278222, 1.0, 2.0, 0.3542074440278222, 1.0, 2.0, 0.5743198546941992, 6.9112, 6.9112, 121.94756008, 1284201.977660462, 1284201.977660462, 276855.2228869521], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2451000.0000, 
sim time next is 2451600.0000, 
raw observation next is [29.8, 35.0, 1.0, 2.0, 0.9983245003929718, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.658414181251398, 6.9112, 121.9229714691213, 1618018.594360542, 1235387.786491987, 244671.1611969678], 
processed observation next is [1.0, 0.391304347826087, 0.6592592592592593, 0.35, 1.0, 1.0, 0.9980053576106808, 0.0, 0.5, -0.1904761904761905, 0.0, 0.5, -0.25, 0.07472141812513984, 0.0, 0.8094417396008743, 0.5778637837001935, 0.44120992374713824, 0.4705214638403227], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.59015185], dtype=float32), 1.3007568]. 
=============================================
[2019-04-27 22:07:50,536] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:07:50,547] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8937
[2019-04-27 22:07:50,552] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1815087.266293281 W.
[2019-04-27 22:07:50,554] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.41666666666666, 72.0, 1.0, 2.0, 0.9648260011776681, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.911200000000001, 6.9112, 121.9260426156618, 1815087.266293281, 1815087.266293281, 371331.5014483724], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3406200.0000, 
sim time next is 3406800.0000, 
raw observation next is [28.63333333333333, 71.0, 1.0, 2.0, 0.8239169140102159, 1.0, 1.0, 0.8239169140102159, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1879337.53743217, 1879337.53743217, 353736.1904568396], 
processed observation next is [1.0, 0.43478260869565216, 0.6160493827160493, 0.71, 1.0, 1.0, 0.7903772785835904, 1.0, 0.5, 0.7903772785835904, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288201359, 0.6711919776543465, 0.6711919776543465, 0.6802619047246916], 
reward next is 0.3197, 
noisyNet noise sample is [array([0.64491916], dtype=float32), 0.24681227]. 
=============================================
[2019-04-27 22:07:51,130] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:07:51,136] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4237
[2019-04-27 22:07:51,144] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1477281.715048664 W.
[2019-04-27 22:07:51,151] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 30.0, 1.0, 2.0, 0.6101415858743415, 1.0, 1.0, 0.6101415858743415, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 1477281.715048664, 1477281.715048663, 275200.0781023161], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2545200.0000, 
sim time next is 2545800.0000, 
raw observation next is [32.1, 30.0, 1.0, 2.0, 0.4186776452155101, 1.0, 2.0, 0.4186776452155101, 1.0, 1.0, 0.6747294779006442, 6.9112, 6.9112, 121.94756008, 1502495.27928746, 1502495.27928746, 305034.5054838987], 
processed observation next is [1.0, 0.4782608695652174, 0.7444444444444445, 0.3, 1.0, 1.0, 0.307949577637512, 1.0, 1.0, 0.307949577637512, 1.0, 0.5, 0.5934118473758052, 0.0, 0.0, 0.8096049824067558, 0.5366054568883786, 0.5366054568883786, 0.5866048182382667], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2078685], dtype=float32), -0.46974844]. 
=============================================
[2019-04-27 22:07:52,026] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:07:52,036] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0119
[2019-04-27 22:07:52,039] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.9, 92.0, 1.0, 2.0, 0.5308996786162566, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 630180.3371304561, 630180.3371304561, 147719.1760136274], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3281400.0000, 
sim time next is 3282000.0000, 
raw observation next is [22.93333333333333, 92.66666666666667, 1.0, 2.0, 0.5361189403161418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 634960.381396709, 634960.381396709, 148513.4950350495], 
processed observation next is [0.0, 1.0, 0.40493827160493817, 0.9266666666666667, 1.0, 1.0, 0.4477606432335022, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.22677156478453894, 0.22677156478453894, 0.2856028750674029], 
reward next is 0.7144, 
noisyNet noise sample is [array([1.0110341], dtype=float32), 1.0988576]. 
=============================================
[2019-04-27 22:07:52,050] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[73.89505]
 [73.89398]
 [73.86405]
 [73.79101]
 [73.54971]], R is [[73.85171509]
 [73.82912445]
 [73.80808258]
 [73.78822327]
 [73.76815033]].
[2019-04-27 22:07:55,925] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:07:55,935] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8026
[2019-04-27 22:07:55,941] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.25, 84.0, 1.0, 2.0, 0.5306915102516391, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 626845.7008107904, 626845.7008107904, 147564.0264374721], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2633400.0000, 
sim time next is 2634000.0000, 
raw observation next is [24.5, 83.66666666666666, 1.0, 2.0, 0.5403562583280824, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 635801.0518117211, 635801.0518117211, 149037.9173712561], 
processed observation next is [0.0, 0.4782608695652174, 0.46296296296296297, 0.8366666666666666, 1.0, 1.0, 0.4528050694381933, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2270718042184718, 0.2270718042184718, 0.2866113795601079], 
reward next is 0.7134, 
noisyNet noise sample is [array([0.42800647], dtype=float32), 1.3301277]. 
=============================================
[2019-04-27 22:07:55,962] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[73.02738 ]
 [73.07683 ]
 [73.10197 ]
 [73.05943 ]
 [72.838745]], R is [[72.95992279]
 [72.94654846]
 [72.93580627]
 [72.92720032]
 [72.91949463]].
[2019-04-27 22:08:04,908] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:08:04,917] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4254
[2019-04-27 22:08:04,920] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 74.0, 1.0, 2.0, 0.6721086874652493, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 765997.88764777, 765997.88764777, 170929.737230214], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2844000.0000, 
sim time next is 2844600.0000, 
raw observation next is [27.91666666666667, 73.5, 1.0, 2.0, 0.6659684489390412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 758996.4331781879, 758996.4331781876, 169799.6796250315], 
processed observation next is [1.0, 0.9565217391304348, 0.5895061728395063, 0.735, 1.0, 1.0, 0.6023433915940967, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2710701547064957, 0.2710701547064956, 0.3265378454327529], 
reward next is 0.6735, 
noisyNet noise sample is [array([1.5239123], dtype=float32), -1.4754511]. 
=============================================
[2019-04-27 22:08:05,547] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:08:05,555] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3761
[2019-04-27 22:08:05,558] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.66666666666667, 92.0, 1.0, 2.0, 0.5222750799583226, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 621748.3819943088, 621748.3819943093, 146394.3492373932], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2857200.0000, 
sim time next is 2857800.0000, 
raw observation next is [22.58333333333333, 91.5, 1.0, 2.0, 0.5165075739330426, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 616533.4396606995, 616533.4396606995, 145530.5175000216], 
processed observation next is [1.0, 0.043478260869565216, 0.3919753086419751, 0.915, 1.0, 1.0, 0.42441377849171735, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.22019051416453553, 0.22019051416453553, 0.2798663798077339], 
reward next is 0.7201, 
noisyNet noise sample is [array([0.38644978], dtype=float32), 0.008952949]. 
=============================================
[2019-04-27 22:08:08,165] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:08:08,173] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3943
[2019-04-27 22:08:08,182] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1402392.48660058 W.
[2019-04-27 22:08:08,188] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.0, 89.0, 1.0, 2.0, 0.6149945747202678, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9790913919296694, 6.9112, 6.9112, 121.9260426156618, 1402392.48660058, 1402392.48660058, 299738.367149838], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2894400.0000, 
sim time next is 2895000.0000, 
raw observation next is [25.0, 89.0, 1.0, 2.0, 0.6080153190129322, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9679801895451012, 6.911200000000001, 6.9112, 121.9260426156618, 1386463.053476449, 1386463.053476449, 296990.7531238872], 
processed observation next is [1.0, 0.5217391304347826, 0.48148148148148145, 0.89, 1.0, 1.0, 0.5333515702534907, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.9599752369313765, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.49516537624158896, 0.49516537624158896, 0.571136063699783], 
reward next is 0.4289, 
noisyNet noise sample is [array([-1.4777644], dtype=float32), -1.813108]. 
=============================================
[2019-04-27 22:08:08,198] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[50.150833]
 [49.24314 ]
 [48.495308]
 [48.54754 ]
 [48.97822 ]], R is [[50.62571335]
 [50.11945724]
 [50.08473587]
 [49.99600983]
 [49.88483429]].
[2019-04-27 22:08:10,960] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:08:10,969] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8896
[2019-04-27 22:08:10,974] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.1, 91.0, 1.0, 2.0, 0.6456726082182056, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 735854.3988570881, 735854.3988570881, 166112.4596339399], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2937600.0000, 
sim time next is 2938200.0000, 
raw observation next is [25.08333333333334, 91.50000000000001, 1.0, 2.0, 0.6465532949932038, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 736858.574518105, 736858.574518105, 166271.1785003643], 
processed observation next is [1.0, 0.0, 0.4845679012345681, 0.9150000000000001, 1.0, 1.0, 0.5792301130871473, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.26316377661360896, 0.26316377661360896, 0.31975226634685444], 
reward next is 0.6802, 
noisyNet noise sample is [array([0.16932125], dtype=float32), 1.7700932]. 
=============================================
[2019-04-27 22:08:15,269] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:08:15,278] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4336
[2019-04-27 22:08:15,284] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.4, 93.5, 1.0, 2.0, 0.6147631605342778, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 704850.5152580119, 704850.5152580119, 160845.2393393603], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3029400.0000, 
sim time next is 3030000.0000, 
raw observation next is [24.53333333333333, 93.33333333333334, 1.0, 2.0, 0.6226979551145905, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 712164.5384646951, 712164.5384646951, 162150.5494010056], 
processed observation next is [1.0, 0.043478260869565216, 0.46419753086419746, 0.9333333333333335, 1.0, 1.0, 0.5508308989459411, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2543444780231054, 0.2543444780231054, 0.3118279796173185], 
reward next is 0.6882, 
noisyNet noise sample is [array([-0.44405076], dtype=float32), -0.26574165]. 
=============================================
[2019-04-27 22:08:15,296] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[60.269043]
 [60.99769 ]
 [61.62037 ]
 [62.13421 ]
 [62.504955]], R is [[59.65359879]
 [59.74774551]
 [59.84276581]
 [59.93811417]
 [60.03392029]].
[2019-04-27 22:08:20,186] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:08:20,196] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8095
[2019-04-27 22:08:20,201] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 62.0, 1.0, 2.0, 0.60747021360841, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 700300.4287062406, 700300.4287062406, 159757.2035214588], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3101400.0000, 
sim time next is 3102000.0000, 
raw observation next is [29.0, 60.66666666666667, 1.0, 2.0, 0.5906186535047285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 684264.2380425144, 684264.2380425144, 157007.7862570397], 
processed observation next is [1.0, 0.9130434782608695, 0.6296296296296297, 0.6066666666666667, 1.0, 1.0, 0.5126412541722959, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2443800850151837, 0.2443800850151837, 0.3019380504943071], 
reward next is 0.6981, 
noisyNet noise sample is [array([-0.23118407], dtype=float32), 1.6137177]. 
=============================================
[2019-04-27 22:08:20,215] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[77.56653 ]
 [77.40443 ]
 [76.85897 ]
 [76.360016]
 [76.127975]], R is [[77.74770355]
 [77.66300201]
 [77.57407379]
 [77.48123169]
 [77.38493347]].
[2019-04-27 22:08:20,630] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:08:20,641] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6978
[2019-04-27 22:08:20,646] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.83333333333334, 59.66666666666667, 1.0, 2.0, 0.5629257792579294, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 657755.5935035872, 657755.5935035876, 152577.2278828065], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3104400.0000, 
sim time next is 3105000.0000, 
raw observation next is [28.75, 60.5, 1.0, 2.0, 0.5646192719639856, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 658772.2371803321, 658772.2371803321, 152818.4459041128], 
processed observation next is [1.0, 0.9565217391304348, 0.6203703703703703, 0.605, 1.0, 1.0, 0.48168960948093525, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.23527579899297574, 0.23527579899297574, 0.29388162673867846], 
reward next is 0.7061, 
noisyNet noise sample is [array([0.25991854], dtype=float32), -0.97518426]. 
=============================================
[2019-04-27 22:08:20,657] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[79.209946]
 [79.0153  ]
 [78.75297 ]
 [78.24327 ]
 [78.013275]], R is [[79.26831055]
 [79.18221283]
 [79.09737396]
 [79.01271057]
 [78.92500305]].
[2019-04-27 22:08:20,825] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:08:20,833] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9006
[2019-04-27 22:08:20,837] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 71.66666666666666, 1.0, 2.0, 0.5370800957428578, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 633053.9766904257, 633053.9766904257, 148548.4300835542], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3822000.0000, 
sim time next is 3822600.0000, 
raw observation next is [26.1, 72.83333333333334, 1.0, 2.0, 0.5411766033706233, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 636827.3106506363, 636827.3106506363, 149174.4675696952], 
processed observation next is [0.0, 0.21739130434782608, 0.5222222222222223, 0.7283333333333334, 1.0, 1.0, 0.45378167067931346, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.22743832523237012, 0.22743832523237012, 0.2868739760955677], 
reward next is 0.7131, 
noisyNet noise sample is [array([-0.55203444], dtype=float32), -0.39586136]. 
=============================================
[2019-04-27 22:08:24,203] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:08:24,208] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0695
[2019-04-27 22:08:24,215] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.86666666666667, 58.0, 1.0, 2.0, 0.61220063289616, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 697689.9967348226, 697689.9967348226, 160190.0366392704], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3181200.0000, 
sim time next is 3181800.0000, 
raw observation next is [30.33333333333334, 63.0, 1.0, 2.0, 0.6409895857272603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 730514.7504372733, 730514.7504372733, 165275.2315169972], 
processed observation next is [1.0, 0.8260869565217391, 0.6790123456790126, 0.63, 1.0, 1.0, 0.5726066496753098, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.26089812515616906, 0.26089812515616906, 0.3178369836865331], 
reward next is 0.6822, 
noisyNet noise sample is [array([0.5984657], dtype=float32), 0.75910234]. 
=============================================
[2019-04-27 22:08:30,286] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:08:30,294] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9772
[2019-04-27 22:08:30,299] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.91666666666667, 91.66666666666666, 1.0, 2.0, 0.5283611796627021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 626987.9529033051, 626987.9529033051, 147300.6103292744], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3311400.0000, 
sim time next is 3312000.0000, 
raw observation next is [23.1, 90.0, 1.0, 2.0, 0.5253814687296308, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 623801.4337682694, 623801.4337682694, 146832.6091107315], 
processed observation next is [0.0, 0.34782608695652173, 0.41111111111111115, 0.9, 1.0, 1.0, 0.4349779389638461, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2227862263458105, 0.2227862263458105, 0.2823704021360221], 
reward next is 0.7176, 
noisyNet noise sample is [array([-0.11903621], dtype=float32), 1.4672915]. 
=============================================
[2019-04-27 22:08:30,324] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[67.63392 ]
 [67.608734]
 [67.595985]
 [67.59882 ]
 [67.59842 ]], R is [[67.78868866]
 [67.82752991]
 [67.86526489]
 [67.9023056 ]
 [67.9388504 ]].
[2019-04-27 22:08:31,403] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:08:31,413] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4716
[2019-04-27 22:08:31,419] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.15, 88.0, 1.0, 2.0, 0.564053196929177, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 659800.8545291289, 659800.8545291289, 152797.0784010385], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3319800.0000, 
sim time next is 3320400.0000, 
raw observation next is [24.3, 87.0, 1.0, 2.0, 0.5679117211798616, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 664117.8870354343, 664117.8870354343, 153435.4845621895], 
processed observation next is [0.0, 0.43478260869565216, 0.4555555555555556, 0.87, 1.0, 1.0, 0.48560919188078755, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.23718495965551223, 0.23718495965551223, 0.2950682395426721], 
reward next is 0.7049, 
noisyNet noise sample is [array([-0.34779298], dtype=float32), 0.027402619]. 
=============================================
[2019-04-27 22:08:31,742] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:08:31,749] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1848
[2019-04-27 22:08:31,759] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.4975013624193814, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 597232.7543458953, 597232.7543458953, 142641.5825532521], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3297600.0000, 
sim time next is 3298200.0000, 
raw observation next is [21.95, 93.83333333333334, 1.0, 2.0, 0.4955509371695133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 595427.8531604705, 595427.8531604705, 142354.280115095], 
processed observation next is [0.0, 0.17391304347826086, 0.36851851851851847, 0.9383333333333335, 1.0, 1.0, 0.3994654013922778, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21265280470016804, 0.21265280470016804, 0.2737582309905673], 
reward next is 0.7262, 
noisyNet noise sample is [array([1.2487452], dtype=float32), 0.79597145]. 
=============================================
[2019-04-27 22:08:33,630] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:08:33,639] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8322
[2019-04-27 22:08:33,648] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1652497.669194792 W.
[2019-04-27 22:08:33,652] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.1, 31.0, 1.0, 2.0, 0.7954074486352488, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9708769745432184, 6.911199999999999, 6.9112, 121.9260426156618, 1652497.669194792, 1652497.669194792, 330136.2238087411], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4201200.0000, 
sim time next is 4201800.0000, 
raw observation next is [34.08333333333334, 31.5, 1.0, 2.0, 0.4531167386656548, 1.0, 1.0, 0.4531167386656548, 1.0, 2.0, 0.7225786706730793, 6.9112, 6.9112, 121.94756008, 1575013.492312456, 1575013.492312456, 321243.1801996711], 
processed observation next is [1.0, 0.6521739130434783, 0.8179012345679015, 0.315, 1.0, 1.0, 0.34894849841149383, 1.0, 0.5, 0.34894849841149383, 1.0, 1.0, 0.653223338341349, 0.0, 0.0, 0.8096049824067558, 0.56250481868302, 0.56250481868302, 0.617775346537829], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.15835969], dtype=float32), 0.7141009]. 
=============================================
[2019-04-27 22:08:34,055] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.5272065e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 22:08:34,067] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9325
[2019-04-27 22:08:34,072] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.98333333333333, 99.0, 1.0, 2.0, 0.6253660777566992, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 728423.4135539647, 728423.4135539647, 163246.6856203015], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3390600.0000, 
sim time next is 3391200.0000, 
raw observation next is [23.0, 100.0, 1.0, 2.0, 0.6298061278446234, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 731469.1706142032, 731469.1706142027, 163941.0100291949], 
processed observation next is [1.0, 0.2608695652173913, 0.4074074074074074, 1.0, 1.0, 1.0, 0.5592930093388374, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.26123898950507257, 0.2612389895050724, 0.31527117313306713], 
reward next is 0.6847, 
noisyNet noise sample is [array([0.7614045], dtype=float32), 0.8064619]. 
=============================================
[2019-04-27 22:08:35,531] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-27 22:08:35,533] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 22:08:35,533] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 22:08:35,533] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 22:08:35,535] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 22:08:35,536] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:08:35,533] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:08:35,534] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 22:08:35,538] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:08:35,541] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:08:35,539] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:08:35,563] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run98
[2019-04-27 22:08:35,589] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run98
[2019-04-27 22:08:35,590] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run98
[2019-04-27 22:08:35,591] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run98
[2019-04-27 22:08:35,615] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run98
[2019-04-27 22:08:47,046] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00505163], dtype=float32), -0.00481579]
[2019-04-27 22:08:47,047] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [38.0, 15.0, 1.0, 2.0, 0.8884734911305073, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.948913741677401, 6.9112, 121.925830037967, 1123052.004191099, 1103739.23130811, 218713.9230885614]
[2019-04-27 22:08:47,048] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:08:47,052] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6416535995874869
[2019-04-27 22:09:10,436] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00505163], dtype=float32), -0.00481579]
[2019-04-27 22:09:10,436] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [32.76666666666667, 52.0, 1.0, 2.0, 0.6532911906545473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 744541.2936869849, 744541.2936869849, 167491.3317536567]
[2019-04-27 22:09:10,437] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:09:10,440] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8652398726600414
[2019-04-27 22:09:20,267] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00505163], dtype=float32), -0.00481579]
[2019-04-27 22:09:20,269] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [23.12236541333333, 47.80388852, 1.0, 2.0, 0.4993549120078463, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 643190.6786754739, 643190.6786754744, 143742.9152466475]
[2019-04-27 22:09:20,271] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 22:09:20,276] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.09527545177455188
[2019-04-27 22:09:30,157] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00505163], dtype=float32), -0.00481579]
[2019-04-27 22:09:30,159] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [24.0, 90.66666666666666, 1.0, 2.0, 0.5740261537050451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 668196.2660648676, 668196.2660648671, 154330.795432965]
[2019-04-27 22:09:30,161] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:09:30,164] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7885158236555676
[2019-04-27 22:09:39,094] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00505163], dtype=float32), -0.00481579]
[2019-04-27 22:09:39,095] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [23.0, 87.0, 1.0, 2.0, 0.4776967815480677, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 572736.6863096366, 572736.6863096366, 139542.2373725759]
[2019-04-27 22:09:39,095] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:09:39,098] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.06869971503363315
[2019-04-27 22:09:45,192] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00505163], dtype=float32), -0.00481579]
[2019-04-27 22:09:45,192] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [32.68877848333334, 90.26885158333333, 1.0, 2.0, 0.4453605615608139, 1.0, 2.0, 0.4453605615608139, 1.0, 1.0, 0.7090285183857022, 6.911200000000001, 6.9112, 121.94756008, 1523475.698239988, 1523475.698239987, 317522.2323847599]
[2019-04-27 22:09:45,193] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 22:09:45,200] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6133249777325364
[2019-04-27 22:09:45,203] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1523475.698239988 W.
[2019-04-27 22:09:45,243] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00505163], dtype=float32), -0.00481579]
[2019-04-27 22:09:45,244] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [25.0, 95.66666666666666, 1.0, 2.0, 0.7025306588959171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 800687.7192626997, 800687.7192626997, 176622.0261513401]
[2019-04-27 22:09:45,245] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 22:09:45,247] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [2.684594e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.6096784717654811
[2019-04-27 22:10:11,410] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00505163], dtype=float32), -0.00481579]
[2019-04-27 22:10:11,411] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [24.8, 85.33333333333334, 1.0, 2.0, 0.5621602593221539, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 654336.6555681797, 654336.6555681797, 152338.7594681168]
[2019-04-27 22:10:11,412] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:10:11,415] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.07017229217422127
[2019-04-27 22:10:24,084] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8923.4199 2120533710.1715 430.0000
[2019-04-27 22:10:24,496] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8582.8795 2248773970.1662 553.0000
[2019-04-27 22:10:24,503] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8770.2626 2170712315.3188 493.0000
[2019-04-27 22:10:24,567] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8099.2132 2445444095.4335 746.0000
[2019-04-27 22:10:24,749] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8700.8700 2195155380.8105 572.0000
[2019-04-27 22:10:25,766] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 2425000, evaluation results [2425000.0, 8099.213190057684, 2445444095.433475, 746.0, 8770.262620159681, 2170712315.3187604, 493.0, 8923.419851553375, 2120533710.171511, 430.0, 8582.879467926026, 2248773970.1662407, 553.0, 8700.870031992405, 2195155380.8105125, 572.0]
[2019-04-27 22:10:27,439] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:10:27,444] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4900
[2019-04-27 22:10:27,453] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2088088.864947259 W.
[2019-04-27 22:10:27,458] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.06666666666666, 69.0, 1.0, 2.0, 0.6102190136329815, 1.0, 2.0, 0.6102190136329815, 1.0, 2.0, 0.971488543149543, 6.911200000000001, 6.9112, 121.94756008, 2088088.864947259, 2088088.864947259, 400971.5480867646], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3408000.0000, 
sim time next is 3408600.0000, 
raw observation next is [29.28333333333333, 68.0, 1.0, 2.0, 0.9278599133022052, 1.0, 2.0, 0.9278599133022052, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 2116710.365890477, 2116710.365890477, 399420.5863595617], 
processed observation next is [1.0, 0.43478260869565216, 0.6401234567901234, 0.68, 1.0, 1.0, 0.9141189444073872, 1.0, 1.0, 0.9141189444073872, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.7559679878180274, 0.7559679878180274, 0.7681165122299264], 
reward next is 0.2319, 
noisyNet noise sample is [array([-1.4046088], dtype=float32), -0.96054053]. 
=============================================
[2019-04-27 22:10:28,931] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:10:28,939] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2128
[2019-04-27 22:10:28,947] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2044556.625645945 W.
[2019-04-27 22:10:28,949] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.16666666666667, 65.66666666666667, 1.0, 2.0, 0.597511781908759, 1.0, 2.0, 0.597511781908759, 1.0, 2.0, 0.9512582163989356, 6.911199999999999, 6.9112, 121.94756008, 2044556.625645945, 2044556.625645945, 394021.5513460909], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3428400.0000, 
sim time next is 3429000.0000, 
raw observation next is [29.95, 66.5, 1.0, 2.0, 0.8990174635569707, 1.0, 2.0, 0.8990174635569707, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 2050837.037367608, 2050837.037367608, 386368.9747161179], 
processed observation next is [1.0, 0.6956521739130435, 0.6648148148148147, 0.665, 1.0, 1.0, 0.8797826947106794, 1.0, 1.0, 0.8797826947106794, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.73244179905986, 0.73244179905986, 0.7430172590694575], 
reward next is 0.2570, 
noisyNet noise sample is [array([-1.7759694], dtype=float32), -0.90055555]. 
=============================================
[2019-04-27 22:10:28,972] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[56.176666]
 [55.71871 ]
 [55.36374 ]
 [54.927994]
 [54.66994 ]], R is [[56.4306488 ]
 [56.10861206]
 [55.79520798]
 [55.48412323]
 [55.16514206]].
[2019-04-27 22:10:42,204] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:10:42,209] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0845
[2019-04-27 22:10:42,214] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 94.0, 1.0, 2.0, 0.6014335430909789, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 694156.5555027412, 694156.5555027412, 158748.8369508204], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4560000.0000, 
sim time next is 4560600.0000, 
raw observation next is [24.0, 94.0, 1.0, 2.0, 0.6004602859199565, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 693034.7523098465, 693034.7523098469, 158580.6028200891], 
processed observation next is [0.0, 0.782608695652174, 0.4444444444444444, 0.94, 1.0, 1.0, 0.5243574832380434, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.24751241153923087, 0.24751241153923104, 0.30496269773094054], 
reward next is 0.6950, 
noisyNet noise sample is [array([-0.601053], dtype=float32), -0.3137099]. 
=============================================
[2019-04-27 22:10:45,107] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:10:45,119] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4771
[2019-04-27 22:10:45,126] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2271137.342717215 W.
[2019-04-27 22:10:45,129] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.0, 47.0, 1.0, 2.0, 0.7005602876804644, 1.0, 2.0, 0.6636448058166668, 1.0, 2.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.94756008, 2271137.342717215, 2271137.342717215, 429390.6660305992], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3769200.0000, 
sim time next is 3769800.0000, 
raw observation next is [34.16666666666667, 46.16666666666667, 1.0, 2.0, 0.6888471557855309, 1.0, 2.0, 0.6577882398692002, 1.0, 2.0, 0.9977734948820727, 6.911200000000001, 6.9112, 121.94756008, 2251069.632452747, 2251069.632452747, 426256.560554855], 
processed observation next is [1.0, 0.6521739130434783, 0.8209876543209879, 0.4616666666666667, 1.0, 1.0, 0.6295799473637272, 1.0, 1.0, 0.5926050474633335, 1.0, 1.0, 0.9972168686025908, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.8039534401616953, 0.8039534401616953, 0.8197241549131826], 
reward next is 0.1803, 
noisyNet noise sample is [array([1.1536406], dtype=float32), -1.6394336]. 
=============================================
[2019-04-27 22:10:57,515] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5634701e-21 1.0000000e+00 1.7144565e-24 1.7195766e-26 4.0867786e-22], sum to 1.0000
[2019-04-27 22:10:57,521] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0454
[2019-04-27 22:10:57,529] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 93.0, 1.0, 2.0, 0.9105997078504262, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260425998823, 1039738.354404601, 1039738.354404601, 219942.2068273345], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3996000.0000, 
sim time next is 3996600.0000, 
raw observation next is [24.56666666666667, 93.16666666666667, 1.0, 2.0, 0.8298231261644942, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.926042615657, 947697.429640825, 947697.429640825, 202272.271709023], 
processed observation next is [1.0, 0.2608695652173913, 0.46543209876543223, 0.9316666666666668, 1.0, 1.0, 0.7974084835291597, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.809462128820104, 0.3384633677288661, 0.3384633677288661, 0.3889851379019673], 
reward next is 0.6110, 
noisyNet noise sample is [array([-0.9721361], dtype=float32), -1.3366929]. 
=============================================
[2019-04-27 22:11:01,401] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:11:01,410] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6904
[2019-04-27 22:11:01,416] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.16666666666666, 99.0, 1.0, 2.0, 0.6659853117778005, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 759015.6610729307, 759015.6610729302, 169802.302650631], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4053000.0000, 
sim time next is 4053600.0000, 
raw observation next is [24.0, 100.0, 1.0, 2.0, 0.66701890898558, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 760194.2238362391, 760194.2238362391, 169991.7295244461], 
processed observation next is [1.0, 0.9565217391304348, 0.4444444444444444, 1.0, 1.0, 1.0, 0.6035939392685477, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2714979370843711, 0.2714979370843711, 0.32690717216239634], 
reward next is 0.6731, 
noisyNet noise sample is [array([-2.6731312], dtype=float32), -1.3157219]. 
=============================================
[2019-04-27 22:11:01,484] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:11:01,492] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7088
[2019-04-27 22:11:01,497] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.66666666666667, 83.33333333333334, 1.0, 2.0, 0.5127554782571664, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 612428.8256038601, 612428.8256038601, 144945.0718299522], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4404000.0000, 
sim time next is 4404600.0000, 
raw observation next is [23.25, 86.0, 1.0, 2.0, 0.5092171707109018, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 608733.8759557354, 608733.8759557354, 144401.4112709802], 
processed observation next is [1.0, 1.0, 0.4166666666666667, 0.86, 1.0, 1.0, 0.4157347270367878, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21740495569847695, 0.21740495569847695, 0.2776950216749619], 
reward next is 0.7223, 
noisyNet noise sample is [array([1.5721059], dtype=float32), 0.74168366]. 
=============================================
[2019-04-27 22:11:05,946] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.8560967e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 22:11:05,951] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2349
[2019-04-27 22:11:05,955] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 100.0, 1.0, 2.0, 0.7070643246684518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 805857.5417372299, 805857.5417372299, 177486.7292199989], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4840800.0000, 
sim time next is 4841400.0000, 
raw observation next is [25.0, 100.0, 1.0, 2.0, 0.7063807153730699, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 805078.0072505722, 805078.0072505722, 177356.5382913137], 
processed observation next is [1.0, 0.0, 0.48148148148148145, 1.0, 1.0, 1.0, 0.650453232586988, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.28752785973234724, 0.28752785973234724, 0.34107026594483403], 
reward next is 0.6589, 
noisyNet noise sample is [array([-0.03516662], dtype=float32), 0.9898735]. 
=============================================
[2019-04-27 22:11:06,739] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:11:06,749] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5519
[2019-04-27 22:11:06,756] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.66666666666667, 90.0, 1.0, 2.0, 0.452092731889277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 550300.0894872755, 550300.0894872759, 135939.8814193387], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4148400.0000, 
sim time next is 4149000.0000, 
raw observation next is [21.5, 91.0, 1.0, 2.0, 0.4495754593660087, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 547601.2038882084, 547601.2038882084, 135574.8591224398], 
processed observation next is [1.0, 0.0, 0.35185185185185186, 0.91, 1.0, 1.0, 0.34473268972143895, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.195571858531503, 0.195571858531503, 0.26072088292776885], 
reward next is 0.7393, 
noisyNet noise sample is [array([0.93197805], dtype=float32), -1.1108986]. 
=============================================
[2019-04-27 22:11:06,776] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[60.937958]
 [63.3403  ]
 [70.4418  ]
 [81.88847 ]
 [81.66082 ]], R is [[60.4646759 ]
 [60.59860992]
 [60.73049927]
 [60.86015701]
 [60.98707962]].
[2019-04-27 22:11:16,025] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-27 22:11:16,026] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 22:11:16,027] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:11:16,028] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 22:11:16,029] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:11:16,029] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 22:11:16,031] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 22:11:16,032] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 22:11:16,033] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:11:16,035] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:11:16,035] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:11:16,067] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run99
[2019-04-27 22:11:16,067] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run99
[2019-04-27 22:11:16,116] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run99
[2019-04-27 22:11:16,140] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run99
[2019-04-27 22:11:16,165] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run99
[2019-04-27 22:11:27,335] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00511583], dtype=float32), -0.009001443]
[2019-04-27 22:11:27,336] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [26.5, 43.5, 1.0, 2.0, 0.3504165216094661, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 442563.7091835291, 442563.7091835291, 121998.0294245697]
[2019-04-27 22:11:27,338] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:11:27,342] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.3959937e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.7383237745754524
[2019-04-27 22:11:28,958] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00511583], dtype=float32), -0.009001443]
[2019-04-27 22:11:28,959] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [32.78822586666666, 39.94068837666666, 1.0, 2.0, 0.4984267740657599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 590670.2835705931, 590670.2835705931, 142504.6570895609]
[2019-04-27 22:11:28,960] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:11:28,964] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8724104984575412
[2019-04-27 22:11:35,760] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00511583], dtype=float32), -0.009001443]
[2019-04-27 22:11:35,761] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [22.73333333333333, 58.0, 1.0, 2.0, 0.311390549152577, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 396619.4900185247, 396619.4900185247, 116991.9120201644]
[2019-04-27 22:11:35,761] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:11:35,767] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3304141312047618
[2019-04-27 22:11:40,248] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00511583], dtype=float32), -0.009001443]
[2019-04-27 22:11:40,249] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [25.67649939, 45.96027585, 1.0, 2.0, 0.2994875702641756, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 380462.5558204444, 380462.5558204444, 115504.9296856327]
[2019-04-27 22:11:40,250] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:11:40,253] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.22569466968601237
[2019-04-27 22:12:06,916] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00511583], dtype=float32), -0.009001443]
[2019-04-27 22:12:06,917] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [28.2, 63.0, 1.0, 2.0, 0.5299910787982017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 618852.1553218529, 618852.1553218529, 147151.9252898069]
[2019-04-27 22:12:06,919] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:12:06,922] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4316123162902832
[2019-04-27 22:12:12,168] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00511583], dtype=float32), -0.009001443]
[2019-04-27 22:12:12,172] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [31.16666666666667, 66.16666666666666, 1.0, 2.0, 0.7485508874732045, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 853167.052026674, 853167.0520266736, 185544.471797749]
[2019-04-27 22:12:12,174] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:12:12,176] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3041356306181078
[2019-04-27 22:12:22,550] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00511583], dtype=float32), -0.009001443]
[2019-04-27 22:12:22,551] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [27.8, 79.0, 1.0, 2.0, 0.6543279616761829, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 745723.4530822105, 745723.4530822105, 167680.3121602566]
[2019-04-27 22:12:22,552] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:12:22,556] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7028734073900554
[2019-04-27 22:12:27,562] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00511583], dtype=float32), -0.009001443]
[2019-04-27 22:12:27,563] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [25.46666666666667, 88.0, 1.0, 2.0, 0.7164246657170265, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.9260426156618, 1531548.272600081, 1531548.272600082, 320230.3536457325]
[2019-04-27 22:12:27,564] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 22:12:27,565] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.14651089112676763
[2019-04-27 22:12:27,567] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1531548.272600081 W.
[2019-04-27 22:12:33,120] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00511583], dtype=float32), -0.009001443]
[2019-04-27 22:12:33,122] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [27.7, 82.0, 1.0, 2.0, 0.6846363209747832, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9977734948820727, 6.911200000000001, 6.9112, 121.9256177221639, 1495265.828010542, 1495265.828010542, 314465.6556121696]
[2019-04-27 22:12:33,123] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:12:33,126] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.9798449e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.3159291543123055
[2019-04-27 22:12:33,128] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1495265.828010542 W.
[2019-04-27 22:12:40,816] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00511583], dtype=float32), -0.009001443]
[2019-04-27 22:12:40,816] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [27.17747783666667, 47.455974295, 1.0, 2.0, 0.5290190606311189, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 657217.5234243851, 657217.5234243846, 148314.1496989883]
[2019-04-27 22:12:40,817] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:12:40,819] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8457955752718248
[2019-04-27 22:12:47,449] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00511583], dtype=float32), -0.009001443]
[2019-04-27 22:12:47,451] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [26.94350298, 74.34142018, 1.0, 2.0, 0.6081580623721181, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 698764.1333464385, 698764.1333464381, 159765.352887082]
[2019-04-27 22:12:47,452] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 22:12:47,454] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.09778785239012999
[2019-04-27 22:12:48,200] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00511583], dtype=float32), -0.009001443]
[2019-04-27 22:12:48,202] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [29.73333333333333, 67.33333333333334, 1.0, 2.0, 0.6289245020785103, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.9260426156618, 1431681.843205494, 1431681.843205495, 304776.6308339225]
[2019-04-27 22:12:48,202] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:12:48,204] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.01404764135657266
[2019-04-27 22:12:48,207] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1431681.843205494 W.
[2019-04-27 22:13:05,063] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8770.2626 2170712315.3188 493.0000
[2019-04-27 22:13:05,335] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8923.4199 2120533710.1715 430.0000
[2019-04-27 22:13:05,368] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8099.2132 2445444095.4335 746.0000
[2019-04-27 22:13:05,370] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8582.8795 2248773970.1662 553.0000
[2019-04-27 22:13:05,404] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8700.8700 2195155380.8105 572.0000
[2019-04-27 22:13:06,419] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 2450000, evaluation results [2450000.0, 8099.213190057684, 2445444095.433475, 746.0, 8770.262620159681, 2170712315.3187604, 493.0, 8923.419851553375, 2120533710.171511, 430.0, 8582.879467926026, 2248773970.1662407, 553.0, 8700.870031992405, 2195155380.8105125, 572.0]
[2019-04-27 22:13:11,616] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:13:11,621] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7515
[2019-04-27 22:13:11,626] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.66666666666666, 98.0, 1.0, 2.0, 0.5400534006777605, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 635370.080430419, 635370.080430419, 148985.4546041971], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4430400.0000, 
sim time next is 4431000.0000, 
raw observation next is [22.83333333333334, 99.0, 1.0, 2.0, 0.5557384120912342, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 650010.2332639744, 650010.2332639744, 151409.5297615861], 
processed observation next is [0.0, 0.2608695652173913, 0.4012345679012348, 0.99, 1.0, 1.0, 0.47111715725146924, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.23214651187999086, 0.23214651187999086, 0.2911721726184348], 
reward next is 0.7088, 
noisyNet noise sample is [array([0.5708239], dtype=float32), 0.29431978]. 
=============================================
[2019-04-27 22:13:11,640] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[73.06233]
 [73.17307]
 [73.27486]
 [73.34984]
 [73.38466]], R is [[72.90738678]
 [72.89179993]
 [72.8807373 ]
 [72.87367249]
 [72.87001038]].
[2019-04-27 22:13:11,996] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3257153e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 22:13:12,005] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0544
[2019-04-27 22:13:12,013] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.25, 92.0, 1.0, 2.0, 0.4941389014223858, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 593184.6952246726, 593184.6952246721, 142114.3455429962], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4415400.0000, 
sim time next is 4416000.0000, 
raw observation next is [22.16666666666667, 92.66666666666667, 1.0, 2.0, 0.4932093234130112, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 592070.1774641596, 592070.1774641596, 141969.0978907556], 
processed observation next is [0.0, 0.08695652173913043, 0.3765432098765434, 0.9266666666666667, 1.0, 1.0, 0.3966777659678705, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21145363480862842, 0.21145363480862842, 0.2730174959437608], 
reward next is 0.7270, 
noisyNet noise sample is [array([0.2091585], dtype=float32), 0.14802723]. 
=============================================
[2019-04-27 22:13:12,029] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[74.0706  ]
 [74.061714]
 [74.09093 ]
 [74.366936]
 [74.31313 ]], R is [[74.26832581]
 [74.25234222]
 [74.23648071]
 [74.22112274]
 [74.20611572]].
[2019-04-27 22:13:14,569] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:13:14,576] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5885
[2019-04-27 22:13:14,580] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.4, 86.0, 1.0, 2.0, 0.6448359271405296, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 738002.1132951621, 738002.1132951621, 166117.0462184343], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4489200.0000, 
sim time next is 4489800.0000, 
raw observation next is [25.33333333333334, 87.33333333333334, 1.0, 2.0, 0.6428756545919334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 735290.3594775404, 735290.3594775404, 165741.6406729849], 
processed observation next is [0.0, 1.0, 0.49382716049382736, 0.8733333333333334, 1.0, 1.0, 0.5748519697523017, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.26260369981340725, 0.26260369981340725, 0.31873392437112485], 
reward next is 0.6813, 
noisyNet noise sample is [array([0.5195648], dtype=float32), 0.45144647]. 
=============================================
[2019-04-27 22:13:16,756] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:13:16,766] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1161
[2019-04-27 22:13:16,772] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.75, 86.5, 1.0, 2.0, 0.5917260333575118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 686071.8443937078, 686071.8443937078, 157221.8747977821], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4531800.0000, 
sim time next is 4532400.0000, 
raw observation next is [24.7, 86.0, 1.0, 2.0, 0.5839012848255937, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 678668.9812822536, 678668.9812822532, 155960.0744193575], 
processed observation next is [0.0, 0.4782608695652174, 0.4703703703703703, 0.86, 1.0, 1.0, 0.5046443866971353, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2423817790293763, 0.24238177902937613, 0.29992322003722593], 
reward next is 0.7001, 
noisyNet noise sample is [array([-1.5224355], dtype=float32), -0.99319893]. 
=============================================
[2019-04-27 22:13:17,812] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.8263817e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 22:13:17,817] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1725
[2019-04-27 22:13:17,825] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 100.0, 1.0, 2.0, 0.4793053141476246, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 578328.5812437377, 578328.5812437373, 139914.1190240014], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4581600.0000, 
sim time next is 4582200.0000, 
raw observation next is [21.0, 100.0, 1.0, 2.0, 0.4780409867187133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 576804.1734844871, 576804.1734844871, 139719.5995613142], 
processed observation next is [1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.3786202222841825, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.20600149053017397, 0.20600149053017397, 0.26869153761791187], 
reward next is 0.7313, 
noisyNet noise sample is [array([0.35611442], dtype=float32), -0.4072279]. 
=============================================
[2019-04-27 22:13:18,655] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.6062054e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.6337264e-38], sum to 1.0000
[2019-04-27 22:13:18,664] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6483
[2019-04-27 22:13:18,670] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.26666666666667, 99.33333333333334, 1.0, 2.0, 0.4944036306303334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 594381.4311792044, 594381.4311792044, 142186.1121619031], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4598400.0000, 
sim time next is 4599000.0000, 
raw observation next is [21.2, 99.5, 1.0, 2.0, 0.4915601988732447, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 591479.1175363577, 591479.1175363577, 141759.4939517889], 
processed observation next is [1.0, 0.21739130434782608, 0.34074074074074073, 0.995, 1.0, 1.0, 0.3947145224681485, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2112425419772706, 0.2112425419772706, 0.2726144114457479], 
reward next is 0.7274, 
noisyNet noise sample is [array([0.00238974], dtype=float32), -0.8105385]. 
=============================================
[2019-04-27 22:13:18,678] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[63.95388]
 [64.03254]
 [64.09175]
 [64.18255]
 [64.37239]], R is [[63.91139603]
 [63.99884796]
 [64.07501221]
 [64.16043091]
 [64.23841095]].
[2019-04-27 22:13:25,954] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.8847466e-29 1.0000000e+00 2.0705605e-34 1.3419184e-33 8.2459019e-34], sum to 1.0000
[2019-04-27 22:13:25,963] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5830
[2019-04-27 22:13:25,966] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 99.0, 1.0, 2.0, 0.6006226317524329, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 699319.206404983, 699319.206404983, 158888.7963096643], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4690200.0000, 
sim time next is 4690800.0000, 
raw observation next is [23.0, 100.0, 1.0, 2.0, 0.6039261973650975, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 701371.7237633121, 701371.7237633121, 159381.6378134763], 
processed observation next is [1.0, 0.30434782608695654, 0.4074074074074074, 1.0, 1.0, 1.0, 0.5284835682917827, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.25048990134404003, 0.25048990134404003, 0.30650314964130054], 
reward next is 0.6935, 
noisyNet noise sample is [array([-1.092065], dtype=float32), 0.37413543]. 
=============================================
[2019-04-27 22:13:27,696] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.228491e-35 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-27 22:13:27,702] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5763
[2019-04-27 22:13:27,712] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2590834.060972773 W.
[2019-04-27 22:13:27,718] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.9, 75.66666666666667, 1.0, 2.0, 0.8871255382157036, 1.0, 2.0, 0.7569274310842865, 1.0, 1.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.94756008, 2590834.060972773, 2590834.060972774, 483308.9482611948], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5581200.0000, 
sim time next is 5581800.0000, 
raw observation next is [29.6, 76.5, 1.0, 2.0, 0.8061711934693122, 1.0, 2.0, 0.7164502587110909, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.94756008, 2452097.283679037, 2452097.283679037, 458988.5612353702], 
processed observation next is [1.0, 0.6086956521739131, 0.6518518518518519, 0.765, 1.0, 1.0, 0.7692514207968002, 1.0, 1.0, 0.6624407841798702, 1.0, 1.0, 0.9972168686025908, 0.0, 0.0, 0.8096049824067558, 0.8757490298853703, 0.8757490298853703, 0.8826703100680195], 
reward next is 0.1173, 
noisyNet noise sample is [array([0.0595844], dtype=float32), -0.1491048]. 
=============================================
[2019-04-27 22:13:35,463] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.6994388e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 22:13:35,469] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3339
[2019-04-27 22:13:35,474] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 89.0, 1.0, 2.0, 0.8938659017340929, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1018901.142621102, 1018901.142621102, 216110.8488387688], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4906800.0000, 
sim time next is 4907400.0000, 
raw observation next is [29.01666666666667, 88.5, 1.0, 2.0, 0.8937539785571856, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1018773.478659324, 1018773.478659324, 216085.5777361782], 
processed observation next is [1.0, 0.8260869565217391, 0.630246913580247, 0.885, 1.0, 1.0, 0.8735166411395067, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.36384767094975856, 0.36384767094975856, 0.4155491879541889], 
reward next is 0.5845, 
noisyNet noise sample is [array([-0.84352], dtype=float32), 0.2973587]. 
=============================================
[2019-04-27 22:13:36,716] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.445644e-36 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-27 22:13:36,725] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4101
[2019-04-27 22:13:36,732] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1655263.792665617 W.
[2019-04-27 22:13:36,735] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.0, 89.0, 1.0, 2.0, 0.7257720897276512, 1.0, 2.0, 0.7257720897276512, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1655263.792665617, 1655263.792665617, 314039.5517138647], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4885200.0000, 
sim time next is 4885800.0000, 
raw observation next is [29.46666666666667, 87.0, 1.0, 2.0, 0.5863796672254955, 1.0, 2.0, 0.5863796672254955, 1.0, 1.0, 0.9335355272755158, 6.911199999999999, 6.9112, 121.94756008, 2006422.141622912, 2006422.141622913, 388004.4887622932], 
processed observation next is [1.0, 0.5652173913043478, 0.6469135802469137, 0.87, 1.0, 1.0, 0.5075948419351137, 1.0, 1.0, 0.5075948419351137, 1.0, 0.5, 0.9169194090943946, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.7165793362938971, 0.7165793362938975, 0.7461624783890254], 
reward next is 0.2538, 
noisyNet noise sample is [array([-1.0452611], dtype=float32), -0.8058504]. 
=============================================
[2019-04-27 22:13:37,569] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:13:37,577] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4084
[2019-04-27 22:13:37,581] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 89.0, 1.0, 2.0, 0.8894622466902191, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 1013878.177146406, 1013878.177146406, 215131.0814757184], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4906200.0000, 
sim time next is 4906800.0000, 
raw observation next is [29.0, 89.0, 1.0, 2.0, 0.8938659017340929, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1018901.142621102, 1018901.142621102, 216110.8488387688], 
processed observation next is [1.0, 0.8260869565217391, 0.6296296296296297, 0.89, 1.0, 1.0, 0.8736498830167773, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.36389326522182214, 0.36389326522182214, 0.41559778622840154], 
reward next is 0.5844, 
noisyNet noise sample is [array([1.1195166], dtype=float32), 0.15444797]. 
=============================================
[2019-04-27 22:13:41,824] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:13:41,830] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8910
[2019-04-27 22:13:41,842] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1523514.625872359 W.
[2019-04-27 22:13:41,846] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.5, 94.0, 1.0, 2.0, 0.6680577779184445, 1.0, 1.0, 0.6680577779184445, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1523514.625872359, 1523514.62587236, 292249.275339079], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4974600.0000, 
sim time next is 4975200.0000, 
raw observation next is [24.2, 95.0, 1.0, 2.0, 0.6319563617690173, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.9260426156618, 1435141.973241386, 1435141.973241386, 305286.5632554275], 
processed observation next is [1.0, 0.6086956521739131, 0.45185185185185184, 0.95, 1.0, 1.0, 0.5618528116297824, 0.0, 0.5, -0.1904761904761905, 1.0, 0.5, 0.9972168686025908, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.5125507047290664, 0.5125507047290664, 0.587089544721976], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.43423858], dtype=float32), 1.193782]. 
=============================================
[2019-04-27 22:13:48,868] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7465389e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 22:13:48,878] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2351
[2019-04-27 22:13:48,883] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.96666666666667, 99.66666666666667, 1.0, 2.0, 0.710499611697434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 809774.8859480429, 809774.8859480424, 178141.7645139608], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5112600.0000, 
sim time next is 5113200.0000, 
raw observation next is [24.93333333333333, 99.33333333333334, 1.0, 2.0, 0.7071922839643207, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 806003.4565820264, 806003.4565820264, 177510.30445858], 
processed observation next is [0.0, 0.17391304347826086, 0.47901234567901224, 0.9933333333333334, 1.0, 1.0, 0.6514193856718103, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2878583773507237, 0.2878583773507237, 0.34136597011265385], 
reward next is 0.6586, 
noisyNet noise sample is [array([1.7860477], dtype=float32), 0.037472222]. 
=============================================
[2019-04-27 22:13:51,086] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:13:51,102] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4280
[2019-04-27 22:13:51,116] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1671812.804168236 W.
[2019-04-27 22:13:51,125] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.0, 58.83333333333334, 1.0, 2.0, 0.4886810389190472, 1.0, 1.0, 0.4886810389190472, 1.0, 2.0, 0.7779961291894606, 6.911199999999999, 6.9112, 121.94756008, 1671812.804168236, 1671812.804168236, 338068.2919097008], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6012600.0000, 
sim time next is 6013200.0000, 
raw observation next is [29.0, 58.66666666666667, 1.0, 2.0, 0.4868937399438331, 1.0, 2.0, 0.4868937399438331, 1.0, 2.0, 0.7751506910126559, 6.911199999999999, 6.9112, 121.94756008, 1665692.627170831, 1665692.627170832, 337202.9055442812], 
processed observation next is [1.0, 0.6086956521739131, 0.6296296296296297, 0.5866666666666667, 1.0, 1.0, 0.3891592142188489, 1.0, 1.0, 0.3891592142188489, 1.0, 1.0, 0.7189383637658198, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.5948902239895825, 0.5948902239895828, 0.6484671260466945], 
reward next is 0.3515, 
noisyNet noise sample is [array([-1.4145099], dtype=float32), -0.32762808]. 
=============================================
[2019-04-27 22:13:56,542] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-27 22:13:56,545] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 22:13:56,546] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:13:56,547] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 22:13:56,547] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:13:56,548] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 22:13:56,548] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 22:13:56,550] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 22:13:56,550] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:13:56,551] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:13:56,552] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:13:56,576] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run100
[2019-04-27 22:13:56,604] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run100
[2019-04-27 22:13:56,628] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run100
[2019-04-27 22:13:56,650] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run100
[2019-04-27 22:13:56,650] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run100
[2019-04-27 22:14:03,495] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00387769], dtype=float32), -0.008488768]
[2019-04-27 22:14:03,498] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [31.99309592333334, 41.45557834666667, 1.0, 2.0, 0.5030923080899464, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 595452.289963968, 595452.289963968, 143206.0211136531]
[2019-04-27 22:14:03,500] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 22:14:03,502] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6016050332278122
[2019-04-27 22:14:08,759] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00387769], dtype=float32), -0.008488768]
[2019-04-27 22:14:08,761] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [25.33333333333333, 20.0, 1.0, 2.0, 0.2896429447397335, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 373626.9346202505, 373626.9346202505, 92145.66316927281]
[2019-04-27 22:14:08,762] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:14:08,765] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [4.418931e-36 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.9172544071446828
[2019-04-27 22:14:11,655] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00387769], dtype=float32), -0.008488768]
[2019-04-27 22:14:11,656] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [26.1, 27.66666666666667, 1.0, 2.0, 0.3716553021434073, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 479452.418296819, 479452.418296819, 114976.8364201185]
[2019-04-27 22:14:11,658] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:14:11,663] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.8189437e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.7365626531959217
[2019-04-27 22:14:22,469] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00387769], dtype=float32), -0.008488768]
[2019-04-27 22:14:22,470] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [22.03032023333333, 81.84783204166666, 1.0, 2.0, 0.4031710645226158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 495772.7801583821, 495772.7801583821, 128975.4836645055]
[2019-04-27 22:14:22,474] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:14:22,478] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [3.7126255e-33 1.0000000e+00 1.0441763e-37 5.0919773e-38 1.1640101e-37], sampled 0.8139621821293757
[2019-04-27 22:14:32,465] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00387769], dtype=float32), -0.008488768]
[2019-04-27 22:14:32,466] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [34.54661491, 24.84922660666667, 1.0, 2.0, 0.810111480930706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 996692.7817271722, 996692.7817271722, 201151.8308546113]
[2019-04-27 22:14:32,467] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:14:32,470] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [3.3151709e-30 1.0000000e+00 1.9005170e-34 3.5908546e-35 3.2157458e-32], sampled 0.9559258917187824
[2019-04-27 22:15:02,228] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00387769], dtype=float32), -0.008488768]
[2019-04-27 22:15:02,231] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [25.25, 87.0, 1.0, 2.0, 0.595365480071792, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 682898.9458019709, 682898.9458019709, 157499.3156364547]
[2019-04-27 22:15:02,233] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:15:02,235] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.4666078e-33 1.0000000e+00 4.0177421e-38 0.0000000e+00 2.8025686e-38], sampled 0.13271279006595937
[2019-04-27 22:15:14,876] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00387769], dtype=float32), -0.008488768]
[2019-04-27 22:15:14,878] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [31.480637275, 84.988493335, 1.0, 2.0, 0.6956282121674653, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.9260426156226, 1507811.507868349, 1507811.507868349, 316453.0766534928]
[2019-04-27 22:15:14,879] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 22:15:14,882] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [6.5983825e-32 1.0000000e+00 9.0620398e-37 4.4538565e-38 4.9406258e-38], sampled 0.11155844037683782
[2019-04-27 22:15:14,883] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1507811.507868349 W.
[2019-04-27 22:15:40,384] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00387769], dtype=float32), -0.008488768]
[2019-04-27 22:15:40,386] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [27.16666666666667, 45.0, 1.0, 2.0, 0.362346433151436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 452491.3205145066, 452491.3205145066, 123513.0243762193]
[2019-04-27 22:15:40,388] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:15:40,390] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.12596991765006316
[2019-04-27 22:15:46,113] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8770.2626 2170712315.3188 493.0000
[2019-04-27 22:15:46,260] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8923.4199 2120533710.1715 430.0000
[2019-04-27 22:15:46,320] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8099.2132 2445444095.4335 746.0000
[2019-04-27 22:15:46,338] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8700.1130 2195220806.1349 572.0000
[2019-04-27 22:15:46,380] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8582.8795 2248773970.1662 553.0000
[2019-04-27 22:15:47,396] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 2475000, evaluation results [2475000.0, 8099.213190057684, 2445444095.433475, 746.0, 8770.262620159681, 2170712315.3187604, 493.0, 8923.419851553375, 2120533710.171511, 430.0, 8582.879467926026, 2248773970.1662407, 553.0, 8700.112961499688, 2195220806.134913, 572.0]
[2019-04-27 22:15:50,231] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:15:50,235] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2845
[2019-04-27 22:15:50,240] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1647633.198219076 W.
[2019-04-27 22:15:50,244] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.0, 58.0, 1.0, 2.0, 0.7224294380410455, 1.0, 2.0, 0.7224294380410455, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1647633.198219076, 1647633.198219077, 312741.878129703], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6016200.0000, 
sim time next is 6016800.0000, 
raw observation next is [29.0, 58.0, 1.0, 2.0, 0.7213276430621746, 1.0, 2.0, 0.7213276430621746, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1645118.038490864, 1645118.038490864, 312316.4348919396], 
processed observation next is [1.0, 0.6521739130434783, 0.6296296296296297, 0.58, 1.0, 1.0, 0.6682471941216364, 1.0, 1.0, 0.6682471941216364, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.58754215660388, 0.58754215660388, 0.6006085286383455], 
reward next is 0.3994, 
noisyNet noise sample is [array([-0.40131074], dtype=float32), 0.22984122]. 
=============================================
[2019-04-27 22:15:54,424] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.4024553e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 22:15:54,437] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3067
[2019-04-27 22:15:54,442] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.23333333333333, 84.33333333333333, 1.0, 2.0, 0.7840229331145561, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 893620.1790110203, 893620.1790110199, 192666.9824748277], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5433000.0000, 
sim time next is 5433600.0000, 
raw observation next is [28.16666666666667, 84.66666666666667, 1.0, 2.0, 0.7832983745810553, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 892793.85463071, 892793.85463071, 192519.244485585], 
processed observation next is [1.0, 0.9130434782608695, 0.5987654320987656, 0.8466666666666667, 1.0, 1.0, 0.7420218745012563, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3188549480823964, 0.3188549480823964, 0.3702293163184327], 
reward next is 0.6298, 
noisyNet noise sample is [array([0.72157174], dtype=float32), 2.1525989]. 
=============================================
[2019-04-27 22:15:56,923] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:15:56,931] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6072
[2019-04-27 22:15:56,934] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666667, 74.5, 1.0, 2.0, 0.7070884734300452, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 805885.0791076405, 805885.0791076405, 177496.602193354], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5421000.0000, 
sim time next is 5421600.0000, 
raw observation next is [29.6, 75.0, 1.0, 2.0, 0.7228416295369249, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 823848.9645240116, 823848.9645240116, 180519.3327335949], 
processed observation next is [1.0, 0.782608695652174, 0.6518518518518519, 0.75, 1.0, 1.0, 0.6700495589725296, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2942317730442899, 0.2942317730442899, 0.347152562949221], 
reward next is 0.6528, 
noisyNet noise sample is [array([0.24253891], dtype=float32), 1.029551]. 
=============================================
[2019-04-27 22:15:59,196] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.5418206e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-27 22:15:59,202] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1588
[2019-04-27 22:15:59,209] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1895848.528677689 W.
[2019-04-27 22:15:59,213] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.48333333333333, 68.0, 1.0, 2.0, 0.8311477929024162, 1.0, 2.0, 0.8311477929024162, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1895848.528677689, 1895848.528677689, 356795.8832894515], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5490600.0000, 
sim time next is 5491200.0000, 
raw observation next is [32.66666666666667, 67.0, 1.0, 2.0, 0.6793433469085507, 1.0, 2.0, 0.65303633543071, 1.0, 1.0, 0.9977734948820727, 6.9112, 6.9112, 121.94756008, 2234787.411544698, 2234787.411544698, 423735.4045281222], 
processed observation next is [1.0, 0.5652173913043478, 0.7654320987654323, 0.67, 1.0, 1.0, 0.618265889176846, 1.0, 1.0, 0.5869480183698929, 1.0, 0.5, 0.9972168686025908, 0.0, 0.0, 0.8096049824067558, 0.7981383612659635, 0.7981383612659635, 0.8148757779386966], 
reward next is 0.1851, 
noisyNet noise sample is [array([-0.4235077], dtype=float32), -0.5008123]. 
=============================================
[2019-04-27 22:15:59,798] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:15:59,804] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3455
[2019-04-27 22:15:59,810] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.91666666666667, 81.0, 1.0, 2.0, 0.6386577400020159, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 727855.9608615381, 727855.9608615381, 164856.7959631], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5507400.0000, 
sim time next is 5508000.0000, 
raw observation next is [26.9, 81.0, 1.0, 2.0, 0.6437222182083935, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 733630.5320873221, 733630.5320873221, 165763.7259427731], 
processed observation next is [1.0, 0.782608695652174, 0.5518518518518518, 0.81, 1.0, 1.0, 0.5758597835814208, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2620109043169008, 0.2620109043169008, 0.31877639604379443], 
reward next is 0.6812, 
noisyNet noise sample is [array([2.0309935], dtype=float32), -0.56966287]. 
=============================================
[2019-04-27 22:15:59,825] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[70.58103]
 [70.72921]
 [70.10481]
 [68.32591]
 [65.26523]], R is [[70.48400879]
 [70.46213531]
 [70.4432373 ]
 [70.42668915]
 [69.72241974]].
[2019-04-27 22:16:07,857] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:16:07,864] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1298
[2019-04-27 22:16:07,870] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.76666666666667, 96.33333333333334, 1.0, 2.0, 0.596024749508781, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 687174.3768280424, 687174.3768280424, 157780.8167898527], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5636400.0000, 
sim time next is 5637000.0000, 
raw observation next is [23.83333333333333, 96.16666666666666, 1.0, 2.0, 0.5976003191338255, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 688299.2423782729, 688299.2423782729, 158019.2681128721], 
processed observation next is [0.0, 0.21739130434782608, 0.43827160493827144, 0.9616666666666666, 1.0, 1.0, 0.5209527608736018, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.24582115799224033, 0.24582115799224033, 0.3038832079093694], 
reward next is 0.6961, 
noisyNet noise sample is [array([-1.5755309], dtype=float32), -0.15011513]. 
=============================================
[2019-04-27 22:16:07,878] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[73.611885]
 [73.6051  ]
 [73.6168  ]
 [73.63085 ]
 [73.63351 ]], R is [[73.57242584]
 [73.53327942]
 [73.49491119]
 [73.45748138]
 [73.42134857]].
[2019-04-27 22:16:08,162] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:16:08,171] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1803
[2019-04-27 22:16:08,176] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 95.0, 1.0, 2.0, 0.6386048357134231, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 727795.6390657937, 727795.6390657937, 164845.6364489348], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5644800.0000, 
sim time next is 5645400.0000, 
raw observation next is [24.75, 94.16666666666667, 1.0, 2.0, 0.6413606050894736, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 730937.790600246, 730937.790600246, 165338.8032336448], 
processed observation next is [0.0, 0.34782608695652173, 0.4722222222222222, 0.9416666666666668, 1.0, 1.0, 0.5730483393922304, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2610492109286593, 0.2610492109286593, 0.3179592369877785], 
reward next is 0.6820, 
noisyNet noise sample is [array([-1.5988575], dtype=float32), -0.009634602]. 
=============================================
[2019-04-27 22:16:15,066] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.1296731e-33 1.0000000e+00 1.1382918e-36 0.0000000e+00 1.0529694e-35], sum to 1.0000
[2019-04-27 22:16:15,077] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0052
[2019-04-27 22:16:15,079] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.63333333333333, 91.0, 1.0, 2.0, 0.4636261297287627, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 563491.4813876618, 563491.4813876613, 137650.8263569096], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5802000.0000, 
sim time next is 5802600.0000, 
raw observation next is [21.56666666666667, 91.5, 1.0, 2.0, 0.4598538989826133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 558960.1968251583, 558960.1968251579, 137082.1408223214], 
processed observation next is [1.0, 0.13043478260869565, 0.35432098765432113, 0.915, 1.0, 1.0, 0.35696892736025393, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.19962864172327083, 0.19962864172327066, 0.2636195015813873], 
reward next is 0.7364, 
noisyNet noise sample is [array([-0.9429439], dtype=float32), -1.3593663]. 
=============================================
[2019-04-27 22:16:24,112] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:16:24,120] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4447
[2019-04-27 22:16:24,126] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.36666666666667, 74.33333333333334, 1.0, 2.0, 0.4733333948453738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 571272.1156407197, 571272.1156407197, 139002.6003826506], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6816000.0000, 
sim time next is 6816600.0000, 
raw observation next is [24.3, 75.0, 1.0, 2.0, 0.474378115301586, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 572217.097322017, 572217.097322017, 139152.044865565], 
processed observation next is [1.0, 0.9130434782608695, 0.4555555555555556, 0.75, 1.0, 1.0, 0.37425966107331665, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.20436324904357747, 0.20436324904357747, 0.2676000862799327], 
reward next is 0.7324, 
noisyNet noise sample is [array([-0.53115374], dtype=float32), -0.055741083]. 
=============================================
[2019-04-27 22:16:36,988] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-27 22:16:36,996] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3707
[2019-04-27 22:16:37,001] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.86666666666667, 67.5, 1.0, 2.0, 0.5392515466586398, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 635332.6529971688, 635332.6529971688, 148891.2081643855], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6210600.0000, 
sim time next is 6211200.0000, 
raw observation next is [26.73333333333333, 68.0, 1.0, 2.0, 0.538118212213793, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 634538.3358142425, 634538.3358142425, 148728.1911120488], 
processed observation next is [1.0, 0.9130434782608695, 0.545679012345679, 0.68, 1.0, 1.0, 0.45014072882594397, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.22662083421937232, 0.22662083421937232, 0.2860157521385554], 
reward next is 0.7140, 
noisyNet noise sample is [array([1.7845062], dtype=float32), 2.0868802]. 
=============================================
[2019-04-27 22:16:37,322] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-27 22:16:37,327] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-27 22:16:37,330] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-27 22:16:37,331] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:16:37,332] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-27 22:16:37,333] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:16:37,334] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-27 22:16:37,333] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-27 22:16:37,336] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:16:37,338] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:16:37,334] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:16:37,363] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run101
[2019-04-27 22:16:37,385] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run101
[2019-04-27 22:16:37,410] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run101
[2019-04-27 22:16:37,431] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run101
[2019-04-27 22:16:37,456] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/27/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run101
[2019-04-27 22:16:53,440] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00339081], dtype=float32), -0.011833914]
[2019-04-27 22:16:53,441] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [29.0, 22.5, 1.0, 2.0, 0.3248479743896885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 419052.3155475209, 419052.3155475209, 115580.1898072051]
[2019-04-27 22:16:53,444] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:16:53,447] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3363267778106276
[2019-04-27 22:16:54,861] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00339081], dtype=float32), -0.011833914]
[2019-04-27 22:16:54,864] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [22.48869254, 56.89433419, 1.0, 2.0, 0.2957913154314523, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 378139.1707783945, 378139.170778394, 115058.4882680281]
[2019-04-27 22:16:54,865] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 22:16:54,867] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4045527906922244
[2019-04-27 22:17:02,322] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00339081], dtype=float32), -0.011833914]
[2019-04-27 22:17:02,323] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [26.0, 69.5, 1.0, 2.0, 0.5250115520129501, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 624217.3686515858, 624217.3686515858, 146805.5613199429]
[2019-04-27 22:17:02,325] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:17:02,327] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.42263233622266483
[2019-04-27 22:17:16,020] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00339081], dtype=float32), -0.011833914]
[2019-04-27 22:17:16,021] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [25.11666666666667, 73.0, 1.0, 2.0, 0.9336355089567304, 1.0, 2.0, 0.9336355089567304, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 129.8041706822043, 2157559.962663975, 2157559.962663976, 405141.6101711816]
[2019-04-27 22:17:16,023] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:17:16,028] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [6.8882650e-26 1.0000000e+00 2.3796183e-29 3.7083710e-30 6.0863609e-28], sampled 0.09034487720229378
[2019-04-27 22:17:16,030] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2157559.962663975 W.
[2019-04-27 22:18:19,769] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00339081], dtype=float32), -0.011833914]
[2019-04-27 22:18:19,770] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [20.868729465, 71.684023155, 1.0, 2.0, 0.4539438564451794, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 576871.836554286, 576871.836554286, 136699.7397828353]
[2019-04-27 22:18:19,771] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:18:19,772] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.1043013e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.49959698489199145
[2019-04-27 22:18:27,809] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00339081], dtype=float32), -0.011833914]
[2019-04-27 22:18:27,809] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [30.73333333333333, 36.66666666666667, 1.0, 2.0, 0.5284274181733352, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8571377107471316, 6.911199999999999, 6.9112, 121.9260426156618, 1277969.018783156, 1277969.018783156, 264841.818471587]
[2019-04-27 22:18:27,810] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 22:18:27,814] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6451643841979028
[2019-04-27 22:18:29,310] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8099.2132 2445444095.4335 746.0000
[2019-04-27 22:18:29,334] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8582.8795 2248773970.1662 553.0000
[2019-04-27 22:18:29,372] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8923.4199 2120533710.1715 430.0000
[2019-04-27 22:18:29,382] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8770.2626 2170712315.3188 493.0000
[2019-04-27 22:18:29,516] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8700.8700 2195155380.8105 572.0000
[2019-04-27 22:18:30,534] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 2500000, evaluation results [2500000.0, 8099.213190057684, 2445444095.433475, 746.0, 8770.262620159681, 2170712315.3187604, 493.0, 8923.419851553375, 2120533710.171511, 430.0, 8582.879467926026, 2248773970.1662407, 553.0, 8700.870031992405, 2195155380.8105125, 572.0]
