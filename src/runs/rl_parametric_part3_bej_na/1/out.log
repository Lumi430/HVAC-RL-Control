Using TensorFlow backend.
[2019-04-02 17:33:10,343] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='part3_v1', activation='linear', agent_num=5, check_args_only=False, clip_norm=5.0, debug_log_prob=0.0005, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part3-NA-Bej-Train-v1', eval_act_func='part3_bej_det_v1', eval_env_res_max_keep=50, eval_epi_num=1, eval_freq=25000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_warm_start=False, job_mode='Train', learning_rate=0.001, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=1000000, metric_func='part3_v1', model_dir='None', model_param=[19, 1], model_type='nn', num_threads=16, output='./Part3-NA-Bej-Train-v1-res1', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part3_v3', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=1.0, rwd_p_para=1.0, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=17, test_env=['Part3-NA-Bej-Test-v1', 'Part3-NA-Bej-Test-v2', 'Part3-NA-Bej-Test-v3', 'Part3-NA-Bej-Test-v4'], test_mode='Multiple', train_act_func='part3_bej_sto_v1', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=50.0, weight_initer='glorot_uniform', window_len=26)
[2019-04-02 17:33:10,344] A3C_AGENT_MAIN INFO:Start compiling...
2019-04-02 17:33:10.377584: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-04-02 17:33:27,252] A3C_AGENT_MAIN INFO:Start the learning...
[2019-04-02 17:33:27,252] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part3-NA-Bej-Train-v1', 'Part3-NA-Bej-Test-v1', 'Part3-NA-Bej-Test-v2', 'Part3-NA-Bej-Test-v3', 'Part3-NA-Bej-Test-v4'] ...
[2019-04-02 17:33:27,264] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation worker starts!
[2019-04-02 17:33:27,266] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation worker starts!
[2019-04-02 17:33:27,270] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation worker starts!
[2019-04-02 17:33:27,273] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation worker starts!
[2019-04-02 17:33:27,279] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation worker starts!
[2019-04-02 17:33:27,279] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-02 17:33:27,280] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-04-02 17:33:27,333] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:33:27,334] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res2/Eplus-env-sub_run1
[2019-04-02 17:33:28,281] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-02 17:33:28,282] A3C_AGENT_WORKER-Thread-3 INFO:Local worker starts!
[2019-04-02 17:33:28,353] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:33:28,354] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res3/Eplus-env-sub_run1
[2019-04-02 17:33:28,521] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-02 17:33:28,522] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 17:33:28,522] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 17:33:28,522] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:33:28,522] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 17:33:28,522] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:33:28,522] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 17:33:28,523] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:33:28,523] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 17:33:28,523] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:33:28,523] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:33:28,525] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run1
[2019-04-02 17:33:28,526] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run1
[2019-04-02 17:33:28,526] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run1
[2019-04-02 17:33:28,553] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run1
[2019-04-02 17:33:28,553] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run1
[2019-04-02 17:33:29,283] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-02 17:33:29,284] A3C_AGENT_WORKER-Thread-9 INFO:Local worker starts!
[2019-04-02 17:33:29,367] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:33:29,369] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res4/Eplus-env-sub_run1
[2019-04-02 17:33:30,285] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-02 17:33:30,290] A3C_AGENT_WORKER-Thread-10 INFO:Local worker starts!
[2019-04-02 17:33:30,362] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:33:30,366] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res5/Eplus-env-sub_run1
[2019-04-02 17:33:31,289] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-02 17:33:31,292] A3C_AGENT_WORKER-Thread-11 INFO:Local worker starts!
[2019-04-02 17:33:31,384] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:33:31,385] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res6/Eplus-env-sub_run1
[2019-04-02 17:33:32,293] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-02 17:33:32,296] A3C_AGENT_WORKER-Thread-12 INFO:Local worker starts!
[2019-04-02 17:33:32,376] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:33:32,378] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res7/Eplus-env-sub_run1
[2019-04-02 17:33:33,297] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-02 17:33:33,302] A3C_AGENT_WORKER-Thread-13 INFO:Local worker starts!
[2019-04-02 17:33:33,360] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:33:33,360] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res8/Eplus-env-sub_run1
[2019-04-02 17:33:34,301] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-02 17:33:34,303] A3C_AGENT_WORKER-Thread-14 INFO:Local worker starts!
[2019-04-02 17:33:34,363] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:33:34,364] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res9/Eplus-env-sub_run1
[2019-04-02 17:33:35,304] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-02 17:33:35,307] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-04-02 17:33:35,365] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:33:35,366] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res10/Eplus-env-sub_run1
[2019-04-02 17:33:36,307] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-02 17:33:36,310] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-04-02 17:33:36,366] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:33:36,367] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res11/Eplus-env-sub_run1
[2019-04-02 17:33:37,310] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-02 17:33:37,311] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-04-02 17:33:37,383] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:33:37,399] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res12/Eplus-env-sub_run1
[2019-04-02 17:33:38,312] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-02 17:33:38,317] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-04-02 17:33:38,373] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:33:38,374] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res13/Eplus-env-sub_run1
[2019-04-02 17:33:39,319] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-02 17:33:39,324] A3C_AGENT_WORKER-Thread-19 INFO:Local worker starts!
[2019-04-02 17:33:39,446] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:33:39,447] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res14/Eplus-env-sub_run1
[2019-04-02 17:33:40,324] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-02 17:33:40,327] A3C_AGENT_WORKER-Thread-20 INFO:Local worker starts!
[2019-04-02 17:33:40,415] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:33:40,416] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res15/Eplus-env-sub_run1
[2019-04-02 17:33:41,327] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-02 17:33:41,331] A3C_AGENT_WORKER-Thread-21 INFO:Local worker starts!
[2019-04-02 17:33:41,410] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:33:41,425] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res16/Eplus-env-sub_run1
[2019-04-02 17:33:42,333] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-02 17:33:42,341] A3C_AGENT_WORKER-Thread-22 INFO:Local worker starts!
[2019-04-02 17:33:42,420] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:33:42,421] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res17/Eplus-env-sub_run1
[2019-04-02 17:34:00,188] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-02 17:34:00,188] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [26.6, 48.5, 1.0, 2.0, 0.1908735530111096, 0.0, 1.0, 0.0, 1.0, 2.0, 0.3165067113440675, 6.9112, 6.9112, 121.9260426156618, 472719.9714244336, 472719.9714244336, 165481.7160875034]
[2019-04-02 17:34:00,189] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 17:34:00,191] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0.05293103 0.39059335 0.12858434 0.14823572 0.27965546], sampled 0.6264328924578988
[2019-04-02 17:34:03,567] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-02 17:34:03,570] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [26.33333333333334, 65.66666666666667, 1.0, 2.0, 0.2463127282113008, 1.0, 1.0, 0.2463127282113008, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 583677.9767435754, 583677.9767435759, 169142.7150863881]
[2019-04-02 17:34:03,571] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 17:34:03,572] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0.05027653 0.27049008 0.15732768 0.16691789 0.3549879 ], sampled 0.5327879025505065
[2019-04-02 17:34:38,283] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-02 17:34:38,284] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [28.4, 53.0, 1.0, 2.0, 0.4460637419343499, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 534102.4651680256, 534102.4651680256, 134750.7019882827]
[2019-04-02 17:34:38,284] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 17:34:38,286] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [0.03464851 0.27723658 0.12199657 0.18115024 0.38496807], sampled 0.4634813651071318
[2019-04-02 17:35:22,571] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 3816.7640 2589488241.2574 269.0000
[2019-04-02 17:35:22,798] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 3828.0820 2532517066.1957 256.0000
[2019-04-02 17:35:22,977] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 3797.6981 2562321495.0828 286.0000
[2019-04-02 17:35:23,028] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 3908.8495 2498763370.4279 202.0000
[2019-04-02 17:35:23,109] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 3707.4386 2768818173.8209 381.0000
[2019-04-02 17:35:24,124] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 3707.438639028382, 2768818173.820901, 381.0, 3828.081994032037, 2532517066.1957173, 256.0, 3908.8495286755438, 2498763370.427884, 202.0, 3816.763985776717, 2589488241.2574253, 269.0, 3797.698059106476, 2562321495.082785, 286.0]
[2019-04-02 17:35:27,843] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.01018467 0.72463053 0.03103781 0.01490324 0.21924369], sum to 1.0000
[2019-04-02 17:35:27,851] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1166
[2019-04-02 17:35:27,929] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.46666666666667, 49.5, 1.0, 2.0, 0.16, 0.0, 2.0, 0.0, 1.0, 1.0, 0.2477686025324115, 6.911199999999999, 6.9112, 121.9260426156618, 355603.8023152149, 355603.8023152154, 148524.4153684408], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 28200.0000, 
sim time next is 28800.0000, 
raw observation next is [22.8, 48.0, 1.0, 2.0, 0.2911862324136336, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 375390.9958524862, 375390.9958524862, 112993.690085798], 
processed observation next is [1.0, 0.34782608695652173, 0.4, 0.48, 1.0, 1.0, 0.1561740862067067, 0.0, 1.0, -0.1904761904761905, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288201359, 0.13406821280445935, 0.13406821280445935, 0.21729555785730387], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.53119147], dtype=float32), -0.082507074]. 
=============================================
[2019-04-02 17:35:46,681] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 7901: loss 0.1599
[2019-04-02 17:35:46,731] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 7902: learning rate 0.0010
[2019-04-02 17:35:46,800] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 7930: loss 0.0696
[2019-04-02 17:35:46,802] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 7930: learning rate 0.0010
[2019-04-02 17:35:46,818] A3C_AGENT_WORKER-Thread-10 INFO:Local step 500, global step 7939: loss 0.1388
[2019-04-02 17:35:46,820] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 500, global step 7939: learning rate 0.0010
[2019-04-02 17:35:46,850] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 7953: loss 0.0189
[2019-04-02 17:35:46,856] A3C_AGENT_WORKER-Thread-9 INFO:Local step 500, global step 7955: loss 0.0119
[2019-04-02 17:35:46,856] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 7955: learning rate 0.0010
[2019-04-02 17:35:46,859] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 500, global step 7955: learning rate 0.0010
[2019-04-02 17:35:46,910] A3C_AGENT_WORKER-Thread-22 INFO:Local step 500, global step 7978: loss 0.0062
[2019-04-02 17:35:46,910] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 500, global step 7978: learning rate 0.0010
[2019-04-02 17:35:46,920] A3C_AGENT_WORKER-Thread-21 INFO:Local step 500, global step 7981: loss 0.0002
[2019-04-02 17:35:46,920] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 500, global step 7981: learning rate 0.0010
[2019-04-02 17:35:46,937] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 7986: loss 0.0144
[2019-04-02 17:35:46,943] A3C_AGENT_WORKER-Thread-13 INFO:Local step 500, global step 7988: loss 0.0261
[2019-04-02 17:35:46,943] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 7987: learning rate 0.0010
[2019-04-02 17:35:46,946] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 500, global step 7988: learning rate 0.0010
[2019-04-02 17:35:46,949] A3C_AGENT_WORKER-Thread-12 INFO:Local step 500, global step 7989: loss 0.0082
[2019-04-02 17:35:46,951] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 500, global step 7989: learning rate 0.0010
[2019-04-02 17:35:46,970] A3C_AGENT_WORKER-Thread-11 INFO:Local step 500, global step 7996: loss 0.0192
[2019-04-02 17:35:46,977] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 500, global step 7996: learning rate 0.0010
[2019-04-02 17:35:46,998] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 8003: loss 0.0088
[2019-04-02 17:35:47,001] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 8004: learning rate 0.0010
[2019-04-02 17:35:47,038] A3C_AGENT_WORKER-Thread-14 INFO:Local step 500, global step 8019: loss 0.0833
[2019-04-02 17:35:47,041] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 500, global step 8019: learning rate 0.0010
[2019-04-02 17:35:47,085] A3C_AGENT_WORKER-Thread-20 INFO:Local step 500, global step 8036: loss 0.1803
[2019-04-02 17:35:47,090] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 500, global step 8037: learning rate 0.0010
[2019-04-02 17:35:47,126] A3C_AGENT_WORKER-Thread-19 INFO:Local step 500, global step 8054: loss 0.1801
[2019-04-02 17:35:47,131] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 500, global step 8054: learning rate 0.0010
[2019-04-02 17:35:47,206] A3C_AGENT_WORKER-Thread-3 INFO:Local step 500, global step 8080: loss 0.1174
[2019-04-02 17:35:47,208] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 500, global step 8081: learning rate 0.0010
[2019-04-02 17:35:47,678] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.7699572e-26 1.0000000e+00 2.8564450e-26 6.8549589e-23 3.9845319e-22], sum to 1.0000
[2019-04-02 17:35:47,690] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7013
[2019-04-02 17:35:47,785] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.7, 32.0, 1.0, 2.0, 0.3312439299839069, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 423883.0265279144, 423883.0265279149, 119524.9374939313], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 309600.0000, 
sim time next is 310200.0000, 
raw observation next is [27.71666666666667, 31.83333333333334, 1.0, 2.0, 0.3312556315785966, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 423848.5355148942, 423848.5355148942, 119526.3398379616], 
processed observation next is [0.0, 0.6086956521739131, 0.5820987654320988, 0.3183333333333334, 1.0, 1.0, 0.20387575187928167, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.15137447696960507, 0.15137447696960507, 0.22985834584223383], 
reward next is 0.7701, 
noisyNet noise sample is [array([1.3639563], dtype=float32), 2.4850762]. 
=============================================
[2019-04-02 17:35:52,498] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.4991173e-28 1.0000000e+00 1.3262423e-28 2.2927438e-24 7.0558237e-23], sum to 1.0000
[2019-04-02 17:35:52,507] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7667
[2019-04-02 17:35:52,609] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 34.5, 1.0, 2.0, 0.8563181533836159, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.922452020368901, 6.9112, 121.926002644575, 1104591.097022721, 1098829.058788648, 211932.8331543257], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 378600.0000, 
sim time next is 379200.0000, 
raw observation next is [26.6, 34.0, 1.0, 2.0, 0.8664285141071355, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.988044668033963, 6.9112, 121.9256190318753, 1150351.335440977, 1111000.133819749, 214196.4493788135], 
processed observation next is [1.0, 0.391304347826087, 0.5407407407407407, 0.34, 1.0, 1.0, 0.8409863263180185, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.007684466803396272, 0.0, 0.8094593166642897, 0.4108397626574918, 0.3967857620784818, 0.41191624880541056], 
reward next is 0.2039, 
noisyNet noise sample is [array([0.606901], dtype=float32), -1.6946385]. 
=============================================
[2019-04-02 17:35:54,501] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.1237200e-10 6.8266386e-01 3.1690374e-01 2.0645923e-04 2.2599981e-04], sum to 1.0000
[2019-04-02 17:35:54,512] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9508
[2019-04-02 17:35:54,602] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.8, 27.0, 1.0, 2.0, 0.5600301471064562, 1.0, 1.0, 0.5600301471064562, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9256883684695, 1389407.011963543, 1389407.011963543, 258828.7665530628], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 406800.0000, 
sim time next is 407400.0000, 
raw observation next is [30.65, 27.33333333333333, 1.0, 2.0, 0.2569894706035736, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4307698431133909, 6.911199999999999, 6.9112, 121.926042507641, 641824.4017614033, 641824.4017614038, 180298.3415663377], 
processed observation next is [1.0, 0.7391304347826086, 0.6907407407407407, 0.27333333333333326, 1.0, 1.0, 0.11546365548044474, 0.0, 0.5, -0.1904761904761905, 1.0, 0.5, 0.2884623038917386, -8.881784197001253e-17, 0.0, 0.8094621281029901, 0.2292230006290726, 0.22922300062907278, 0.3467275799352648], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.55795825], dtype=float32), 0.8942371]. 
=============================================
[2019-04-02 17:36:04,031] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.0100836e-28 1.0000000e+00 8.6335442e-22 2.3077023e-26 9.5441342e-25], sum to 1.0000
[2019-04-02 17:36:04,047] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6317
[2019-04-02 17:36:04,138] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.6, 61.83333333333334, 1.0, 2.0, 0.4820516630407299, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 610325.6266807457, 610325.6266807457, 140995.0138968359], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 546600.0000, 
sim time next is 547200.0000, 
raw observation next is [22.8, 61.0, 1.0, 2.0, 0.4468045129692679, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 565374.5075995541, 565374.5075995541, 135605.1980425162], 
processed observation next is [1.0, 0.34782608695652173, 0.4, 0.61, 1.0, 1.0, 0.3414339440110332, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.20191946699984076, 0.20191946699984076, 0.2607792270048388], 
reward next is 0.7392, 
noisyNet noise sample is [array([-0.9408167], dtype=float32), -0.8252512]. 
=============================================
[2019-04-02 17:36:07,905] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 15876: loss 0.2514
[2019-04-02 17:36:07,907] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 15876: learning rate 0.0010
[2019-04-02 17:36:07,917] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1000, global step 15880: loss 0.5530
[2019-04-02 17:36:07,919] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1000, global step 15880: learning rate 0.0010
[2019-04-02 17:36:08,081] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 15940: loss -1.2391
[2019-04-02 17:36:08,087] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 15941: learning rate 0.0010
[2019-04-02 17:36:08,128] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1000, global step 15958: loss -1.0792
[2019-04-02 17:36:08,136] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1000, global step 15960: learning rate 0.0010
[2019-04-02 17:36:08,156] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 15970: loss 0.0748
[2019-04-02 17:36:08,163] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 15970: learning rate 0.0010
[2019-04-02 17:36:08,165] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 15971: loss 0.0139
[2019-04-02 17:36:08,171] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 15971: learning rate 0.0010
[2019-04-02 17:36:08,190] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 15981: loss 0.2756
[2019-04-02 17:36:08,193] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 15982: learning rate 0.0010
[2019-04-02 17:36:08,203] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1000, global step 15987: loss 0.1965
[2019-04-02 17:36:08,208] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1000, global step 15987: learning rate 0.0010
[2019-04-02 17:36:08,220] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1000, global step 15990: loss 0.1097
[2019-04-02 17:36:08,224] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1000, global step 15993: learning rate 0.0010
[2019-04-02 17:36:08,230] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1000, global step 15995: loss 0.0563
[2019-04-02 17:36:08,232] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1000, global step 15995: learning rate 0.0010
[2019-04-02 17:36:08,244] A3C_AGENT_WORKER-Thread-22 INFO:Local step 1000, global step 15998: loss 0.0531
[2019-04-02 17:36:08,247] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 1000, global step 15999: learning rate 0.0010
[2019-04-02 17:36:08,292] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1000, global step 16016: loss 0.2980
[2019-04-02 17:36:08,295] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1000, global step 16016: learning rate 0.0010
[2019-04-02 17:36:08,302] A3C_AGENT_WORKER-Thread-21 INFO:Local step 1000, global step 16021: loss 0.0317
[2019-04-02 17:36:08,306] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 1000, global step 16022: learning rate 0.0010
[2019-04-02 17:36:08,394] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1000, global step 16059: loss 0.5002
[2019-04-02 17:36:08,396] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1000, global step 16059: learning rate 0.0010
[2019-04-02 17:36:08,415] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1000, global step 16067: loss 0.0151
[2019-04-02 17:36:08,417] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1000, global step 16067: learning rate 0.0010
[2019-04-02 17:36:08,520] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1000, global step 16104: loss 0.1222
[2019-04-02 17:36:08,523] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1000, global step 16105: learning rate 0.0010
[2019-04-02 17:36:28,539] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.9252739e-36 1.0000000e+00 6.1162564e-32 5.3034513e-30 4.3508003e-30], sum to 1.0000
[2019-04-02 17:36:28,548] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9605
[2019-04-02 17:36:28,646] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 66.0, 1.0, 2.0, 0.3330647910775799, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 421678.1685356441, 421678.1685356436, 119737.2455207498], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 891000.0000, 
sim time next is 891600.0000, 
raw observation next is [22.06666666666667, 66.0, 1.0, 2.0, 0.3346968079896203, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 423451.2892866416, 423451.2892866416, 119945.7465275098], 
processed observation next is [0.0, 0.30434782608695654, 0.3728395061728396, 0.66, 1.0, 1.0, 0.2079723904638337, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1512326033166577, 0.1512326033166577, 0.23066489716828806], 
reward next is 0.7693, 
noisyNet noise sample is [array([0.28407606], dtype=float32), 0.9930325]. 
=============================================
[2019-04-02 17:36:29,093] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 23833: loss 0.2847
[2019-04-02 17:36:29,097] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 23834: learning rate 0.0010
[2019-04-02 17:36:29,206] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1500, global step 23876: loss 0.1151
[2019-04-02 17:36:29,209] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1500, global step 23877: learning rate 0.0010
[2019-04-02 17:36:29,342] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 23925: loss 0.0030
[2019-04-02 17:36:29,347] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 23926: learning rate 0.0010
[2019-04-02 17:36:29,391] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1500, global step 23947: loss 0.0064
[2019-04-02 17:36:29,396] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1500, global step 23947: learning rate 0.0010
[2019-04-02 17:36:29,418] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1500, global step 23955: loss 0.0202
[2019-04-02 17:36:29,423] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1500, global step 23955: learning rate 0.0010
[2019-04-02 17:36:29,467] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 23976: loss 0.0268
[2019-04-02 17:36:29,469] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 23976: learning rate 0.0010
[2019-04-02 17:36:29,476] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 23978: loss 0.0338
[2019-04-02 17:36:29,477] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 23978: learning rate 0.0010
[2019-04-02 17:36:29,486] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 23980: loss 0.1288
[2019-04-02 17:36:29,490] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 23981: learning rate 0.0010
[2019-04-02 17:36:29,503] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1500, global step 23985: loss 0.0883
[2019-04-02 17:36:29,506] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1500, global step 23986: learning rate 0.0010
[2019-04-02 17:36:29,526] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1500, global step 23993: loss 0.0166
[2019-04-02 17:36:29,531] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1500, global step 23993: learning rate 0.0010
[2019-04-02 17:36:29,563] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1500, global step 24007: loss 0.0373
[2019-04-02 17:36:29,565] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1500, global step 24007: learning rate 0.0010
[2019-04-02 17:36:29,590] A3C_AGENT_WORKER-Thread-22 INFO:Local step 1500, global step 24017: loss 0.0339
[2019-04-02 17:36:29,592] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 1500, global step 24017: learning rate 0.0010
[2019-04-02 17:36:29,632] A3C_AGENT_WORKER-Thread-21 INFO:Local step 1500, global step 24035: loss 0.0619
[2019-04-02 17:36:29,634] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 1500, global step 24035: learning rate 0.0010
[2019-04-02 17:36:29,683] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1500, global step 24055: loss 0.0007
[2019-04-02 17:36:29,686] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1500, global step 24055: learning rate 0.0010
[2019-04-02 17:36:29,769] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1500, global step 24084: loss 0.0194
[2019-04-02 17:36:29,774] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1500, global step 24084: learning rate 0.0010
[2019-04-02 17:36:29,961] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1500, global step 24157: loss 0.1580
[2019-04-02 17:36:29,965] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1500, global step 24158: learning rate 0.0010
[2019-04-02 17:36:32,212] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-02 17:36:32,217] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 17:36:32,218] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:36:32,220] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 17:36:32,220] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:36:32,222] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 17:36:32,222] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:36:32,223] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 17:36:32,224] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 17:36:32,225] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:36:32,225] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:36:32,243] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run2
[2019-04-02 17:36:32,243] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run2
[2019-04-02 17:36:32,244] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run2
[2019-04-02 17:36:32,301] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run2
[2019-04-02 17:36:32,302] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run2
[2019-04-02 17:36:33,210] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.0678495], dtype=float32), 0.15282759]
[2019-04-02 17:36:33,211] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [15.5, 85.0, 1.0, 2.0, 0.2003944577492719, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 258480.8270723998, 258480.8270724003, 83342.13489199836]
[2019-04-02 17:36:33,211] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 17:36:33,212] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [3.2554356e-27 1.0000000e+00 1.1992069e-21 5.7330131e-25 4.1986546e-24], sampled 0.4957396011284081
[2019-04-02 17:36:36,636] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0678495], dtype=float32), 0.15282759]
[2019-04-02 17:36:36,637] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [34.32723272, 13.3064862, 1.0, 2.0, 0.3775525640657537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 485017.386010933, 485017.386010933, 125693.3866002367]
[2019-04-02 17:36:36,638] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 17:36:36,644] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [2.8209431e-30 1.0000000e+00 7.0215396e-22 3.2938598e-27 9.2264876e-26], sampled 0.17133634446418877
[2019-04-02 17:36:59,953] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0678495], dtype=float32), 0.15282759]
[2019-04-02 17:36:59,955] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [25.32535497333333, 72.11607992, 1.0, 2.0, 0.5025620718342779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 599694.8742818688, 599694.8742818688, 143308.3560834799]
[2019-04-02 17:36:59,957] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 17:36:59,961] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.2643637e-31 1.0000000e+00 8.5420105e-23 1.1413054e-28 8.2042636e-27], sampled 0.8247723316775148
[2019-04-02 17:37:23,332] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0678495], dtype=float32), 0.15282759]
[2019-04-02 17:37:23,333] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [37.28353722, 37.49990923, 1.0, 2.0, 0.716278772972921, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.9260426156618, 1531381.750215302, 1531381.750215303, 320206.8065988087]
[2019-04-02 17:37:23,333] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 17:37:23,334] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [2.9775751e-32 1.0000000e+00 4.9939060e-24 5.1855005e-30 3.4053463e-28], sampled 0.09053505396330364
[2019-04-02 17:37:23,334] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1531381.750215302 W.
[2019-04-02 17:37:39,137] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0678495], dtype=float32), 0.15282759]
[2019-04-02 17:37:39,138] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [28.0, 70.0, 1.0, 2.0, 0.7464014997144897, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.911200000000001, 6.9112, 121.9260426156618, 1565764.682659113, 1565764.682659113, 325829.7288267894]
[2019-04-02 17:37:39,141] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 17:37:39,145] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [2.6149359e-36 1.0000000e+00 7.1190796e-33 7.2958201e-36 8.2717626e-35], sampled 0.8700254936710903
[2019-04-02 17:37:39,146] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1565764.682659113 W.
[2019-04-02 17:37:52,066] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.0678495], dtype=float32), 0.15282759]
[2019-04-02 17:37:52,068] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [26.0, 64.33333333333334, 1.0, 2.0, 0.4660661742311069, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 562178.9199811325, 562178.9199811325, 137884.9613130523]
[2019-04-02 17:37:52,068] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 17:37:52,071] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [8.1614155e-32 1.0000000e+00 1.5108814e-22 8.6755741e-29 7.5056001e-27], sampled 0.3334990681088592
[2019-04-02 17:38:32,340] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0678495], dtype=float32), 0.15282759]
[2019-04-02 17:38:32,342] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [27.0, 40.0, 1.0, 2.0, 0.3254182045360555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 411616.0128365167, 411616.0128365162, 118749.0406795288]
[2019-04-02 17:38:32,343] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 17:38:32,346] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.4179615e-30 1.0000000e+00 8.2840025e-23 1.3040241e-27 2.9587094e-26], sampled 0.17050576943694695
[2019-04-02 17:38:38,937] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8584.5265 2248663206.3456 553.0000
[2019-04-02 17:38:39,380] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8701.5773 2195106196.3969 572.0000
[2019-04-02 17:38:39,511] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8924.1625 2120466133.0938 430.0000
[2019-04-02 17:38:39,563] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8771.0839 2170604073.8558 493.0000
[2019-04-02 17:38:39,728] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8100.7222 2445312557.4025 746.0000
[2019-04-02 17:38:40,746] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 25000, evaluation results [25000.0, 8100.722247181571, 2445312557.402538, 746.0, 8771.08387552638, 2170604073.8558493, 493.0, 8924.162528137549, 2120466133.0938108, 430.0, 8584.526542641555, 2248663206.345631, 553.0, 8701.577341373517, 2195106196.396876, 572.0]
[2019-04-02 17:38:43,101] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0915072e-36 1.0000000e+00 4.5729473e-28 3.1261432e-32 4.3410145e-32], sum to 1.0000
[2019-04-02 17:38:43,111] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0535
[2019-04-02 17:38:43,212] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.51666666666667, 60.83333333333334, 1.0, 2.0, 0.2724598695039858, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 351456.4280997167, 351456.4280997167, 109856.3376129677], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 969000.0000, 
sim time next is 969600.0000, 
raw observation next is [20.63333333333333, 60.66666666666667, 1.0, 2.0, 0.2588584172150412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 333907.5741308543, 333907.5741308543, 109453.434273958], 
processed observation next is [1.0, 0.21739130434782608, 0.3197530864197529, 0.6066666666666667, 1.0, 1.0, 0.11768859192266812, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.11925270504673367, 0.11925270504673367, 0.21048737360376538], 
reward next is 0.7895, 
noisyNet noise sample is [array([0.9814348], dtype=float32), -0.050663404]. 
=============================================
[2019-04-02 17:38:46,356] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4090826e-12 6.6120435e-08 9.9999988e-01 1.2055672e-12 1.1259689e-09], sum to 1.0000
[2019-04-02 17:38:46,370] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8948
[2019-04-02 17:38:46,476] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [21.3, 65.0, 1.0, 2.0, 0.16, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2652169237139355, 6.911199999999999, 6.9112, 121.9260426156618, 389668.3233718909, 389668.3233718913, 154636.775983957], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1018800.0000, 
sim time next is 1019400.0000, 
raw observation next is [21.78333333333333, 61.50000000000001, 1.0, 2.0, 0.16, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2637722519446325, 6.911199999999999, 6.9112, 121.9260426156618, 387102.0610127106, 387102.0610127111, 154200.7527221495], 
processed observation next is [1.0, 0.8260869565217391, 0.3623456790123456, 0.6150000000000001, 1.0, 1.0, 0.0, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.0797153149307906, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.13825073607596808, 0.13825073607596824, 0.29653990908105676], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7179654], dtype=float32), 0.004082488]. 
=============================================
[2019-04-02 17:38:58,260] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5161635e-26 1.0000000e+00 2.2243628e-12 1.5656462e-24 4.8198176e-20], sum to 1.0000
[2019-04-02 17:38:58,270] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5142
[2019-04-02 17:38:58,366] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.66666666666667, 78.0, 1.0, 2.0, 0.3541033725295138, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 446331.411384368, 446331.411384368, 122476.5992925784], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1189200.0000, 
sim time next is 1189800.0000, 
raw observation next is [20.6, 78.5, 1.0, 2.0, 0.3545446156438301, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 446883.88297647, 446883.88297647, 122535.2918662251], 
processed observation next is [1.0, 0.782608695652174, 0.3185185185185186, 0.785, 1.0, 1.0, 0.23160073290932157, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1596013867773107, 0.1596013867773107, 0.2356447920504329], 
reward next is 0.7644, 
noisyNet noise sample is [array([0.80719495], dtype=float32), -0.5832751]. 
=============================================
[2019-04-02 17:38:58,779] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 31802: loss 0.0219
[2019-04-02 17:38:58,782] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 31803: learning rate 0.0010
[2019-04-02 17:38:58,945] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2000, global step 31863: loss 0.2195
[2019-04-02 17:38:58,952] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2000, global step 31865: learning rate 0.0010
[2019-04-02 17:38:59,106] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 31927: loss 0.1091
[2019-04-02 17:38:59,108] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 31927: learning rate 0.0010
[2019-04-02 17:38:59,114] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 31929: loss 0.0853
[2019-04-02 17:38:59,118] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 31929: learning rate 0.0010
[2019-04-02 17:38:59,118] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2000, global step 31930: loss 0.1570
[2019-04-02 17:38:59,122] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2000, global step 31930: learning rate 0.0010
[2019-04-02 17:38:59,130] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2000, global step 31932: loss 0.1283
[2019-04-02 17:38:59,132] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2000, global step 31932: learning rate 0.0010
[2019-04-02 17:38:59,176] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 31948: loss 0.0623
[2019-04-02 17:38:59,179] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 31949: learning rate 0.0010
[2019-04-02 17:38:59,266] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 31984: loss 0.0066
[2019-04-02 17:38:59,269] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 31985: learning rate 0.0010
[2019-04-02 17:38:59,286] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2000, global step 31992: loss 0.0012
[2019-04-02 17:38:59,291] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2000, global step 31992: learning rate 0.0010
[2019-04-02 17:38:59,354] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2000, global step 32018: loss 0.0562
[2019-04-02 17:38:59,357] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2000, global step 32019: learning rate 0.0010
[2019-04-02 17:38:59,372] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2000, global step 32025: loss 0.0936
[2019-04-02 17:38:59,374] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2000, global step 32025: learning rate 0.0010
[2019-04-02 17:38:59,376] A3C_AGENT_WORKER-Thread-21 INFO:Local step 2000, global step 32025: loss 0.0793
[2019-04-02 17:38:59,378] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 2000, global step 32025: learning rate 0.0010
[2019-04-02 17:38:59,410] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2000, global step 32036: loss 0.1109
[2019-04-02 17:38:59,413] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2000, global step 32036: learning rate 0.0010
[2019-04-02 17:38:59,482] A3C_AGENT_WORKER-Thread-22 INFO:Local step 2000, global step 32062: loss 0.1115
[2019-04-02 17:38:59,490] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 2000, global step 32062: learning rate 0.0010
[2019-04-02 17:38:59,512] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2000, global step 32069: loss 0.1368
[2019-04-02 17:38:59,515] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2000, global step 32070: learning rate 0.0010
[2019-04-02 17:38:59,997] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2000, global step 32256: loss 0.0325
[2019-04-02 17:39:00,001] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2000, global step 32257: learning rate 0.0010
[2019-04-02 17:39:03,227] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.0960853e-15 4.2725943e-02 9.5727408e-01 7.3413904e-14 2.4317641e-13], sum to 1.0000
[2019-04-02 17:39:03,240] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6742
[2019-04-02 17:39:03,346] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.26666666666667, 60.0, 1.0, 2.0, 0.7298028327134523, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 899476.3311386802, 899476.3311386802, 184625.5844175305], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1257600.0000, 
sim time next is 1258200.0000, 
raw observation next is [25.45, 59.5, 1.0, 2.0, 0.3897611825435842, 0.0, 2.0, 0.0, 1.0, 1.0, 0.636427038548962, 6.911199999999999, 6.9112, 121.9260426156618, 950788.5948200511, 950788.5948200516, 218527.5532660121], 
processed observation next is [1.0, 0.5652173913043478, 0.4981481481481481, 0.595, 1.0, 1.0, 0.27352521731379076, 0.0, 1.0, -0.1904761904761905, 1.0, 0.5, 0.5455337981862024, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.3395673552928754, 0.3395673552928756, 0.42024529474233097], 
reward next is 0.5798, 
noisyNet noise sample is [array([-0.46426788], dtype=float32), -0.78159523]. 
=============================================
[2019-04-02 17:39:07,247] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.9366283e-25 1.0000000e+00 7.5001522e-22 6.9433656e-29 1.4173331e-24], sum to 1.0000
[2019-04-02 17:39:07,261] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6841
[2019-04-02 17:39:07,268] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.96666666666667, 88.66666666666667, 1.0, 2.0, 0.3307690045769958, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 418607.9252199156, 418607.9252199156, 119438.9526042556], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1308000.0000, 
sim time next is 1308600.0000, 
raw observation next is [18.9, 89.0, 1.0, 2.0, 0.3297583092513613, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 417463.4867095518, 417463.4867095518, 119310.0104438398], 
processed observation next is [1.0, 0.13043478260869565, 0.2555555555555555, 0.89, 1.0, 1.0, 0.20209322529923965, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1490941023962685, 0.1490941023962685, 0.229442327776615], 
reward next is 0.7706, 
noisyNet noise sample is [array([1.0389001], dtype=float32), -1.0354948]. 
=============================================
[2019-04-02 17:39:19,979] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 39760: loss 0.1498
[2019-04-02 17:39:19,981] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 39760: learning rate 0.0010
[2019-04-02 17:39:20,185] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2500, global step 39836: loss 0.0164
[2019-04-02 17:39:20,188] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2500, global step 39837: learning rate 0.0010
[2019-04-02 17:39:20,226] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 39849: loss 0.0042
[2019-04-02 17:39:20,229] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 39849: learning rate 0.0010
[2019-04-02 17:39:20,404] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 39914: loss 0.0092
[2019-04-02 17:39:20,409] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 39914: learning rate 0.0010
[2019-04-02 17:39:20,516] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2500, global step 39958: loss 0.0967
[2019-04-02 17:39:20,521] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2500, global step 39959: learning rate 0.0010
[2019-04-02 17:39:20,541] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 39968: loss 0.0462
[2019-04-02 17:39:20,543] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 39968: learning rate 0.0010
[2019-04-02 17:39:20,561] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2500, global step 39976: loss 0.0308
[2019-04-02 17:39:20,564] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2500, global step 39977: learning rate 0.0010
[2019-04-02 17:39:20,616] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2500, global step 39996: loss 0.0046
[2019-04-02 17:39:20,618] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2500, global step 39996: learning rate 0.0010
[2019-04-02 17:39:20,667] A3C_AGENT_WORKER-Thread-22 INFO:Local step 2500, global step 40014: loss 0.0000
[2019-04-02 17:39:20,672] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 2500, global step 40015: learning rate 0.0010
[2019-04-02 17:39:20,714] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2500, global step 40034: loss 0.0018
[2019-04-02 17:39:20,716] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2500, global step 40034: learning rate 0.0010
[2019-04-02 17:39:20,724] A3C_AGENT_WORKER-Thread-21 INFO:Local step 2500, global step 40034: loss 0.0249
[2019-04-02 17:39:20,725] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 40034: loss 0.0235
[2019-04-02 17:39:20,728] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 2500, global step 40035: learning rate 0.0010
[2019-04-02 17:39:20,733] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 40035: learning rate 0.0010
[2019-04-02 17:39:20,758] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2500, global step 40048: loss 0.0112
[2019-04-02 17:39:20,762] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2500, global step 40048: learning rate 0.0010
[2019-04-02 17:39:20,798] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2500, global step 40062: loss 0.0394
[2019-04-02 17:39:20,802] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2500, global step 40062: learning rate 0.0010
[2019-04-02 17:39:20,905] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2500, global step 40099: loss 0.0724
[2019-04-02 17:39:20,908] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2500, global step 40101: learning rate 0.0010
[2019-04-02 17:39:21,231] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2500, global step 40225: loss 0.0857
[2019-04-02 17:39:21,233] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2500, global step 40226: learning rate 0.0010
[2019-04-02 17:39:28,998] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.2874605e-20 1.0000000e+00 1.0468126e-22 1.3248161e-21 4.1894730e-23], sum to 1.0000
[2019-04-02 17:39:29,013] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4530
[2019-04-02 17:39:29,108] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 43.0, 1.0, 2.0, 0.519736329918954, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8552188930019534, 6.911199999999999, 6.9112, 121.9260426156618, 1278824.499201624, 1278824.499201625, 260451.5205299493], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1609200.0000, 
sim time next is 1609800.0000, 
raw observation next is [27.55, 42.83333333333334, 1.0, 2.0, 0.8993231652636864, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.032257440877722, 6.9112, 121.9254237545427, 1181195.776188792, 1119203.861122633, 221242.8595431516], 
processed observation next is [1.0, 0.6521739130434783, 0.575925925925926, 0.42833333333333345, 1.0, 1.0, 0.8801466253139124, 0.0, 1.0, -0.1904761904761905, 0.0, 0.5, -0.25, 0.01210574408777223, 0.0, 0.8094580202258753, 0.42185563435314, 0.3997156646866547, 0.4254670375829838], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6043156], dtype=float32), -2.4654534]. 
=============================================
[2019-04-02 17:39:32,560] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0504806e-30 1.0000000e+00 1.5660817e-32 0.0000000e+00 1.2902530e-35], sum to 1.0000
[2019-04-02 17:39:32,573] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5360
[2019-04-02 17:39:32,583] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.33333333333334, 82.66666666666667, 1.0, 2.0, 0.5498980897402374, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 698191.1266040233, 698191.1266040228, 152004.7875259118], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1676400.0000, 
sim time next is 1677000.0000, 
raw observation next is [19.41666666666666, 82.33333333333334, 1.0, 2.0, 0.5443379154668907, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 690867.2874932265, 690867.2874932265, 151070.7490033311], 
processed observation next is [1.0, 0.391304347826087, 0.27469135802469113, 0.8233333333333335, 1.0, 1.0, 0.4575451374605841, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2467383169618666, 0.2467383169618666, 0.2905206711602521], 
reward next is 0.7095, 
noisyNet noise sample is [array([1.065771], dtype=float32), 1.0677514]. 
=============================================
[2019-04-02 17:39:32,601] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[89.43464 ]
 [89.39758 ]
 [89.38371 ]
 [89.36901 ]
 [89.354805]], R is [[89.2973938 ]
 [89.11209869]
 [88.91940308]
 [88.73104095]
 [88.53949738]].
[2019-04-02 17:39:38,788] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.2923381e-25 1.0000000e+00 8.9117695e-25 2.1466640e-32 2.1450944e-30], sum to 1.0000
[2019-04-02 17:39:38,803] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7308
[2019-04-02 17:39:38,910] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.86666666666667, 78.16666666666667, 1.0, 2.0, 0.4572692586259895, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 567856.8617584355, 567856.8617584355, 137007.1901651579], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1753800.0000, 
sim time next is 1754400.0000, 
raw observation next is [22.03333333333333, 77.33333333333334, 1.0, 2.0, 0.4460606358676412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 553516.7202851606, 553516.7202851601, 135315.3300766536], 
processed observation next is [1.0, 0.30434782608695654, 0.37160493827160485, 0.7733333333333334, 1.0, 1.0, 0.34054837603290616, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1976845429589859, 0.19768454295898574, 0.26022178860894923], 
reward next is 0.7398, 
noisyNet noise sample is [array([0.70794195], dtype=float32), 2.5985103]. 
=============================================
[2019-04-02 17:39:39,159] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.7011868e-21 1.0000000e+00 2.3224866e-18 6.3367895e-27 1.7006434e-21], sum to 1.0000
[2019-04-02 17:39:39,171] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0087
[2019-04-02 17:39:39,179] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.46666666666667, 68.66666666666666, 1.0, 2.0, 0.8809121859698446, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156566, 1075189.661093496, 1075189.661093496, 216437.699444364], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1766400.0000, 
sim time next is 1767000.0000, 
raw observation next is [24.58333333333333, 68.33333333333334, 1.0, 2.0, 0.9081555634360187, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.968540364257877, 6.9112, 121.9257423784489, 1136744.289216264, 1107380.969998616, 222612.5713342432], 
processed observation next is [1.0, 0.43478260869565216, 0.46604938271604923, 0.6833333333333335, 1.0, 1.0, 0.8906613850428794, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.005734036425787714, 0.0, 0.809460135557267, 0.40598010329152284, 0.3954932035709343, 0.4281010987196985], 
reward next is 0.2852, 
noisyNet noise sample is [array([-0.86972797], dtype=float32), 0.42358354]. 
=============================================
[2019-04-02 17:39:39,188] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[63.980007]
 [63.877254]
 [63.729065]
 [63.301643]
 [63.05731 ]], R is [[63.71403122]
 [63.66066742]
 [63.63280487]
 [63.59423828]
 [62.95829773]].
[2019-04-02 17:39:40,725] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.1751525e-20 1.0000000e+00 3.4271311e-23 9.4999545e-27 8.5220959e-22], sum to 1.0000
[2019-04-02 17:39:40,735] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6626
[2019-04-02 17:39:40,740] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 63.5, 1.0, 2.0, 0.5456396597429648, 1.0, 1.0, 0.5456396597429648, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1281204.66188533, 1281204.66188533, 251565.1770393257], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1788600.0000, 
sim time next is 1789200.0000, 
raw observation next is [26.4, 63.0, 1.0, 2.0, 1.02, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.595861156329439, 6.9112, 121.9232416997742, 1570159.349862822, 1219559.697751297, 248503.6321188953], 
processed observation next is [1.0, 0.7391304347826086, 0.5333333333333333, 0.63, 1.0, 1.0, 1.0238095238095237, 0.0, 0.5, -0.1904761904761905, 0.0, 1.0, -0.25, 0.06846611563294394, 0.0, 0.8094435336513894, 0.5607711963795793, 0.4355570349111775, 0.4778916002286448], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.3254026], dtype=float32), -1.4867928]. 
=============================================
[2019-04-02 17:39:41,422] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3000, global step 47760: loss 2.9705
[2019-04-02 17:39:41,425] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3000, global step 47760: learning rate 0.0010
[2019-04-02 17:39:41,503] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3000, global step 47789: loss 3.3117
[2019-04-02 17:39:41,504] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3000, global step 47789: learning rate 0.0010
[2019-04-02 17:39:41,707] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3000, global step 47863: loss 8.3577
[2019-04-02 17:39:41,709] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3000, global step 47864: learning rate 0.0010
[2019-04-02 17:39:41,780] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3000, global step 47894: loss 4.4290
[2019-04-02 17:39:41,782] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3000, global step 47894: learning rate 0.0010
[2019-04-02 17:39:41,866] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3000, global step 47926: loss 2.4014
[2019-04-02 17:39:41,868] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3000, global step 47926: learning rate 0.0010
[2019-04-02 17:39:41,949] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3000, global step 47959: loss 4.7537
[2019-04-02 17:39:41,951] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3000, global step 47960: learning rate 0.0010
[2019-04-02 17:39:42,027] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3000, global step 47988: loss 1.2128
[2019-04-02 17:39:42,028] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3000, global step 47989: learning rate 0.0010
[2019-04-02 17:39:42,050] A3C_AGENT_WORKER-Thread-22 INFO:Local step 3000, global step 47997: loss 4.6889
[2019-04-02 17:39:42,054] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3000, global step 48000: loss 2.4445
[2019-04-02 17:39:42,055] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 3000, global step 47999: learning rate 0.0010
[2019-04-02 17:39:42,057] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3000, global step 48000: learning rate 0.0010
[2019-04-02 17:39:42,099] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3000, global step 48013: loss 3.5591
[2019-04-02 17:39:42,102] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3000, global step 48014: learning rate 0.0010
[2019-04-02 17:39:42,174] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3000, global step 48043: loss 1.1038
[2019-04-02 17:39:42,177] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3000, global step 48043: learning rate 0.0010
[2019-04-02 17:39:42,224] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3000, global step 48061: loss 2.2675
[2019-04-02 17:39:42,225] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3000, global step 48061: learning rate 0.0010
[2019-04-02 17:39:42,233] A3C_AGENT_WORKER-Thread-21 INFO:Local step 3000, global step 48064: loss 0.4081
[2019-04-02 17:39:42,238] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 3000, global step 48065: learning rate 0.0010
[2019-04-02 17:39:42,302] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3000, global step 48088: loss 0.8320
[2019-04-02 17:39:42,306] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3000, global step 48090: learning rate 0.0010
[2019-04-02 17:39:42,377] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3000, global step 48117: loss 0.0344
[2019-04-02 17:39:42,382] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3000, global step 48117: learning rate 0.0010
[2019-04-02 17:39:42,687] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3000, global step 48233: loss 1.8932
[2019-04-02 17:39:42,690] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3000, global step 48234: learning rate 0.0010
[2019-04-02 17:39:47,387] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-02 17:39:47,391] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 17:39:47,392] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 17:39:47,394] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 17:39:47,396] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 17:39:47,395] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:39:47,398] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:39:47,399] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:39:47,394] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 17:39:47,398] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:39:47,405] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:39:47,424] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run3
[2019-04-02 17:39:47,424] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run3
[2019-04-02 17:39:47,461] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run3
[2019-04-02 17:39:47,484] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run3
[2019-04-02 17:39:47,484] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run3
[2019-04-02 17:39:55,155] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.13029192], dtype=float32), 0.24667133]
[2019-04-02 17:39:55,157] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [23.00762149, 42.12980094333334, 1.0, 2.0, 0.2731034591674227, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 352286.8097466027, 352286.8097466027, 101634.9807934032]
[2019-04-02 17:39:55,158] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 17:39:55,160] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [3.4005502e-24 1.0000000e+00 2.2224369e-20 2.6411437e-27 6.5502576e-24], sampled 0.7584293744049151
[2019-04-02 17:40:10,145] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.13029192], dtype=float32), 0.24667133]
[2019-04-02 17:40:10,145] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [29.8, 31.5, 1.0, 2.0, 0.3418296607124494, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 430671.4635898568, 430671.4635898568, 120854.0810909905]
[2019-04-02 17:40:10,145] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 17:40:10,148] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [4.0970611e-24 1.0000000e+00 2.9841316e-20 2.6569887e-27 6.2457711e-24], sampled 0.8650979982967856
[2019-04-02 17:40:22,159] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.13029192], dtype=float32), 0.24667133]
[2019-04-02 17:40:22,159] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [27.6087953, 70.546442595, 1.0, 2.0, 0.5851636170201447, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 672640.2673601877, 672640.2673601877, 155827.1768704466]
[2019-04-02 17:40:22,159] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 17:40:22,161] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.2450421e-23 1.0000000e+00 1.1987279e-19 7.9614662e-27 2.0101465e-23], sampled 0.00732303083206165
[2019-04-02 17:40:24,342] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.13029192], dtype=float32), 0.24667133]
[2019-04-02 17:40:24,343] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [23.945093725, 93.76078135, 1.0, 2.0, 0.565847454182332, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 655849.0097480261, 655849.0097480261, 152829.891231253]
[2019-04-02 17:40:24,344] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 17:40:24,348] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.6176274e-23 1.0000000e+00 2.1769264e-19 3.5790265e-27 1.7603177e-23], sampled 0.4096231682059892
[2019-04-02 17:40:46,720] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.13029192], dtype=float32), 0.24667133]
[2019-04-02 17:40:46,721] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [22.83333333333334, 96.66666666666666, 1.0, 2.0, 0.6206477385354021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 728547.207370908, 728547.207370908, 162656.4185023598]
[2019-04-02 17:40:46,723] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 17:40:46,725] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [4.7670208e-23 1.0000000e+00 1.0069039e-18 1.1716746e-26 6.4543937e-23], sampled 0.24871636454619372
[2019-04-02 17:40:49,465] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.13029192], dtype=float32), 0.24667133]
[2019-04-02 17:40:49,465] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [26.5, 68.16666666666667, 1.0, 2.0, 0.5292703047248103, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 626999.795098962, 626999.795098962, 147406.4072229618]
[2019-04-02 17:40:49,466] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 17:40:49,471] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [3.9331393e-23 1.0000000e+00 6.1859865e-19 2.0285694e-26 5.4602276e-23], sampled 0.8169844674670287
[2019-04-02 17:41:07,852] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.13029192], dtype=float32), 0.24667133]
[2019-04-02 17:41:07,852] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [27.7, 60.0, 1.0, 2.0, 0.5075816671668806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 603301.25459124, 603301.25459124, 144012.543108094]
[2019-04-02 17:41:07,853] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 17:41:07,856] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.0356193e-23 1.0000000e+00 2.5721877e-20 8.9335397e-28 3.7780052e-24], sampled 0.944386178601686
[2019-04-02 17:41:28,218] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.13029192], dtype=float32), 0.24667133]
[2019-04-02 17:41:28,218] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [29.90356782666667, 79.82802536, 1.0, 2.0, 0.8625199438267092, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 983147.5457546286, 983147.5457546286, 209206.9587008113]
[2019-04-02 17:41:28,220] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 17:41:28,223] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [5.6759221e-22 1.0000000e+00 5.0406044e-17 9.1839285e-26 9.3981167e-22], sampled 0.5607636694703961
[2019-04-02 17:41:40,652] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.13029192], dtype=float32), 0.24667133]
[2019-04-02 17:41:40,652] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [32.16666666666667, 59.33333333333333, 1.0, 2.0, 0.9180184591434112, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.9260426156618, 1761653.024091106, 1761653.024091107, 360889.445457777]
[2019-04-02 17:41:40,655] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 17:41:40,660] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [6.8135828e-31 1.0000000e+00 1.1372650e-33 2.7255506e-37 6.7080877e-35], sampled 0.6768971223315908
[2019-04-02 17:41:40,660] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1761653.024091106 W.
[2019-04-02 17:41:44,904] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.13029192], dtype=float32), 0.24667133]
[2019-04-02 17:41:44,904] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [24.20066607666666, 69.11307551333333, 1.0, 2.0, 0.4179668963700637, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 513624.4132346618, 513624.4132346618, 131076.5584897358]
[2019-04-02 17:41:44,905] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 17:41:44,908] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [3.34323669e-23 1.00000000e+00 2.56552549e-19 1.75924668e-27
 1.27935584e-23], sampled 0.8716813449532472
[2019-04-02 17:41:54,772] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8701.2692 2195266425.1124 572.0000
[2019-04-02 17:41:54,923] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8584.5265 2248663206.3456 553.0000
[2019-04-02 17:41:54,953] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8924.2184 2120437061.2969 430.0000
[2019-04-02 17:41:55,050] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8100.6463 2445352063.9460 746.0000
[2019-04-02 17:41:55,057] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8771.9095 2170601483.0346 493.0000
[2019-04-02 17:41:56,072] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 50000, evaluation results [50000.0, 8100.6462730594485, 2445352063.9460416, 746.0, 8771.90946536216, 2170601483.03465, 493.0, 8924.218435439243, 2120437061.2969296, 430.0, 8584.526542641555, 2248663206.345631, 553.0, 8701.269209228363, 2195266425.1123557, 572.0]
[2019-04-02 17:41:57,961] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2386805e-22 1.0000000e+00 6.1336164e-17 2.8626503e-27 5.1095842e-24], sum to 1.0000
[2019-04-02 17:41:57,971] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8276
[2019-04-02 17:41:57,976] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.56666666666667, 92.0, 1.0, 2.0, 0.3665851010460713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 458466.5080188482, 458466.5080188482, 124096.5719820149], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1910400.0000, 
sim time next is 1911000.0000, 
raw observation next is [19.58333333333334, 92.0, 1.0, 2.0, 0.3656142412663756, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 457160.9042486206, 457160.9042486206, 123963.8114241449], 
processed observation next is [1.0, 0.08695652173913043, 0.2808641975308644, 0.92, 1.0, 1.0, 0.24477885865044713, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1632717515173645, 0.1632717515173645, 0.2383919450464325], 
reward next is 0.7616, 
noisyNet noise sample is [array([1.3877119], dtype=float32), -0.8090807]. 
=============================================
[2019-04-02 17:41:57,995] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[64.67787 ]
 [64.78313 ]
 [64.812935]
 [64.903885]
 [65.00693 ]], R is [[64.66213989]
 [64.77687073]
 [64.88996887]
 [64.99663544]
 [65.08796692]].
[2019-04-02 17:41:58,487] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0479359e-21 1.0000000e+00 2.3424373e-16 1.4177696e-26 2.2827537e-22], sum to 1.0000
[2019-04-02 17:41:58,498] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9919
[2019-04-02 17:41:58,508] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.66666666666667, 92.0, 1.0, 2.0, 0.3698207687598741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 461735.2258255981, 461735.2258255981, 124520.6615421738], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1906800.0000, 
sim time next is 1907400.0000, 
raw observation next is [19.58333333333333, 92.0, 1.0, 2.0, 0.3663638010087946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 457919.1518491962, 457919.1518491962, 124061.7455398165], 
processed observation next is [1.0, 0.043478260869565216, 0.280864197530864, 0.92, 1.0, 1.0, 0.24567119167713647, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16354255423185576, 0.16354255423185576, 0.2385802798842625], 
reward next is 0.7614, 
noisyNet noise sample is [array([-1.8772147], dtype=float32), 1.3234693]. 
=============================================
[2019-04-02 17:42:03,664] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6486131e-24 1.0000000e+00 3.5002702e-11 7.6011276e-24 2.3960157e-20], sum to 1.0000
[2019-04-02 17:42:03,674] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9675
[2019-04-02 17:42:03,790] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.71666666666667, 83.66666666666667, 1.0, 2.0, 0.5158818936842905, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 614304.2007000119, 614304.2007000115, 145375.1164685083], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1977000.0000, 
sim time next is 1977600.0000, 
raw observation next is [23.33333333333334, 84.33333333333334, 1.0, 2.0, 0.502483169565479, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 601442.6010756982, 601442.6010756982, 143362.7858336034], 
processed observation next is [1.0, 0.9130434782608695, 0.4197530864197533, 0.8433333333333334, 1.0, 1.0, 0.4077180590065226, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21480092895560648, 0.21480092895560648, 0.27569766506462196], 
reward next is 0.7243, 
noisyNet noise sample is [array([0.18809797], dtype=float32), -0.4089921]. 
=============================================
[2019-04-02 17:42:08,046] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.8538340e-21 2.0438439e-01 7.9561561e-01 5.9202332e-23 7.8043541e-16], sum to 1.0000
[2019-04-02 17:42:08,062] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1968
[2019-04-02 17:42:08,073] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 63.0, 1.0, 2.0, 0.2887443496480297, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4599852459017017, 6.911199999999999, 6.9112, 121.9260426156618, 663793.2016338893, 663793.2016338898, 192143.7810388608], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2043000.0000, 
sim time next is 2043600.0000, 
raw observation next is [28.66666666666666, 63.0, 1.0, 2.0, 0.5772777173692822, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 666975.0936751642, 666975.0936751642, 154652.4156911822], 
processed observation next is [0.0, 0.6521739130434783, 0.6172839506172837, 0.63, 1.0, 1.0, 0.4967591873443836, 0.0, 1.0, -0.1904761904761905, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288201359, 0.23820539059827292, 0.23820539059827292, 0.2974084917138119], 
reward next is 0.7026, 
noisyNet noise sample is [array([-0.14619584], dtype=float32), -0.16318843]. 
=============================================
[2019-04-02 17:42:11,300] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3500, global step 55709: loss -3.1127
[2019-04-02 17:42:11,308] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3500, global step 55710: learning rate 0.0010
[2019-04-02 17:42:11,423] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3500, global step 55754: loss 0.0116
[2019-04-02 17:42:11,429] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3500, global step 55756: learning rate 0.0010
[2019-04-02 17:42:11,725] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3500, global step 55867: loss 0.0318
[2019-04-02 17:42:11,728] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3500, global step 55867: learning rate 0.0010
[2019-04-02 17:42:11,841] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3500, global step 55909: loss 0.0091
[2019-04-02 17:42:11,842] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3500, global step 55910: learning rate 0.0010
[2019-04-02 17:42:11,870] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3500, global step 55922: loss 0.0393
[2019-04-02 17:42:11,875] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3500, global step 55922: learning rate 0.0010
[2019-04-02 17:42:11,905] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3500, global step 55934: loss 0.0675
[2019-04-02 17:42:11,909] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3500, global step 55934: learning rate 0.0010
[2019-04-02 17:42:11,995] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3500, global step 55967: loss 0.1597
[2019-04-02 17:42:11,998] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3500, global step 55967: learning rate 0.0010
[2019-04-02 17:42:12,020] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3500, global step 55975: loss 0.0612
[2019-04-02 17:42:12,022] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3500, global step 55975: learning rate 0.0010
[2019-04-02 17:42:12,141] A3C_AGENT_WORKER-Thread-22 INFO:Local step 3500, global step 56019: loss 0.0250
[2019-04-02 17:42:12,146] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 3500, global step 56020: learning rate 0.0010
[2019-04-02 17:42:12,161] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3500, global step 56028: loss 0.0370
[2019-04-02 17:42:12,168] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3500, global step 56030: learning rate 0.0010
[2019-04-02 17:42:12,183] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3500, global step 56033: loss 0.0084
[2019-04-02 17:42:12,187] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3500, global step 56034: learning rate 0.0010
[2019-04-02 17:42:12,211] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3500, global step 56045: loss 0.0001
[2019-04-02 17:42:12,212] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3500, global step 56045: learning rate 0.0010
[2019-04-02 17:42:12,292] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3500, global step 56074: loss 0.0002
[2019-04-02 17:42:12,293] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3500, global step 56074: learning rate 0.0010
[2019-04-02 17:42:12,338] A3C_AGENT_WORKER-Thread-21 INFO:Local step 3500, global step 56090: loss 0.0265
[2019-04-02 17:42:12,340] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 3500, global step 56090: learning rate 0.0010
[2019-04-02 17:42:12,442] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3500, global step 56134: loss 0.0821
[2019-04-02 17:42:12,447] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3500, global step 56134: learning rate 0.0010
[2019-04-02 17:42:12,960] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3500, global step 56327: loss 0.0006
[2019-04-02 17:42:12,966] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3500, global step 56328: learning rate 0.0010
[2019-04-02 17:42:32,603] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4000, global step 63714: loss 0.4299
[2019-04-02 17:42:32,605] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4000, global step 63714: learning rate 0.0010
[2019-04-02 17:42:32,769] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4000, global step 63781: loss 1.3054
[2019-04-02 17:42:32,770] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4000, global step 63781: learning rate 0.0010
[2019-04-02 17:42:32,974] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4000, global step 63858: loss 0.7736
[2019-04-02 17:42:32,978] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4000, global step 63858: learning rate 0.0010
[2019-04-02 17:42:33,087] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4000, global step 63899: loss 0.3926
[2019-04-02 17:42:33,090] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4000, global step 63900: learning rate 0.0010
[2019-04-02 17:42:33,137] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4000, global step 63921: loss 0.2928
[2019-04-02 17:42:33,140] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4000, global step 63921: learning rate 0.0010
[2019-04-02 17:42:33,169] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4000, global step 63932: loss 0.1020
[2019-04-02 17:42:33,174] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4000, global step 63932: learning rate 0.0010
[2019-04-02 17:42:33,193] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4000, global step 63938: loss 0.1195
[2019-04-02 17:42:33,198] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4000, global step 63939: learning rate 0.0010
[2019-04-02 17:42:33,275] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4000, global step 63965: loss 0.0208
[2019-04-02 17:42:33,283] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4000, global step 63967: learning rate 0.0010
[2019-04-02 17:42:33,331] A3C_AGENT_WORKER-Thread-22 INFO:Local step 4000, global step 63989: loss 0.0049
[2019-04-02 17:42:33,335] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 4000, global step 63990: learning rate 0.0010
[2019-04-02 17:42:33,376] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4000, global step 64006: loss 0.0127
[2019-04-02 17:42:33,378] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4000, global step 64007: learning rate 0.0010
[2019-04-02 17:42:33,415] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4000, global step 64015: loss 0.0021
[2019-04-02 17:42:33,416] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4000, global step 64016: learning rate 0.0010
[2019-04-02 17:42:33,559] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4000, global step 64075: loss 0.0030
[2019-04-02 17:42:33,565] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4000, global step 64075: learning rate 0.0010
[2019-04-02 17:42:33,631] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4000, global step 64100: loss 0.0117
[2019-04-02 17:42:33,633] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4000, global step 64100: learning rate 0.0010
[2019-04-02 17:42:33,668] A3C_AGENT_WORKER-Thread-21 INFO:Local step 4000, global step 64112: loss 0.0343
[2019-04-02 17:42:33,671] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 4000, global step 64113: learning rate 0.0010
[2019-04-02 17:42:33,788] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4000, global step 64162: loss 0.0480
[2019-04-02 17:42:33,793] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4000, global step 64164: learning rate 0.0010
[2019-04-02 17:42:34,289] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4000, global step 64347: loss 0.0487
[2019-04-02 17:42:34,293] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4000, global step 64348: learning rate 0.0010
[2019-04-02 17:42:36,253] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.9885464e-01 5.6865743e-15 1.1454027e-03 1.9397160e-17 2.8689929e-15], sum to 1.0000
[2019-04-02 17:42:36,266] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7545
[2019-04-02 17:42:36,272] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.63333333333333, 60.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5831659721531088, 6.9112, 6.9112, 121.9260426156618, 427806.3755617131, 427806.3755617131, 125997.8645966175], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2442000.0000, 
sim time next is 2442600.0000, 
raw observation next is [23.2, 58.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6003126911795054, 6.9112, 6.9112, 121.9260426156618, 441477.4308393403, 441477.4308393403, 128047.325820318], 
processed observation next is [1.0, 0.2608695652173913, 0.4148148148148148, 0.585, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5003908639743817, 0.0, 0.0, 0.8094621288201359, 0.1576705110140501, 0.1576705110140501, 0.24624485734676538], 
reward next is 0.7538, 
noisyNet noise sample is [array([-0.4123972], dtype=float32), 0.7174018]. 
=============================================
[2019-04-02 17:42:37,362] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.3497330e-06 1.4156017e-15 9.9999762e-01 1.3449910e-24 6.8466652e-21], sum to 1.0000
[2019-04-02 17:42:37,372] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9261
[2019-04-02 17:42:37,379] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.33333333333334, 33.0, 1.0, 2.0, 0.4834759741540183, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7958210707898564, 6.911199999999999, 6.9112, 121.9260426156618, 1189932.462587897, 1189932.462587897, 247879.1997284408], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2452800.0000, 
sim time next is 2453400.0000, 
raw observation next is [30.6, 32.0, 1.0, 2.0, 0.4857014592376724, 0.0, 2.0, 0.0, 1.0, 2.0, 0.799903424174612, 6.911199999999997, 6.9112, 121.9260426156618, 1196028.414721749, 1196028.41472175, 248590.3975991192], 
processed observation next is [1.0, 0.391304347826087, 0.688888888888889, 0.32, 1.0, 1.0, 0.38773983242580046, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.7498792802182649, -2.6645352591003756e-16, 0.0, 0.8094621288201359, 0.4271530052577675, 0.42715300525776784, 0.47805845692138305], 
reward next is 0.5219, 
noisyNet noise sample is [array([-0.9869969], dtype=float32), -1.082484]. 
=============================================
[2019-04-02 17:42:37,573] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.8769981e-12 1.4266473e-16 1.0000000e+00 6.0068872e-32 7.9712595e-25], sum to 1.0000
[2019-04-02 17:42:37,583] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6398
[2019-04-02 17:42:37,597] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [33.6, 24.0, 1.0, 2.0, 0.5308084041485569, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8713596362943679, 6.9112, 6.9112, 121.9260426156618, 1302916.448270869, 1302916.448270869, 264595.4106665261], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2462400.0000, 
sim time next is 2463000.0000, 
raw observation next is [33.71666666666667, 23.83333333333333, 1.0, 2.0, 0.7507773160753294, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9559356254154072, 6.911199999999999, 6.9112, 121.9260426156618, 1633645.150455373, 1633645.150455373, 314558.3717387099], 
processed observation next is [1.0, 0.5217391304347826, 0.804320987654321, 0.23833333333333329, 1.0, 1.0, 0.7033063286611064, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.944919531769259, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.5834446965912047, 0.5834446965912047, 0.6049199456513652], 
reward next is 0.3951, 
noisyNet noise sample is [array([-0.57076156], dtype=float32), -0.09969764]. 
=============================================
[2019-04-02 17:42:37,618] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[64.36728 ]
 [64.25338 ]
 [64.14857 ]
 [64.06118 ]
 [63.969204]], R is [[64.01737976]
 [63.86836624]
 [63.72766876]
 [63.59611893]
 [63.47847748]].
[2019-04-02 17:42:42,967] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.2258629e-10 6.5886754e-09 1.0000000e+00 2.9320247e-26 1.5318728e-22], sum to 1.0000
[2019-04-02 17:42:42,979] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7000
[2019-04-02 17:42:42,986] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.63333333333333, 31.33333333333334, 1.0, 2.0, 0.6154410169135489, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9568230765612754, 6.911199999999999, 6.9112, 121.9260426156618, 1463580.523881316, 1463580.523881316, 290878.2900339089], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2544000.0000, 
sim time next is 2544600.0000, 
raw observation next is [31.81666666666667, 30.66666666666666, 1.0, 2.0, 0.6235241843493342, 0.0, 2.0, 0.0, 1.0, 2.0, 0.956790609612666, 6.911200000000001, 6.9112, 121.9260426156618, 1473546.340000025, 1473546.340000025, 292236.2626700189], 
processed observation next is [1.0, 0.43478260869565216, 0.7339506172839507, 0.3066666666666666, 1.0, 1.0, 0.5518145051777789, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.9459882620158324, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.5262665500000089, 0.5262665500000089, 0.5619928128269593], 
reward next is 0.4380, 
noisyNet noise sample is [array([0.44341105], dtype=float32), -0.8398812]. 
=============================================
[2019-04-02 17:42:45,393] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.5836264e-10 5.4512782e-08 1.0000000e+00 1.8543371e-22 2.6372146e-14], sum to 1.0000
[2019-04-02 17:42:45,407] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9681
[2019-04-02 17:42:45,419] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.58333333333333, 50.16666666666666, 1.0, 2.0, 0.2530711794177045, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4059741483679522, 6.9112, 6.9112, 121.9260426156618, 598942.5097923034, 598942.5097923034, 182334.3691422478], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2577000.0000, 
sim time next is 2577600.0000, 
raw observation next is [29.4, 51.0, 1.0, 2.0, 0.2535975098721855, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4068058433282919, 6.9112, 6.9112, 121.9260426156618, 600140.3972606272, 600140.3972606272, 182467.8416287983], 
processed observation next is [1.0, 0.8695652173913043, 0.6444444444444444, 0.51, 1.0, 1.0, 0.11142560699069701, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.2585073041603648, 0.0, 0.0, 0.8094621288201359, 0.21433585616450973, 0.21433585616450973, 0.3508996954399968], 
reward next is 0.6491, 
noisyNet noise sample is [array([-1.2139263], dtype=float32), 0.23467714]. 
=============================================
[2019-04-02 17:42:48,014] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.664672e-23 4.994169e-18 1.000000e+00 7.087130e-34 9.186241e-26], sum to 1.0000
[2019-04-02 17:42:48,027] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6636
[2019-04-02 17:42:48,031] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [21.15, 94.33333333333334, 1.0, 2.0, 0.22907430762822, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3715848804551346, 6.911200000000001, 6.9112, 121.9260426156618, 553771.4441294904, 553771.44412949, 175659.2747760012], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2603400.0000, 
sim time next is 2604000.0000, 
raw observation next is [21.1, 94.66666666666667, 1.0, 2.0, 0.2292848839663652, 0.0, 2.0, 0.0, 1.0, 2.0, 0.371968664842022, 6.911199999999999, 6.9112, 121.9260426156618, 554373.5919424235, 554373.591942424, 175701.4495959613], 
processed observation next is [0.0, 0.13043478260869565, 0.3370370370370371, 0.9466666666666668, 1.0, 1.0, 0.08248200472186334, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.2149608310525275, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.19799056855086553, 0.1979905685508657, 0.3378874030691564], 
reward next is 0.6621, 
noisyNet noise sample is [array([1.3635465], dtype=float32), 1.5757786]. 
=============================================
[2019-04-02 17:42:48,044] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[56.782326]
 [56.953476]
 [57.15383 ]
 [57.288338]
 [57.426773]], R is [[56.77994537]
 [56.87434006]
 [56.96775818]
 [57.05992889]
 [57.15084839]].
[2019-04-02 17:42:54,012] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4500, global step 71779: loss 0.0115
[2019-04-02 17:42:54,018] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4500, global step 71779: learning rate 0.0010
[2019-04-02 17:42:54,072] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4500, global step 71799: loss 0.0050
[2019-04-02 17:42:54,075] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4500, global step 71799: learning rate 0.0010
[2019-04-02 17:42:54,171] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4500, global step 71839: loss 0.0009
[2019-04-02 17:42:54,174] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4500, global step 71839: learning rate 0.0010
[2019-04-02 17:42:54,374] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4500, global step 71913: loss -4.6218
[2019-04-02 17:42:54,377] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4500, global step 71914: learning rate 0.0010
[2019-04-02 17:42:54,444] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4500, global step 71934: loss 0.0339
[2019-04-02 17:42:54,453] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4500, global step 71936: loss 0.0352
[2019-04-02 17:42:54,453] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4500, global step 71936: learning rate 0.0010
[2019-04-02 17:42:54,462] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4500, global step 71937: learning rate 0.0010
[2019-04-02 17:42:54,505] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4500, global step 71957: loss 0.0330
[2019-04-02 17:42:54,509] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4500, global step 71959: learning rate 0.0010
[2019-04-02 17:42:54,535] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4500, global step 71969: loss 0.0152
[2019-04-02 17:42:54,538] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4500, global step 71969: learning rate 0.0010
[2019-04-02 17:42:54,578] A3C_AGENT_WORKER-Thread-22 INFO:Local step 4500, global step 71987: loss 0.0105
[2019-04-02 17:42:54,581] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 4500, global step 71987: learning rate 0.0010
[2019-04-02 17:42:54,601] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4500, global step 71994: loss 0.0009
[2019-04-02 17:42:54,603] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4500, global step 71994: learning rate 0.0010
[2019-04-02 17:42:54,613] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4500, global step 71996: loss 0.0076
[2019-04-02 17:42:54,615] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4500, global step 71997: learning rate 0.0010
[2019-04-02 17:42:54,818] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4500, global step 72073: loss 0.0506
[2019-04-02 17:42:54,824] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4500, global step 72073: learning rate 0.0010
[2019-04-02 17:42:54,858] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4500, global step 72086: loss 0.0622
[2019-04-02 17:42:54,863] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4500, global step 72086: learning rate 0.0010
[2019-04-02 17:42:54,976] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4500, global step 72134: loss 0.0015
[2019-04-02 17:42:54,979] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4500, global step 72134: learning rate 0.0010
[2019-04-02 17:42:54,988] A3C_AGENT_WORKER-Thread-21 INFO:Local step 4500, global step 72138: loss 0.0434
[2019-04-02 17:42:54,992] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 4500, global step 72138: learning rate 0.0010
[2019-04-02 17:42:55,333] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4500, global step 72264: loss 0.0555
[2019-04-02 17:42:55,334] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4500, global step 72264: learning rate 0.0010
[2019-04-02 17:42:58,377] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.5136822e-24 2.8763791e-18 1.0000000e+00 0.0000000e+00 1.6922736e-31], sum to 1.0000
[2019-04-02 17:42:58,393] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2302
[2019-04-02 17:42:58,398] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.3, 67.0, 1.0, 2.0, 0.3313962324733744, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5275935949194175, 6.9112, 6.9112, 121.9260426156618, 755375.0179610882, 755375.0179610882, 203874.9573957678], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2748600.0000, 
sim time next is 2749200.0000, 
raw observation next is [29.06666666666667, 68.66666666666667, 1.0, 2.0, 0.3348141345092257, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5330350062737911, 6.9112, 6.9112, 121.9260426156618, 763169.5636305645, 763169.5636305645, 204838.0831305286], 
processed observation next is [0.0, 0.8260869565217391, 0.6320987654320989, 0.6866666666666668, 1.0, 1.0, 0.20811206489193537, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.4162937578422388, 0.0, 0.0, 0.8094621288201359, 0.27256055843948734, 0.27256055843948734, 0.3939193906356319], 
reward next is 0.6061, 
noisyNet noise sample is [array([0.5893583], dtype=float32), 1.4245477]. 
=============================================
[2019-04-02 17:43:02,646] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-02 17:43:02,647] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 17:43:02,648] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 17:43:02,650] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 17:43:02,650] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:43:02,651] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:43:02,653] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 17:43:02,654] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 17:43:02,656] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:43:02,656] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:43:02,651] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:43:02,679] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run4
[2019-04-02 17:43:02,680] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run4
[2019-04-02 17:43:02,699] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run4
[2019-04-02 17:43:02,700] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run4
[2019-04-02 17:43:02,718] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run4
[2019-04-02 17:43:27,563] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00061337], dtype=float32), 0.26713297]
[2019-04-02 17:43:27,564] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [28.421185155, 31.97246635666666, 1.0, 2.0, 0.1626912375239577, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2794182775522762, 6.911199999999999, 6.9112, 121.9260426156618, 412043.4036131313, 412043.4036131317, 157494.6907639167]
[2019-04-02 17:43:27,565] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 17:43:27,570] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [3.6669805e-25 1.9764210e-19 1.0000000e+00 1.5679552e-29 9.8815238e-26], sampled 0.882440722065012
[2019-04-02 17:43:31,128] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00061337], dtype=float32), 0.26713297]
[2019-04-02 17:43:31,129] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [20.9, 83.0, 1.0, 2.0, 0.2122664938275642, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3520925739363455, 6.911200000000001, 6.9112, 121.9260426156618, 525861.1878925218, 525861.1878925215, 170283.3375131706]
[2019-04-02 17:43:31,130] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 17:43:31,132] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [4.5860188e-26 2.4450712e-21 1.0000000e+00 3.0528540e-31 6.6413036e-27], sampled 0.6501598877416682
[2019-04-02 17:43:35,815] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00061337], dtype=float32), 0.26713297]
[2019-04-02 17:43:35,815] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [18.75, 92.33333333333334, 1.0, 2.0, 0.1629335890272134, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2760107443655323, 6.911199999999999, 6.9112, 121.9260426156618, 409622.2910905469, 409622.2910905473, 158248.8854810344]
[2019-04-02 17:43:35,817] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 17:43:35,819] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [9.0045773e-26 1.2800461e-20 1.0000000e+00 8.2644626e-31 7.7912032e-27], sampled 0.8727247152672513
[2019-04-02 17:43:56,930] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00061337], dtype=float32), 0.26713297]
[2019-04-02 17:43:56,931] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [30.49986285, 68.50629782666667, 1.0, 2.0, 0.3646919711441772, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5806014952498406, 6.9112, 6.9112, 121.9260426156618, 831309.5206869314, 831309.5206869314, 213459.4082039396]
[2019-04-02 17:43:56,933] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 17:43:56,935] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [3.4045946e-27 5.7356934e-21 1.0000000e+00 1.9649298e-31 4.3722030e-27], sampled 0.4093688048899282
[2019-04-02 17:43:59,632] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00061337], dtype=float32), 0.26713297]
[2019-04-02 17:43:59,633] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [27.19852954, 78.206709535, 1.0, 2.0, 0.3237763183105313, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5154624434088922, 6.911199999999999, 6.9112, 121.9260426156618, 737998.0442721626, 737998.0442721631, 201744.4771480609]
[2019-04-02 17:43:59,634] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 17:43:59,636] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [6.7946197e-28 3.9374516e-21 1.0000000e+00 7.4748933e-32 1.9577361e-27], sampled 0.4927289674701816
[2019-04-02 17:43:59,659] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00061337], dtype=float32), 0.26713297]
[2019-04-02 17:43:59,660] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [26.19466708333333, 88.29747544666667, 1.0, 2.0, 0.5624287589043453, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8954049012016172, 6.9112, 6.9112, 121.9260426156618, 1282424.568265653, 1282424.568265653, 279529.5831476573]
[2019-04-02 17:43:59,660] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 17:43:59,664] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [5.8017118e-28 1.4637829e-21 1.0000000e+00 5.4830787e-32 1.6964265e-27], sampled 0.023762009381851246
[2019-04-02 17:44:50,090] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00061337], dtype=float32), 0.26713297]
[2019-04-02 17:44:50,091] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [25.08716076833333, 40.58126731833333, 1.0, 2.0, 0.16, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2558328368148285, 6.911199999999999, 6.9112, 121.9260426156618, 372063.1708963233, 372063.1708963237, 151511.4544821076]
[2019-04-02 17:44:50,095] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 17:44:50,097] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [7.7776917e-27 1.6315047e-21 1.0000000e+00 3.3461321e-31 9.1473160e-27], sampled 0.01863163352803543
[2019-04-02 17:44:52,274] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00061337], dtype=float32), 0.26713297]
[2019-04-02 17:44:52,274] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [22.72169607166667, 81.29681835666666, 1.0, 2.0, 0.2261377753940704, 0.0, 2.0, 0.0, 1.0, 2.0, 0.367120236900485, 6.9112, 6.9112, 121.9260426156618, 547319.8238052527, 547319.8238052527, 174905.30583901]
[2019-04-02 17:44:52,274] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 17:44:52,277] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [6.0033716e-28 4.0709544e-22 1.0000000e+00 2.5311065e-32 1.2216945e-27], sampled 0.42847801670606234
[2019-04-02 17:45:08,046] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00061337], dtype=float32), 0.26713297]
[2019-04-02 17:45:08,048] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [20.92197396833333, 76.99079037833333, 1.0, 2.0, 0.1696596479007734, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2858743263157974, 6.911199999999999, 6.9112, 121.9260426156618, 425138.8315218734, 425138.8315218739, 159957.4444077945]
[2019-04-02 17:45:08,049] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 17:45:08,051] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.9556089e-27 3.0736682e-22 1.0000000e+00 5.6007862e-32 2.1047156e-27], sampled 0.6393216416249325
[2019-04-02 17:45:10,040] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 7492.5206 2565975221.9196 47.0000
[2019-04-02 17:45:10,170] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7157.8595 2831362235.9031 210.0000
[2019-04-02 17:45:10,430] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 6809.5120 2600787981.7632 61.0000
[2019-04-02 17:45:10,469] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 7162.7684 2623880762.9586 97.0000
[2019-04-02 17:45:10,582] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 6579.9288 2661677694.6124 110.0000
[2019-04-02 17:45:11,597] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 75000, evaluation results [75000.0, 7157.859463619724, 2831362235.9030914, 210.0, 6809.511993196244, 2600787981.7632065, 61.0, 7492.520639979948, 2565975221.919632, 47.0, 6579.928819146758, 2661677694.612359, 110.0, 7162.768394127867, 2623880762.9585886, 97.0]
[2019-04-02 17:45:16,063] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.1586053e-18 1.4487333e-07 9.9999988e-01 2.7098036e-29 5.1080045e-24], sum to 1.0000
[2019-04-02 17:45:16,075] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8518
[2019-04-02 17:45:16,079] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.4121460664817952, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6587722836872072, 6.911199999999999, 6.9112, 121.9260426156618, 964900.7504894309, 964900.7504894313, 227326.4266483565], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2863800.0000, 
sim time next is 2864400.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.3563329201933258, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5697015071866703, 6.911199999999999, 6.9112, 121.9260426156618, 834916.6055221698, 834916.6055221702, 210469.3293899106], 
processed observation next is [1.0, 0.13043478260869565, 0.37037037037037035, 1.0, 1.0, 1.0, 0.23372966689681643, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.4621268839833379, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.29818450197220353, 0.29818450197220364, 0.40474871036521265], 
reward next is 0.5953, 
noisyNet noise sample is [array([-0.47854847], dtype=float32), -0.3440424]. 
=============================================
[2019-04-02 17:45:19,396] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.3986237e-32 2.2504862e-29 1.0000000e+00 0.0000000e+00 2.8230315e-30], sum to 1.0000
[2019-04-02 17:45:19,406] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2172
[2019-04-02 17:45:19,420] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.0, 87.0, 1.0, 2.0, 0.3372478732733392, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5369095976475976, 6.911199999999999, 6.9112, 121.9260426156618, 768719.7674911079, 768719.7674911084, 205526.2690416901], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2921400.0000, 
sim time next is 2922000.0000, 
raw observation next is [26.0, 86.33333333333334, 1.0, 2.0, 0.333571716841605, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5310570368842203, 6.9112, 6.9112, 121.9260426156618, 760336.2136723698, 760336.2136723698, 204486.9081543941], 
processed observation next is [1.0, 0.8260869565217391, 0.5185185185185185, 0.8633333333333334, 1.0, 1.0, 0.206632996240006, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.4138212961052754, 0.0, 0.0, 0.8094621288201359, 0.2715486477401321, 0.2715486477401321, 0.39324405414306557], 
reward next is 0.6068, 
noisyNet noise sample is [array([0.8121913], dtype=float32), -1.5721917]. 
=============================================
[2019-04-02 17:45:19,437] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[54.333435]
 [54.127045]
 [54.236607]
 [54.37121 ]
 [53.868595]], R is [[54.29878616]
 [54.36055374]
 [54.42015839]
 [54.47832108]
 [54.5353508 ]].
[2019-04-02 17:45:23,344] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.9982271e-25 2.2418818e-23 1.0000000e+00 8.9561488e-31 1.8730757e-24], sum to 1.0000
[2019-04-02 17:45:23,354] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2918
[2019-04-02 17:45:23,359] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.33333333333334, 80.66666666666667, 1.0, 2.0, 0.5680173603478682, 1.0, 2.0, 0.5680173603478682, 1.0, 1.0, 0.9043021367077481, 6.9112, 6.9112, 121.94756008, 1943523.266132532, 1943523.266132532, 378225.3747284306], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2976600.0000, 
sim time next is 2977200.0000, 
raw observation next is [28.4, 80.0, 1.0, 2.0, 1.02, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9977734948820727, 7.704468369767572, 6.9112, 121.9229753970669, 2284717.868011116, 1878503.780303255, 381287.9755717222], 
processed observation next is [1.0, 0.4782608695652174, 0.6074074074074074, 0.8, 1.0, 1.0, 1.0238095238095237, 0.0, 0.5, -0.1904761904761905, 1.0, 1.0, 0.9972168686025908, 0.07932683697675724, 0.0, 0.8094417656783484, 0.8159706671468272, 0.6708942072511624, 0.7332461068686965], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.4273194], dtype=float32), -0.8055213]. 
=============================================
[2019-04-02 17:45:24,219] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5000, global step 79741: loss 2.8055
[2019-04-02 17:45:24,222] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5000, global step 79741: learning rate 0.0010
[2019-04-02 17:45:24,407] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5000, global step 79790: loss 1.9606
[2019-04-02 17:45:24,414] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5000, global step 79792: learning rate 0.0010
[2019-04-02 17:45:24,505] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5000, global step 79803: loss 1.8153
[2019-04-02 17:45:24,508] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5000, global step 79804: learning rate 0.0010
[2019-04-02 17:45:24,804] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5000, global step 79899: loss 1.3395
[2019-04-02 17:45:24,807] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5000, global step 79900: learning rate 0.0010
[2019-04-02 17:45:24,810] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5000, global step 79900: loss 2.3256
[2019-04-02 17:45:24,888] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5000, global step 79900: learning rate 0.0010
[2019-04-02 17:45:24,975] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5000, global step 79913: loss 1.1910
[2019-04-02 17:45:24,977] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5000, global step 79913: learning rate 0.0010
[2019-04-02 17:45:25,097] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5000, global step 79940: loss 1.8545
[2019-04-02 17:45:25,100] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5000, global step 79940: learning rate 0.0010
[2019-04-02 17:45:25,310] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5000, global step 80000: loss 1.2561
[2019-04-02 17:45:25,313] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5000, global step 80000: learning rate 0.0010
[2019-04-02 17:45:25,414] A3C_AGENT_WORKER-Thread-22 INFO:Local step 5000, global step 80008: loss 1.7836
[2019-04-02 17:45:25,419] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 5000, global step 80009: learning rate 0.0010
[2019-04-02 17:45:25,517] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5000, global step 80025: loss 1.5439
[2019-04-02 17:45:25,520] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5000, global step 80026: learning rate 0.0010
[2019-04-02 17:45:25,660] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5000, global step 80062: loss 1.4657
[2019-04-02 17:45:25,665] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5000, global step 80063: learning rate 0.0010
[2019-04-02 17:45:25,752] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5000, global step 80075: loss 1.4897
[2019-04-02 17:45:25,755] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5000, global step 80077: learning rate 0.0010
[2019-04-02 17:45:25,839] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5000, global step 80086: loss 1.3640
[2019-04-02 17:45:25,840] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5000, global step 80086: learning rate 0.0010
[2019-04-02 17:45:25,948] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5000, global step 80106: loss 0.9982
[2019-04-02 17:45:25,950] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5000, global step 80107: learning rate 0.0010
[2019-04-02 17:45:26,029] A3C_AGENT_WORKER-Thread-21 INFO:Local step 5000, global step 80117: loss 0.9062
[2019-04-02 17:45:26,030] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 5000, global step 80117: learning rate 0.0010
[2019-04-02 17:45:26,473] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5000, global step 80265: loss 0.1569
[2019-04-02 17:45:26,476] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5000, global step 80265: learning rate 0.0010
[2019-04-02 17:45:30,010] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.5091275e-19 2.9251997e-18 1.0000000e+00 3.0507051e-28 1.5505095e-22], sum to 1.0000
[2019-04-02 17:45:30,024] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6190
[2019-04-02 17:45:30,031] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.16666666666666, 89.0, 1.0, 2.0, 0.9105296542046277, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.911200000000001, 6.9112, 121.9260426156618, 1753104.300611161, 1753104.300611161, 359253.7781326969], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3057000.0000, 
sim time next is 3057600.0000, 
raw observation next is [27.33333333333334, 89.0, 1.0, 2.0, 0.8515000076133143, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.9260426156618, 1685722.831401423, 1685722.831401423, 346698.3509234809], 
processed observation next is [1.0, 0.391304347826087, 0.5679012345679014, 0.89, 1.0, 1.0, 0.8232142947777552, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.9972168686025908, 0.0, 0.0, 0.8094621288201359, 0.6020438683576511, 0.6020438683576511, 0.6667275979297709], 
reward next is 0.3333, 
noisyNet noise sample is [array([0.77335995], dtype=float32), 1.0507711]. 
=============================================
[2019-04-02 17:45:30,789] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.0626386e-30 1.5692050e-30 1.0000000e+00 2.1478789e-37 3.4070077e-29], sum to 1.0000
[2019-04-02 17:45:30,799] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3114
[2019-04-02 17:45:30,806] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 2 has been changed to 4 for the demand 2486036.574263319 W.
[2019-04-02 17:45:30,812] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.83333333333334, 78.5, 1.0, 2.0, 0.8259762830954849, 1.0, 2.0, 0.7263528035241771, 1.0, 1.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.94756008, 2486036.574263319, 2486036.574263319, 464807.5157117847], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3066600.0000, 
sim time next is 3067200.0000, 
raw observation next is [31.0, 79.0, 1.0, 2.0, 0.8997370537388608, 1.0, 2.0, 0.763233188845865, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.94756008, 2612449.184785881, 2612449.184785881, 487225.2660614388], 
processed observation next is [1.0, 0.5217391304347826, 0.7037037037037037, 0.79, 1.0, 1.0, 0.88063934968912, 1.0, 1.0, 0.7181347486260298, 1.0, 1.0, 0.9972168686025908, 0.0, 0.0, 0.8096049824067558, 0.9330175659949576, 0.9330175659949576, 0.9369716655027669], 
reward next is 0.0630, 
noisyNet noise sample is [array([0.30911833], dtype=float32), 0.6578184]. 
=============================================
[2019-04-02 17:45:41,126] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2840601e-22 4.5340316e-12 1.0000000e+00 5.9484330e-28 5.0079252e-25], sum to 1.0000
[2019-04-02 17:45:41,139] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2439
[2019-04-02 17:45:41,150] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.15, 68.5, 1.0, 2.0, 0.2377720339238287, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3840184567335847, 6.911199999999999, 6.9112, 121.9260426156618, 570792.5515275891, 570792.5515275896, 178058.720078547], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3223800.0000, 
sim time next is 3224400.0000, 
raw observation next is [25.53333333333333, 65.33333333333333, 1.0, 2.0, 0.2350720844290662, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3801591580385306, 6.9112, 6.9112, 121.9260426156618, 565582.0547052455, 565582.0547052455, 177310.9038518941], 
processed observation next is [0.0, 0.30434782608695654, 0.5012345679012346, 0.6533333333333333, 1.0, 1.0, 0.08937152908222168, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.22519894754816325, 0.0, 0.0, 0.8094621288201359, 0.20199359096615913, 0.20199359096615913, 0.34098250740748864], 
reward next is 0.6590, 
noisyNet noise sample is [array([1.6779033], dtype=float32), -0.10427235]. 
=============================================
[2019-04-02 17:45:42,460] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5695807e-17 2.1615717e-08 1.0000000e+00 1.2780222e-26 3.4481635e-24], sum to 1.0000
[2019-04-02 17:45:42,474] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8726
[2019-04-02 17:45:42,480] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.15, 68.5, 1.0, 2.0, 0.2377720339238287, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3840184567335847, 6.911199999999999, 6.9112, 121.9260426156618, 570792.5515275891, 570792.5515275896, 178058.720078547], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3223800.0000, 
sim time next is 3224400.0000, 
raw observation next is [25.53333333333333, 65.33333333333333, 1.0, 2.0, 0.2350720844290662, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3801591580385306, 6.9112, 6.9112, 121.9260426156618, 565582.0547052455, 565582.0547052455, 177310.9038518941], 
processed observation next is [0.0, 0.30434782608695654, 0.5012345679012346, 0.6533333333333333, 1.0, 1.0, 0.08937152908222168, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.22519894754816325, 0.0, 0.0, 0.8094621288201359, 0.20199359096615913, 0.20199359096615913, 0.34098250740748864], 
reward next is 0.6590, 
noisyNet noise sample is [array([-0.03148675], dtype=float32), 0.43390378]. 
=============================================
[2019-04-02 17:45:45,112] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5088399e-33 1.1110630e-28 1.0000000e+00 0.0000000e+00 8.4962381e-34], sum to 1.0000
[2019-04-02 17:45:45,127] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4917
[2019-04-02 17:45:45,135] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.33333333333334, 84.0, 1.0, 2.0, 0.3038714651902286, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4839031285724442, 6.911199999999999, 6.9112, 121.9260426156618, 695763.6349880208, 695763.6349880212, 196236.0519879839], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3273600.0000, 
sim time next is 3274200.0000, 
raw observation next is [25.0, 86.5, 1.0, 2.0, 0.3043955231514885, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4847205804107912, 6.911199999999999, 6.9112, 121.9260426156618, 696634.2780834283, 696634.2780834287, 196383.545635805], 
processed observation next is [0.0, 0.9130434782608695, 0.48148148148148145, 0.865, 1.0, 1.0, 0.1718994323232006, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.35590072551348895, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.24879795645836725, 0.24879795645836741, 0.37766066468424037], 
reward next is 0.6223, 
noisyNet noise sample is [array([0.5292269], dtype=float32), 0.69110006]. 
=============================================
[2019-04-02 17:45:46,353] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5500, global step 87716: loss 0.0966
[2019-04-02 17:45:46,356] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5500, global step 87717: learning rate 0.0010
[2019-04-02 17:45:46,446] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5500, global step 87742: loss 0.1365
[2019-04-02 17:45:46,448] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5500, global step 87742: learning rate 0.0010
[2019-04-02 17:45:46,598] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5500, global step 87807: loss 0.0867
[2019-04-02 17:45:46,603] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5500, global step 87808: learning rate 0.0010
[2019-04-02 17:45:46,795] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5500, global step 87883: loss 0.1031
[2019-04-02 17:45:46,797] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5500, global step 87883: learning rate 0.0010
[2019-04-02 17:45:46,815] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5500, global step 87887: loss 0.0342
[2019-04-02 17:45:46,819] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5500, global step 87888: learning rate 0.0010
[2019-04-02 17:45:46,937] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5500, global step 87930: loss 0.0538
[2019-04-02 17:45:46,940] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5500, global step 87931: learning rate 0.0010
[2019-04-02 17:45:46,972] A3C_AGENT_WORKER-Thread-22 INFO:Local step 5500, global step 87943: loss 0.0256
[2019-04-02 17:45:46,975] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 5500, global step 87944: learning rate 0.0010
[2019-04-02 17:45:47,029] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5500, global step 87969: loss 0.0181
[2019-04-02 17:45:47,033] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5500, global step 87970: learning rate 0.0010
[2019-04-02 17:45:47,230] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5500, global step 88045: loss 0.0736
[2019-04-02 17:45:47,238] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5500, global step 88046: learning rate 0.0010
[2019-04-02 17:45:47,261] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5500, global step 88056: loss 0.0384
[2019-04-02 17:45:47,265] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5500, global step 88056: learning rate 0.0010
[2019-04-02 17:45:47,273] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5500, global step 88058: loss 0.0783
[2019-04-02 17:45:47,276] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5500, global step 88058: learning rate 0.0010
[2019-04-02 17:45:47,333] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5500, global step 88077: loss 0.0188
[2019-04-02 17:45:47,337] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5500, global step 88077: learning rate 0.0010
[2019-04-02 17:45:47,444] A3C_AGENT_WORKER-Thread-21 INFO:Local step 5500, global step 88121: loss 0.0416
[2019-04-02 17:45:47,449] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 5500, global step 88123: learning rate 0.0010
[2019-04-02 17:45:47,462] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5500, global step 88126: loss 0.0219
[2019-04-02 17:45:47,466] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5500, global step 88128: learning rate 0.0010
[2019-04-02 17:45:47,537] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5500, global step 88153: loss 0.0043
[2019-04-02 17:45:47,539] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5500, global step 88153: learning rate 0.0010
[2019-04-02 17:45:47,808] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5500, global step 88253: loss 0.1039
[2019-04-02 17:45:47,812] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5500, global step 88253: learning rate 0.0010
[2019-04-02 17:45:52,818] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0101299e-14 7.0841952e-12 1.0000000e+00 1.3511666e-22 9.6260203e-21], sum to 1.0000
[2019-04-02 17:45:52,830] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7076
[2019-04-02 17:45:52,838] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.0, 94.0, 1.0, 2.0, 0.3792407045392335, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6038150017642676, 6.911199999999999, 6.9112, 121.9260426156618, 866011.312407829, 866011.3124078294, 217757.069386689], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3384000.0000, 
sim time next is 3384600.0000, 
raw observation next is [23.81666666666667, 94.00000000000001, 1.0, 2.0, 0.367778748019184, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5857017469069722, 6.911199999999999, 6.9112, 121.9260426156618, 842697.0895974961, 842697.0895974966, 214296.4574429476], 
processed observation next is [1.0, 0.17391304347826086, 0.43765432098765444, 0.9400000000000002, 1.0, 1.0, 0.24735565240379045, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.4821271836337152, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.30096324628482, 0.3009632462848202, 0.41210857200566847], 
reward next is 0.5879, 
noisyNet noise sample is [array([-1.6973205], dtype=float32), -0.18654916]. 
=============================================
[2019-04-02 17:45:55,605] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5853638e-32 1.0000000e+00 1.4545045e-28 5.2208894e-38 0.0000000e+00], sum to 1.0000
[2019-04-02 17:45:55,615] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9900
[2019-04-02 17:45:55,623] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2083758.476582542 W.
[2019-04-02 17:45:55,713] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.33333333333333, 59.83333333333333, 1.0, 2.0, 0.9134322881532916, 1.0, 2.0, 0.9134322881532916, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 2083758.476582542, 2083758.476582542, 392855.28908687], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3419400.0000, 
sim time next is 3420000.0000, 
raw observation next is [30.2, 60.0, 1.0, 2.0, 0.9165871121277902, 1.0, 2.0, 0.9165871121277902, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 2090963.814300566, 2090963.814300566, 394284.5689441916], 
processed observation next is [1.0, 0.6086956521739131, 0.674074074074074, 0.6, 1.0, 1.0, 0.900698943009274, 1.0, 1.0, 0.900698943009274, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.7467727908216307, 0.7467727908216307, 0.7582395556619069], 
reward next is 0.2418, 
noisyNet noise sample is [array([-0.27576792], dtype=float32), 0.4170695]. 
=============================================
[2019-04-02 17:45:55,726] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[52.53858 ]
 [52.592884]
 [52.331917]
 [52.000225]
 [52.460434]], R is [[53.05222321]
 [52.76621246]
 [52.4661026 ]
 [52.17696381]
 [51.65519333]].
[2019-04-02 17:45:58,846] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.6312583e-19 1.0000000e+00 2.2457002e-11 3.0082043e-26 2.6321819e-27], sum to 1.0000
[2019-04-02 17:45:58,855] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2695
[2019-04-02 17:45:58,861] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 94.0, 1.0, 2.0, 0.6658284007805595, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 758836.7429282625, 758836.7429282625, 169774.4979504457], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3459600.0000, 
sim time next is 3460200.0000, 
raw observation next is [24.91666666666667, 93.50000000000001, 1.0, 2.0, 0.660580565573337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 752852.9101768514, 752852.9101768514, 168813.910160873], 
processed observation next is [1.0, 0.043478260869565216, 0.47839506172839524, 0.9350000000000002, 1.0, 1.0, 0.5959292447301631, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2688760393488755, 0.2688760393488755, 0.32464213492475574], 
reward next is 0.6754, 
noisyNet noise sample is [array([-1.2606342], dtype=float32), -0.7466157]. 
=============================================
[2019-04-02 17:46:07,572] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6000, global step 95651: loss -75.6171
[2019-04-02 17:46:07,574] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6000, global step 95651: learning rate 0.0010
[2019-04-02 17:46:07,893] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6000, global step 95769: loss -120.2614
[2019-04-02 17:46:07,899] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6000, global step 95770: learning rate 0.0010
[2019-04-02 17:46:08,212] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6000, global step 95884: loss -25.6649
[2019-04-02 17:46:08,215] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6000, global step 95884: learning rate 0.0010
[2019-04-02 17:46:08,217] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6000, global step 95884: loss 28.8425
[2019-04-02 17:46:08,220] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6000, global step 95885: learning rate 0.0010
[2019-04-02 17:46:08,342] A3C_AGENT_WORKER-Thread-22 INFO:Local step 6000, global step 95932: loss -39.0814
[2019-04-02 17:46:08,344] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 6000, global step 95932: learning rate 0.0010
[2019-04-02 17:46:08,422] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6000, global step 95963: loss -65.4048
[2019-04-02 17:46:08,424] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6000, global step 95964: learning rate 0.0010
[2019-04-02 17:46:08,470] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6000, global step 95981: loss 86.1539
[2019-04-02 17:46:08,472] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6000, global step 95981: learning rate 0.0010
[2019-04-02 17:46:08,480] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6000, global step 95983: loss -10.0988
[2019-04-02 17:46:08,482] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6000, global step 95983: learning rate 0.0010
[2019-04-02 17:46:08,607] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6000, global step 96030: loss -11.9335
[2019-04-02 17:46:08,611] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6000, global step 96030: learning rate 0.0010
[2019-04-02 17:46:08,644] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6000, global step 96044: loss 22.6808
[2019-04-02 17:46:08,646] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6000, global step 96044: learning rate 0.0010
[2019-04-02 17:46:08,752] A3C_AGENT_WORKER-Thread-21 INFO:Local step 6000, global step 96084: loss -53.7198
[2019-04-02 17:46:08,752] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6000, global step 96084: loss 17.6419
[2019-04-02 17:46:08,754] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 6000, global step 96084: learning rate 0.0010
[2019-04-02 17:46:08,756] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6000, global step 96084: learning rate 0.0010
[2019-04-02 17:46:08,805] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6000, global step 96104: loss -120.4094
[2019-04-02 17:46:08,807] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6000, global step 96104: learning rate 0.0010
[2019-04-02 17:46:08,859] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6000, global step 96122: loss 55.3461
[2019-04-02 17:46:08,862] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6000, global step 96123: learning rate 0.0010
[2019-04-02 17:46:08,873] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6000, global step 96127: loss 96.2885
[2019-04-02 17:46:08,875] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6000, global step 96128: learning rate 0.0010
[2019-04-02 17:46:09,223] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6000, global step 96260: loss 23.8637
[2019-04-02 17:46:09,230] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6000, global step 96261: learning rate 0.0010
[2019-04-02 17:46:09,246] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-02 17:46:09,254] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0600
[2019-04-02 17:46:09,260] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.25, 79.66666666666667, 1.0, 2.0, 0.6450449498187676, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 751366.7768818693, 751366.7768818693, 166778.3114229011], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3604200.0000, 
sim time next is 3604800.0000, 
raw observation next is [25.2, 80.33333333333334, 1.0, 2.0, 0.5416034692933774, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 633841.0947013455, 633841.0947013455, 149097.824084309], 
processed observation next is [1.0, 0.7391304347826086, 0.4888888888888889, 0.8033333333333335, 1.0, 1.0, 0.45428984439687786, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.22637181953619484, 0.22637181953619484, 0.2867265847775173], 
reward next is 0.7133, 
noisyNet noise sample is [array([-0.58956796], dtype=float32), 0.4335218]. 
=============================================
[2019-04-02 17:46:15,876] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.5810176e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 17:46:15,885] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5594
[2019-04-02 17:46:15,890] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 90.66666666666667, 1.0, 2.0, 0.7017913058679535, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 799844.6247119415, 799844.6247119415, 176483.4245743322], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3701400.0000, 
sim time next is 3702000.0000, 
raw observation next is [25.8, 91.33333333333334, 1.0, 2.0, 0.6968489582287243, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 794208.8207127714, 794208.8207127709, 175547.9829762611], 
processed observation next is [1.0, 0.8695652173913043, 0.5111111111111112, 0.9133333333333334, 1.0, 1.0, 0.6391059026532432, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.28364600739741835, 0.2836460073974182, 0.33759227495434824], 
reward next is 0.6624, 
noisyNet noise sample is [array([0.2796289], dtype=float32), 1.2016987]. 
=============================================
[2019-04-02 17:46:15,919] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[51.686375]
 [51.96611 ]
 [52.438942]
 [52.671783]
 [53.198215]], R is [[51.6681633 ]
 [51.81209183]
 [51.95312881]
 [52.09183121]
 [52.22803879]].
[2019-04-02 17:46:19,184] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-02 17:46:19,185] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 17:46:19,187] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 17:46:19,187] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:46:19,188] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:46:19,188] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 17:46:19,189] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 17:46:19,189] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:46:19,190] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:46:19,190] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 17:46:19,192] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:46:19,210] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run5
[2019-04-02 17:46:19,211] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run5
[2019-04-02 17:46:19,212] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run5
[2019-04-02 17:46:19,265] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run5
[2019-04-02 17:46:19,291] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run5
[2019-04-02 17:47:18,121] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05640102], dtype=float32), 0.37486678]
[2019-04-02 17:47:18,122] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [30.0, 75.0, 1.0, 2.0, 1.02, 1.0, 2.0, 1.02, 1.0, 2.0, 0.9977734948820727, 28.30351402536945, 6.9112, 135.961979289126, 15264747.28889678, 3048873.591748985, 515104.8043190093]
[2019-04-02 17:47:18,122] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 17:47:18,124] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.1696463e-20 1.0000000e+00 4.1662208e-08 9.7104832e-20 7.6771974e-21], sampled 0.05845616786316332
[2019-04-02 17:47:18,125] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 0, 1] for the demand 15264747.28889678 W.
[2019-04-02 17:48:25,852] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8100.6619 2445343923.2757 746.0000
[2019-04-02 17:48:25,996] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8701.6686 2195058721.8889 572.0000
[2019-04-02 17:48:26,356] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8923.5143 2120416988.3376 430.0000
[2019-04-02 17:48:26,415] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8771.0838 2170637693.1962 493.0000
[2019-04-02 17:48:26,452] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8584.5265 2248663206.3456 553.0000
[2019-04-02 17:48:27,467] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 100000, evaluation results [100000.0, 8100.661928194693, 2445343923.2757144, 746.0, 8771.08379319977, 2170637693.1962304, 493.0, 8923.514325042122, 2120416988.33764, 430.0, 8584.526542641555, 2248663206.345631, 553.0, 8701.66863850428, 2195058721.8888793, 572.0]
[2019-04-02 17:48:34,591] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.42570199e-29 1.00000000e+00 2.41655077e-19 2.82344076e-30
 1.06074224e-35], sum to 1.0000
[2019-04-02 17:48:34,601] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9089
[2019-04-02 17:48:34,607] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.0, 57.5, 1.0, 2.0, 0.7350849463596973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 837810.7458758174, 837810.7458758178, 182898.1780494148], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3846600.0000, 
sim time next is 3847200.0000, 
raw observation next is [33.0, 57.0, 1.0, 2.0, 0.7467232737189728, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 851082.8576060015, 851082.8576060015, 185183.0156712345], 
processed observation next is [0.0, 0.5217391304347826, 0.7777777777777778, 0.57, 1.0, 1.0, 0.6984800877606819, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.30395816343071486, 0.30395816343071486, 0.35612118398314324], 
reward next is 0.6439, 
noisyNet noise sample is [array([-0.42672178], dtype=float32), 0.73995996]. 
=============================================
[2019-04-02 17:48:36,989] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6500, global step 103582: loss 0.1395
[2019-04-02 17:48:36,992] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6500, global step 103583: learning rate 0.0010
[2019-04-02 17:48:37,575] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6500, global step 103800: loss 0.0264
[2019-04-02 17:48:37,579] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6500, global step 103801: learning rate 0.0010
[2019-04-02 17:48:37,614] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6500, global step 103815: loss 0.0026
[2019-04-02 17:48:37,618] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6500, global step 103816: learning rate 0.0010
[2019-04-02 17:48:37,672] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6500, global step 103838: loss 0.0027
[2019-04-02 17:48:37,678] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6500, global step 103838: learning rate 0.0010
[2019-04-02 17:48:37,898] A3C_AGENT_WORKER-Thread-22 INFO:Local step 6500, global step 103923: loss 0.0024
[2019-04-02 17:48:37,900] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 6500, global step 103924: learning rate 0.0010
[2019-04-02 17:48:37,921] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6500, global step 103934: loss 0.0118
[2019-04-02 17:48:37,922] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6500, global step 103934: learning rate 0.0010
[2019-04-02 17:48:38,027] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6500, global step 103969: loss 0.1028
[2019-04-02 17:48:38,032] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6500, global step 103970: learning rate 0.0010
[2019-04-02 17:48:38,145] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6500, global step 104009: loss 0.0022
[2019-04-02 17:48:38,148] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6500, global step 104009: learning rate 0.0010
[2019-04-02 17:48:38,164] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6500, global step 104016: loss 0.0030
[2019-04-02 17:48:38,169] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6500, global step 104016: learning rate 0.0010
[2019-04-02 17:48:38,231] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6500, global step 104042: loss 0.0559
[2019-04-02 17:48:38,234] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6500, global step 104043: learning rate 0.0010
[2019-04-02 17:48:38,360] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6500, global step 104093: loss 0.0616
[2019-04-02 17:48:38,365] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6500, global step 104093: learning rate 0.0010
[2019-04-02 17:48:38,386] A3C_AGENT_WORKER-Thread-21 INFO:Local step 6500, global step 104102: loss 0.0107
[2019-04-02 17:48:38,391] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 6500, global step 104104: learning rate 0.0010
[2019-04-02 17:48:38,413] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6500, global step 104111: loss 0.0429
[2019-04-02 17:48:38,415] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6500, global step 104111: learning rate 0.0010
[2019-04-02 17:48:38,416] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6500, global step 104111: loss 0.1018
[2019-04-02 17:48:38,420] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6500, global step 104113: learning rate 0.0010
[2019-04-02 17:48:38,490] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6500, global step 104139: loss 0.0733
[2019-04-02 17:48:38,496] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6500, global step 104140: learning rate 0.0010
[2019-04-02 17:48:38,885] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6500, global step 104288: loss 0.0500
[2019-04-02 17:48:38,888] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6500, global step 104289: learning rate 0.0010
[2019-04-02 17:48:39,224] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.4283908e-22 1.0000000e+00 5.8035921e-21 7.9000540e-34 5.0024326e-30], sum to 1.0000
[2019-04-02 17:48:39,235] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7849
[2019-04-02 17:48:39,242] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666666, 87.33333333333334, 1.0, 2.0, 0.7508021557287434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 855734.3860518129, 855734.3860518129, 185987.3922403012], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3921000.0000, 
sim time next is 3921600.0000, 
raw observation next is [27.33333333333334, 85.66666666666667, 1.0, 2.0, 0.746611303941366, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 850955.1684547858, 850955.1684547854, 185158.3410257567], 
processed observation next is [0.0, 0.391304347826087, 0.5679012345679014, 0.8566666666666667, 1.0, 1.0, 0.698346790406388, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.3039125601624235, 0.30391256016242335, 0.3560737327418398], 
reward next is 0.6439, 
noisyNet noise sample is [array([2.2324426], dtype=float32), 0.6501763]. 
=============================================
[2019-04-02 17:48:53,540] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.0023787e-18 1.0000000e+00 0.0000000e+00 2.3058695e-28 1.8404105e-31], sum to 1.0000
[2019-04-02 17:48:53,551] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7226
[2019-04-02 17:48:53,559] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2002261.523041549 W.
[2019-04-02 17:48:53,565] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.0, 73.33333333333334, 1.0, 2.0, 0.8777474496088347, 1.0, 2.0, 0.8777474496088347, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 2002261.523041549, 2002261.523041549, 376928.6652250413], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4115400.0000, 
sim time next is 4116000.0000, 
raw observation next is [29.0, 72.66666666666667, 1.0, 2.0, 0.6042869163630963, 1.0, 2.0, 0.6042869163630963, 1.0, 1.0, 0.9620444511009654, 6.911199999999999, 6.9112, 121.94756008, 2067766.51315357, 2067766.51315357, 397716.275683505], 
processed observation next is [1.0, 0.6521739130434783, 0.6296296296296297, 0.7266666666666667, 1.0, 1.0, 0.5289129956703527, 1.0, 1.0, 0.5289129956703527, 1.0, 0.5, 0.9525555638762068, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.7384880404119892, 0.7384880404119892, 0.7648389916990481], 
reward next is 0.2352, 
noisyNet noise sample is [array([-0.08241449], dtype=float32), -0.08370902]. 
=============================================
[2019-04-02 17:48:53,578] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[44.028572]
 [43.342976]
 [43.149635]
 [43.31888 ]
 [43.692123]], R is [[43.17469406]
 [43.01808548]
 [42.79035568]
 [42.60298157]
 [42.44360352]].
[2019-04-02 17:48:58,461] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7000, global step 111660: loss -92.2956
[2019-04-02 17:48:58,463] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7000, global step 111660: learning rate 0.0010
[2019-04-02 17:48:58,875] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7000, global step 111814: loss 34.9396
[2019-04-02 17:48:58,880] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7000, global step 111815: learning rate 0.0010
[2019-04-02 17:48:59,046] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7000, global step 111875: loss -49.1438
[2019-04-02 17:48:59,050] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7000, global step 111876: learning rate 0.0010
[2019-04-02 17:48:59,117] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7000, global step 111905: loss -108.2102
[2019-04-02 17:48:59,118] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7000, global step 111906: learning rate 0.0010
[2019-04-02 17:48:59,154] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7000, global step 111918: loss 111.5148
[2019-04-02 17:48:59,156] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7000, global step 111918: learning rate 0.0010
[2019-04-02 17:48:59,163] A3C_AGENT_WORKER-Thread-22 INFO:Local step 7000, global step 111918: loss 30.1020
[2019-04-02 17:48:59,166] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 7000, global step 111918: learning rate 0.0010
[2019-04-02 17:48:59,343] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7000, global step 111989: loss 68.4027
[2019-04-02 17:48:59,346] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7000, global step 111989: learning rate 0.0010
[2019-04-02 17:48:59,363] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7000, global step 111995: loss 152.6409
[2019-04-02 17:48:59,365] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7000, global step 111995: learning rate 0.0010
[2019-04-02 17:48:59,468] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7000, global step 112031: loss -0.1199
[2019-04-02 17:48:59,470] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7000, global step 112031: learning rate 0.0010
[2019-04-02 17:48:59,477] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7000, global step 112033: loss 16.9556
[2019-04-02 17:48:59,480] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7000, global step 112034: learning rate 0.0010
[2019-04-02 17:48:59,553] A3C_AGENT_WORKER-Thread-21 INFO:Local step 7000, global step 112064: loss -87.2322
[2019-04-02 17:48:59,556] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 7000, global step 112064: learning rate 0.0010
[2019-04-02 17:48:59,581] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7000, global step 112075: loss -29.6642
[2019-04-02 17:48:59,582] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7000, global step 112075: learning rate 0.0010
[2019-04-02 17:48:59,584] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7000, global step 112075: loss -41.2999
[2019-04-02 17:48:59,586] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7000, global step 112075: learning rate 0.0010
[2019-04-02 17:48:59,620] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7000, global step 112085: loss 35.5892
[2019-04-02 17:48:59,621] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7000, global step 112085: learning rate 0.0010
[2019-04-02 17:48:59,781] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7000, global step 112148: loss -132.5123
[2019-04-02 17:48:59,785] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7000, global step 112148: learning rate 0.0010
[2019-04-02 17:49:00,061] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7000, global step 112248: loss 6.7125
[2019-04-02 17:49:00,064] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7000, global step 112249: learning rate 0.0010
[2019-04-02 17:49:12,843] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.5140500e-17 1.0000000e+00 6.9719311e-27 2.3166183e-27 1.4034148e-24], sum to 1.0000
[2019-04-02 17:49:12,852] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6210
[2019-04-02 17:49:12,857] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.86666666666667, 70.66666666666667, 1.0, 2.0, 0.6862697645720194, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 782145.4091733866, 782145.4091733866, 173560.4175165981], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4390800.0000, 
sim time next is 4391400.0000, 
raw observation next is [28.33333333333333, 71.83333333333333, 1.0, 2.0, 0.6762334810210587, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 770701.2513124467, 770701.2513124467, 171691.8299294921], 
processed observation next is [1.0, 0.8260869565217391, 0.6049382716049381, 0.7183333333333333, 1.0, 1.0, 0.6145636678822127, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2752504468973024, 0.2752504468973024, 0.330176596018254], 
reward next is 0.6698, 
noisyNet noise sample is [array([0.6092537], dtype=float32), 1.0385914]. 
=============================================
[2019-04-02 17:49:15,118] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1076969e-16 1.0000000e+00 5.7143436e-18 1.8523137e-31 2.0477657e-26], sum to 1.0000
[2019-04-02 17:49:15,127] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3677
[2019-04-02 17:49:15,137] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.06666666666667, 93.5, 1.0, 2.0, 0.4904930150644758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 588766.5316148774, 588766.5316148774, 141543.9065313584], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4427400.0000, 
sim time next is 4428000.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.4912626564596281, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 589742.1493895198, 589742.1493895198, 141665.6250266383], 
processed observation next is [0.0, 0.2608695652173913, 0.37037037037037035, 0.94, 1.0, 1.0, 0.394360305309081, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2106221962105428, 0.2106221962105428, 0.2724338942819967], 
reward next is 0.7276, 
noisyNet noise sample is [array([-1.3082772], dtype=float32), 0.9381692]. 
=============================================
[2019-04-02 17:49:15,162] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[63.794014]
 [63.762077]
 [63.7254  ]
 [63.69357 ]
 [63.66078 ]], R is [[63.96017075]
 [64.04837036]
 [64.13580322]
 [64.22225189]
 [64.30770874]].
[2019-04-02 17:49:15,385] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.0038749e-15 1.0000000e+00 1.4282605e-14 7.0131961e-27 1.6253246e-20], sum to 1.0000
[2019-04-02 17:49:15,391] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6209
[2019-04-02 17:49:15,396] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.25, 85.66666666666667, 1.0, 2.0, 0.548256388018073, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 645055.1733294473, 645055.1733294473, 150333.445910439], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4435800.0000, 
sim time next is 4436400.0000, 
raw observation next is [24.6, 85.33333333333334, 1.0, 2.0, 0.5598483584174666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 654852.9203018256, 654852.9203018253, 152094.198472189], 
processed observation next is [0.0, 0.34782608695652173, 0.46666666666666673, 0.8533333333333334, 1.0, 1.0, 0.476009950496984, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.23387604296493772, 0.2338760429649376, 0.29248884321574803], 
reward next is 0.7075, 
noisyNet noise sample is [array([-1.2188551], dtype=float32), -0.09677762]. 
=============================================
[2019-04-02 17:49:17,457] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.7847121e-17 1.0000000e+00 2.4141706e-14 1.5433402e-29 1.2058331e-25], sum to 1.0000
[2019-04-02 17:49:17,469] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5296
[2019-04-02 17:49:17,569] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 72.5, 1.0, 2.0, 0.656081921955531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 747723.3787008525, 747723.3787008525, 167995.4560335104], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4453800.0000, 
sim time next is 4454400.0000, 
raw observation next is [28.33333333333334, 71.0, 1.0, 2.0, 0.6513164823279147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 742289.6732965625, 742289.673296562, 167131.263466623], 
processed observation next is [0.0, 0.5652173913043478, 0.6049382716049385, 0.71, 1.0, 1.0, 0.5849005741998985, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.26510345474877234, 0.26510345474877217, 0.32140627589735193], 
reward next is 0.6786, 
noisyNet noise sample is [array([-0.6299691], dtype=float32), -1.1154188]. 
=============================================
[2019-04-02 17:49:19,367] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7500, global step 119613: loss 0.1417
[2019-04-02 17:49:19,369] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7500, global step 119613: learning rate 0.0010
[2019-04-02 17:49:19,511] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.4329237e-19 1.0000000e+00 3.1328046e-17 4.2998638e-31 3.4962089e-21], sum to 1.0000
[2019-04-02 17:49:19,521] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2467
[2019-04-02 17:49:19,527] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.03333333333333, 83.66666666666666, 1.0, 2.0, 0.7065642282954179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 805287.2709330095, 805287.2709330095, 177390.4616102606], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4481400.0000, 
sim time next is 4482000.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.7077329746065462, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 806620.0185117348, 806620.0185117348, 177613.2096852566], 
processed observation next is [0.0, 0.9130434782608695, 0.5555555555555556, 0.84, 1.0, 1.0, 0.652063065007793, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2880785780399053, 0.2880785780399053, 0.3415638647793396], 
reward next is 0.6584, 
noisyNet noise sample is [array([0.27591178], dtype=float32), 0.060618885]. 
=============================================
[2019-04-02 17:49:19,544] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[67.78836]
 [67.78393]
 [67.76443]
 [67.73005]
 [67.6914 ]], R is [[67.81298828]
 [67.79372406]
 [67.77483368]
 [67.75601959]
 [67.73732758]].
[2019-04-02 17:49:19,593] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7500, global step 119696: loss 0.0041
[2019-04-02 17:49:19,596] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7500, global step 119696: learning rate 0.0010
[2019-04-02 17:49:20,083] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.2777600e-23 1.0000000e+00 1.4965662e-19 7.1858242e-31 3.9662732e-22], sum to 1.0000
[2019-04-02 17:49:20,083] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7500, global step 119880: loss 0.0622
[2019-04-02 17:49:20,088] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7500, global step 119880: learning rate 0.0010
[2019-04-02 17:49:20,096] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1322
[2019-04-02 17:49:20,101] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.83333333333334, 94.00000000000001, 1.0, 2.0, 0.6653498770612468, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 758291.105356137, 758291.105356137, 169685.964532902], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4493400.0000, 
sim time next is 4494000.0000, 
raw observation next is [24.66666666666667, 94.0, 1.0, 2.0, 0.6585278009620904, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 750512.2624758733, 750512.2624758729, 168438.8443565942], 
processed observation next is [0.0, 0.0, 0.469135802469136, 0.94, 1.0, 1.0, 0.5934854773358219, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2680400937413833, 0.26804009374138316, 0.32392085453191194], 
reward next is 0.6761, 
noisyNet noise sample is [array([-0.34467712], dtype=float32), -1.0773947]. 
=============================================
[2019-04-02 17:49:20,117] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[70.724976]
 [71.466255]
 [72.40002 ]
 [72.36696 ]
 [72.3263  ]], R is [[70.23791504]
 [70.20922089]
 [70.17966461]
 [70.15270996]
 [70.12815094]].
[2019-04-02 17:49:20,121] A3C_AGENT_WORKER-Thread-22 INFO:Local step 7500, global step 119891: loss 0.0316
[2019-04-02 17:49:20,124] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 7500, global step 119891: learning rate 0.0010
[2019-04-02 17:49:20,136] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7500, global step 119895: loss 0.0455
[2019-04-02 17:49:20,140] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7500, global step 119896: learning rate 0.0010
[2019-04-02 17:49:20,205] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7500, global step 119931: loss 0.0148
[2019-04-02 17:49:20,208] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7500, global step 119931: learning rate 0.0010
[2019-04-02 17:49:20,265] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7500, global step 119954: loss 0.0063
[2019-04-02 17:49:20,269] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7500, global step 119954: learning rate 0.0010
[2019-04-02 17:49:20,411] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7500, global step 120009: loss 0.0762
[2019-04-02 17:49:20,418] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7500, global step 120010: learning rate 0.0010
[2019-04-02 17:49:20,465] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7500, global step 120030: loss 0.0670
[2019-04-02 17:49:20,470] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7500, global step 120031: learning rate 0.0010
[2019-04-02 17:49:20,481] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7500, global step 120034: loss 0.0643
[2019-04-02 17:49:20,486] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7500, global step 120035: learning rate 0.0010
[2019-04-02 17:49:20,493] A3C_AGENT_WORKER-Thread-21 INFO:Local step 7500, global step 120036: loss 0.1024
[2019-04-02 17:49:20,494] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 7500, global step 120036: learning rate 0.0010
[2019-04-02 17:49:20,512] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7500, global step 120047: loss 0.1973
[2019-04-02 17:49:20,515] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7500, global step 120047: learning rate 0.0010
[2019-04-02 17:49:20,670] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7500, global step 120110: loss 0.0046
[2019-04-02 17:49:20,672] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7500, global step 120111: learning rate 0.0010
[2019-04-02 17:49:20,794] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7500, global step 120157: loss 0.0282
[2019-04-02 17:49:20,797] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7500, global step 120158: learning rate 0.0010
[2019-04-02 17:49:20,947] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7500, global step 120212: loss 0.0950
[2019-04-02 17:49:20,949] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7500, global step 120213: learning rate 0.0010
[2019-04-02 17:49:21,219] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7500, global step 120317: loss 0.0618
[2019-04-02 17:49:21,222] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7500, global step 120318: learning rate 0.0010
[2019-04-02 17:49:28,121] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.1845772e-26 1.0000000e+00 5.5984778e-25 2.5278097e-37 2.9553307e-30], sum to 1.0000
[2019-04-02 17:49:28,134] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9515
[2019-04-02 17:49:28,143] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.46666666666667, 86.33333333333334, 1.0, 2.0, 0.9739310075978798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.143917879090234, 6.9112, 121.925215938918, 1259093.196824242, 1139921.650648362, 236091.7921877085], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4609200.0000, 
sim time next is 4609800.0000, 
raw observation next is [24.85, 87.0, 1.0, 2.0, 1.02, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.589144088841367, 6.9112, 121.9191360016604, 2041776.385397689, 1182567.712401032, 246666.3398866761], 
processed observation next is [1.0, 0.34782608695652173, 0.475925925925926, 0.87, 1.0, 1.0, 1.0238095238095237, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.16779440888413671, 0.0, 0.809416276085539, 0.729205851927746, 0.4223456115717971, 0.4743583459359156], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.4858748], dtype=float32), -0.59252495]. 
=============================================
[2019-04-02 17:49:33,622] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-02 17:49:33,625] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 17:49:33,626] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:49:33,627] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 17:49:33,629] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:49:33,629] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 17:49:33,630] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:49:33,631] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 17:49:33,631] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 17:49:33,632] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:49:33,634] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:49:33,658] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run6
[2019-04-02 17:49:33,659] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run6
[2019-04-02 17:49:33,698] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run6
[2019-04-02 17:49:33,698] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run6
[2019-04-02 17:49:33,699] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run6
[2019-04-02 17:49:58,096] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.02173212], dtype=float32), 0.40155184]
[2019-04-02 17:49:58,099] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [29.33333333333334, 36.83333333333334, 1.0, 2.0, 0.3733911700201121, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 466178.2980297515, 466178.2980297515, 125006.3353275724]
[2019-04-02 17:49:58,101] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 17:49:58,104] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [2.6710882e-24 1.0000000e+00 5.6776760e-22 3.9314561e-32 1.6632069e-29], sampled 0.47094156640997964
[2019-04-02 17:50:00,255] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.02173212], dtype=float32), 0.40155184]
[2019-04-02 17:50:00,257] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [22.6, 60.0, 1.0, 2.0, 0.3201855186915131, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 406795.8760764506, 406795.8760764506, 118097.2646627972]
[2019-04-02 17:50:00,259] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 17:50:00,262] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [4.1574054e-25 1.0000000e+00 4.0579111e-24 6.6317865e-34 4.3452083e-31], sampled 0.5217366782272339
[2019-04-02 17:50:35,307] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.02173212], dtype=float32), 0.40155184]
[2019-04-02 17:50:35,308] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [26.47386330666667, 81.57383851333333, 1.0, 2.0, 0.6405022608543216, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 729959.0978674651, 729959.0978674651, 165184.9343061992]
[2019-04-02 17:50:35,308] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 17:50:35,309] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [3.4710779e-22 1.0000000e+00 5.0666053e-19 1.2661647e-30 3.0914163e-27], sampled 0.4006386689775726
[2019-04-02 17:50:55,316] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.02173212], dtype=float32), 0.40155184]
[2019-04-02 17:50:55,321] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [28.0, 66.0, 1.0, 2.0, 0.570366964910409, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 662440.3201548529, 662440.3201548529, 153647.9168701875]
[2019-04-02 17:50:55,322] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 17:50:55,325] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [4.1272442e-23 1.0000000e+00 1.0250760e-19 2.7246859e-31 4.1322276e-28], sampled 0.4509833808656337
[2019-04-02 17:51:08,213] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.02173212], dtype=float32), 0.40155184]
[2019-04-02 17:51:08,216] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [25.16666666666667, 89.0, 1.0, 2.0, 0.6307226070130738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 720715.3648694527, 720715.3648694522, 163538.1386670792]
[2019-04-02 17:51:08,216] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 17:51:08,217] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [4.1032435e-22 1.0000000e+00 4.3551940e-18 9.5873515e-30 2.1463429e-26], sampled 0.22979361026078682
[2019-04-02 17:51:16,171] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.02173212], dtype=float32), 0.40155184]
[2019-04-02 17:51:16,174] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [23.0, 89.0, 1.0, 2.0, 0.4901360121592232, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 584529.3948139787, 584529.3948139787, 141351.6499240973]
[2019-04-02 17:51:16,177] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 17:51:16,180] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.5115034e-23 1.0000000e+00 1.4256915e-21 1.4244767e-32 1.5770250e-29], sampled 0.3912986858315165
[2019-04-02 17:51:20,091] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.02173212], dtype=float32), 0.40155184]
[2019-04-02 17:51:20,092] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [24.16666666666667, 42.0, 1.0, 2.0, 0.2834216597875697, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 365427.2031072191, 365427.2031072191, 113539.3881441376]
[2019-04-02 17:51:20,095] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 17:51:20,098] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.9855144e-24 1.0000000e+00 1.1209980e-23 5.0273131e-33 2.3016665e-30], sampled 0.1838215993815443
[2019-04-02 17:51:40,741] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8099.9706 2445378853.1784 746.0000
[2019-04-02 17:51:41,093] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8583.7010 2248665784.9182 553.0000
[2019-04-02 17:51:41,239] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8924.2933 2120398140.2949 430.0000
[2019-04-02 17:51:41,281] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8771.0845 2170631810.8436 493.0000
[2019-04-02 17:51:41,375] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8700.9646 2195038593.1110 572.0000
[2019-04-02 17:51:42,391] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 125000, evaluation results [125000.0, 8099.970618414005, 2445378853.1783895, 746.0, 8771.084470098069, 2170631810.843588, 493.0, 8924.293283520115, 2120398140.2948759, 430.0, 8583.700958564144, 2248665784.918216, 553.0, 8700.964635042716, 2195038593.1109576, 572.0]
[2019-04-02 17:51:46,804] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.2660287e-25 1.0000000e+00 1.2330738e-24 7.9633516e-34 8.0877048e-27], sum to 1.0000
[2019-04-02 17:51:46,813] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2474
[2019-04-02 17:51:46,820] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.15, 91.5, 1.0, 2.0, 0.6631309782993798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 755761.0069403094, 755761.0069403094, 169279.6475984121], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4746600.0000, 
sim time next is 4747200.0000, 
raw observation next is [25.2, 90.66666666666666, 1.0, 2.0, 0.6562128468906139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 747872.6640079087, 747872.6640079087, 168018.2088162292], 
processed observation next is [1.0, 0.9565217391304348, 0.4888888888888889, 0.9066666666666666, 1.0, 1.0, 0.5907295796316832, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2670973800028245, 0.2670973800028245, 0.32311194003120997], 
reward next is 0.6769, 
noisyNet noise sample is [array([-0.73046803], dtype=float32), -0.4140239]. 
=============================================
[2019-04-02 17:51:47,864] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.2770063e-22 1.0000000e+00 6.2581460e-25 2.3902133e-32 4.3932286e-30], sum to 1.0000
[2019-04-02 17:51:47,874] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7048
[2019-04-02 17:51:47,884] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.03333333333333, 93.66666666666667, 1.0, 2.0, 0.6885567323147725, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 794751.7437175903, 794751.7437175903, 174484.7017349176], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4762200.0000, 
sim time next is 4762800.0000, 
raw observation next is [24.0, 94.0, 1.0, 2.0, 0.6850679220476161, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 790593.1290413323, 790593.1290413323, 173822.8758559487], 
processed observation next is [1.0, 0.13043478260869565, 0.4444444444444444, 0.94, 1.0, 1.0, 0.6250808595804954, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.28235468894333293, 0.28235468894333293, 0.3342747612614398], 
reward next is 0.6657, 
noisyNet noise sample is [array([-0.4339105], dtype=float32), -0.3192618]. 
=============================================
[2019-04-02 17:51:49,390] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8000, global step 127647: loss -85.2364
[2019-04-02 17:51:49,394] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8000, global step 127647: learning rate 0.0010
[2019-04-02 17:51:49,505] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8154877e-25 1.0000000e+00 1.4142684e-35 3.9763912e-38 3.6513878e-33], sum to 1.0000
[2019-04-02 17:51:49,513] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1861
[2019-04-02 17:51:49,525] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1313018.143160542 W.
[2019-04-02 17:51:49,636] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.98333333333333, 94.0, 1.0, 2.0, 1.008350475745488, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.221214573775065, 6.9112, 121.9247308789282, 1313018.143160542, 1154264.680561047, 243009.1960403197], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4787400.0000, 
sim time next is 4788000.0000, 
raw observation next is [24.0, 94.0, 1.0, 2.0, 0.5649618939322195, 1.0, 1.0, 0.5649618939322195, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9258559303543, 1288205.366446436, 1288205.366446436, 256203.2168727831], 
processed observation next is [1.0, 0.43478260869565216, 0.4444444444444444, 0.94, 1.0, 1.0, 0.4820974927764518, 1.0, 0.5, 0.4820974927764518, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094608894238332, 0.46007334515944137, 0.46007334515944137, 0.4926984939861213], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.24403751], dtype=float32), -0.4403774]. 
=============================================
[2019-04-02 17:51:49,650] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[39.738403]
 [39.515457]
 [39.561443]
 [39.39375 ]
 [39.59313 ]], R is [[39.97018051]
 [39.57048035]
 [39.64435196]
 [39.24790955]
 [38.8554306 ]].
[2019-04-02 17:51:49,725] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8000, global step 127740: loss 34.5386
[2019-04-02 17:51:49,726] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8000, global step 127740: learning rate 0.0010
[2019-04-02 17:51:49,918] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8000, global step 127815: loss -37.3101
[2019-04-02 17:51:49,922] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8000, global step 127815: learning rate 0.0010
[2019-04-02 17:51:50,147] A3C_AGENT_WORKER-Thread-22 INFO:Local step 8000, global step 127902: loss -20.8415
[2019-04-02 17:51:50,153] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 8000, global step 127903: learning rate 0.0010
[2019-04-02 17:51:50,158] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8000, global step 127905: loss -66.0992
[2019-04-02 17:51:50,162] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8000, global step 127906: learning rate 0.0010
[2019-04-02 17:51:50,321] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8000, global step 127964: loss -32.9369
[2019-04-02 17:51:50,322] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8000, global step 127964: learning rate 0.0010
[2019-04-02 17:51:50,424] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8000, global step 128000: loss -113.0951
[2019-04-02 17:51:50,428] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8000, global step 128000: learning rate 0.0010
[2019-04-02 17:51:50,451] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8000, global step 128009: loss -76.1889
[2019-04-02 17:51:50,455] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8000, global step 128010: learning rate 0.0010
[2019-04-02 17:51:50,484] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8000, global step 128020: loss -43.0367
[2019-04-02 17:51:50,487] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8000, global step 128022: learning rate 0.0010
[2019-04-02 17:51:50,514] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8000, global step 128033: loss -63.1495
[2019-04-02 17:51:50,516] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8000, global step 128033: learning rate 0.0010
[2019-04-02 17:51:50,531] A3C_AGENT_WORKER-Thread-21 INFO:Local step 8000, global step 128039: loss 2.4213
[2019-04-02 17:51:50,533] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 8000, global step 128039: learning rate 0.0010
[2019-04-02 17:51:50,538] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8000, global step 128041: loss -62.3054
[2019-04-02 17:51:50,540] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8000, global step 128041: learning rate 0.0010
[2019-04-02 17:51:50,624] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8000, global step 128075: loss -132.6853
[2019-04-02 17:51:50,625] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8000, global step 128075: learning rate 0.0010
[2019-04-02 17:51:50,721] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8000, global step 128109: loss -80.4635
[2019-04-02 17:51:50,723] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8000, global step 128109: learning rate 0.0010
[2019-04-02 17:51:51,035] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8000, global step 128229: loss -161.3226
[2019-04-02 17:51:51,036] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8000, global step 128229: learning rate 0.0010
[2019-04-02 17:51:51,126] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8000, global step 128257: loss -59.2349
[2019-04-02 17:51:51,131] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8000, global step 128260: learning rate 0.0010
[2019-04-02 17:51:54,092] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.0825084e-24 1.0000000e+00 1.2489850e-24 2.3421668e-33 1.2391239e-31], sum to 1.0000
[2019-04-02 17:51:54,103] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3823
[2019-04-02 17:51:54,108] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 94.0, 1.0, 2.0, 0.8665634584606117, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 987759.5360775262, 987759.5360775262, 210072.2405908297], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4849200.0000, 
sim time next is 4849800.0000, 
raw observation next is [25.0, 94.00000000000001, 1.0, 2.0, 0.7995105086572982, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 911283.231819984, 911283.231819984, 195834.7669785132], 
processed observation next is [1.0, 0.13043478260869565, 0.48148148148148145, 0.9400000000000002, 1.0, 1.0, 0.7613220341158312, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3254582970785657, 0.3254582970785657, 0.37660532111252537], 
reward next is 0.6234, 
noisyNet noise sample is [array([0.9989122], dtype=float32), 0.859621]. 
=============================================
[2019-04-02 17:51:55,348] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.3378446e-26 1.0000000e+00 4.5181009e-21 1.1592456e-31 1.2511379e-28], sum to 1.0000
[2019-04-02 17:51:55,361] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5879
[2019-04-02 17:51:55,371] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1440986.633656215 W.
[2019-04-02 17:51:55,378] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.86666666666667, 93.33333333333334, 1.0, 2.0, 1.02, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.454057462420761, 6.9112, 121.924252405975, 1440986.633656215, 1162999.17452786, 245586.4915850116], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4868400.0000, 
sim time next is 4869000.0000, 
raw observation next is [25.9, 93.5, 1.0, 2.0, 0.4261464242504919, 1.0, 1.0, 0.4261464242504919, 1.0, 1.0, 0.6784389860269041, 6.9112, 6.9112, 121.94756008, 1457686.023775539, 1457686.023775539, 308725.0581002511], 
processed observation next is [1.0, 0.34782608695652173, 0.5148148148148147, 0.935, 1.0, 1.0, 0.3168409812505855, 1.0, 0.5, 0.3168409812505855, 1.0, 0.5, 0.5980487325336301, 0.0, 0.0, 0.8096049824067558, 0.5206021513484067, 0.5206021513484067, 0.5937020348081752], 
reward next is 0.4063, 
noisyNet noise sample is [array([1.0024978], dtype=float32), -0.44101378]. 
=============================================
[2019-04-02 17:51:55,392] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[60.005592]
 [60.94726 ]
 [60.944954]
 [60.696476]
 [60.594936]], R is [[58.05595016]
 [57.47539139]
 [57.5128746 ]
 [57.57536316]
 [57.61984634]].
[2019-04-02 17:52:10,520] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8500, global step 135531: loss 4.5689
[2019-04-02 17:52:10,522] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8500, global step 135531: learning rate 0.0010
[2019-04-02 17:52:10,840] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8500, global step 135647: loss 3.7830
[2019-04-02 17:52:10,844] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8500, global step 135647: learning rate 0.0010
[2019-04-02 17:52:11,159] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8500, global step 135770: loss 4.3132
[2019-04-02 17:52:11,161] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8500, global step 135770: learning rate 0.0010
[2019-04-02 17:52:11,387] A3C_AGENT_WORKER-Thread-22 INFO:Local step 8500, global step 135852: loss 2.7477
[2019-04-02 17:52:11,389] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 8500, global step 135852: learning rate 0.0010
[2019-04-02 17:52:11,608] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8500, global step 135935: loss 4.1160
[2019-04-02 17:52:11,611] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8500, global step 135935: learning rate 0.0010
[2019-04-02 17:52:11,645] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8500, global step 135948: loss 4.1181
[2019-04-02 17:52:11,646] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8500, global step 135948: learning rate 0.0010
[2019-04-02 17:52:11,846] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8500, global step 136023: loss 4.1302
[2019-04-02 17:52:11,850] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8500, global step 136023: learning rate 0.0010
[2019-04-02 17:52:11,866] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8500, global step 136029: loss 3.9740
[2019-04-02 17:52:11,870] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8500, global step 136030: learning rate 0.0010
[2019-04-02 17:52:11,896] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8500, global step 136043: loss 3.9162
[2019-04-02 17:52:11,897] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8500, global step 136043: learning rate 0.0010
[2019-04-02 17:52:11,919] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8500, global step 136051: loss 3.7529
[2019-04-02 17:52:11,930] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8500, global step 136052: learning rate 0.0010
[2019-04-02 17:52:11,948] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8500, global step 136057: loss 3.7617
[2019-04-02 17:52:11,950] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8500, global step 136057: learning rate 0.0010
[2019-04-02 17:52:11,955] A3C_AGENT_WORKER-Thread-21 INFO:Local step 8500, global step 136059: loss 2.9986
[2019-04-02 17:52:11,958] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 8500, global step 136061: learning rate 0.0010
[2019-04-02 17:52:12,019] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8500, global step 136087: loss 3.7116
[2019-04-02 17:52:12,026] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8500, global step 136088: learning rate 0.0010
[2019-04-02 17:52:12,371] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8500, global step 136220: loss 2.4746
[2019-04-02 17:52:12,375] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8500, global step 136221: learning rate 0.0010
[2019-04-02 17:52:12,398] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8500, global step 136226: loss 3.5473
[2019-04-02 17:52:12,404] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8500, global step 136227: learning rate 0.0010
[2019-04-02 17:52:12,631] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8500, global step 136313: loss 3.0088
[2019-04-02 17:52:12,634] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8500, global step 136314: learning rate 0.0010
[2019-04-02 17:52:15,187] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.5907433e-22 1.0000000e+00 5.8601207e-21 2.2823394e-34 2.0141680e-31], sum to 1.0000
[2019-04-02 17:52:15,197] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3204
[2019-04-02 17:52:15,204] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.65, 75.5, 1.0, 2.0, 0.8046689791088548, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 917166.3821859591, 917166.3821859587, 196917.8264600176], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5142600.0000, 
sim time next is 5143200.0000, 
raw observation next is [30.76666666666667, 75.33333333333334, 1.0, 2.0, 0.8476405933036, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 966176.553821545, 966176.553821545, 205991.2983587798], 
processed observation next is [0.0, 0.5217391304347826, 0.6950617283950619, 0.7533333333333334, 1.0, 1.0, 0.8186197539328571, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.34506305493626604, 0.34506305493626604, 0.3961371122284227], 
reward next is 0.6039, 
noisyNet noise sample is [array([-0.15960912], dtype=float32), 1.3674288]. 
=============================================
[2019-04-02 17:52:23,948] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1953598e-16 1.0000000e+00 1.5884044e-16 1.6623476e-25 5.6046862e-21], sum to 1.0000
[2019-04-02 17:52:23,957] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7520
[2019-04-02 17:52:23,962] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.51666666666667, 88.33333333333334, 1.0, 2.0, 0.6998923328327609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 797679.2035729453, 797679.2035729453, 176124.6613501184], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5260200.0000, 
sim time next is 5260800.0000, 
raw observation next is [26.43333333333334, 88.66666666666667, 1.0, 2.0, 0.7052407743460064, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 803778.1095693308, 803778.1095693308, 177139.2011854372], 
processed observation next is [1.0, 0.9130434782608695, 0.5345679012345682, 0.8866666666666667, 1.0, 1.0, 0.6490961599357219, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2870636105604753, 0.2870636105604753, 0.34065230997199464], 
reward next is 0.6593, 
noisyNet noise sample is [array([0.722607], dtype=float32), 1.334986]. 
=============================================
[2019-04-02 17:52:25,823] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.4277109e-13 1.0000000e+00 1.0260092e-15 9.4958753e-24 4.8242668e-21], sum to 1.0000
[2019-04-02 17:52:25,833] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3260
[2019-04-02 17:52:25,839] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.4, 88.66666666666667, 1.0, 2.0, 0.5555690286505225, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 657943.4551882119, 657943.4551882119, 151716.8572907148], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5291400.0000, 
sim time next is 5292000.0000, 
raw observation next is [23.3, 89.0, 1.0, 2.0, 0.5494410286935945, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 651378.1674455381, 651378.1674455381, 150726.937108558], 
processed observation next is [1.0, 0.2608695652173913, 0.41851851851851857, 0.89, 1.0, 1.0, 0.4636202722542791, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2326350598019779, 0.2326350598019779, 0.2898594944395346], 
reward next is 0.7101, 
noisyNet noise sample is [array([1.2948438], dtype=float32), 0.4793696]. 
=============================================
[2019-04-02 17:52:25,855] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[75.57338]
 [75.60353]
 [75.66631]
 [75.73173]
 [75.7236 ]], R is [[75.80561829]
 [75.75579834]
 [75.70379639]
 [75.65026855]
 [75.59996796]].
[2019-04-02 17:52:31,975] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9000, global step 143563: loss -74.8197
[2019-04-02 17:52:31,979] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9000, global step 143564: learning rate 0.0010
[2019-04-02 17:52:32,022] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.1086244e-24 1.0000000e+00 6.4964987e-19 1.2704943e-31 1.6353734e-22], sum to 1.0000
[2019-04-02 17:52:32,038] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4328
[2019-04-02 17:52:32,042] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 91.0, 1.0, 2.0, 0.7179484108209931, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 824888.1739753722, 824888.1739753722, 179902.991792306], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5382000.0000, 
sim time next is 5382600.0000, 
raw observation next is [24.65, 91.00000000000001, 1.0, 2.0, 0.7248544647170104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 832007.4164577109, 832007.4164577109, 181198.6426863543], 
processed observation next is [1.0, 0.30434782608695654, 0.46851851851851845, 0.9100000000000001, 1.0, 1.0, 0.6724457913297743, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2971455058777539, 0.2971455058777539, 0.348458928242989], 
reward next is 0.6515, 
noisyNet noise sample is [array([-0.23398305], dtype=float32), 0.50346434]. 
=============================================
[2019-04-02 17:52:32,281] A3C_AGENT_WORKER-Thread-10 INFO:Local step 9000, global step 143673: loss -71.5743
[2019-04-02 17:52:32,284] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 9000, global step 143674: learning rate 0.0010
[2019-04-02 17:52:32,588] A3C_AGENT_WORKER-Thread-9 INFO:Local step 9000, global step 143786: loss -72.2283
[2019-04-02 17:52:32,590] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 9000, global step 143786: learning rate 0.0010
[2019-04-02 17:52:32,719] A3C_AGENT_WORKER-Thread-22 INFO:Local step 9000, global step 143835: loss -46.6902
[2019-04-02 17:52:32,723] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 9000, global step 143835: learning rate 0.0010
[2019-04-02 17:52:32,987] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9000, global step 143935: loss -171.0900
[2019-04-02 17:52:32,989] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9000, global step 143935: learning rate 0.0010
[2019-04-02 17:52:33,120] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9000, global step 143984: loss 14.6484
[2019-04-02 17:52:33,122] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9000, global step 143984: learning rate 0.0010
[2019-04-02 17:52:33,248] A3C_AGENT_WORKER-Thread-21 INFO:Local step 9000, global step 144031: loss 23.4676
[2019-04-02 17:52:33,252] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 9000, global step 144031: learning rate 0.0010
[2019-04-02 17:52:33,271] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9000, global step 144042: loss -161.0838
[2019-04-02 17:52:33,273] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9000, global step 144042: learning rate 0.0010
[2019-04-02 17:52:33,286] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9000, global step 144047: loss 46.5601
[2019-04-02 17:52:33,290] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9000, global step 144047: learning rate 0.0010
[2019-04-02 17:52:33,341] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9000, global step 144065: loss -23.4218
[2019-04-02 17:52:33,343] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9000, global step 144065: learning rate 0.0010
[2019-04-02 17:52:33,377] A3C_AGENT_WORKER-Thread-19 INFO:Local step 9000, global step 144077: loss 18.5776
[2019-04-02 17:52:33,379] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 9000, global step 144078: learning rate 0.0010
[2019-04-02 17:52:33,468] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9000, global step 144112: loss -68.5266
[2019-04-02 17:52:33,472] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9000, global step 144113: learning rate 0.0010
[2019-04-02 17:52:33,525] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9000, global step 144131: loss 3.8484
[2019-04-02 17:52:33,527] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9000, global step 144133: learning rate 0.0010
[2019-04-02 17:52:33,595] A3C_AGENT_WORKER-Thread-20 INFO:Local step 9000, global step 144157: loss -36.1407
[2019-04-02 17:52:33,597] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 9000, global step 144157: learning rate 0.0010
[2019-04-02 17:52:33,625] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9000, global step 144169: loss -128.8775
[2019-04-02 17:52:33,627] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9000, global step 144169: learning rate 0.0010
[2019-04-02 17:52:33,747] A3C_AGENT_WORKER-Thread-3 INFO:Local step 9000, global step 144214: loss 68.3462
[2019-04-02 17:52:33,749] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 9000, global step 144215: learning rate 0.0010
[2019-04-02 17:52:36,703] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.43639695e-28 1.00000000e+00 1.04779564e-29 6.09068577e-34
 1.53646064e-30], sum to 1.0000
[2019-04-02 17:52:36,713] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1203
[2019-04-02 17:52:36,719] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.25, 93.5, 1.0, 2.0, 0.5118523817470552, 0.0, 1.0, 0.0, 1.0, 2.0, 0.8148856616096013, 6.9112, 6.9112, 121.9260423751758, 1167014.763737313, 1167014.763737313, 261135.2663109231], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5452200.0000, 
sim time next is 5452800.0000, 
raw observation next is [26.23333333333333, 93.66666666666667, 1.0, 2.0, 0.9600357051598862, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9259003563077, 1094380.73827696, 1094380.73827696, 231225.4168597931], 
processed observation next is [1.0, 0.08695652173913043, 0.5271604938271603, 0.9366666666666668, 1.0, 1.0, 0.952423458523674, 0.0, 1.0, -0.1904761904761905, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094611843659639, 0.39085026367034287, 0.39085026367034287, 0.4446642631919098], 
reward next is 0.5553, 
noisyNet noise sample is [array([1.5948597], dtype=float32), -0.7371869]. 
=============================================
[2019-04-02 17:52:49,184] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-02 17:52:49,192] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 17:52:49,192] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:52:49,194] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 17:52:49,196] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 17:52:49,196] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:52:49,196] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 17:52:49,197] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:52:49,197] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 17:52:49,198] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:52:49,200] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:52:49,220] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run7
[2019-04-02 17:52:49,247] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run7
[2019-04-02 17:52:49,252] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run7
[2019-04-02 17:52:49,286] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run7
[2019-04-02 17:52:49,288] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run7
[2019-04-02 17:52:56,497] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04415052], dtype=float32), 0.3539782]
[2019-04-02 17:52:56,499] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [29.0, 32.33333333333333, 1.0, 2.0, 0.6743124343654219, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 852758.2923797703, 852758.2923797703, 174266.0546637208]
[2019-04-02 17:52:56,499] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 17:52:56,503] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [9.3989578e-26 1.0000000e+00 7.8508423e-27 3.6402457e-34 1.6703846e-30], sampled 0.8645649572029119
[2019-04-02 17:53:14,982] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04415052], dtype=float32), 0.3539782]
[2019-04-02 17:53:14,984] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [23.3, 68.0, 1.0, 2.0, 0.3970176048178098, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 493155.7517088855, 493155.751708885, 128226.6928431847]
[2019-04-02 17:53:14,985] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 17:53:14,988] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.7975128e-28 1.0000000e+00 8.6153820e-30 3.1744183e-38 1.3933364e-34], sampled 0.18504474498409962
[2019-04-02 17:53:16,190] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04415052], dtype=float32), 0.3539782]
[2019-04-02 17:53:16,192] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [24.56020993666667, 62.85725187833333, 1.0, 2.0, 0.3836273416813699, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 474951.0511482218, 474951.0511482218, 126328.1795977264]
[2019-04-02 17:53:16,193] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 17:53:16,195] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [6.7492283e-29 1.0000000e+00 7.7952005e-31 5.6651033e-38 1.1507798e-34], sampled 0.7446723288489537
[2019-04-02 17:53:22,684] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04415052], dtype=float32), 0.3539782]
[2019-04-02 17:53:22,685] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [26.35024450333334, 58.91464356666667, 1.0, 2.0, 0.4484329247882733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 545440.0867175888, 545440.0867175888, 135381.2700235489]
[2019-04-02 17:53:22,685] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 17:53:22,688] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [2.0521203e-27 1.0000000e+00 2.1774894e-28 1.0378350e-36 6.0062655e-33], sampled 0.8581097565003143
[2019-04-02 17:53:41,496] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04415052], dtype=float32), 0.3539782]
[2019-04-02 17:53:41,497] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [27.24922691, 50.710719895, 1.0, 2.0, 0.4048424762016172, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 498675.5037320531, 498675.5037320531, 129233.7996691054]
[2019-04-02 17:53:41,498] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 17:53:41,501] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [9.2258847e-27 1.0000000e+00 5.3605070e-29 7.5976898e-36 2.2799921e-32], sampled 0.6303481619390214
[2019-04-02 17:54:35,343] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04415052], dtype=float32), 0.3539782]
[2019-04-02 17:54:35,346] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [25.08333333333333, 49.5, 1.0, 2.0, 0.777579102555398, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.926042615646, 981585.6727528515, 981585.672752851, 194838.4921853856]
[2019-04-02 17:54:35,346] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 17:54:35,349] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [2.5238733e-26 1.0000000e+00 3.1560266e-32 1.8733438e-35 8.7491092e-32], sampled 0.06417391678379503
[2019-04-02 17:54:36,165] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04415052], dtype=float32), 0.3539782]
[2019-04-02 17:54:36,166] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [24.8, 90.0, 1.0, 2.0, 0.5912487113051594, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 678184.4228136328, 678184.4228136324, 156794.7170284333]
[2019-04-02 17:54:36,166] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 17:54:36,170] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [2.1717760e-26 1.0000000e+00 2.1084592e-28 3.5171055e-35 2.2601714e-31], sampled 0.22826592610257812
[2019-04-02 17:54:37,073] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04415052], dtype=float32), 0.3539782]
[2019-04-02 17:54:37,075] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [24.23687312, 44.54508331, 1.0, 2.0, 0.2862476671366732, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 366987.7959840809, 366987.7959840809, 113893.1269980257]
[2019-04-02 17:54:37,076] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 17:54:37,080] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.6987375e-28 1.0000000e+00 2.8642298e-30 2.4060427e-37 5.6155849e-34], sampled 0.5278403166363177
[2019-04-02 17:54:47,231] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04415052], dtype=float32), 0.3539782]
[2019-04-02 17:54:47,232] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [19.75755391666667, 95.80720356833334, 1.0, 2.0, 0.3982786116769176, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 493435.2156179147, 493435.2156179147, 128375.7041466189]
[2019-04-02 17:54:47,233] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 17:54:47,237] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [3.5351017e-28 1.0000000e+00 3.6312502e-31 6.8862035e-37 1.0146192e-33], sampled 0.46686577796013184
[2019-04-02 17:54:56,307] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8099.9424 2445324199.3218 746.0000
[2019-04-02 17:54:56,526] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8771.0852 2170656631.3663 493.0000
[2019-04-02 17:54:56,561] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8700.9646 2195038593.1110 572.0000
[2019-04-02 17:54:56,935] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8924.2933 2120398140.2949 430.0000
[2019-04-02 17:54:57,007] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8584.5265 2248663206.3456 553.0000
[2019-04-02 17:54:58,023] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 150000, evaluation results [150000.0, 8099.942407935641, 2445324199.321786, 746.0, 8771.085197215923, 2170656631.3662677, 493.0, 8924.293283520115, 2120398140.2948759, 430.0, 8584.526542641555, 2248663206.345631, 553.0, 8700.964635042716, 2195038593.1109576, 572.0]
[2019-04-02 17:54:59,008] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.5612766e-27 1.0000000e+00 2.2953330e-25 1.4012343e-36 1.1879631e-32], sum to 1.0000
[2019-04-02 17:54:59,016] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5697
[2019-04-02 17:54:59,025] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.95, 96.0, 1.0, 2.0, 0.6028838149844531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 692934.2739085577, 692934.2739085577, 158862.2123546475], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5638200.0000, 
sim time next is 5638800.0000, 
raw observation next is [24.0, 96.0, 1.0, 2.0, 0.6061968031043733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 695966.023321951, 695966.0233219514, 159398.8328567443], 
processed observation next is [0.0, 0.2608695652173913, 0.4444444444444444, 0.96, 1.0, 1.0, 0.5311866703623491, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.24855929404355395, 0.24855929404355406, 0.3065362170322006], 
reward next is 0.6935, 
noisyNet noise sample is [array([-0.57036066], dtype=float32), 0.02559232]. 
=============================================
[2019-04-02 17:55:00,743] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.778002e-30 1.000000e+00 6.545447e-21 0.000000e+00 2.501746e-29], sum to 1.0000
[2019-04-02 17:55:00,753] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0549
[2019-04-02 17:55:00,759] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.25, 82.66666666666667, 1.0, 2.0, 0.6920683116177165, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999998, 6.9112, 121.9260426156618, 788757.4452950526, 788757.4452950535, 174648.6955599671], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5655000.0000, 
sim time next is 5655600.0000, 
raw observation next is [27.4, 82.0, 1.0, 2.0, 0.6941929828995457, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 791180.2049876829, 791180.2049876829, 175048.5603713311], 
processed observation next is [0.0, 0.4782608695652174, 0.5703703703703703, 0.82, 1.0, 1.0, 0.6359440272613639, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2825643589241725, 0.2825643589241725, 0.33663184686794445], 
reward next is 0.6634, 
noisyNet noise sample is [array([1.8689432], dtype=float32), 0.9481858]. 
=============================================
[2019-04-02 17:55:02,109] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9500, global step 151544: loss 0.0383
[2019-04-02 17:55:02,115] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9500, global step 151545: learning rate 0.0010
[2019-04-02 17:55:02,388] A3C_AGENT_WORKER-Thread-10 INFO:Local step 9500, global step 151653: loss 0.0032
[2019-04-02 17:55:02,393] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 9500, global step 151654: learning rate 0.0010
[2019-04-02 17:55:02,650] A3C_AGENT_WORKER-Thread-9 INFO:Local step 9500, global step 151749: loss 0.0166
[2019-04-02 17:55:02,656] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 9500, global step 151750: learning rate 0.0010
[2019-04-02 17:55:02,737] A3C_AGENT_WORKER-Thread-22 INFO:Local step 9500, global step 151781: loss 0.0234
[2019-04-02 17:55:02,739] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 9500, global step 151782: learning rate 0.0010
[2019-04-02 17:55:03,055] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9500, global step 151899: loss 0.1666
[2019-04-02 17:55:03,060] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9500, global step 151899: learning rate 0.0010
[2019-04-02 17:55:03,269] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9500, global step 151980: loss 0.0326
[2019-04-02 17:55:03,271] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9500, global step 151980: learning rate 0.0010
[2019-04-02 17:55:03,392] A3C_AGENT_WORKER-Thread-21 INFO:Local step 9500, global step 152025: loss 0.0229
[2019-04-02 17:55:03,394] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 9500, global step 152026: learning rate 0.0010
[2019-04-02 17:55:03,411] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9500, global step 152030: loss 0.0128
[2019-04-02 17:55:03,417] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9500, global step 152032: learning rate 0.0010
[2019-04-02 17:55:03,440] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9500, global step 152044: loss 0.0023
[2019-04-02 17:55:03,446] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9500, global step 152044: learning rate 0.0010
[2019-04-02 17:55:03,522] A3C_AGENT_WORKER-Thread-19 INFO:Local step 9500, global step 152072: loss 0.0042
[2019-04-02 17:55:03,528] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 9500, global step 152074: learning rate 0.0010
[2019-04-02 17:55:03,582] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9500, global step 152094: loss 0.0017
[2019-04-02 17:55:03,586] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9500, global step 152096: learning rate 0.0010
[2019-04-02 17:55:03,664] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9500, global step 152126: loss 0.0109
[2019-04-02 17:55:03,669] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9500, global step 152126: learning rate 0.0010
[2019-04-02 17:55:03,684] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9500, global step 152134: loss 0.0060
[2019-04-02 17:55:03,685] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9500, global step 152134: learning rate 0.0010
[2019-04-02 17:55:03,862] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9500, global step 152198: loss 0.0079
[2019-04-02 17:55:03,866] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9500, global step 152199: learning rate 0.0010
[2019-04-02 17:55:03,957] A3C_AGENT_WORKER-Thread-3 INFO:Local step 9500, global step 152233: loss 0.0059
[2019-04-02 17:55:03,960] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 9500, global step 152233: learning rate 0.0010
[2019-04-02 17:55:03,974] A3C_AGENT_WORKER-Thread-20 INFO:Local step 9500, global step 152239: loss 0.0015
[2019-04-02 17:55:03,977] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 9500, global step 152240: learning rate 0.0010
[2019-04-02 17:55:20,547] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.647306e-27 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-02 17:55:20,555] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6049
[2019-04-02 17:55:20,559] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.65, 85.5, 1.0, 2.0, 0.4713366857674947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 569760.3210193092, 569760.3210193092, 138726.7495181194], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5959800.0000, 
sim time next is 5960400.0000, 
raw observation next is [22.63333333333333, 85.0, 1.0, 2.0, 0.4680733910078664, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 566588.464542882, 566588.4645428816, 138254.3504390669], 
processed observation next is [1.0, 1.0, 0.393827160493827, 0.85, 1.0, 1.0, 0.3667540369141266, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2023530230510293, 0.20235302305102915, 0.26587375084435944], 
reward next is 0.7341, 
noisyNet noise sample is [array([-0.28045982], dtype=float32), -0.8715676]. 
=============================================
[2019-04-02 17:55:23,371] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10000, global step 159499: loss -64.4513
[2019-04-02 17:55:23,377] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10000, global step 159500: learning rate 0.0010
[2019-04-02 17:55:23,788] A3C_AGENT_WORKER-Thread-10 INFO:Local step 10000, global step 159653: loss -45.7465
[2019-04-02 17:55:23,791] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 10000, global step 159653: learning rate 0.0010
[2019-04-02 17:55:24,124] A3C_AGENT_WORKER-Thread-9 INFO:Local step 10000, global step 159774: loss -36.4704
[2019-04-02 17:55:24,128] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 10000, global step 159775: learning rate 0.0010
[2019-04-02 17:55:24,392] A3C_AGENT_WORKER-Thread-22 INFO:Local step 10000, global step 159875: loss -53.7196
[2019-04-02 17:55:24,395] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 10000, global step 159875: learning rate 0.0010
[2019-04-02 17:55:24,475] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10000, global step 159901: loss -25.0020
[2019-04-02 17:55:24,476] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10000, global step 159902: learning rate 0.0010
[2019-04-02 17:55:24,689] A3C_AGENT_WORKER-Thread-21 INFO:Local step 10000, global step 159983: loss -111.6099
[2019-04-02 17:55:24,692] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 10000, global step 159983: learning rate 0.0010
[2019-04-02 17:55:24,717] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10000, global step 159991: loss -69.8742
[2019-04-02 17:55:24,721] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10000, global step 159994: learning rate 0.0010
[2019-04-02 17:55:24,739] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10000, global step 160000: loss -56.3853
[2019-04-02 17:55:24,739] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10000, global step 160000: learning rate 0.0010
[2019-04-02 17:55:24,773] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10000, global step 160010: loss -68.7069
[2019-04-02 17:55:24,775] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10000, global step 160010: learning rate 0.0010
[2019-04-02 17:55:24,824] A3C_AGENT_WORKER-Thread-19 INFO:Local step 10000, global step 160027: loss -99.2193
[2019-04-02 17:55:24,829] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 10000, global step 160029: learning rate 0.0010
[2019-04-02 17:55:24,940] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10000, global step 160073: loss -30.6809
[2019-04-02 17:55:24,942] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10000, global step 160073: learning rate 0.0010
[2019-04-02 17:55:25,017] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10000, global step 160099: loss -41.4343
[2019-04-02 17:55:25,018] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10000, global step 160099: learning rate 0.0010
[2019-04-02 17:55:25,061] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10000, global step 160115: loss -22.8171
[2019-04-02 17:55:25,064] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10000, global step 160116: learning rate 0.0010
[2019-04-02 17:55:25,090] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10000, global step 160125: loss 2.0626
[2019-04-02 17:55:25,093] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10000, global step 160125: learning rate 0.0010
[2019-04-02 17:55:25,410] A3C_AGENT_WORKER-Thread-20 INFO:Local step 10000, global step 160241: loss -61.0063
[2019-04-02 17:55:25,412] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 10000, global step 160241: learning rate 0.0010
[2019-04-02 17:55:25,474] A3C_AGENT_WORKER-Thread-3 INFO:Local step 10000, global step 160263: loss -47.1629
[2019-04-02 17:55:25,477] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 10000, global step 160264: learning rate 0.0010
[2019-04-02 17:55:26,288] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.5486132e-20 1.0000000e+00 1.8055240e-30 2.1481576e-37 1.6590809e-30], sum to 1.0000
[2019-04-02 17:55:26,299] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4776
[2019-04-02 17:55:26,310] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1689820.639933631 W.
[2019-04-02 17:55:26,323] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.0, 60.0, 1.0, 2.0, 0.8550900266549156, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9977734948820727, 6.911200000000001, 6.9112, 121.9260426156291, 1689820.639933631, 1689820.63993363, 347436.5484363934], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6010800.0000, 
sim time next is 6011400.0000, 
raw observation next is [29.0, 59.5, 1.0, 2.0, 0.7231578969763438, 1.0, 1.0, 0.7231578969763438, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1649296.116363716, 1649296.116363716, 313023.6243660438], 
processed observation next is [1.0, 0.5652173913043478, 0.6296296296296297, 0.595, 1.0, 1.0, 0.6704260678289807, 1.0, 0.5, 0.6704260678289807, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.5890343272727557, 0.5890343272727557, 0.6019685083962381], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.41489345], dtype=float32), 1.1374955]. 
=============================================
[2019-04-02 17:55:27,311] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.7421825e-19 1.0000000e+00 1.8437354e-11 5.3623424e-31 1.8964275e-24], sum to 1.0000
[2019-04-02 17:55:27,322] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3695
[2019-04-02 17:55:27,330] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.35, 69.5, 1.0, 2.0, 0.5302533657670122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 626787.686622951, 626787.686622951, 147511.1128162339], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6035400.0000, 
sim time next is 6036000.0000, 
raw observation next is [26.23333333333333, 70.0, 1.0, 2.0, 0.5293400796480651, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 626073.4126661824, 626073.4126661824, 147377.7736004535], 
processed observation next is [1.0, 0.8695652173913043, 0.5271604938271603, 0.7, 1.0, 1.0, 0.43969057100960124, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.22359764738077942, 0.22359764738077942, 0.28341879538548753], 
reward next is 0.7166, 
noisyNet noise sample is [array([1.3082322], dtype=float32), 0.35670167]. 
=============================================
[2019-04-02 17:55:27,347] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[50.662945]
 [50.573166]
 [50.853714]
 [50.56803 ]
 [50.704124]], R is [[51.19896317]
 [51.40329742]
 [51.60541153]
 [51.8053627 ]
 [52.00329208]].
[2019-04-02 17:55:29,476] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5511619e-20 1.0000000e+00 2.4750489e-25 3.4746750e-33 1.3651690e-32], sum to 1.0000
[2019-04-02 17:55:29,488] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0284
[2019-04-02 17:55:29,496] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.4, 86.0, 1.0, 2.0, 0.5579521260775825, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 665225.9178012228, 665225.9178012228, 152286.3034165575], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6066000.0000, 
sim time next is 6066600.0000, 
raw observation next is [23.46666666666667, 85.66666666666667, 1.0, 2.0, 0.5993366234318728, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 714337.592929791, 714337.592929791, 159351.5911071184], 
processed observation next is [1.0, 0.21739130434782608, 0.42469135802469143, 0.8566666666666667, 1.0, 1.0, 0.5230197897998485, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.25512056890349677, 0.25512056890349677, 0.3064453675136892], 
reward next is 0.6936, 
noisyNet noise sample is [array([0.6289672], dtype=float32), 0.01094357]. 
=============================================
[2019-04-02 17:55:31,093] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1726278e-20 1.0000000e+00 4.1837299e-20 1.9042282e-31 1.7860776e-28], sum to 1.0000
[2019-04-02 17:55:31,102] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8171
[2019-04-02 17:55:31,111] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1499979.059207733 W.
[2019-04-02 17:55:31,117] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.53333333333333, 70.33333333333334, 1.0, 2.0, 1.02, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.497077956287765, 6.9112, 121.9239894088543, 1499979.059207733, 1199962.232711029, 247589.6578655135], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6078000.0000, 
sim time next is 6078600.0000, 
raw observation next is [27.55, 65.5, 1.0, 2.0, 0.7118693721253574, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9864932046143152, 6.911200000000003, 6.9112, 121.9256898098176, 1536566.016544863, 1536566.016544861, 317425.1583540627], 
processed observation next is [1.0, 0.34782608695652173, 0.575925925925926, 0.655, 1.0, 1.0, 0.6569873477682826, 0.0, 1.0, -0.1904761904761905, 1.0, 0.5, 0.9831165057678939, 2.6645352591003756e-16, 0.0, 0.8094597865562226, 0.5487735773374511, 0.5487735773374504, 0.610432996834736], 
reward next is 0.3896, 
noisyNet noise sample is [array([0.68130714], dtype=float32), -0.4793135]. 
=============================================
[2019-04-02 17:55:31,164] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.3936053e-20 1.0000000e+00 1.0624396e-34 3.8817174e-37 8.0205896e-38], sum to 1.0000
[2019-04-02 17:55:31,173] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9016
[2019-04-02 17:55:31,180] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1329804.046830516 W.
[2019-04-02 17:55:31,187] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.51666666666667, 56.83333333333334, 1.0, 2.0, 0.3887932599240695, 1.0, 1.0, 0.3887932599240695, 1.0, 2.0, 0.6189715319116064, 6.911199999999999, 6.9112, 121.94756008, 1329804.046830516, 1329804.046830516, 292207.7672393529], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6083400.0000, 
sim time next is 6084000.0000, 
raw observation next is [29.3, 58.0, 1.0, 2.0, 0.7294466983540168, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9973597932285804, 6.911199999999999, 6.9112, 121.9260426156618, 1546772.927674136, 1546772.927674137, 322574.6444812125], 
processed observation next is [1.0, 0.43478260869565216, 0.6407407407407407, 0.58, 1.0, 1.0, 0.6779127361357342, 0.0, 0.5, -0.1904761904761905, 1.0, 1.0, 0.9966997415357254, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.5524189027407628, 0.5524189027407632, 0.6203358547715625], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.8351791], dtype=float32), 1.15824]. 
=============================================
[2019-04-02 17:55:31,202] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[55.206615]
 [54.475655]
 [53.74056 ]
 [53.70044 ]
 [54.645283]], R is [[54.59864426]
 [54.05265808]
 [53.51213074]
 [52.97700882]
 [52.44723892]].
[2019-04-02 17:55:38,530] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.2801107e-03 9.9371994e-01 2.3809044e-28 3.8763629e-25 7.3235243e-21], sum to 1.0000
[2019-04-02 17:55:38,538] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7937
[2019-04-02 17:55:38,549] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2171026.374230124 W.
[2019-04-02 17:55:38,559] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.91666666666667, 52.66666666666667, 1.0, 2.0, 1.02, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9902206785731065, 7.463974856762873, 6.9112, 121.923683364327, 2171026.374230124, 1887961.718477709, 381124.4474303401], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6192600.0000, 
sim time next is 6193200.0000, 
raw observation next is [29.93333333333333, 52.33333333333334, 1.0, 2.0, 0.8546235872020105, 1.0, 1.0, 0.8546235872020105, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.925709713789, 1949455.275181035, 1949455.275181035, 366840.843552638], 
processed observation next is [1.0, 0.6956521739130435, 0.6641975308641974, 0.5233333333333334, 1.0, 1.0, 0.8269328419071553, 1.0, 0.5, 0.8269328419071553, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.8094599186978941, 0.6962340268503696, 0.6962340268503696, 0.7054631606781501], 
reward next is 0.2945, 
noisyNet noise sample is [array([0.17706583], dtype=float32), -1.7914858]. 
=============================================
[2019-04-02 17:55:44,740] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10500, global step 167487: loss 0.1330
[2019-04-02 17:55:44,743] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10500, global step 167489: learning rate 0.0010
[2019-04-02 17:55:45,130] A3C_AGENT_WORKER-Thread-10 INFO:Local step 10500, global step 167631: loss 0.0357
[2019-04-02 17:55:45,134] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 10500, global step 167632: learning rate 0.0010
[2019-04-02 17:55:45,489] A3C_AGENT_WORKER-Thread-9 INFO:Local step 10500, global step 167764: loss 0.1645
[2019-04-02 17:55:45,492] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 10500, global step 167764: learning rate 0.0010
[2019-04-02 17:55:45,756] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10500, global step 167857: loss 0.0019
[2019-04-02 17:55:45,761] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10500, global step 167857: learning rate 0.0010
[2019-04-02 17:55:45,856] A3C_AGENT_WORKER-Thread-22 INFO:Local step 10500, global step 167897: loss 0.0254
[2019-04-02 17:55:45,862] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 10500, global step 167898: learning rate 0.0010
[2019-04-02 17:55:46,079] A3C_AGENT_WORKER-Thread-21 INFO:Local step 10500, global step 167984: loss 0.0265
[2019-04-02 17:55:46,081] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 10500, global step 167984: learning rate 0.0010
[2019-04-02 17:55:46,150] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10500, global step 168009: loss 0.0010
[2019-04-02 17:55:46,151] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10500, global step 168009: learning rate 0.0010
[2019-04-02 17:55:46,160] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10500, global step 168011: loss 0.0110
[2019-04-02 17:55:46,163] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10500, global step 168011: loss 0.0186
[2019-04-02 17:55:46,163] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10500, global step 168011: learning rate 0.0010
[2019-04-02 17:55:46,166] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10500, global step 168013: learning rate 0.0010
[2019-04-02 17:55:46,303] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10500, global step 168065: loss 0.0651
[2019-04-02 17:55:46,305] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10500, global step 168065: learning rate 0.0010
[2019-04-02 17:55:46,323] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10500, global step 168073: loss 0.0533
[2019-04-02 17:55:46,327] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10500, global step 168074: learning rate 0.0010
[2019-04-02 17:55:46,340] A3C_AGENT_WORKER-Thread-19 INFO:Local step 10500, global step 168077: loss 0.0720
[2019-04-02 17:55:46,342] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 10500, global step 168078: learning rate 0.0010
[2019-04-02 17:55:46,470] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10500, global step 168124: loss 0.0308
[2019-04-02 17:55:46,472] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10500, global step 168125: learning rate 0.0010
[2019-04-02 17:55:46,528] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.4658284e-23 1.0000000e+00 1.6565491e-25 4.7905758e-37 1.4795096e-33], sum to 1.0000
[2019-04-02 17:55:46,534] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8767
[2019-04-02 17:55:46,542] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.91666666666667, 66.83333333333333, 1.0, 2.0, 0.5944717488279332, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 688405.3955770566, 688405.3955770566, 157654.7867830165], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6293400.0000, 
sim time next is 6294000.0000, 
raw observation next is [27.73333333333334, 67.66666666666667, 1.0, 2.0, 0.5923677373533767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 686441.7799248466, 686441.7799248466, 157315.0000815664], 
processed observation next is [0.0, 0.8695652173913043, 0.5827160493827164, 0.6766666666666667, 1.0, 1.0, 0.5147234968492579, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.24515777854458806, 0.24515777854458806, 0.3025288463107046], 
reward next is 0.6975, 
noisyNet noise sample is [array([-1.0462795], dtype=float32), 0.08029024]. 
=============================================
[2019-04-02 17:55:46,582] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[75.807236]
 [75.71929 ]
 [75.601295]
 [75.53971 ]
 [75.47351 ]], R is [[75.80834961]
 [75.74708557]
 [75.68557739]
 [75.62359619]
 [75.56124115]].
[2019-04-02 17:55:46,627] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10500, global step 168181: loss 0.0241
[2019-04-02 17:55:46,629] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10500, global step 168181: learning rate 0.0010
[2019-04-02 17:55:46,962] A3C_AGENT_WORKER-Thread-3 INFO:Local step 10500, global step 168304: loss 0.0590
[2019-04-02 17:55:46,966] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 10500, global step 168306: learning rate 0.0010
[2019-04-02 17:55:47,017] A3C_AGENT_WORKER-Thread-20 INFO:Local step 10500, global step 168328: loss 0.0454
[2019-04-02 17:55:47,019] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 10500, global step 168328: learning rate 0.0010
[2019-04-02 17:55:52,799] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.3508579e-26 1.0000000e+00 6.2716869e-28 1.3663021e-36 2.8971716e-32], sum to 1.0000
[2019-04-02 17:55:52,811] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1507
[2019-04-02 17:55:52,820] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.76666666666667, 82.0, 1.0, 2.0, 0.6821823696665255, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 777484.6202186167, 777484.6202186163, 172796.7605566508], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6394800.0000, 
sim time next is 6395400.0000, 
raw observation next is [26.65, 82.5, 1.0, 2.0, 0.6825589677407253, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 777914.0476021738, 777914.0476021738, 172866.6964049963], 
processed observation next is [1.0, 0.0, 0.5425925925925925, 0.825, 1.0, 1.0, 0.6220940092151492, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2778264455722049, 0.2778264455722049, 0.33243595462499287], 
reward next is 0.6676, 
noisyNet noise sample is [array([-1.1063728], dtype=float32), -0.89080197]. 
=============================================
[2019-04-02 17:56:04,840] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-02 17:56:04,841] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 17:56:04,842] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 17:56:04,842] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:56:04,844] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:56:04,844] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 17:56:04,845] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:56:04,845] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 17:56:04,846] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:56:04,848] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 17:56:04,848] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:56:04,871] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run8
[2019-04-02 17:56:04,871] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run8
[2019-04-02 17:56:04,872] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run8
[2019-04-02 17:56:04,891] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run8
[2019-04-02 17:56:04,936] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run8
[2019-04-02 17:56:28,563] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.01749008], dtype=float32), 0.3064601]
[2019-04-02 17:56:28,565] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [31.0, 36.0, 1.0, 2.0, 0.4104703635880589, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 504518.3705632236, 504518.3705632231, 130005.4385301216]
[2019-04-02 17:56:28,566] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 17:56:28,568] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [4.6293197e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.6114094702064927
[2019-04-02 17:56:39,535] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.01749008], dtype=float32), 0.3064601]
[2019-04-02 17:56:39,538] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [22.58333333333334, 88.33333333333334, 1.0, 2.0, 0.4716703677687719, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 567753.6552885142, 567753.6552885142, 138698.0692721086]
[2019-04-02 17:56:39,539] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 17:56:39,550] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.964564e-37 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.8879395485794926
[2019-04-02 17:56:46,786] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.01749008], dtype=float32), 0.3064601]
[2019-04-02 17:56:46,789] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [28.05435958, 64.92407085, 1.0, 2.0, 0.5634649567366093, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 655820.5274334579, 655820.5274334579, 152554.8149375361]
[2019-04-02 17:56:46,791] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 17:56:46,793] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [4.248311e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.6418764506813126
[2019-04-02 17:56:50,220] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.01749008], dtype=float32), 0.3064601]
[2019-04-02 17:56:50,220] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [30.0, 61.0, 1.0, 2.0, 0.6341665197077936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 722735.0607195784, 722735.0607195784, 164054.3814565828]
[2019-04-02 17:56:50,220] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 17:56:50,222] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [3.2077125e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.8828812700495469
[2019-04-02 17:56:51,278] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.01749008], dtype=float32), 0.3064601]
[2019-04-02 17:56:51,279] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [26.42235981333333, 78.18582709333333, 1.0, 2.0, 0.6251285168489474, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 717462.7394414172, 717462.7394414167, 162702.594212128]
[2019-04-02 17:56:51,279] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 17:56:51,283] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.2179378e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.037884693246639345
[2019-04-02 17:56:58,711] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.01749008], dtype=float32), 0.3064601]
[2019-04-02 17:56:58,712] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [22.5027818, 111.7329841666667, 1.0, 2.0, 0.604235789049436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 710305.0346209714, 710305.0346209714, 159812.228004992]
[2019-04-02 17:56:58,713] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 17:56:58,715] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [2.2285556e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.08092528346790184
[2019-04-02 17:57:19,270] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.01749008], dtype=float32), 0.3064601]
[2019-04-02 17:57:19,272] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [26.46666666666667, 82.0, 1.0, 2.0, 0.6291086586529124, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 716968.1163802242, 716968.1163802242, 163157.3084101519]
[2019-04-02 17:57:19,273] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 17:57:19,281] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.2218746e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.0928703028639446
[2019-04-02 17:57:46,503] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.01749008], dtype=float32), 0.3064601]
[2019-04-02 17:57:46,505] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [27.43333333333333, 72.33333333333333, 1.0, 2.0, 0.6155978042853035, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 706392.8666301301, 706392.8666301301, 161019.7107148675]
[2019-04-02 17:57:46,507] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 17:57:46,510] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.2210598e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.404589446358501
[2019-04-02 17:58:11,961] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8771.9095 2170601483.0346 493.0000
[2019-04-02 17:58:12,012] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8924.2933 2120398140.2949 430.0000
[2019-04-02 17:58:12,485] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8701.7435 2195019782.1641 572.0000
[2019-04-02 17:58:12,608] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8100.7222 2445312557.4025 746.0000
[2019-04-02 17:58:12,670] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8584.5265 2248663206.3456 553.0000
[2019-04-02 17:58:13,686] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 175000, evaluation results [175000.0, 8100.722247181571, 2445312557.402538, 746.0, 8771.90946536216, 2170601483.03465, 493.0, 8924.293283520115, 2120398140.2948759, 430.0, 8584.526542641555, 2248663206.345631, 553.0, 8701.74352259033, 2195019782.1641326, 572.0]
[2019-04-02 17:58:14,759] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11000, global step 175416: loss 1.6197
[2019-04-02 17:58:14,761] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11000, global step 175416: learning rate 0.0010
[2019-04-02 17:58:15,446] A3C_AGENT_WORKER-Thread-10 INFO:Local step 11000, global step 175673: loss 2.3341
[2019-04-02 17:58:15,450] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 11000, global step 175673: learning rate 0.0010
[2019-04-02 17:58:15,581] A3C_AGENT_WORKER-Thread-9 INFO:Local step 11000, global step 175721: loss 2.1816
[2019-04-02 17:58:15,586] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 11000, global step 175722: learning rate 0.0010
[2019-04-02 17:58:15,935] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11000, global step 175854: loss 3.6325
[2019-04-02 17:58:15,940] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11000, global step 175854: learning rate 0.0010
[2019-04-02 17:58:16,156] A3C_AGENT_WORKER-Thread-22 INFO:Local step 11000, global step 175934: loss 2.1388
[2019-04-02 17:58:16,163] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 11000, global step 175935: learning rate 0.0010
[2019-04-02 17:58:16,227] A3C_AGENT_WORKER-Thread-21 INFO:Local step 11000, global step 175960: loss 2.4163
[2019-04-02 17:58:16,228] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11000, global step 175960: loss 1.8855
[2019-04-02 17:58:16,231] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 11000, global step 175961: learning rate 0.0010
[2019-04-02 17:58:16,232] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11000, global step 175961: learning rate 0.0010
[2019-04-02 17:58:16,371] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11000, global step 176013: loss 1.6722
[2019-04-02 17:58:16,379] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11000, global step 176013: learning rate 0.0010
[2019-04-02 17:58:16,402] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11000, global step 176025: loss 1.3787
[2019-04-02 17:58:16,407] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11000, global step 176025: learning rate 0.0010
[2019-04-02 17:58:16,424] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11000, global step 176032: loss 1.0568
[2019-04-02 17:58:16,429] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11000, global step 176034: learning rate 0.0010
[2019-04-02 17:58:16,491] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11000, global step 176058: loss 0.9740
[2019-04-02 17:58:16,494] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11000, global step 176060: learning rate 0.0010
[2019-04-02 17:58:16,727] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11000, global step 176146: loss 1.3662
[2019-04-02 17:58:16,729] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11000, global step 176147: learning rate 0.0010
[2019-04-02 17:58:16,733] A3C_AGENT_WORKER-Thread-19 INFO:Local step 11000, global step 176148: loss 0.7939
[2019-04-02 17:58:16,734] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 11000, global step 176148: learning rate 0.0010
[2019-04-02 17:58:16,792] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11000, global step 176172: loss 0.8353
[2019-04-02 17:58:16,795] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11000, global step 176172: learning rate 0.0010
[2019-04-02 17:58:17,105] A3C_AGENT_WORKER-Thread-3 INFO:Local step 11000, global step 176287: loss 0.4414
[2019-04-02 17:58:17,111] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 11000, global step 176287: learning rate 0.0010
[2019-04-02 17:58:17,193] A3C_AGENT_WORKER-Thread-20 INFO:Local step 11000, global step 176317: loss 0.4274
[2019-04-02 17:58:17,196] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 11000, global step 176318: learning rate 0.0010
[2019-04-02 17:58:29,174] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3937191e-23 1.0000000e+00 6.0627820e-32 0.0000000e+00 1.2251496e-32], sum to 1.0000
[2019-04-02 17:58:29,185] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7828
[2019-04-02 17:58:29,195] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1371272.950480525 W.
[2019-04-02 17:58:29,206] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.98333333333333, 50.5, 1.0, 2.0, 0.5739724364107475, 1.0, 1.0, 0.5739724364107475, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9257318538691, 1371272.950480525, 1371272.950480525, 261958.8170802592], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6792600.0000, 
sim time next is 6793200.0000, 
raw observation next is [28.0, 51.0, 1.0, 2.0, 0.5531728653057261, 1.0, 2.0, 0.5531728653057261, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260425209013, 1316452.968875252, 1316452.968875253, 254761.2483259983], 
processed observation next is [1.0, 0.6521739130434783, 0.5925925925925926, 0.51, 1.0, 1.0, 0.4680629348877691, 1.0, 1.0, 0.4680629348877691, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621281910248, 0.47016177459830427, 0.47016177459830466, 0.48992547754999677], 
reward next is 0.5101, 
noisyNet noise sample is [array([1.5881877], dtype=float32), -1.444927]. 
=============================================
[2019-04-02 17:58:31,394] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.4487703e-27 1.0000000e+00 3.4763870e-32 0.0000000e+00 1.6568730e-38], sum to 1.0000
[2019-04-02 17:58:31,404] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1059
[2019-04-02 17:58:31,409] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.75, 71.5, 1.0, 2.0, 0.4706683240800819, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 568457.7567627896, 568457.7567627896, 138608.7559849348], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6813000.0000, 
sim time next is 6813600.0000, 
raw observation next is [24.66666666666666, 72.0, 1.0, 2.0, 0.4709568442032145, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 568848.1030863775, 568848.103086377, 138654.1338251148], 
processed observation next is [1.0, 0.8695652173913043, 0.4691358024691356, 0.72, 1.0, 1.0, 0.37018671928954106, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2031600368165634, 0.20316003681656322, 0.2666425650482977], 
reward next is 0.7334, 
noisyNet noise sample is [array([1.1143178], dtype=float32), 1.7349594]. 
=============================================
[2019-04-02 17:58:31,779] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.940927e-23 1.000000e+00 8.675470e-30 0.000000e+00 5.105608e-32], sum to 1.0000
[2019-04-02 17:58:31,791] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5517
[2019-04-02 17:58:31,797] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.36666666666667, 74.33333333333334, 1.0, 2.0, 0.4733333948453738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 571272.1156407197, 571272.1156407197, 139002.6003826506], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6816000.0000, 
sim time next is 6816600.0000, 
raw observation next is [24.3, 75.0, 1.0, 2.0, 0.474378115301586, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 572217.097322017, 572217.097322017, 139152.044865565], 
processed observation next is [1.0, 0.9130434782608695, 0.4555555555555556, 0.75, 1.0, 1.0, 0.37425966107331665, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.20436324904357747, 0.20436324904357747, 0.2676000862799327], 
reward next is 0.7324, 
noisyNet noise sample is [array([0.22847258], dtype=float32), -0.10122794]. 
=============================================
[2019-04-02 17:58:36,145] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11500, global step 183437: loss 0.0158
[2019-04-02 17:58:36,147] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11500, global step 183439: learning rate 0.0010
[2019-04-02 17:58:36,650] A3C_AGENT_WORKER-Thread-10 INFO:Local step 11500, global step 183623: loss 0.4279
[2019-04-02 17:58:36,652] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 11500, global step 183623: learning rate 0.0010
[2019-04-02 17:58:36,718] A3C_AGENT_WORKER-Thread-9 INFO:Local step 11500, global step 183650: loss 0.3893
[2019-04-02 17:58:36,724] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 11500, global step 183650: learning rate 0.0010
[2019-04-02 17:58:37,314] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11500, global step 183879: loss 0.0032
[2019-04-02 17:58:37,316] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11500, global step 183879: learning rate 0.0010
[2019-04-02 17:58:37,405] A3C_AGENT_WORKER-Thread-22 INFO:Local step 11500, global step 183909: loss 0.0023
[2019-04-02 17:58:37,407] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 11500, global step 183910: learning rate 0.0010
[2019-04-02 17:58:37,502] A3C_AGENT_WORKER-Thread-21 INFO:Local step 11500, global step 183946: loss 0.0091
[2019-04-02 17:58:37,504] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 11500, global step 183946: learning rate 0.0010
[2019-04-02 17:58:37,618] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11500, global step 183985: loss 0.0235
[2019-04-02 17:58:37,622] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11500, global step 183986: learning rate 0.0010
[2019-04-02 17:58:37,741] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11500, global step 184029: loss 0.0005
[2019-04-02 17:58:37,744] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11500, global step 184029: learning rate 0.0010
[2019-04-02 17:58:37,787] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11500, global step 184046: loss 0.0092
[2019-04-02 17:58:37,792] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11500, global step 184047: learning rate 0.0010
[2019-04-02 17:58:37,880] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11500, global step 184083: loss 0.0317
[2019-04-02 17:58:37,881] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11500, global step 184083: learning rate 0.0010
[2019-04-02 17:58:37,983] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11500, global step 184121: loss 0.0256
[2019-04-02 17:58:37,987] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11500, global step 184121: learning rate 0.0010
[2019-04-02 17:58:38,019] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11500, global step 184133: loss 0.0048
[2019-04-02 17:58:38,024] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11500, global step 184135: learning rate 0.0010
[2019-04-02 17:58:38,117] A3C_AGENT_WORKER-Thread-19 INFO:Local step 11500, global step 184168: loss 0.0006
[2019-04-02 17:58:38,119] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 11500, global step 184168: learning rate 0.0010
[2019-04-02 17:58:38,173] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11500, global step 184189: loss 0.0450
[2019-04-02 17:58:38,178] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11500, global step 184189: learning rate 0.0010
[2019-04-02 17:58:38,330] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.8563585e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 17:58:38,337] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2468
[2019-04-02 17:58:38,342] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.43333333333333, 57.83333333333334, 1.0, 2.0, 0.4466430704283983, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 544704.7356019783, 544704.7356019778, 135158.0936875749], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6897000.0000, 
sim time next is 6897600.0000, 
raw observation next is [26.3, 58.0, 1.0, 2.0, 0.4424390781885845, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 540342.1858348153, 540342.1858348148, 134556.7806394899], 
processed observation next is [0.0, 0.8695652173913043, 0.5296296296296297, 0.58, 1.0, 1.0, 0.336236997843553, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1929793520838626, 0.19297935208386244, 0.25876303969132675], 
reward next is 0.7412, 
noisyNet noise sample is [array([-0.552601], dtype=float32), 0.6416429]. 
=============================================
[2019-04-02 17:58:38,494] A3C_AGENT_WORKER-Thread-20 INFO:Local step 11500, global step 184310: loss 0.0194
[2019-04-02 17:58:38,498] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 11500, global step 184310: learning rate 0.0010
[2019-04-02 17:58:38,504] A3C_AGENT_WORKER-Thread-3 INFO:Local step 11500, global step 184312: loss 0.0202
[2019-04-02 17:58:38,506] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 11500, global step 184312: learning rate 0.0010
[2019-04-02 17:58:39,782] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.602727e-37 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-02 17:58:39,794] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2524
[2019-04-02 17:58:39,800] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.03333333333333, 81.0, 1.0, 2.0, 0.4165695959733413, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 513268.1903908714, 513268.1903908714, 130909.6147166401], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6924000.0000, 
sim time next is 6924600.0000, 
raw observation next is [21.95, 81.5, 1.0, 2.0, 0.4159822314287137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 512658.361617891, 512658.361617891, 130828.1050242323], 
processed observation next is [0.0, 0.13043478260869565, 0.36851851851851847, 0.815, 1.0, 1.0, 0.30474075170084963, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.18309227200638964, 0.18309227200638964, 0.2515925096619852], 
reward next is 0.7484, 
noisyNet noise sample is [array([1.9545035], dtype=float32), 0.29168212]. 
=============================================
[2019-04-02 17:58:46,357] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3275138e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 17:58:46,372] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9852
[2019-04-02 17:58:46,379] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.26666666666667, 88.0, 1.0, 2.0, 0.4331538285825101, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 532591.7098035072, 532591.7098035068, 133288.2918898822], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7021200.0000, 
sim time next is 7021800.0000, 
raw observation next is [21.35, 87.5, 1.0, 2.0, 0.4307008946851896, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 529442.604689321, 529442.604689321, 132925.9251009003], 
processed observation next is [1.0, 0.2608695652173913, 0.3462962962962963, 0.875, 1.0, 1.0, 0.32226296986332087, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.18908664453190036, 0.18908664453190036, 0.2556267790401929], 
reward next is 0.7444, 
noisyNet noise sample is [array([0.01325106], dtype=float32), -1.1749378]. 
=============================================
[2019-04-02 17:58:46,455] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.167025e-35 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-02 17:58:46,467] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4872
[2019-04-02 17:58:46,472] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.01666666666667, 89.33333333333333, 1.0, 2.0, 0.4887504962153975, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 601673.7167324402, 601673.7167324397, 141728.3146024143], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7019400.0000, 
sim time next is 7020000.0000, 
raw observation next is [21.1, 89.0, 1.0, 2.0, 0.4831983363350192, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 594478.1486132552, 594478.1486132548, 140853.16819205], 
processed observation next is [1.0, 0.2608695652173913, 0.3370370370370371, 0.89, 1.0, 1.0, 0.38475992420835614, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.212313624504734, 0.21231362450473384, 0.27087147729240385], 
reward next is 0.7291, 
noisyNet noise sample is [array([-0.5506094], dtype=float32), 0.49699366]. 
=============================================
[2019-04-02 17:58:46,483] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[69.374756]
 [69.34602 ]
 [69.3034  ]
 [69.26572 ]
 [69.09345 ]], R is [[69.46734619]
 [69.50012207]
 [69.53242493]
 [69.56433105]
 [69.59220123]].
[2019-04-02 17:58:50,163] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.8084984e-32 1.0000000e+00 2.3595698e-36 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 17:58:50,173] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3663
[2019-04-02 17:58:50,179] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 81.66666666666667, 1.0, 2.0, 0.4839150257246977, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 582093.0024440693, 582093.0024440688, 140565.4579456629], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7076400.0000, 
sim time next is 7077000.0000, 
raw observation next is [23.5, 81.83333333333334, 1.0, 2.0, 0.4849588231539975, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 583120.1167974209, 583120.1167974209, 140719.1871152882], 
processed observation next is [1.0, 0.9130434782608695, 0.42592592592592593, 0.8183333333333335, 1.0, 1.0, 0.38685574184999705, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.20825718457050746, 0.20825718457050746, 0.27061382137555423], 
reward next is 0.7294, 
noisyNet noise sample is [array([-1.1064684], dtype=float32), -0.60018665]. 
=============================================
[2019-04-02 17:58:50,191] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[66.26365]
 [66.12807]
 [66.13928]
 [65.9116 ]
 [65.7312 ]], R is [[66.32476044]
 [66.3911972 ]
 [66.45707703]
 [66.52237701]
 [66.58705139]].
[2019-04-02 17:58:54,576] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4974568e-27 1.0000000e+00 3.2182909e-29 1.3376998e-36 1.7428228e-33], sum to 1.0000
[2019-04-02 17:58:54,591] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5818
[2019-04-02 17:58:54,596] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.4, 76.0, 1.0, 2.0, 0.7472884164114342, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 924230.9975375833, 924230.9975375833, 188221.9712616723], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7123200.0000, 
sim time next is 7123800.0000, 
raw observation next is [22.5, 75.5, 1.0, 2.0, 0.7662290357505965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 947215.1546053814, 947215.154605381, 192078.4794313849], 
processed observation next is [1.0, 0.43478260869565216, 0.3888888888888889, 0.755, 1.0, 1.0, 0.7217012330364244, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.33829112664477906, 0.3382911266447789, 0.3693816912142018], 
reward next is 0.6306, 
noisyNet noise sample is [array([0.826213], dtype=float32), -0.028577704]. 
=============================================
[2019-04-02 17:58:57,441] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12000, global step 191412: loss 0.0248
[2019-04-02 17:58:57,444] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12000, global step 191412: learning rate 0.0010
[2019-04-02 17:58:57,908] A3C_AGENT_WORKER-Thread-10 INFO:Local step 12000, global step 191586: loss 0.0090
[2019-04-02 17:58:57,915] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 12000, global step 191586: learning rate 0.0010
[2019-04-02 17:58:58,142] A3C_AGENT_WORKER-Thread-9 INFO:Local step 12000, global step 191673: loss 0.0120
[2019-04-02 17:58:58,147] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 12000, global step 191674: learning rate 0.0010
[2019-04-02 17:58:58,502] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12000, global step 191806: loss 0.0153
[2019-04-02 17:58:58,504] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12000, global step 191807: learning rate 0.0010
[2019-04-02 17:58:58,677] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.2601674e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.8869467e-38], sum to 1.0000
[2019-04-02 17:58:58,686] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8372
[2019-04-02 17:58:58,695] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.63333333333333, 92.33333333333333, 1.0, 2.0, 0.3800729449551913, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 474551.2949743198, 474551.2949743198, 125921.6284981176], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7188000.0000, 
sim time next is 7188600.0000, 
raw observation next is [19.61666666666667, 92.16666666666667, 1.0, 2.0, 0.3741321870940775, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 467367.4500184584, 467367.4500184584, 125112.0376787789], 
processed observation next is [1.0, 0.17391304347826086, 0.28209876543209894, 0.9216666666666667, 1.0, 1.0, 0.2549192703500922, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16691694643516372, 0.16691694643516372, 0.24060007245919018], 
reward next is 0.7594, 
noisyNet noise sample is [array([1.8165244], dtype=float32), 1.0198689]. 
=============================================
[2019-04-02 17:58:58,781] A3C_AGENT_WORKER-Thread-21 INFO:Local step 12000, global step 191907: loss 0.0239
[2019-04-02 17:58:58,785] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 12000, global step 191907: learning rate 0.0010
[2019-04-02 17:58:58,893] A3C_AGENT_WORKER-Thread-22 INFO:Local step 12000, global step 191950: loss 0.0320
[2019-04-02 17:58:58,893] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12000, global step 191950: loss 0.0249
[2019-04-02 17:58:58,901] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 12000, global step 191951: learning rate 0.0010
[2019-04-02 17:58:58,901] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12000, global step 191951: learning rate 0.0010
[2019-04-02 17:58:59,066] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12000, global step 192014: loss 0.0304
[2019-04-02 17:58:59,067] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12000, global step 192014: learning rate 0.0010
[2019-04-02 17:58:59,145] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12000, global step 192043: loss 0.0192
[2019-04-02 17:58:59,152] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12000, global step 192045: learning rate 0.0010
[2019-04-02 17:58:59,225] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12000, global step 192073: loss 0.0198
[2019-04-02 17:58:59,226] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12000, global step 192073: learning rate 0.0010
[2019-04-02 17:58:59,361] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12000, global step 192122: loss 0.0604
[2019-04-02 17:58:59,366] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12000, global step 192123: learning rate 0.0010
[2019-04-02 17:58:59,397] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12000, global step 192138: loss 0.0266
[2019-04-02 17:58:59,404] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12000, global step 192139: learning rate 0.0010
[2019-04-02 17:58:59,484] A3C_AGENT_WORKER-Thread-19 INFO:Local step 12000, global step 192168: loss 0.0061
[2019-04-02 17:58:59,486] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 12000, global step 192168: learning rate 0.0010
[2019-04-02 17:58:59,682] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12000, global step 192245: loss 0.0011
[2019-04-02 17:58:59,684] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12000, global step 192245: learning rate 0.0010
[2019-04-02 17:58:59,980] A3C_AGENT_WORKER-Thread-20 INFO:Local step 12000, global step 192357: loss 0.0058
[2019-04-02 17:58:59,985] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 12000, global step 192358: learning rate 0.0010
[2019-04-02 17:58:59,990] A3C_AGENT_WORKER-Thread-3 INFO:Local step 12000, global step 192359: loss 0.0069
[2019-04-02 17:58:59,993] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 12000, global step 192359: learning rate 0.0010
[2019-04-02 17:59:05,370] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.4807244e-33 1.0000000e+00 6.3011782e-37 0.0000000e+00 2.6194259e-38], sum to 1.0000
[2019-04-02 17:59:05,384] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7666
[2019-04-02 17:59:05,390] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.31666666666667, 88.83333333333334, 1.0, 2.0, 0.3833628222725802, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 477081.2906134371, 477081.2906134371, 126344.4282265207], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7275000.0000, 
sim time next is 7275600.0000, 
raw observation next is [20.3, 89.0, 1.0, 2.0, 0.3792595934097932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 471953.4272550129, 471953.4272550134, 125779.0837667355], 
processed observation next is [1.0, 0.21739130434782608, 0.3074074074074074, 0.89, 1.0, 1.0, 0.26102332548784907, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.16855479544821889, 0.16855479544821905, 0.24188285339756826], 
reward next is 0.7581, 
noisyNet noise sample is [array([-0.23716924], dtype=float32), -0.4717983]. 
=============================================
[2019-04-02 17:59:17,190] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.9161284e-30 1.0000000e+00 6.5211970e-36 0.0000000e+00 2.5600887e-35], sum to 1.0000
[2019-04-02 17:59:17,203] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9667
[2019-04-02 17:59:17,211] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.6, 94.5, 1.0, 2.0, 0.3704060100999986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 461335.3546405231, 461335.3546405231, 124578.4777416606], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7453800.0000, 
sim time next is 7454400.0000, 
raw observation next is [19.66666666666666, 94.33333333333334, 1.0, 2.0, 0.372261010647395, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 463345.9992650067, 463345.9992650067, 124824.5912623287], 
processed observation next is [0.0, 0.2608695652173913, 0.2839506172839504, 0.9433333333333335, 1.0, 1.0, 0.2526916793421369, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.16548071402321668, 0.16548071402321668, 0.24004729088909366], 
reward next is 0.7600, 
noisyNet noise sample is [array([-2.1650493], dtype=float32), -2.1344917]. 
=============================================
[2019-04-02 17:59:18,303] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.2277487e-32 1.0000000e+00 3.2818562e-35 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 17:59:18,311] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6852
[2019-04-02 17:59:18,316] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.88333333333333, 93.66666666666667, 1.0, 2.0, 0.3773618861454158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 468789.9574950189, 468789.9574950189, 125502.2172711109], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7456200.0000, 
sim time next is 7456800.0000, 
raw observation next is [19.96666666666667, 93.33333333333334, 1.0, 2.0, 0.3790467052541288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 470591.9665960503, 470591.9665960503, 125726.9518179108], 
processed observation next is [0.0, 0.30434782608695654, 0.2950617283950618, 0.9333333333333335, 1.0, 1.0, 0.2607698872072962, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.1680685594985894, 0.1680685594985894, 0.24178259964982846], 
reward next is 0.7582, 
noisyNet noise sample is [array([1.5663421], dtype=float32), 0.3747921]. 
=============================================
[2019-04-02 17:59:18,640] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12500, global step 199366: loss 0.1194
[2019-04-02 17:59:18,643] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12500, global step 199366: learning rate 0.0010
[2019-04-02 17:59:19,419] A3C_AGENT_WORKER-Thread-10 INFO:Local step 12500, global step 199654: loss 0.0768
[2019-04-02 17:59:19,420] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 12500, global step 199654: learning rate 0.0010
[2019-04-02 17:59:19,432] A3C_AGENT_WORKER-Thread-9 INFO:Local step 12500, global step 199658: loss 0.0952
[2019-04-02 17:59:19,433] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 12500, global step 199658: learning rate 0.0010
[2019-04-02 17:59:19,882] A3C_AGENT_WORKER-Thread-21 INFO:Local step 12500, global step 199823: loss 0.0008
[2019-04-02 17:59:19,887] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 12500, global step 199823: learning rate 0.0010
[2019-04-02 17:59:19,974] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12500, global step 199862: loss 0.0214
[2019-04-02 17:59:19,984] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12500, global step 199862: learning rate 0.0010
[2019-04-02 17:59:20,146] A3C_AGENT_WORKER-Thread-22 INFO:Local step 12500, global step 199922: loss 0.0062
[2019-04-02 17:59:20,150] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 12500, global step 199922: learning rate 0.0010
[2019-04-02 17:59:20,184] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12500, global step 199936: loss 0.0101
[2019-04-02 17:59:20,186] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12500, global step 199936: learning rate 0.0010
[2019-04-02 17:59:20,360] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-02 17:59:20,363] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 17:59:20,364] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:59:20,365] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 17:59:20,367] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 17:59:20,367] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:59:20,368] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 17:59:20,369] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:59:20,370] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:59:20,372] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 17:59:20,374] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 17:59:20,396] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run9
[2019-04-02 17:59:20,397] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run9
[2019-04-02 17:59:20,429] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run9
[2019-04-02 17:59:20,431] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run9
[2019-04-02 17:59:20,466] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run9
[2019-04-02 17:59:42,568] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.1327769], dtype=float32), 0.27745524]
[2019-04-02 17:59:42,569] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [37.85, 17.5, 1.0, 2.0, 0.5947911003794369, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9585026054159275, 6.911199999999999, 6.9112, 121.9260426156618, 1433794.950721097, 1433794.950721098, 288658.5409410999]
[2019-04-02 17:59:42,570] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 17:59:42,573] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.8501673e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.12422736614549157
[2019-04-02 17:59:42,574] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1433794.950721097 W.
[2019-04-02 17:59:46,264] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.1327769], dtype=float32), 0.27745524]
[2019-04-02 17:59:46,266] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [22.83333333333334, 73.0, 1.0, 2.0, 0.3954846662359483, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 489956.2350213508, 489956.2350213513, 127983.5819133463]
[2019-04-02 17:59:46,267] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 17:59:46,270] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.3647924e-27 1.0000000e+00 1.2531701e-32 0.0000000e+00 4.2432620e-36], sampled 0.5574420699154397
[2019-04-02 18:00:37,989] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.1327769], dtype=float32), 0.27745524]
[2019-04-02 18:00:37,991] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [23.9, 94.0, 1.0, 2.0, 0.6344926246877193, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.9260426156618, 1438036.512174214, 1438036.512174214, 305716.1476552707]
[2019-04-02 18:00:37,991] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 18:00:37,996] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.5033914e-28 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.6177301226181776
[2019-04-02 18:00:37,997] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1438036.512174214 W.
[2019-04-02 18:00:55,031] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.1327769], dtype=float32), 0.27745524]
[2019-04-02 18:00:55,034] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [27.36666666666667, 46.0, 1.0, 2.0, 0.3674101540974513, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 456828.9431559573, 456828.9431559573, 124156.8081111136]
[2019-04-02 18:00:55,035] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 18:00:55,038] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [4.1696752e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.41188920972888854
[2019-04-02 18:01:21,851] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.1327769], dtype=float32), 0.27745524]
[2019-04-02 18:01:21,857] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [22.7, 54.66666666666666, 1.0, 2.0, 0.2871261133678694, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 367445.8807973326, 367445.8807973326, 114000.9320076829]
[2019-04-02 18:01:21,859] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 18:01:21,860] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.7533968e-28 1.0000000e+00 2.0098766e-32 0.0000000e+00 5.7557897e-37], sampled 0.8003259543808987
[2019-04-02 18:01:24,291] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.1327769], dtype=float32), 0.27745524]
[2019-04-02 18:01:24,293] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [19.63911443, 89.051623595, 1.0, 2.0, 0.3425331405382834, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 430114.7735321165, 430114.7735321165, 120925.7817175852]
[2019-04-02 18:01:24,293] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 18:01:24,297] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [7.6383763e-28 1.0000000e+00 1.5556144e-32 0.0000000e+00 1.6066211e-36], sampled 0.44767728317387345
[2019-04-02 18:01:27,858] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8771.9095 2170601483.0346 493.0000
[2019-04-02 18:01:27,924] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8701.7435 2195019782.1641 572.0000
[2019-04-02 18:01:28,194] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 8100.7222 2445312557.4025 746.0000
[2019-04-02 18:01:28,250] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8584.5265 2248663206.3456 553.0000
[2019-04-02 18:01:28,280] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8924.2933 2120398140.2949 430.0000
[2019-04-02 18:01:29,295] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 200000, evaluation results [200000.0, 8100.722247181571, 2445312557.402538, 746.0, 8771.90946536216, 2170601483.03465, 493.0, 8924.293283520115, 2120398140.2948759, 430.0, 8584.526542641555, 2248663206.345631, 553.0, 8701.74352259033, 2195019782.1641326, 572.0]
[2019-04-02 18:01:29,420] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12500, global step 200058: loss 0.0007
[2019-04-02 18:01:29,423] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12500, global step 200058: learning rate 0.0010
[2019-04-02 18:01:29,441] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12500, global step 200064: loss 0.0112
[2019-04-02 18:01:29,447] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12500, global step 200065: learning rate 0.0010
[2019-04-02 18:01:29,545] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12500, global step 200096: loss 0.0378
[2019-04-02 18:01:29,548] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12500, global step 200097: learning rate 0.0010
[2019-04-02 18:01:29,569] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12500, global step 200107: loss 0.0452
[2019-04-02 18:01:29,572] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12500, global step 200108: learning rate 0.0010
[2019-04-02 18:01:29,618] A3C_AGENT_WORKER-Thread-19 INFO:Local step 12500, global step 200128: loss 0.0229
[2019-04-02 18:01:29,622] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 12500, global step 200128: learning rate 0.0010
[2019-04-02 18:01:29,637] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12500, global step 200137: loss 0.0268
[2019-04-02 18:01:29,639] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12500, global step 200137: learning rate 0.0010
[2019-04-02 18:01:30,041] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12500, global step 200282: loss 0.0091
[2019-04-02 18:01:30,045] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12500, global step 200282: learning rate 0.0010
[2019-04-02 18:01:30,246] A3C_AGENT_WORKER-Thread-20 INFO:Local step 12500, global step 200361: loss 0.0029
[2019-04-02 18:01:30,252] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 12500, global step 200361: learning rate 0.0010
[2019-04-02 18:01:30,303] A3C_AGENT_WORKER-Thread-3 INFO:Local step 12500, global step 200382: loss 0.0200
[2019-04-02 18:01:30,307] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 12500, global step 200382: learning rate 0.0010
[2019-04-02 18:01:32,673] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0469633e-24 1.0000000e+00 1.9643053e-26 2.4623675e-37 2.4968132e-32], sum to 1.0000
[2019-04-02 18:01:32,683] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2926
[2019-04-02 18:01:32,690] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 86.83333333333333, 1.0, 2.0, 0.4817097468288104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 576662.2134818739, 576662.2134818734, 140127.9247231103], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7552200.0000, 
sim time next is 7552800.0000, 
raw observation next is [23.3, 86.0, 1.0, 2.0, 0.4859719934882878, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 580847.5959282393, 580847.5959282388, 140753.2304730742], 
processed observation next is [0.0, 0.43478260869565216, 0.41851851851851857, 0.86, 1.0, 1.0, 0.3880618970098664, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.20744556997437116, 0.207445569974371, 0.27067928937129654], 
reward next is 0.7293, 
noisyNet noise sample is [array([1.4410447], dtype=float32), 2.6262856]. 
=============================================
[2019-04-02 18:01:33,969] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.4202597e-30 1.0000000e+00 1.6142474e-33 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:01:33,982] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6561
[2019-04-02 18:01:33,990] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 62.0, 1.0, 2.0, 0.5080096640461012, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 604229.4878441992, 604229.4878441987, 144096.2115306186], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7578000.0000, 
sim time next is 7578600.0000, 
raw observation next is [27.13333333333333, 63.16666666666667, 1.0, 2.0, 0.5071988925467272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 603236.615391143, 603236.615391143, 143966.9606240462], 
processed observation next is [0.0, 0.7391304347826086, 0.5604938271604937, 0.6316666666666667, 1.0, 1.0, 0.41333201493658, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.21544164835397964, 0.21544164835397964, 0.2768595396616273], 
reward next is 0.7231, 
noisyNet noise sample is [array([-0.42286792], dtype=float32), 0.82514966]. 
=============================================
[2019-04-02 18:01:34,174] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.8700194e-31 1.0000000e+00 8.9964375e-34 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:01:34,183] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0654
[2019-04-02 18:01:34,190] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 65.33333333333333, 1.0, 2.0, 0.5641669426975701, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 655540.187773559, 655540.1877735585, 152622.7493541865], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7571400.0000, 
sim time next is 7572000.0000, 
raw observation next is [28.0, 64.66666666666667, 1.0, 2.0, 0.5603499506481299, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 652563.397347733, 652563.3973477326, 152051.7144729476], 
processed observation next is [0.0, 0.6521739130434783, 0.5925925925925926, 0.6466666666666667, 1.0, 1.0, 0.4766070841049165, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.23305835619561893, 0.23305835619561877, 0.29240714321720696], 
reward next is 0.7076, 
noisyNet noise sample is [array([-0.04162901], dtype=float32), 0.69110566]. 
=============================================
[2019-04-02 18:01:34,207] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[81.32036 ]
 [81.21339 ]
 [81.08873 ]
 [81.00632 ]
 [80.912544]], R is [[81.31977844]
 [81.21308136]
 [81.10726166]
 [81.00493622]
 [80.90605927]].
[2019-04-02 18:01:48,923] A3C_AGENT_WORKER-Thread-15 INFO:Local step 13000, global step 207362: loss 0.1015
[2019-04-02 18:01:48,925] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 13000, global step 207362: learning rate 0.0010
[2019-04-02 18:01:49,622] A3C_AGENT_WORKER-Thread-9 INFO:Local step 13000, global step 207627: loss 0.1369
[2019-04-02 18:01:49,624] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 13000, global step 207627: learning rate 0.0010
[2019-04-02 18:01:49,676] A3C_AGENT_WORKER-Thread-10 INFO:Local step 13000, global step 207646: loss 0.1090
[2019-04-02 18:01:49,679] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 13000, global step 207647: learning rate 0.0010
[2019-04-02 18:01:50,207] A3C_AGENT_WORKER-Thread-21 INFO:Local step 13000, global step 207848: loss 0.3467
[2019-04-02 18:01:50,211] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 13000, global step 207848: learning rate 0.0010
[2019-04-02 18:01:50,302] A3C_AGENT_WORKER-Thread-22 INFO:Local step 13000, global step 207878: loss 0.3953
[2019-04-02 18:01:50,308] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 13000, global step 207881: learning rate 0.0010
[2019-04-02 18:01:50,427] A3C_AGENT_WORKER-Thread-11 INFO:Local step 13000, global step 207929: loss 0.3793
[2019-04-02 18:01:50,430] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 13000, global step 207929: learning rate 0.0010
[2019-04-02 18:01:50,627] A3C_AGENT_WORKER-Thread-16 INFO:Local step 13000, global step 208004: loss 0.2024
[2019-04-02 18:01:50,627] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 13000, global step 208004: learning rate 0.0010
[2019-04-02 18:01:50,671] A3C_AGENT_WORKER-Thread-2 INFO:Local step 13000, global step 208019: loss 0.0858
[2019-04-02 18:01:50,673] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 13000, global step 208019: learning rate 0.0010
[2019-04-02 18:01:50,784] A3C_AGENT_WORKER-Thread-12 INFO:Local step 13000, global step 208060: loss 0.1746
[2019-04-02 18:01:50,788] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 13000, global step 208060: learning rate 0.0010
[2019-04-02 18:01:50,801] A3C_AGENT_WORKER-Thread-14 INFO:Local step 13000, global step 208068: loss 0.1303
[2019-04-02 18:01:50,803] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 13000, global step 208068: learning rate 0.0010
[2019-04-02 18:01:50,864] A3C_AGENT_WORKER-Thread-18 INFO:Local step 13000, global step 208090: loss 0.1618
[2019-04-02 18:01:50,867] A3C_AGENT_WORKER-Thread-19 INFO:Local step 13000, global step 208092: loss 0.1374
[2019-04-02 18:01:50,867] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 13000, global step 208092: learning rate 0.0010
[2019-04-02 18:01:50,870] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 13000, global step 208093: learning rate 0.0010
[2019-04-02 18:01:50,901] A3C_AGENT_WORKER-Thread-13 INFO:Local step 13000, global step 208102: loss 0.0742
[2019-04-02 18:01:50,904] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 13000, global step 208102: learning rate 0.0010
[2019-04-02 18:01:51,384] A3C_AGENT_WORKER-Thread-17 INFO:Local step 13000, global step 208288: loss 0.3762
[2019-04-02 18:01:51,387] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 13000, global step 208288: learning rate 0.0010
[2019-04-02 18:01:51,545] A3C_AGENT_WORKER-Thread-20 INFO:Local step 13000, global step 208344: loss 0.5068
[2019-04-02 18:01:51,546] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 13000, global step 208344: learning rate 0.0010
[2019-04-02 18:01:51,652] A3C_AGENT_WORKER-Thread-3 INFO:Local step 13000, global step 208386: loss 0.7435
[2019-04-02 18:01:51,653] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 13000, global step 208386: learning rate 0.0010
[2019-04-02 18:01:54,890] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.3516396e-21 9.0188646e-14 1.0000000e+00 7.2697202e-32 6.1798650e-23], sum to 1.0000
[2019-04-02 18:01:54,900] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4688
[2019-04-02 18:01:54,904] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [21.5, 83.0, 1.0, 2.0, 0.2107721107157802, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3465570350119225, 6.911199999999999, 6.9112, 121.9260426156618, 517955.4542522862, 517955.4542522866, 170481.9974133676], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7873200.0000, 
sim time next is 7873800.0000, 
raw observation next is [21.35, 83.5, 1.0, 2.0, 0.2493258542142423, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4101419858536435, 6.911199999999999, 6.9112, 121.9260426156618, 613027.3366099423, 613027.3366099427, 179600.9435036966], 
processed observation next is [1.0, 0.13043478260869565, 0.3462962962962963, 0.835, 1.0, 1.0, 0.10634030263600275, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.2626774823170543, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2189383345035508, 0.21893833450355096, 0.3453864298148011], 
reward next is 0.6546, 
noisyNet noise sample is [array([-0.6772414], dtype=float32), 1.0543106]. 
=============================================
[2019-04-02 18:02:00,606] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:02:00,610] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:02:00,649] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res10/Eplus-env-sub_run2
[2019-04-02 18:02:01,221] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:02:01,222] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:02:01,277] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:02:01,277] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:02:01,532] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res5/Eplus-env-sub_run2
[2019-04-02 18:02:01,533] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res4/Eplus-env-sub_run2
[2019-04-02 18:02:01,827] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:02:01,827] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:02:01,831] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res16/Eplus-env-sub_run2
[2019-04-02 18:02:01,844] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:02:01,845] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:02:01,851] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res17/Eplus-env-sub_run2
[2019-04-02 18:02:01,908] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:02:01,908] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:02:01,910] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res6/Eplus-env-sub_run2
[2019-04-02 18:02:01,933] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:02:01,933] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:02:01,936] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res11/Eplus-env-sub_run2
[2019-04-02 18:02:02,005] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:02:02,005] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:02:02,009] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:02:02,009] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:02:02,010] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res2/Eplus-env-sub_run2
[2019-04-02 18:02:02,022] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:02:02,022] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:02:02,023] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:02:02,023] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:02:02,027] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:02:02,028] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:02:02,029] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res7/Eplus-env-sub_run2
[2019-04-02 18:02:02,040] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res13/Eplus-env-sub_run2
[2019-04-02 18:02:02,058] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:02:02,059] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:02:02,063] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res9/Eplus-env-sub_run2
[2019-04-02 18:02:02,080] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res14/Eplus-env-sub_run2
[2019-04-02 18:02:02,099] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:02:02,101] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:02:02,102] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res15/Eplus-env-sub_run2
[2019-04-02 18:02:02,116] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res8/Eplus-env-sub_run2
[2019-04-02 18:02:02,153] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:02:02,154] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:02:02,155] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res3/Eplus-env-sub_run2
[2019-04-02 18:02:02,190] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:02:02,190] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:02:02,191] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res12/Eplus-env-sub_run2
[2019-04-02 18:02:10,730] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.7460458e-06 8.6401522e-01 1.3598008e-01 9.6063568e-27 1.4826008e-15], sum to 1.0000
[2019-04-02 18:02:10,742] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7864
[2019-04-02 18:02:10,753] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1701667.574725129 W.
[2019-04-02 18:02:10,758] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [37.3, 8.5, 1.0, 2.0, 0.7864501946639233, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9608330472130198, 6.911199999999999, 6.9112, 121.9260426156618, 1701667.574725129, 1701667.57472513, 316345.0347839416], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 144600.0000, 
sim time next is 145200.0000, 
raw observation next is [37.3, 8.0, 1.0, 2.0, 0.7969288166093188, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9635647316446372, 6.9112, 6.9112, 121.9260426156618, 1719025.892855943, 1719025.892855943, 317499.125763223], 
processed observation next is [1.0, 0.6956521739130435, 0.9370370370370369, 0.08, 1.0, 1.0, 0.75824859120157, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.9544559145557965, 0.0, 0.0, 0.8094621288201359, 0.6139378188771225, 0.6139378188771225, 0.6105752418523519], 
reward next is 0.3894, 
noisyNet noise sample is [array([-0.388282], dtype=float32), -0.8599706]. 
=============================================
[2019-04-02 18:02:15,158] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.653053e-19 8.097934e-17 1.000000e+00 5.541004e-33 3.555501e-33], sum to 1.0000
[2019-04-02 18:02:15,167] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7113
[2019-04-02 18:02:15,172] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [33.05, 13.5, 1.0, 2.0, 0.1865351041410435, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3319757771258929, 6.911199999999999, 6.9112, 121.9260426156618, 479147.3829352713, 479147.3829352718, 160875.7316409504], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 156600.0000, 
sim time next is 157200.0000, 
raw observation next is [32.76666666666667, 14.0, 1.0, 2.0, 0.1852523400274333, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3299378838895095, 6.911199999999999, 6.9112, 121.9260426156618, 475964.0519821961, 475964.0519821966, 160566.0555479517], 
processed observation next is [1.0, 0.8260869565217391, 0.769135802469136, 0.14, 1.0, 1.0, 0.030062309556468204, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.16242235486188686, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1699871614222129, 0.16998716142221307, 0.30878087605375326], 
reward next is 0.6912, 
noisyNet noise sample is [array([0.31002793], dtype=float32), 0.76036185]. 
=============================================
[2019-04-02 18:02:21,570] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0808450e-20 2.9804630e-16 1.0000000e+00 2.8762080e-30 3.4605467e-26], sum to 1.0000
[2019-04-02 18:02:21,583] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9735
[2019-04-02 18:02:21,589] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.36666666666667, 41.66666666666666, 1.0, 2.0, 0.16, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2665034823688849, 6.911199999999999, 6.9112, 121.9260426156618, 385329.7352244559, 385329.7352244564, 152488.3071351311], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 254400.0000, 
sim time next is 255000.0000, 
raw observation next is [24.03333333333333, 42.83333333333334, 1.0, 2.0, 0.16, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2639431434763062, 6.911199999999999, 6.9112, 121.9260426156618, 381129.0549531843, 381129.0549531847, 151897.2222470715], 
processed observation next is [0.0, 0.9565217391304348, 0.4456790123456789, 0.42833333333333345, 1.0, 1.0, 0.0, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.07992892934538275, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.13611751962613727, 0.13611751962613738, 0.2921100427828298], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.40549624], dtype=float32), -1.1222554]. 
=============================================
[2019-04-02 18:02:21,612] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[49.919044]
 [50.05332 ]
 [50.191402]
 [50.328705]
 [50.465256]], R is [[49.28087234]
 [48.78806305]
 [48.30018234]
 [47.81718063]
 [47.33900833]].
[2019-04-02 18:02:21,804] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.5722587e-28 9.9217608e-24 1.0000000e+00 2.1023007e-34 1.4607210e-28], sum to 1.0000
[2019-04-02 18:02:21,814] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2124
[2019-04-02 18:02:21,820] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.03333333333333, 36.0, 1.0, 2.0, 0.16, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2796766557496985, 6.9112, 6.9112, 121.9260426156618, 406544.4496806834, 406544.4496806834, 155413.6627300909], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 251400.0000, 
sim time next is 252000.0000, 
raw observation next is [25.7, 37.0, 1.0, 2.0, 0.16, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2769513881863179, 6.9112, 6.9112, 121.9260426156618, 402092.6995052445, 402092.6995052445, 154783.475775403], 
processed observation next is [0.0, 0.9565217391304348, 0.5074074074074074, 0.37, 1.0, 1.0, 0.0, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.0961892352328974, 0.0, 0.0, 0.8094621288201359, 0.1436045355375873, 0.1436045355375873, 0.29766053033731343], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8624673], dtype=float32), -1.2177709]. 
=============================================
[2019-04-02 18:02:21,844] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[45.981922]
 [46.121147]
 [46.25375 ]
 [46.388638]
 [46.518597]], R is [[45.40162277]
 [44.94760513]
 [45.19820786]
 [45.4455452 ]
 [45.68962479]].
[2019-04-02 18:02:22,021] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.3566094e-32 2.1795969e-31 1.0000000e+00 2.8175783e-29 9.2955184e-26], sum to 1.0000
[2019-04-02 18:02:22,033] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3093
[2019-04-02 18:02:22,041] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [18.65, 74.66666666666667, 1.0, 2.0, 0.1636504063346753, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2921191040263524, 6.911199999999999, 6.9112, 121.9260426156618, 420743.4607489873, 420743.4607489877, 155882.9027056857], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 447000.0000, 
sim time next is 447600.0000, 
raw observation next is [18.5, 75.33333333333334, 1.0, 2.0, 0.16, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2557884328784305, 6.9112, 6.9112, 121.9260426156618, 367999.8403038155, 367999.8403038155, 150107.0395763707], 
processed observation next is [1.0, 0.17391304347826086, 0.24074074074074073, 0.7533333333333334, 1.0, 1.0, 0.0, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.06973554109803812, 0.0, 0.0, 0.8094621288201359, 0.13142851439421982, 0.13142851439421982, 0.2886673838007129], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.46450537], dtype=float32), 1.6828585]. 
=============================================
[2019-04-02 18:02:22,708] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.0000000e+00 1.0409605e-34 2.5234708e-32], sum to 1.0000
[2019-04-02 18:02:22,719] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5250
[2019-04-02 18:02:22,732] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [21.05, 49.66666666666667, 1.0, 2.0, 0.16, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2380157129790522, 6.9112, 6.9112, 121.9260426156618, 339887.2556789263, 339887.2556789263, 134274.0848479303], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 265800.0000, 
sim time next is 266400.0000, 
raw observation next is [20.9, 50.0, 1.0, 2.0, 0.16, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2374066522158388, 6.911199999999999, 6.9112, 121.9260426156618, 339017.3225872527, 339017.3225872531, 133478.7653174924], 
processed observation next is [0.0, 0.08695652173913043, 0.32962962962962955, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.04675831526979847, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1210776152097331, 0.12107761520973326, 0.25668993330287], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.4756556], dtype=float32), -0.7560658]. 
=============================================
[2019-04-02 18:02:36,169] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.4451772e-25 0.0000000e+00 1.0000000e+00 9.1268237e-34 7.4341873e-28], sum to 1.0000
[2019-04-02 18:02:36,180] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8981
[2019-04-02 18:02:36,188] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [23.0, 54.16666666666667, 1.0, 2.0, 0.2127575652071705, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3678784263587374, 6.911199999999999, 6.9112, 121.9260426156618, 540590.2507510791, 540590.2507510795, 168079.1500418488], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 460200.0000, 
sim time next is 460800.0000, 
raw observation next is [23.3, 53.0, 1.0, 2.0, 0.1872637053272887, 0.0, 2.0, 0.0, 1.0, 2.0, 0.323376461275208, 6.9112, 6.9112, 121.9260426156618, 475514.5054049026, 475514.5054049026, 162482.720910467], 
processed observation next is [1.0, 0.34782608695652173, 0.41851851851851857, 0.53, 1.0, 1.0, 0.03245679205629607, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.15422057659401, 0.0, 0.0, 0.8094621288201359, 0.1698266090731795, 0.1698266090731795, 0.3124667709816673], 
reward next is 0.6875, 
noisyNet noise sample is [array([0.17513141], dtype=float32), 0.68543756]. 
=============================================
[2019-04-02 18:02:36,870] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-02 18:02:36,871] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 18:02:36,871] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:02:36,872] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 18:02:36,873] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:02:36,874] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 18:02:36,874] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:02:36,875] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 18:02:36,875] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 18:02:36,881] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:02:36,881] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:02:36,900] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run10
[2019-04-02 18:02:36,918] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run10
[2019-04-02 18:02:36,920] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run10
[2019-04-02 18:02:36,921] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run10
[2019-04-02 18:02:36,921] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run10
[2019-04-02 18:02:47,024] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.08858233], dtype=float32), -0.012556498]
[2019-04-02 18:02:47,026] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [25.4, 43.33333333333334, 1.0, 2.0, 0.16, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2620859801894608, 6.911199999999999, 6.9112, 121.9260426156618, 385590.8516744935, 385590.851674494, 154335.7060192348]
[2019-04-02 18:02:47,027] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 18:02:47,033] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [5.5833298e-12 3.9351214e-30 1.0000000e+00 2.0133625e-30 6.9098752e-25], sampled 0.5835685929679153
[2019-04-02 18:02:57,434] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.08858233], dtype=float32), -0.012556498]
[2019-04-02 18:02:57,435] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [18.45032757333333, 91.68118770000001, 1.0, 2.0, 0.16, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2720183233640822, 6.911199999999999, 6.9112, 121.9260426156618, 402245.508880163, 402245.5088801634, 157001.2743185525]
[2019-04-02 18:02:57,437] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 18:02:57,439] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.8720013e-12 2.0553068e-29 1.0000000e+00 2.7672162e-29 1.8534591e-23], sampled 0.6063675236944603
[2019-04-02 18:03:06,606] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.08858233], dtype=float32), -0.012556498]
[2019-04-02 18:03:06,606] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [28.66666666666667, 56.0, 1.0, 1.0, 0.2533168793715339, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4057039720407216, 6.9112, 6.9112, 121.9260426156618, 596815.8605633937, 596815.8605633937, 182532.6962096371]
[2019-04-02 18:03:06,607] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 18:03:06,611] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [5.0196968e-02 6.0926360e-15 9.4980299e-01 6.6845036e-25 4.8400422e-18], sampled 0.5776544875167586
[2019-04-02 18:04:02,334] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.08858233], dtype=float32), -0.012556498]
[2019-04-02 18:04:02,335] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [25.1, 92.66666666666667, 1.0, 2.0, 0.3112560681857145, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4955297974541043, 6.911199999999999, 6.9112, 121.9260426156618, 709446.8644723085, 709446.864472309, 198295.3552942983]
[2019-04-02 18:04:02,336] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 18:04:02,338] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.5178496e-02 8.4963245e-18 9.8482150e-01 1.2646163e-25 2.1285324e-18], sampled 0.527311594105748
[2019-04-02 18:04:20,649] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.08858233], dtype=float32), -0.012556498]
[2019-04-02 18:04:20,652] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [26.22944308666667, 82.47522507166667, 1.0, 2.0, 0.3235871999934476, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5151613608889414, 6.911199999999999, 6.9112, 121.9260426156618, 737566.7709302064, 737566.7709302069, 201691.1490547567]
[2019-04-02 18:04:20,654] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 18:04:20,657] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [3.1734610e-03 4.8159952e-18 9.9682653e-01 1.9823623e-26 8.1135296e-20], sampled 0.6320042970167583
[2019-04-02 18:04:38,062] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.08858233], dtype=float32), -0.012556498]
[2019-04-02 18:04:38,065] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [24.38561706, 69.88090165666667, 1.0, 2.0, 0.2116842153649267, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3447134986160106, 6.911199999999999, 6.9112, 121.9260426156618, 514485.2482979838, 514485.2482979842, 171317.8146310953]
[2019-04-02 18:04:38,066] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 18:04:38,070] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.9419572e-07 5.8736276e-24 9.9999976e-01 2.0187321e-28 8.2784760e-22], sampled 0.0659674022895016
[2019-04-02 18:04:43,693] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.08858233], dtype=float32), -0.012556498]
[2019-04-02 18:04:43,693] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [31.27165793, 48.42610053, 1.0, 2.0, 0.8448370659773885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9259741426657, 977893.7362740557, 977893.7362740552, 206156.9327607357]
[2019-04-02 18:04:43,693] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 18:04:43,696] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.45669915e-02 5.80836558e-19 9.85433042e-01 7.73297296e-26
 6.77873933e-18], sampled 0.5443515074838493
[2019-04-02 18:04:44,701] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 7205.2929 2548860666.6191 232.0000
[2019-04-02 18:04:44,704] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 6808.0725 2798466909.6527 513.0000
[2019-04-02 18:04:44,758] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 6242.5786 2641017547.5684 381.0000
[2019-04-02 18:04:44,995] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 6836.6111 2605068110.9204 361.0000
[2019-04-02 18:04:45,004] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 6436.5654 2578892393.3216 302.0000
[2019-04-02 18:04:46,023] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 225000, evaluation results [225000.0, 6808.072516891217, 2798466909.6527143, 513.0, 6436.565377024517, 2578892393.321583, 302.0, 7205.292939840373, 2548860666.6191344, 232.0, 6242.578644539932, 2641017547.5683603, 381.0, 6836.611058301764, 2605068110.92044, 361.0]
[2019-04-02 18:04:46,393] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.2412034e-08 6.5269113e-24 1.0000000e+00 1.5210614e-27 4.3149784e-17], sum to 1.0000
[2019-04-02 18:04:46,404] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5746
[2019-04-02 18:04:46,409] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.1, 43.0, 1.0, 2.0, 0.4843963309080438, 0.0, 1.0, 0.0, 1.0, 1.0, 0.802196746534606, 6.9112, 6.9112, 121.9260425953408, 1198914.046580171, 1198914.046580171, 247695.1698713644], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 468000.0000, 
sim time next is 468600.0000, 
raw observation next is [27.41666666666667, 42.16666666666667, 1.0, 2.0, 0.4707073371378974, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7791993134545266, 6.9112, 6.9112, 121.9260426156556, 1164580.991258348, 1164580.991258348, 243131.2320441024], 
processed observation next is [1.0, 0.43478260869565216, 0.5709876543209879, 0.4216666666666667, 1.0, 1.0, 0.36988968706892555, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.7239991418181582, 0.0, 0.0, 0.8094621288200948, 0.4159217825922672, 0.4159217825922672, 0.46756006162327385], 
reward next is 0.5324, 
noisyNet noise sample is [array([-0.11392364], dtype=float32), 0.3353962]. 
=============================================
[2019-04-02 18:04:48,798] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [6.6264167e-14 6.6838286e-33 1.0000000e+00 2.1283151e-27 2.3582602e-25], sum to 1.0000
[2019-04-02 18:04:48,811] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5692
[2019-04-02 18:04:48,819] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.4, 50.5, 1.0, 2.0, 0.1622301355779161, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2771948345089993, 6.911199999999999, 6.9112, 121.9260426156618, 409807.9541437739, 409807.9541437743, 157657.3586906341], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 513000.0000, 
sim time next is 513600.0000, 
raw observation next is [24.13333333333333, 51.33333333333333, 1.0, 2.0, 0.1607622820259587, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2752390150265093, 6.9112, 6.9112, 121.9260426156618, 406519.795439013, 406519.795439013, 157248.6968382803], 
processed observation next is [1.0, 0.9565217391304348, 0.44938271604938257, 0.5133333333333333, 1.0, 1.0, 0.0009074786023317981, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.09404876878313664, 0.0, 0.0, 0.8094621288201359, 0.14518564122821892, 0.14518564122821892, 0.30240134007361597], 
reward next is 0.6976, 
noisyNet noise sample is [array([-1.0931009], dtype=float32), -0.14450638]. 
=============================================
[2019-04-02 18:04:57,013] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.5924156e-07 1.2294831e-19 9.9999952e-01 6.5747478e-36 3.4602019e-26], sum to 1.0000
[2019-04-02 18:04:57,024] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1812
[2019-04-02 18:04:57,032] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [21.21666666666667, 66.0, 1.0, 2.0, 0.1630168736829151, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2814844817553047, 6.911199999999999, 6.9112, 121.9260426156618, 413914.1876815257, 413914.1876815262, 157297.3900356189], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 619800.0000, 
sim time next is 620400.0000, 
raw observation next is [21.03333333333333, 67.0, 1.0, 2.0, 0.16, 0.0, 2.0, 0.0, 1.0, 2.0, 0.271458787848209, 6.9112, 6.9112, 121.9260426156618, 398861.2292857512, 398861.2292857512, 155709.573558379], 
processed observation next is [1.0, 0.17391304347826086, 0.3345679012345678, 0.67, 1.0, 1.0, 0.0, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.08932348481026121, 0.0, 0.0, 0.8094621288201359, 0.14245043903062543, 0.14245043903062543, 0.2994414876122673], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.307745], dtype=float32), -0.09496638]. 
=============================================
[2019-04-02 18:04:58,949] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.6314639e-05 1.7133814e-23 9.9996364e-01 4.6464997e-25 1.3664973e-21], sum to 1.0000
[2019-04-02 18:04:58,961] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5722
[2019-04-02 18:04:58,969] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [35.13333333333334, 17.66666666666667, 1.0, 2.0, 0.4127712345065208, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 517133.9881786142, 517133.9881786142, 130543.6448775422], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 667200.0000, 
sim time next is 667800.0000, 
raw observation next is [34.90000000000001, 18.0, 1.0, 2.0, 0.2008648577152569, 0.0, 2.0, 0.0, 1.0, 1.0, 0.3379583651362972, 6.911199999999999, 6.9112, 121.9260426156618, 502881.3411477959, 502881.3411477964, 166867.3009154494], 
processed observation next is [1.0, 0.7391304347826086, 0.8481481481481487, 0.18, 1.0, 1.0, 0.04864864013721058, 0.0, 1.0, -0.1904761904761905, 1.0, 0.5, 0.17244795642037145, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.17960047898135567, 0.17960047898135587, 0.32089865560663344], 
reward next is 0.6791, 
noisyNet noise sample is [array([-0.49124956], dtype=float32), -1.2393732]. 
=============================================
[2019-04-02 18:05:01,986] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 1.2718602e-11 1.3065544e-18 3.6319741e-29 1.9495998e-25], sum to 1.0000
[2019-04-02 18:05:01,998] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9101
[2019-04-02 18:05:02,010] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.06666666666667, 35.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5774187792204577, 6.911199999999999, 6.9112, 121.9260426156618, 424763.7503991032, 424763.7503991037, 126053.5410013316], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 688200.0000, 
sim time next is 688800.0000, 
raw observation next is [27.93333333333333, 35.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5751388736408727, 6.9112, 6.9112, 121.9260426156618, 422721.3811450418, 422721.3811450418, 125671.7414608921], 
processed observation next is [1.0, 1.0, 0.5901234567901233, 0.35333333333333344, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.4689235920510908, 0.0, 0.0, 0.8094621288201359, 0.15097192183751493, 0.15097192183751493, 0.24167642588633095], 
reward next is 0.7583, 
noisyNet noise sample is [array([-0.10537577], dtype=float32), 1.6076096]. 
=============================================
[2019-04-02 18:05:04,246] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.00000000e+00 1.15944542e-22 7.07487801e-12 2.98844684e-29
 1.26446245e-20], sum to 1.0000
[2019-04-02 18:05:04,257] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0894
[2019-04-02 18:05:04,267] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1262541.127998949 W.
[2019-04-02 18:05:04,278] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.6, 25.66666666666666, 1.0, 2.0, 0.3430190638042739, 1.0, 1.0, 0.3430190638042739, 1.0, 2.0, 0.5629285462785487, 6.911199999999999, 6.9112, 121.94756008, 1262541.127998949, 1262541.12799895, 271550.8045520126], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 740400.0000, 
sim time next is 741000.0000, 
raw observation next is [31.9, 24.33333333333333, 1.0, 2.0, 0.5079751572942013, 1.0, 2.0, 0.5079751572942013, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156415, 1254918.025428018, 1254918.025428018, 241553.6661644415], 
processed observation next is [1.0, 0.5652173913043478, 0.7370370370370369, 0.2433333333333333, 1.0, 1.0, 0.41425613963595387, 1.0, 1.0, 0.41425613963595387, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288200012, 0.44818500908143505, 0.44818500908143505, 0.4645262810854644], 
reward next is 0.5355, 
noisyNet noise sample is [array([-0.51870733], dtype=float32), 0.47331697]. 
=============================================
[2019-04-02 18:05:04,290] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[66.193054]
 [66.39953 ]
 [65.47602 ]
 [66.51355 ]
 [65.6742  ]], R is [[65.26941681]
 [65.09450531]
 [64.44355774]
 [63.79912186]
 [63.72077179]].
[2019-04-02 18:05:04,309] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 4.1637380e-25 6.4781965e-12 8.8586594e-32 3.5660307e-24], sum to 1.0000
[2019-04-02 18:05:04,317] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6652
[2019-04-02 18:05:04,328] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1381554.67965102 W.
[2019-04-02 18:05:04,334] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.4, 42.83333333333334, 1.0, 2.0, 0.3833860303224373, 1.0, 1.0, 0.3833860303224373, 1.0, 2.0, 0.6192713297335914, 6.911199999999999, 6.9112, 121.94756008, 1381554.67965102, 1381554.679651021, 289383.0236494272], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 733800.0000, 
sim time next is 734400.0000, 
raw observation next is [28.7, 41.0, 1.0, 2.0, 0.565050475818088, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9208742666320954, 6.911199999999999, 6.9112, 121.9260426156618, 1375489.873102988, 1375489.873102989, 277757.6930861016], 
processed observation next is [1.0, 0.5217391304347826, 0.6185185185185185, 0.41, 1.0, 1.0, 0.48220294740248565, 0.0, 0.5, -0.1904761904761905, 1.0, 1.0, 0.9010928332901191, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.4912463832510671, 0.4912463832510675, 0.5341494097809646], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7377458], dtype=float32), 0.72020006]. 
=============================================
[2019-04-02 18:05:11,410] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.9060720e-15 3.0232084e-24 1.0000000e+00 1.0656568e-37 1.8976453e-22], sum to 1.0000
[2019-04-02 18:05:11,421] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6727
[2019-04-02 18:05:11,428] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.66666666666667, 36.0, 1.0, 2.0, 0.2206468959250344, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3581809704034777, 6.911199999999999, 6.9112, 121.9260426156618, 533971.5955793224, 533971.5955793229, 173616.1345124559], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 829200.0000, 
sim time next is 829800.0000, 
raw observation next is [31.7, 36.0, 1.0, 2.0, 0.2210136950853966, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3586743321595426, 6.911199999999999, 6.9112, 121.9260426156618, 534639.0548523964, 534639.0548523968, 173721.8761662148], 
processed observation next is [0.0, 0.6086956521739131, 0.7296296296296296, 0.36, 1.0, 1.0, 0.0726353512921388, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.1983429151994282, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.19094251959014155, 0.19094251959014172, 0.3340805310888746], 
reward next is 0.6659, 
noisyNet noise sample is [array([0.5748914], dtype=float32), 1.1289955]. 
=============================================
[2019-04-02 18:05:12,963] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2042257e-27 1.9287047e-38 1.0000000e+00 0.0000000e+00 7.9795820e-36], sum to 1.0000
[2019-04-02 18:05:12,977] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2157
[2019-04-02 18:05:12,983] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.9, 43.5, 1.0, 2.0, 0.1934638828947066, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3203499151090076, 6.911199999999999, 6.9112, 121.9260426156618, 478558.5917548466, 478558.591754847, 166139.675884252], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 851400.0000, 
sim time next is 852000.0000, 
raw observation next is [27.73333333333333, 44.33333333333333, 1.0, 2.0, 0.1956961215447846, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3239066014822067, 6.911199999999999, 6.9112, 121.9260426156618, 483900.083880879, 483900.0838808795, 166662.4744519475], 
processed observation next is [0.0, 0.8695652173913043, 0.5827160493827159, 0.4433333333333333, 1.0, 1.0, 0.04249538279141024, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.15488325185275834, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.17282145852888536, 0.17282145852888553, 0.3205047585614375], 
reward next is 0.6795, 
noisyNet noise sample is [array([-0.02259967], dtype=float32), 1.0875497]. 
=============================================
[2019-04-02 18:05:13,005] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[67.695656]
 [67.80339 ]
 [67.91618 ]
 [68.02493 ]
 [68.09116 ]], R is [[67.58615112]
 [67.59079742]
 [67.5956192 ]
 [67.60050201]
 [67.60513306]].
[2019-04-02 18:05:19,565] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [9.9999988e-01 9.9503623e-08 9.4815479e-15 6.9789779e-28 3.1846094e-21], sum to 1.0000
[2019-04-02 18:05:19,574] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2299
[2019-04-02 18:05:19,579] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.15, 57.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.561100099924654, 6.9112, 6.9112, 121.9260426156618, 400642.3004280109, 400642.3004280109, 117175.9524612793], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 959400.0000, 
sim time next is 960000.0000, 
raw observation next is [21.1, 57.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5460263215726527, 6.9112, 6.9112, 121.9260426156618, 389876.4346396258, 389876.4346396258, 115766.8042410253], 
processed observation next is [1.0, 0.08695652173913043, 0.3370370370370371, 0.5733333333333333, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.4325329019658159, 0.0, 0.0, 0.8094621288201359, 0.13924158379986634, 0.13924158379986634, 0.2226284696942794], 
reward next is 0.7774, 
noisyNet noise sample is [array([-1.9672818], dtype=float32), -0.5219214]. 
=============================================
[2019-04-02 18:05:19,600] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[66.32581 ]
 [66.31124 ]
 [66.118484]
 [65.36681 ]
 [65.150345]], R is [[66.17441559]
 [66.28733063]
 [66.38024139]
 [66.44618988]
 [66.5714798 ]].
[2019-04-02 18:05:24,258] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 1.6813938e-22 1.1172242e-08 3.9030813e-31 4.3740539e-20], sum to 1.0000
[2019-04-02 18:05:24,266] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8340
[2019-04-02 18:05:24,277] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1143336.647245492 W.
[2019-04-02 18:05:24,284] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.85, 45.0, 1.0, 2.0, 0.4629002757949024, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7648386546113197, 6.911199999999999, 6.9112, 121.926042615652, 1143336.647245492, 1143336.647245492, 240691.320472054], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1009800.0000, 
sim time next is 1010400.0000, 
raw observation next is [26.93333333333334, 44.66666666666667, 1.0, 2.0, 0.4601803014486597, 1.0, 1.0, 0.4601803014486597, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1132987.972896025, 1132987.972896026, 226572.5256664837], 
processed observation next is [1.0, 0.6956521739130435, 0.5530864197530867, 0.4466666666666667, 1.0, 1.0, 0.3573575017245949, 1.0, 0.5, 0.3573575017245949, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.40463856174858037, 0.4046385617485807, 0.43571639551246866], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.89631015], dtype=float32), 1.8988403]. 
=============================================
[2019-04-02 18:05:30,896] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.9998391e-01 4.5044504e-21 1.6150252e-05 4.5752015e-25 1.4725699e-20], sum to 1.0000
[2019-04-02 18:05:30,910] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8755
[2019-04-02 18:05:30,921] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 976608.4496253007 W.
[2019-04-02 18:05:30,928] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.36666666666667, 45.33333333333333, 1.0, 2.0, 0.264617198283038, 1.0, 2.0, 0.264617198283038, 1.0, 1.0, 0.4354936472887178, 6.911199999999999, 6.9112, 121.94756008, 976608.4496253007, 976608.4496253012, 241189.2092857132], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1096800.0000, 
sim time next is 1097400.0000, 
raw observation next is [26.33333333333334, 45.66666666666667, 1.0, 2.0, 0.76049450166402, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 950389.7495059663, 950389.7495059663, 191136.6316101296], 
processed observation next is [1.0, 0.6956521739130435, 0.5308641975308644, 0.4566666666666667, 1.0, 1.0, 0.7148744067428809, 0.0, 0.5, -0.1904761904761905, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3394249105378451, 0.3394249105378451, 0.3675704454040954], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.4492864], dtype=float32), -0.12401531]. 
=============================================
[2019-04-02 18:05:31,969] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 1.5471863e-18 8.8199718e-27 0.0000000e+00 6.2970850e-32], sum to 1.0000
[2019-04-02 18:05:31,977] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6137
[2019-04-02 18:05:31,988] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.0, 75.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4844770698434122, 6.9112, 6.9112, 121.9260426156618, 347270.8559488923, 347270.8559488923, 114412.8721391275], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1134000.0000, 
sim time next is 1134600.0000, 
raw observation next is [18.96666666666667, 75.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.554415130494269, 6.911199999999999, 6.9112, 121.9260426156618, 397386.799626735, 397386.7996267354, 119853.3669008218], 
processed observation next is [1.0, 0.13043478260869565, 0.25802469135802475, 0.7516666666666667, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.44301891311783625, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1419238570095482, 0.14192385700954835, 0.23048724404004192], 
reward next is 0.7695, 
noisyNet noise sample is [array([-0.39409983], dtype=float32), -0.031466477]. 
=============================================
[2019-04-02 18:05:32,914] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 7.9144540e-16 3.0346312e-27 5.9155519e-36 3.7895573e-29], sum to 1.0000
[2019-04-02 18:05:32,927] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1474
[2019-04-02 18:05:32,932] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.76666666666667, 70.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5205687698724641, 6.9112, 6.9112, 121.9260426156618, 373825.8434021463, 373825.8434021463, 117412.0138132721], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1149600.0000, 
sim time next is 1150200.0000, 
raw observation next is [19.85, 69.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5143255633282249, 6.9112, 6.9112, 121.9260426156618, 369427.2340138152, 369427.2340138152, 116953.9707406988], 
processed observation next is [1.0, 0.30434782608695654, 0.2907407407407408, 0.695, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.39290695416028104, 0.0, 0.0, 0.8094621288201359, 0.13193829786207684, 0.13193829786207684, 0.22491148219365154], 
reward next is 0.7751, 
noisyNet noise sample is [array([0.8913971], dtype=float32), 1.1933373]. 
=============================================
[2019-04-02 18:05:37,736] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.98732693e-07 1.30962964e-23 9.99999762e-01 1.32999288e-33
 1.86712920e-20], sum to 1.0000
[2019-04-02 18:05:37,748] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5514
[2019-04-02 18:05:37,762] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [19.7, 84.5, 1.0, 2.0, 0.2777300499028016, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4679527588330267, 6.911199999999999, 6.9112, 121.9260426156618, 696050.2756260941, 696050.2756260944, 185183.9992375969], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1240200.0000, 
sim time next is 1240800.0000, 
raw observation next is [19.9, 83.33333333333334, 1.0, 2.0, 0.2832288010360223, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4763563408675065, 6.911199999999999, 6.9112, 121.9260426156618, 709004.7268373884, 709004.7268373888, 186701.4949568309], 
processed observation next is [1.0, 0.34782608695652173, 0.2925925925925925, 0.8333333333333335, 1.0, 1.0, 0.1467009536143123, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.3454454260843831, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2532159738704958, 0.253215973870496, 0.359041336455444], 
reward next is 0.6410, 
noisyNet noise sample is [array([1.1299186], dtype=float32), -0.3485257]. 
=============================================
[2019-04-02 18:05:38,308] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.9196351e-23 3.7738213e-35 1.0000000e+00 0.0000000e+00 1.6376749e-32], sum to 1.0000
[2019-04-02 18:05:38,323] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0019
[2019-04-02 18:05:38,329] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [18.55, 94.00000000000001, 1.0, 2.0, 0.1710356345876364, 0.0, 2.0, 0.0, 1.0, 2.0, 0.289507469531715, 6.911199999999999, 6.9112, 121.9260426156618, 429795.7249121649, 429795.7249121654, 160008.3303740716], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1210200.0000, 
sim time next is 1210800.0000, 
raw observation next is [18.5, 94.0, 1.0, 2.0, 0.1702629722698127, 0.0, 2.0, 0.0, 1.0, 2.0, 0.28849773351162, 6.911199999999999, 6.9112, 121.9260426156618, 428115.8957466062, 428115.8957466067, 159788.6933619746], 
processed observation next is [1.0, 0.0, 0.24074074074074073, 0.94, 1.0, 1.0, 0.012217824130729416, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.11062216688952499, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1528985341952165, 0.15289853419521668, 0.3072859487730281], 
reward next is 0.6927, 
noisyNet noise sample is [array([-0.81721157], dtype=float32), -0.8167377]. 
=============================================
[2019-04-02 18:05:50,387] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.34883249e-12 1.18766374e-25 1.00000000e+00 4.92899075e-31
 1.02557695e-26], sum to 1.0000
[2019-04-02 18:05:50,397] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7369
[2019-04-02 18:05:50,403] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.96666666666667, 55.0, 1.0, 2.0, 0.1859907312104361, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3102826367389294, 6.911199999999999, 6.9112, 121.9260426156618, 462855.702713709, 462855.7027137094, 164053.7514582438], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1372200.0000, 
sim time next is 1372800.0000, 
raw observation next is [24.73333333333333, 56.00000000000001, 1.0, 2.0, 0.1848903596070087, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3086678555476157, 6.911199999999999, 6.9112, 121.9260426156618, 460363.8368436251, 460363.8368436255, 163770.5150744889], 
processed observation next is [1.0, 0.9130434782608695, 0.4716049382716048, 0.56, 1.0, 1.0, 0.02963138048453416, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.13583481943451964, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1644156560155804, 0.16441565601558056, 0.314943298220171], 
reward next is 0.6851, 
noisyNet noise sample is [array([0.03680487], dtype=float32), -1.2388368]. 
=============================================
[2019-04-02 18:05:52,549] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-02 18:05:52,551] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 18:05:52,552] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 18:05:52,553] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 18:05:52,554] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:05:52,554] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:05:52,555] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 18:05:52,559] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:05:52,560] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:05:52,559] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 18:05:52,565] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:05:52,573] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run11
[2019-04-02 18:05:52,594] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run11
[2019-04-02 18:05:52,595] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run11
[2019-04-02 18:05:52,612] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run11
[2019-04-02 18:05:52,641] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run11
[2019-04-02 18:05:54,625] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.1720501], dtype=float32), -0.1184353]
[2019-04-02 18:05:54,626] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [25.0, 28.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5315473506928874, 6.911200000000001, 6.9112, 121.9260426156618, 379535.5294912393, 379535.5294912388, 101680.3532519568]
[2019-04-02 18:05:54,627] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 18:05:54,630] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.0000000e+00 1.3576864e-23 2.0264327e-28 0.0000000e+00 0.0000000e+00], sampled 0.8624567760387584
[2019-04-02 18:05:55,529] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.1720501], dtype=float32), -0.1184353]
[2019-04-02 18:05:55,530] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [41.15916006333334, 8.523035337833335, 1.0, 2.0, 0.4732527766525689, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7791992481667435, 6.9112, 6.9112, 121.92604250557, 1165054.742371567, 1165054.742371567, 244415.8211756299]
[2019-04-02 18:05:55,530] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 18:05:55,534] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.0000000e+00 1.2832713e-30 9.7525188e-10 0.0000000e+00 1.0056694e-30], sampled 0.16008671479436476
[2019-04-02 18:05:55,534] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1165054.742371567 W.
[2019-04-02 18:06:25,791] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.1720501], dtype=float32), -0.1184353]
[2019-04-02 18:06:25,794] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [28.86666666666667, 64.33333333333334, 1.0, 1.0, 0.5993312529188879, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 687830.3042890569, 687830.3042890569, 158199.5646450332]
[2019-04-02 18:06:25,795] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 18:06:25,799] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.0000000e+00 1.4823199e-22 3.4449084e-36 0.0000000e+00 0.0000000e+00], sampled 0.9427319534256579
[2019-04-02 18:06:25,799] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 687830.3042890569 W.
[2019-04-02 18:06:29,735] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.1720501], dtype=float32), -0.1184353]
[2019-04-02 18:06:29,736] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [26.75, 60.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7469997566779013, 6.911200000000001, 6.9112, 121.9260426156618, 556621.2289412594, 556621.228941259, 152783.6516229236]
[2019-04-02 18:06:29,740] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 18:06:29,743] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.0000000e+00 3.3836634e-24 8.1775586e-32 0.0000000e+00 0.0000000e+00], sampled 0.9590158826916898
[2019-04-02 18:06:34,366] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.1720501], dtype=float32), -0.1184353]
[2019-04-02 18:06:34,370] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [29.20368212666667, 55.59258984333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8417123818721486, 6.9112, 6.9112, 121.9260426156618, 620085.6642846663, 620085.6642846663, 167386.7583872732]
[2019-04-02 18:06:34,373] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 18:06:34,376] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.000000e+00 5.610362e-26 8.505781e-32 0.000000e+00 0.000000e+00], sampled 0.6575719771408997
[2019-04-02 18:07:10,807] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.1720501], dtype=float32), -0.1184353]
[2019-04-02 18:07:10,808] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [26.03752048, 104.7847543, 1.0, 2.0, 0.7543728836182914, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.9260426156618, 1574863.704092871, 1574863.704092872, 327352.1034608589]
[2019-04-02 18:07:10,808] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 18:07:10,811] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.000000e+00 6.314968e-29 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.2618984963410914
[2019-04-02 18:07:10,811] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1574863.704092871 W.
[2019-04-02 18:07:14,607] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.1720501], dtype=float32), -0.1184353]
[2019-04-02 18:07:14,609] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [21.89883545, 93.88659217, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8251724149621518, 6.9112, 6.9112, 121.9260426156618, 612417.1322540673, 612417.1322540673, 163713.2138706072]
[2019-04-02 18:07:14,611] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 18:07:14,614] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [9.9999630e-01 3.3254942e-30 3.7124223e-06 0.0000000e+00 3.8089184e-30], sampled 0.9605685281640166
[2019-04-02 18:07:18,320] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.1720501], dtype=float32), -0.1184353]
[2019-04-02 18:07:18,321] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [29.828070175, 45.82323488999999, 1.0, 2.0, 0.655956435002204, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 790224.1544335805, 790224.15443358, 169828.4161064244]
[2019-04-02 18:07:18,322] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 18:07:18,325] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [9.9999619e-01 2.5877950e-29 3.7899702e-06 9.8255371e-37 1.9425232e-27], sampled 0.845253586310341
[2019-04-02 18:07:18,326] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 790224.1544335805 W.
[2019-04-02 18:07:26,471] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.1720501], dtype=float32), -0.1184353]
[2019-04-02 18:07:26,473] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [25.53333333333333, 60.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6918705448413677, 6.9112, 6.9112, 121.9260426156618, 517025.6151074707, 517025.6151074707, 143833.5475215414]
[2019-04-02 18:07:26,476] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 18:07:26,481] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.0000000e+00 4.8076674e-25 3.6768705e-32 0.0000000e+00 0.0000000e+00], sampled 0.35561243933195774
[2019-04-02 18:07:28,889] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.1720501], dtype=float32), -0.1184353]
[2019-04-02 18:07:28,890] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [21.83333333333333, 86.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7547724927580701, 6.9112, 6.9112, 121.9260426156618, 563708.7818919438, 563708.7818919438, 152257.7763902843]
[2019-04-02 18:07:28,890] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 18:07:28,892] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.00000000e+00 2.54936791e-28 1.10304535e-17 0.00000000e+00
 1.35123053e-34], sampled 0.002255649758234113
[2019-04-02 18:07:53,419] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.1720501], dtype=float32), -0.1184353]
[2019-04-02 18:07:53,419] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [21.313579895, 80.19552054666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6232900797002574, 6.911199999999999, 6.9112, 121.9260426156618, 464416.1847969544, 464416.1847969549, 134074.5748495861]
[2019-04-02 18:07:53,419] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 18:07:53,421] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.0000000e+00 2.8914009e-22 1.3974517e-33 0.0000000e+00 0.0000000e+00], sampled 0.32394581438821224
[2019-04-02 18:08:00,022] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7341.9188 2572733546.7502 770.0000
[2019-04-02 18:08:00,076] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8118.2584 2258895849.5510 477.0000
[2019-04-02 18:08:00,116] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 7996.4343 2326700693.5318 639.0000
[2019-04-02 18:08:00,265] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 7867.6972 2305795587.9549 473.0000
[2019-04-02 18:08:00,387] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 7831.5566 2379958324.0193 556.0000
[2019-04-02 18:08:01,404] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 250000, evaluation results [250000.0, 7341.918780768005, 2572733546.7502313, 770.0, 7867.697179639032, 2305795587.954941, 473.0, 8118.258357457457, 2258895849.5509624, 477.0, 7831.5565770054955, 2379958324.0193434, 556.0, 7996.434301179687, 2326700693.531805, 639.0]
[2019-04-02 18:08:03,247] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 1.9500868e-29 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:08:03,252] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8530
[2019-04-02 18:08:03,260] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [33.3, 23.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6503325402961917, 6.9112, 6.9112, 121.9260426156618, 482983.6902552626, 482983.6902552626, 135428.4044089636], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1430400.0000, 
sim time next is 1431000.0000, 
raw observation next is [33.45, 22.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6521600155536517, 6.9112, 6.9112, 121.9260426156618, 484215.8251909763, 484215.8251909763, 135518.8276278271], 
processed observation next is [0.0, 0.5652173913043478, 0.7944444444444445, 0.225, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5652000194420645, 0.0, 0.0, 0.8094621288201359, 0.17293422328249156, 0.17293422328249156, 0.26061313005351366], 
reward next is 0.7394, 
noisyNet noise sample is [array([1.7357379], dtype=float32), -0.16860667]. 
=============================================
[2019-04-02 18:08:03,289] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[87.99859]
 [87.87866]
 [87.76324]
 [87.61133]
 [87.47267]], R is [[87.97911072]
 [87.83888245]
 [87.70033264]
 [87.56410217]
 [87.42984772]].
[2019-04-02 18:08:03,718] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 1.8703271e-16 1.2267287e-27 0.0000000e+00 5.8847623e-36], sum to 1.0000
[2019-04-02 18:08:03,731] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2652
[2019-04-02 18:08:03,736] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [34.53333333333333, 21.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6755113701821037, 6.9112, 6.9112, 121.9260426156618, 502653.7160788787, 502653.7160788787, 138706.5563451725], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1441200.0000, 
sim time next is 1441800.0000, 
raw observation next is [34.3, 21.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6666137184701724, 6.9112, 6.9112, 121.9260426156618, 495984.5007160059, 495984.5007160059, 137760.4886741337], 
processed observation next is [0.0, 0.6956521739130435, 0.8259259259259258, 0.215, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5832671480877154, 0.0, 0.0, 0.8094621288201359, 0.17713732168428784, 0.17713732168428784, 0.26492401668102633], 
reward next is 0.7351, 
noisyNet noise sample is [array([-0.441673], dtype=float32), -0.10886522]. 
=============================================
[2019-04-02 18:08:06,911] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 1.2287490e-13 4.2593942e-23 0.0000000e+00 4.2541438e-33], sum to 1.0000
[2019-04-02 18:08:06,923] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7149
[2019-04-02 18:08:06,934] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.65, 60.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5665701979538158, 6.9112, 6.9112, 121.9260426156618, 415917.5257062832, 415917.5257062832, 124674.4312976804], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1477800.0000, 
sim time next is 1478400.0000, 
raw observation next is [22.56666666666667, 61.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5688899719549176, 6.911200000000001, 6.9112, 121.9260426156618, 417904.4919986141, 417904.4919986136, 125012.5501859155], 
processed observation next is [0.0, 0.08695652173913043, 0.39135802469135816, 0.6133333333333334, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.461112464943647, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.14925160428521933, 0.14925160428521914, 0.24040875035752982], 
reward next is 0.7596, 
noisyNet noise sample is [array([1.1753607], dtype=float32), -0.07551846]. 
=============================================
[2019-04-02 18:08:15,896] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 1.6935694e-26 5.4381211e-34 2.9086573e-38 1.8938607e-32], sum to 1.0000
[2019-04-02 18:08:15,903] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9217
[2019-04-02 18:08:15,915] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1129390.682460809 W.
[2019-04-02 18:08:15,920] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.96666666666667, 41.16666666666667, 1.0, 2.0, 0.88788488716854, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.95799954501331, 6.9112, 121.9257662534438, 1129390.682460809, 1105425.186839355, 218637.3819609106], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1615800.0000, 
sim time next is 1616400.0000, 
raw observation next is [28.0, 41.0, 1.0, 2.0, 0.2986934592154817, 1.0, 1.0, 0.2986934592154817, 1.0, 1.0, 0.4886840677285496, 6.911200000000001, 6.9112, 121.94756008, 1095575.822521726, 1095575.822521725, 254230.651443353], 
processed observation next is [1.0, 0.7391304347826086, 0.5925925925925926, 0.41, 1.0, 1.0, 0.16511126097081152, 1.0, 0.5, 0.16511126097081152, 1.0, 0.5, 0.360855084660687, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.391277079472045, 0.39127707947204465, 0.488905098929525], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.56454766], dtype=float32), -1.5230299]. 
=============================================
[2019-04-02 18:08:18,609] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 1.2705593e-28 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:08:18,617] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7623
[2019-04-02 18:08:18,622] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.86666666666667, 65.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5541619293609317, 6.9112, 6.9112, 121.9260426156618, 406688.9905649445, 406688.9905649445, 123540.3341810506], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1647600.0000, 
sim time next is 1648200.0000, 
raw observation next is [21.78333333333333, 65.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5462529811451657, 6.911200000000001, 6.9112, 121.9260426156618, 400852.4696627444, 400852.4696627439, 122845.5406463556], 
processed observation next is [1.0, 0.043478260869565216, 0.3623456790123456, 0.655, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.4328162264314571, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.143161596308123, 0.14316159630812283, 0.2362414243199146], 
reward next is 0.7638, 
noisyNet noise sample is [array([-1.1678419], dtype=float32), 0.19409429]. 
=============================================
[2019-04-02 18:08:18,863] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.000000e+00 6.525184e-27 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-02 18:08:18,880] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2225
[2019-04-02 18:08:18,885] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.0, 75.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6374517740549779, 6.9112, 6.9112, 121.9260426156618, 465827.5534411847, 465827.5534411847, 130124.9696485178], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1653600.0000, 
sim time next is 1654200.0000, 
raw observation next is [19.8, 77.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6370643206652238, 6.9112, 6.9112, 121.9260426156618, 465393.8590127878, 465393.8590127878, 130025.4307172869], 
processed observation next is [1.0, 0.13043478260869565, 0.2888888888888889, 0.77, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5463304008315297, 0.0, 0.0, 0.8094621288201359, 0.1662120925045671, 0.1662120925045671, 0.25004890522555173], 
reward next is 0.7500, 
noisyNet noise sample is [array([0.35450175], dtype=float32), -0.26641122]. 
=============================================
[2019-04-02 18:08:24,587] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 2.3183434e-26 1.2439082e-22 0.0000000e+00 3.4321028e-38], sum to 1.0000
[2019-04-02 18:08:24,595] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9189
[2019-04-02 18:08:24,599] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.9, 83.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7059635547842378, 6.9112, 6.9112, 121.9260426156618, 525861.1878925223, 525861.1878925223, 142352.8291991637], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1737000.0000, 
sim time next is 1737600.0000, 
raw observation next is [20.83333333333334, 83.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6720161734770572, 6.9112, 6.9112, 121.9260426156618, 500494.5412398362, 500494.5412398362, 138747.8847528074], 
processed observation next is [1.0, 0.08695652173913043, 0.3271604938271607, 0.8333333333333335, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5900202168463214, 0.0, 0.0, 0.8094621288201359, 0.17874805044279865, 0.17874805044279865, 0.2668228552938604], 
reward next is 0.7332, 
noisyNet noise sample is [array([-0.3748808], dtype=float32), -0.8822724]. 
=============================================
[2019-04-02 18:08:24,897] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 1.7263990e-26 3.8452070e-17 0.0000000e+00 5.2398253e-30], sum to 1.0000
[2019-04-02 18:08:24,907] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1346
[2019-04-02 18:08:24,910] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.2, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6160631733621726, 6.911200000000001, 6.9112, 121.9260426156618, 458310.6505138577, 458310.6505138573, 132701.4806605205], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1742400.0000, 
sim time next is 1743000.0000, 
raw observation next is [20.13333333333333, 87.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6586659273737349, 6.9112, 6.9112, 121.9260426156618, 489935.8632362562, 489935.8632362562, 136845.3812813262], 
processed observation next is [1.0, 0.17391304347826086, 0.30123456790123443, 0.8733333333333333, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5733324092171685, 0.0, 0.0, 0.8094621288201359, 0.17497709401294864, 0.17497709401294864, 0.26316419477178116], 
reward next is 0.7368, 
noisyNet noise sample is [array([-0.927781], dtype=float32), 1.0248114]. 
=============================================
[2019-04-02 18:08:24,926] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[73.377205]
 [73.47289 ]
 [73.575386]
 [73.64338 ]
 [73.69021 ]], R is [[73.30554962]
 [73.31729889]
 [73.32849884]
 [73.33907318]
 [73.34889221]].
[2019-04-02 18:08:31,662] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 6.5465457e-21 2.2647184e-32 0.0000000e+00 1.4192217e-31], sum to 1.0000
[2019-04-02 18:08:31,671] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8680
[2019-04-02 18:08:31,682] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [18.66666666666667, 90.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5757305284061894, 6.911200000000001, 6.9112, 121.9260426156618, 423708.548619583, 423708.5486195825, 125996.8565631136], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1830000.0000, 
sim time next is 1830600.0000, 
raw observation next is [18.7, 90.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5915043159688579, 6.9112, 6.9112, 121.9260426156618, 435393.6746881725, 435393.6746881725, 127447.648860873], 
processed observation next is [1.0, 0.17391304347826086, 0.24814814814814812, 0.905, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.48938039496107233, 0.0, 0.0, 0.8094621288201359, 0.1554977409600616, 0.1554977409600616, 0.24509163242475576], 
reward next is 0.7549, 
noisyNet noise sample is [array([-0.23228386], dtype=float32), 0.05698322]. 
=============================================
[2019-04-02 18:08:42,748] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.000000e+00 4.711265e-24 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-02 18:08:42,762] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0466
[2019-04-02 18:08:42,772] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.5, 91.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6125538666154928, 6.911199999999999, 6.9112, 121.9260426156618, 454934.7871474361, 454934.7871474365, 131762.9280738588], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1987200.0000, 
sim time next is 1987800.0000, 
raw observation next is [19.5, 91.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6133957700512734, 6.9112, 6.9112, 121.9260426156618, 455564.3234768047, 455564.3234768047, 131846.5411813513], 
processed observation next is [0.0, 0.0, 0.2777777777777778, 0.9100000000000001, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5167447125640917, 0.0, 0.0, 0.8094621288201359, 0.1627015440988588, 0.1627015440988588, 0.25355104073336787], 
reward next is 0.7464, 
noisyNet noise sample is [array([-0.02277813], dtype=float32), 0.71973544]. 
=============================================
[2019-04-02 18:09:07,906] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-02 18:09:07,908] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 18:09:07,911] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:09:07,912] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 18:09:07,913] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:09:07,913] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 18:09:07,914] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 18:09:07,916] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:09:07,916] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:09:07,918] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 18:09:07,920] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:09:07,941] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run12
[2019-04-02 18:09:07,941] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run12
[2019-04-02 18:09:07,958] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run12
[2019-04-02 18:09:07,978] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run12
[2019-04-02 18:09:07,998] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run12
[2019-04-02 18:09:18,753] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.13786113], dtype=float32), -0.12790854]
[2019-04-02 18:09:18,754] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [22.13333333333333, 61.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6875820709774637, 6.9112, 6.9112, 121.9260426156618, 503076.7288244056, 503076.7288244056, 135092.1494495476]
[2019-04-02 18:09:18,754] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 18:09:18,756] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.0000000e+00 6.1064786e-20 5.6723735e-29 0.0000000e+00 4.0467254e-36], sampled 0.1358498488941594
[2019-04-02 18:09:32,493] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.13786113], dtype=float32), -0.12790854]
[2019-04-02 18:09:32,494] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [26.0, 42.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5534567205481288, 6.911199999999999, 6.9112, 121.9260426156618, 405327.2751852109, 405327.2751852114, 123090.7367505947]
[2019-04-02 18:09:32,495] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 18:09:32,497] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.0000000e+00 2.6697897e-20 7.4958275e-31 0.0000000e+00 3.3718772e-37], sampled 0.3044171095589371
[2019-04-02 18:09:49,167] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.13786113], dtype=float32), -0.12790854]
[2019-04-02 18:09:49,168] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [24.16666666666667, 59.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.673684003282029, 6.911200000000001, 6.9112, 121.9260426156618, 500881.019755821, 500881.0197558205, 138180.3260842547]
[2019-04-02 18:09:49,168] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 18:09:49,172] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.0000000e+00 8.3005504e-22 3.9119363e-38 0.0000000e+00 0.0000000e+00], sampled 0.16941385481164983
[2019-04-02 18:09:55,211] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.13786113], dtype=float32), -0.12790854]
[2019-04-02 18:09:55,212] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [28.33333333333334, 70.66666666666667, 1.0, 2.0, 0.9875907916320859, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.068202415225877, 6.9112, 121.9251226392438, 1206272.635530561, 1125873.972860057, 237741.3241480957]
[2019-04-02 18:09:55,214] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 18:09:55,220] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.0000000e+00 1.5244027e-33 1.9125623e-25 0.0000000e+00 9.6566620e-35], sampled 0.18298428328404492
[2019-04-02 18:09:55,222] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1206272.635530561 W.
[2019-04-02 18:10:18,671] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.13786113], dtype=float32), -0.12790854]
[2019-04-02 18:10:18,674] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [24.57432416666667, 92.17264939333334, 1.0, 2.0, 0.6217620236685641, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 712443.6069476744, 712443.6069476744, 162052.2852054732]
[2019-04-02 18:10:18,676] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 18:10:18,679] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.0000000e+00 7.7103815e-31 1.0204195e-08 6.9806012e-36 3.4572936e-30], sampled 0.5488009497369618
[2019-04-02 18:10:18,680] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 712443.6069476744 W.
[2019-04-02 18:10:43,979] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.13786113], dtype=float32), -0.12790854]
[2019-04-02 18:10:43,979] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [23.70209583833334, 60.61544987500001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6612902154857055, 6.9112, 6.9112, 121.9260426156618, 490368.3524108046, 490368.3524108046, 135994.4365719707]
[2019-04-02 18:10:43,980] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 18:10:43,984] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.0000000e+00 3.0358848e-25 1.9463926e-37 0.0000000e+00 0.0000000e+00], sampled 0.17837100856860022
[2019-04-02 18:10:56,518] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.13786113], dtype=float32), -0.12790854]
[2019-04-02 18:10:56,519] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [27.61624618166667, 48.23652947833334, 1.0, 1.0, 0.5468490701191527, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260317731643, 673947.8632739439, 673947.8632739444, 151136.6646226019]
[2019-04-02 18:10:56,520] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 18:10:56,522] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.0000000e+00 2.5820310e-18 5.3041504e-26 6.3093560e-37 5.7386321e-34], sampled 0.7858223514524342
[2019-04-02 18:11:15,051] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8519.8191 2226068474.8051 537.0000
[2019-04-02 18:11:15,710] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7758.3787 2534419843.9636 828.0000
[2019-04-02 18:11:15,873] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8489.6485 2262145954.7556 533.0000
[2019-04-02 18:11:15,895] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8291.9518 2299772179.1175 686.0000
[2019-04-02 18:11:15,909] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8270.3138 2344205669.2195 609.0000
[2019-04-02 18:11:16,924] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 275000, evaluation results [275000.0, 7758.378700551073, 2534419843.9635854, 828.0, 8489.648498847351, 2262145954.7555985, 533.0, 8519.819148712293, 2226068474.8051286, 537.0, 8270.313787151825, 2344205669.2195253, 609.0, 8291.951832640525, 2299772179.11753, 686.0]
[2019-04-02 18:11:19,808] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 2.8716671e-24 2.5746669e-31 2.6814266e-34 4.5123916e-32], sum to 1.0000
[2019-04-02 18:11:19,820] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9737
[2019-04-02 18:11:19,833] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1326128.730397939 W.
[2019-04-02 18:11:19,839] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.9, 37.0, 1.0, 2.0, 0.5528450349510645, 1.0, 2.0, 0.5528450349510645, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1326128.730397939, 1326128.730397939, 255042.7553958364], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2382000.0000, 
sim time next is 2382600.0000, 
raw observation next is [31.0, 37.0, 1.0, 2.0, 0.3737412537793705, 1.0, 2.0, 0.3737412537793705, 1.0, 1.0, 0.6001141068072128, 6.9112, 6.9112, 121.94756008, 1330541.012571313, 1330541.012571313, 285577.5127214907], 
processed observation next is [1.0, 0.5652173913043478, 0.7037037037037037, 0.37, 1.0, 1.0, 0.25445387354686966, 1.0, 1.0, 0.25445387354686966, 1.0, 0.5, 0.5001426335090159, 0.0, 0.0, 0.8096049824067558, 0.4751932187754689, 0.4751932187754689, 0.5491875244644052], 
reward next is 0.4508, 
noisyNet noise sample is [array([1.1833006], dtype=float32), 0.23906305]. 
=============================================
[2019-04-02 18:11:23,941] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 1.2147337e-26 9.0865146e-29 8.5070614e-36 2.9216005e-31], sum to 1.0000
[2019-04-02 18:11:23,955] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3125
[2019-04-02 18:11:23,966] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1281172.970524047 W.
[2019-04-02 18:11:23,974] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.55, 36.16666666666666, 1.0, 2.0, 0.5245397908731361, 1.0, 2.0, 0.5245397908731361, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1281172.970524047, 1281172.970524047, 246502.042525832], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2451000.0000, 
sim time next is 2451600.0000, 
raw observation next is [29.8, 35.0, 1.0, 2.0, 0.5465032461054983, 1.0, 2.0, 0.5465032461054983, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1335216.850714903, 1335216.850714904, 253735.8059636691], 
processed observation next is [1.0, 0.391304347826087, 0.6592592592592593, 0.35, 1.0, 1.0, 0.46012291203035505, 1.0, 1.0, 0.46012291203035505, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.47686316096960824, 0.47686316096960857, 0.48795347300705594], 
reward next is 0.5120, 
noisyNet noise sample is [array([-0.8136466], dtype=float32), -0.84616953]. 
=============================================
[2019-04-02 18:11:24,435] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 3.0991013e-28 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:11:24,442] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4433
[2019-04-02 18:11:24,451] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1054630.452786243 W.
[2019-04-02 18:11:24,457] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.55, 40.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9579701877603664, 7.575581331833416, 6.9112, 121.9239129933196, 1054630.452786243, 714413.7716595014, 170628.7237346017], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2448600.0000, 
sim time next is 2449200.0000, 
raw observation next is [28.8, 39.66666666666667, 1.0, 1.0, 0.4898780169316238, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8062905840384561, 6.9112, 6.9112, 121.9256425362521, 1205600.485798243, 1205600.485798243, 250063.0604454084], 
processed observation next is [1.0, 0.34782608695652173, 0.6222222222222222, 0.3966666666666667, 1.0, 0.5, 0.39271192491859974, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.75786323004807, 0.0, 0.0, 0.8094594727089088, 0.4305716020708011, 0.4305716020708011, 0.4808905008565546], 
reward next is 0.5191, 
noisyNet noise sample is [array([-0.6605662], dtype=float32), 0.56676626]. 
=============================================
[2019-04-02 18:11:31,860] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 3.0919524e-16 4.8510832e-35 4.4656826e-37 1.8689815e-36], sum to 1.0000
[2019-04-02 18:11:31,872] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2195
[2019-04-02 18:11:31,880] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1307758.317323941 W.
[2019-04-02 18:11:31,892] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.3, 30.0, 1.0, 2.0, 0.5478696660165194, 1.0, 2.0, 0.5478696660165194, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1307758.317323941, 1307758.317323941, 253151.6418581393], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2553000.0000, 
sim time next is 2553600.0000, 
raw observation next is [33.40000000000001, 30.0, 1.0, 2.0, 0.4448404057732852, 1.0, 2.0, 0.4448404057732852, 1.0, 1.0, 0.7124170643675574, 6.9112, 6.9112, 121.94756008, 1572907.920308819, 1572907.920308819, 317287.8626246726], 
processed observation next is [1.0, 0.5652173913043478, 0.7925925925925931, 0.3, 1.0, 1.0, 0.33909572115867287, 1.0, 1.0, 0.33909572115867287, 1.0, 0.5, 0.6405213304594467, 0.0, 0.0, 0.8096049824067558, 0.561752828681721, 0.561752828681721, 0.6101689665859088], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.57154083], dtype=float32), -0.5590177]. 
=============================================
[2019-04-02 18:11:37,317] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.000000e+00 9.693324e-25 8.324290e-32 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-02 18:11:37,329] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1425
[2019-04-02 18:11:37,336] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.58333333333334, 85.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8356139129789883, 6.9112, 6.9112, 121.9260426156618, 618440.391247483, 618440.391247483, 165711.4021184869], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2631000.0000, 
sim time next is 2631600.0000, 
raw observation next is [23.5, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8234041449486944, 6.911200000000001, 6.9112, 121.9260426156618, 610414.210572402, 610414.2105724015, 163798.9019511378], 
processed observation next is [0.0, 0.4782608695652174, 0.42592592592592593, 0.85, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.779255181185868, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.21800507520442927, 0.2180050752044291, 0.3149978883675727], 
reward next is 0.6850, 
noisyNet noise sample is [array([1.0195383], dtype=float32), 1.3891095]. 
=============================================
[2019-04-02 18:11:37,876] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.4307782e-02 4.6228289e-17 9.6569222e-01 1.1287428e-28 1.1573035e-24], sum to 1.0000
[2019-04-02 18:11:37,888] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3124
[2019-04-02 18:11:37,896] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.0, 74.0, 1.0, 2.0, 0.3067463675843619, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4883668081697158, 6.911200000000001, 6.9112, 121.9260426156618, 699693.6998991542, 699693.6998991539, 197058.3068158139], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2649000.0000, 
sim time next is 2649600.0000, 
raw observation next is [27.0, 74.0, 1.0, 2.0, 0.3064284683003169, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4878753976701641, 6.9112, 6.9112, 121.9260426156618, 699390.9522000978, 699390.9522000978, 196965.5996685097], 
processed observation next is [0.0, 0.6956521739130435, 0.5555555555555556, 0.74, 1.0, 1.0, 0.17431960511942488, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.35984424708770507, 0.0, 0.0, 0.8094621288201359, 0.2497824829286064, 0.2497824829286064, 0.37877999936251866], 
reward next is 0.6212, 
noisyNet noise sample is [array([-0.08504746], dtype=float32), 0.3893296]. 
=============================================
[2019-04-02 18:11:39,634] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.9801612e-01 3.2637261e-17 1.9838354e-03 4.0310135e-30 1.4514260e-27], sum to 1.0000
[2019-04-02 18:11:39,650] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8948
[2019-04-02 18:11:39,661] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 688882.8743650343 W.
[2019-04-02 18:11:39,666] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.33333333333334, 76.66666666666667, 1.0, 2.0, 0.3015523770031503, 1.0, 2.0, 0.3015523770031503, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 688882.8743650343, 688882.8743650347, 180912.006690931], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2659200.0000, 
sim time next is 2659800.0000, 
raw observation next is [26.16666666666667, 77.33333333333333, 1.0, 2.0, 0.2985119082246315, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4754649815929801, 6.911199999999999, 6.9112, 121.9260426156618, 685114.0522935303, 685114.0522935308, 194763.3366423597], 
processed observation next is [0.0, 0.782608695652174, 0.5246913580246916, 0.7733333333333333, 1.0, 1.0, 0.164895128838847, 0.0, 0.5, -0.1904761904761905, 1.0, 0.5, 0.3443312269912251, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.24468359010483226, 0.24468359010483243, 0.37454487815838405], 
reward next is 0.6255, 
noisyNet noise sample is [array([1.3997629], dtype=float32), -0.24068876]. 
=============================================
[2019-04-02 18:11:57,842] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.00000000e+00 4.08328510e-31 8.25159908e-26 5.02908934e-36
 1.20147535e-33], sum to 1.0000
[2019-04-02 18:11:57,850] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5003
[2019-04-02 18:11:57,863] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 892371.6554852028 W.
[2019-04-02 18:11:57,872] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.0, 94.00000000000001, 1.0, 2.0, 0.2609760837944969, 1.0, 2.0, 0.2609760837944969, 1.0, 2.0, 0.4154824247985144, 6.911200000000001, 6.9112, 121.94756008, 892371.6554852028, 892371.6554852024, 241551.1050097964], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2959800.0000, 
sim time next is 2960400.0000, 
raw observation next is [25.0, 94.0, 1.0, 2.0, 0.3578253624023797, 1.0, 2.0, 0.3578253624023797, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 815648.8673148486, 815648.8673148486, 194916.2363683615], 
processed observation next is [1.0, 0.2608695652173913, 0.48148148148148145, 0.94, 1.0, 1.0, 0.2355063838123568, 1.0, 1.0, 0.2355063838123568, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2913031668981602, 0.2913031668981602, 0.3748389160930029], 
reward next is 0.6252, 
noisyNet noise sample is [array([0.87674874], dtype=float32), -0.14160106]. 
=============================================
[2019-04-02 18:11:58,545] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 2.9022435e-28 2.1770011e-19 3.7468165e-32 1.7529956e-28], sum to 1.0000
[2019-04-02 18:11:58,552] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6984
[2019-04-02 18:11:58,563] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 763874.2274289472 W.
[2019-04-02 18:11:58,566] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.0, 87.66666666666666, 1.0, 2.0, 0.6702462546151963, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 763874.2274289472, 763874.2274289468, 170586.9196804013], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2925600.0000, 
sim time next is 2926200.0000, 
raw observation next is [26.0, 88.33333333333334, 1.0, 2.0, 0.3368228908122546, 1.0, 1.0, 0.3368228908122546, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 767750.5811596058, 767750.5811596058, 189532.4073211354], 
processed observation next is [1.0, 0.8695652173913043, 0.5185185185185185, 0.8833333333333334, 1.0, 1.0, 0.21050344144316022, 1.0, 0.5, 0.21050344144316022, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.27419663612843065, 0.27419663612843065, 0.36448539869449115], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.5254054], dtype=float32), -0.19258945]. 
=============================================
[2019-04-02 18:12:23,461] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-02 18:12:23,465] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 18:12:23,466] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 18:12:23,468] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:12:23,468] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 18:12:23,469] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:12:23,470] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:12:23,470] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 18:12:23,473] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 18:12:23,475] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:12:23,477] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:12:23,496] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run13
[2019-04-02 18:12:23,516] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run13
[2019-04-02 18:12:23,517] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run13
[2019-04-02 18:12:23,518] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run13
[2019-04-02 18:12:23,551] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run13
[2019-04-02 18:13:30,510] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.16414802], dtype=float32), -0.12571093]
[2019-04-02 18:13:30,513] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6860950208441902, 6.911200000000001, 6.9112, 121.9260426156618, 512709.520101499, 512709.5201014986, 143315.5053791279]
[2019-04-02 18:13:30,514] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 18:13:30,519] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.0000000e+00 1.7716983e-28 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.8407322100076107
[2019-04-02 18:13:37,097] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.16414802], dtype=float32), -0.12571093]
[2019-04-02 18:13:37,099] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [27.03333333333333, 82.66666666666667, 1.0, 2.0, 0.6723215884298231, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 766240.6507664088, 766240.6507664088, 170970.8861828357]
[2019-04-02 18:13:37,103] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 18:13:37,105] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [9.999999e-01 6.580161e-31 7.624363e-08 6.851629e-33 6.626662e-31], sampled 0.4233714933060959
[2019-04-02 18:13:37,108] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 766240.6507664088 W.
[2019-04-02 18:13:58,009] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.16414802], dtype=float32), -0.12571093]
[2019-04-02 18:13:58,012] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [28.25904548, 51.64812179499999, 1.0, 2.0, 0.5360251394641457, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8602787897048572, 6.9112, 6.9112, 121.9260426156618, 1270637.718222494, 1270637.718222494, 268668.4630521405]
[2019-04-02 18:13:58,016] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 18:13:58,020] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.04799743327716299
[2019-04-02 18:13:58,020] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1270637.718222494 W.
[2019-04-02 18:14:30,967] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8358.8860 2339678324.3766 616.0000
[2019-04-02 18:14:30,970] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7818.6487 2530925237.2667 831.0000
[2019-04-02 18:14:31,004] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8620.4337 2219749520.7987 543.0000
[2019-04-02 18:14:31,094] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8559.5129 2258291576.2407 536.0000
[2019-04-02 18:14:31,100] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8388.6525 2293825186.0711 697.0000
[2019-04-02 18:14:32,115] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 300000, evaluation results [300000.0, 7818.648737419051, 2530925237.2666926, 831.0, 8559.51293803055, 2258291576.240747, 536.0, 8620.433676219576, 2219749520.7987285, 543.0, 8358.885977070893, 2339678324.3765626, 616.0, 8388.652543432778, 2293825186.0711293, 697.0]
[2019-04-02 18:14:33,213] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.000000e+00 1.006662e-33 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-02 18:14:33,229] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0074
[2019-04-02 18:14:33,236] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8949826515083509, 6.9112, 6.9112, 121.9260426156618, 655995.1903157387, 655995.1903157387, 175068.7546796787], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3317400.0000, 
sim time next is 3318000.0000, 
raw observation next is [24.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8955875585291686, 6.9112, 6.9112, 121.9260426156618, 656438.4264757908, 656438.4264757908, 175148.7816979109], 
processed observation next is [0.0, 0.391304347826087, 0.4444444444444444, 0.89, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8694844481614608, 0.0, 0.0, 0.8094621288201359, 0.2344422951699253, 0.2344422951699253, 0.33682458018829015], 
reward next is 0.6632, 
noisyNet noise sample is [array([0.62381935], dtype=float32), 1.0357877]. 
=============================================
[2019-04-02 18:14:33,256] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[70.30291 ]
 [70.24838 ]
 [70.20958 ]
 [70.184235]
 [70.15557 ]], R is [[70.31646729]
 [70.27663422]
 [70.2374649 ]
 [70.19910431]
 [70.16215515]].
[2019-04-02 18:14:38,179] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.00000000e+00 1.46008445e-30 1.40727611e-22 2.25111407e-28
 1.56579984e-26], sum to 1.0000
[2019-04-02 18:14:38,187] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9921
[2019-04-02 18:14:38,195] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1808478.306383049 W.
[2019-04-02 18:14:38,202] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.4, 77.0, 1.0, 2.0, 0.9590368096833521, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.911200000000001, 6.9112, 121.9260426156618, 1808478.306383049, 1808478.306383049, 370018.5156386691], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3403200.0000, 
sim time next is 3403800.0000, 
raw observation next is [27.6, 76.0, 1.0, 2.0, 0.5341116657943213, 1.0, 1.0, 0.5341116657943213, 1.0, 2.0, 0.8503231667471202, 6.911199999999999, 6.9112, 121.94756008, 1827393.175791308, 1827393.175791309, 360646.7834327876], 
processed observation next is [1.0, 0.391304347826087, 0.5777777777777778, 0.76, 1.0, 1.0, 0.4453710307075254, 1.0, 0.5, 0.4453710307075254, 1.0, 1.0, 0.8129039584339001, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.6526404199254672, 0.6526404199254675, 0.6935515066015145], 
reward next is 0.3064, 
noisyNet noise sample is [array([-0.05775139], dtype=float32), 0.40496737]. 
=============================================
[2019-04-02 18:14:41,391] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.00000000e+00 6.58834855e-24 1.23502615e-26 2.82914833e-27
 9.09287425e-26], sum to 1.0000
[2019-04-02 18:14:41,403] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7094
[2019-04-02 18:14:41,413] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1506616.489180533 W.
[2019-04-02 18:14:41,417] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 59.0, 1.0, 2.0, 0.6606552646365921, 1.0, 2.0, 0.6606552646365921, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1506616.489180533, 1506616.489180533, 289539.0694545883], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3414000.0000, 
sim time next is 3414600.0000, 
raw observation next is [31.0, 59.0, 1.0, 2.0, 0.6225078918494493, 1.0, 2.0, 0.6225078918494493, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1419541.230298599, 1419541.230298599, 275870.0640902536], 
processed observation next is [1.0, 0.5217391304347826, 0.7037037037037037, 0.59, 1.0, 1.0, 0.5506046331541063, 1.0, 1.0, 0.5506046331541063, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.5069790108209282, 0.5069790108209282, 0.5305193540197185], 
reward next is 0.4695, 
noisyNet noise sample is [array([1.2045403], dtype=float32), 2.3243392]. 
=============================================
[2019-04-02 18:14:56,167] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 3.5739854e-23 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:14:56,175] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2330
[2019-04-02 18:14:56,179] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.4, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9048583728661702, 6.9112, 6.9112, 121.9260426156618, 662067.7846247307, 662067.7846247307, 176623.0443050754], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3622800.0000, 
sim time next is 3623400.0000, 
raw observation next is [23.6, 91.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8894371258386028, 6.9112, 6.9112, 121.9260426156618, 652516.1608753932, 652516.1608753932, 174207.0753689294], 
processed observation next is [1.0, 0.9565217391304348, 0.4296296296296297, 0.91, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8617964072982534, 0.0, 0.0, 0.8094621288201359, 0.23304148602692615, 0.23304148602692615, 0.33501360647871037], 
reward next is 0.6650, 
noisyNet noise sample is [array([0.62092686], dtype=float32), 1.5620421]. 
=============================================
[2019-04-02 18:14:58,052] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 4.1783943e-24 0.0000000e+00 7.3326186e-37 0.0000000e+00], sum to 1.0000
[2019-04-02 18:14:58,065] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8853
[2019-04-02 18:14:58,076] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1949533.341758962 W.
[2019-04-02 18:14:58,082] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.5697719603553174, 1.0, 2.0, 0.5697719603553174, 1.0, 1.0, 0.9070955170629399, 6.911200000000001, 6.9112, 121.94756008, 1949533.341758962, 1949533.341758962, 379151.9534152023], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3685200.0000, 
sim time next is 3685800.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5804758666878331, 1.0, 2.0, 0.5804758666878331, 1.0, 2.0, 0.924136484546197, 6.911200000000001, 6.9112, 121.94756008, 1986198.577746873, 1986198.577746872, 384840.5016557844], 
processed observation next is [1.0, 0.6521739130434783, 0.5555555555555556, 0.89, 1.0, 1.0, 0.5005665079617061, 1.0, 1.0, 0.5005665079617061, 1.0, 1.0, 0.9051706056827463, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.7093566349095975, 0.7093566349095972, 0.7400778877995854], 
reward next is 0.2599, 
noisyNet noise sample is [array([-0.17227283], dtype=float32), -2.1105888]. 
=============================================
[2019-04-02 18:15:06,479] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 2.2733152e-23 6.0132287e-35 1.8426425e-33 3.8675491e-34], sum to 1.0000
[2019-04-02 18:15:06,489] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0534
[2019-04-02 18:15:06,497] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 717501.9396928785 W.
[2019-04-02 18:15:06,503] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.5, 79.0, 1.0, 2.0, 0.3147884232590946, 1.0, 1.0, 0.3147884232590946, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 717501.9396928785, 717501.939692879, 184047.3582251899], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3803400.0000, 
sim time next is 3804000.0000, 
raw observation next is [26.33333333333334, 80.66666666666667, 1.0, 2.0, 0.316375853770246, 1.0, 2.0, 0.316375853770246, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 721121.8955817649, 721121.8955817653, 184436.9041933611], 
processed observation next is [0.0, 0.0, 0.5308641975308644, 0.8066666666666668, 1.0, 1.0, 0.18616173067886432, 1.0, 1.0, 0.18616173067886432, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2575435341363446, 0.2575435341363448, 0.3546863542180021], 
reward next is 0.6453, 
noisyNet noise sample is [array([0.16001049], dtype=float32), 0.79057956]. 
=============================================
[2019-04-02 18:15:06,517] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[32.702923]
 [32.61679 ]
 [32.845387]
 [32.302567]
 [32.289795]], R is [[33.16835403]
 [32.83666992]
 [33.12547684]
 [33.44050217]
 [33.10609818]].
[2019-04-02 18:15:22,016] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 6.0677118e-24 0.0000000e+00 2.0697076e-34 3.0414231e-34], sum to 1.0000
[2019-04-02 18:15:22,025] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1385
[2019-04-02 18:15:22,033] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1044024.006286544 W.
[2019-04-02 18:15:22,038] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.75, 91.83333333333333, 1.0, 2.0, 0.4579453945756327, 0.0, 2.0, 0.0, 1.0, 1.0, 0.7290639824046912, 6.9112, 6.9112, 121.9260426156618, 1044024.006286544, 1044024.006286544, 242655.389264504], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3991800.0000, 
sim time next is 3992400.0000, 
raw observation next is [24.7, 92.0, 1.0, 2.0, 0.442314994329764, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7041798761666707, 6.9112, 6.9112, 121.9260426156618, 1008366.369758757, 1008366.369758757, 237516.9004768865], 
processed observation next is [1.0, 0.21739130434782608, 0.4703703703703703, 0.92, 1.0, 1.0, 0.3360892789640048, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6302248452083383, 0.0, 0.0, 0.8094621288201359, 0.3601308463424132, 0.3601308463424132, 0.45676327014785867], 
reward next is 0.5432, 
noisyNet noise sample is [array([1.1928046], dtype=float32), -0.38443595]. 
=============================================
[2019-04-02 18:15:30,390] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 3.6065936e-24 0.0000000e+00 2.0724776e-37 3.1826286e-37], sum to 1.0000
[2019-04-02 18:15:30,400] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3654
[2019-04-02 18:15:30,410] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1998517.500014849 W.
[2019-04-02 18:15:30,416] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.86666666666667, 66.0, 1.0, 2.0, 0.5840721063108381, 1.0, 1.0, 0.5840721063108381, 1.0, 2.0, 0.9298618151473688, 6.9112, 6.9112, 121.94756008, 1998517.500014849, 1998517.500014849, 386765.5753198219], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4110000.0000, 
sim time next is 4110600.0000, 
raw observation next is [28.83333333333333, 65.0, 1.0, 2.0, 0.8467538951286943, 1.0, 2.0, 0.8467538951286943, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260425317581, 1931484.547971711, 1931484.547971712, 363451.7089560908], 
processed observation next is [1.0, 0.5652173913043478, 0.6234567901234566, 0.65, 1.0, 1.0, 0.8175641608674932, 1.0, 1.0, 0.8175641608674932, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.8094621282631026, 0.6898159099898968, 0.6898159099898972, 0.6989455941463284], 
reward next is 0.3011, 
noisyNet noise sample is [array([-0.19079447], dtype=float32), 0.6786218]. 
=============================================
[2019-04-02 18:15:38,688] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-02 18:15:38,690] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 18:15:38,691] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:15:38,693] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 18:15:38,695] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 18:15:38,696] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 18:15:38,696] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:15:38,697] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:15:38,698] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 18:15:38,700] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:15:38,700] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:15:38,723] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run14
[2019-04-02 18:15:38,724] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run14
[2019-04-02 18:15:38,743] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run14
[2019-04-02 18:15:38,762] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run14
[2019-04-02 18:15:38,763] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run14
[2019-04-02 18:15:40,635] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.15077466], dtype=float32), -0.12655354]
[2019-04-02 18:15:40,636] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [26.8, 53.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6838738468218539, 6.9112, 6.9112, 121.9260426156618, 511019.7248349458, 511019.7248349458, 142669.5381334081]
[2019-04-02 18:15:40,636] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 18:15:40,640] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.0000000e+00 1.3398968e-27 1.4792936e-35 0.0000000e+00 0.0000000e+00], sampled 0.39171737682038255
[2019-04-02 18:15:52,716] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.15077466], dtype=float32), -0.12655354]
[2019-04-02 18:15:52,718] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [26.602480095, 47.84081562, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6073248053262199, 6.911200000000001, 6.9112, 121.9260426156618, 451477.1962585914, 451477.196258591, 131588.2509535941]
[2019-04-02 18:15:52,720] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 18:15:52,723] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.0000000e+00 2.3220838e-29 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.9872665923758192
[2019-04-02 18:16:20,292] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.15077466], dtype=float32), -0.12655354]
[2019-04-02 18:16:20,294] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [25.97286399, 40.81282595, 1.0, 2.0, 0.7209681206328179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 917732.9026359434, 917732.9026359434, 183389.5862187528]
[2019-04-02 18:16:20,294] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 18:16:20,296] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.0000000e+00 2.4088387e-26 2.3210907e-15 2.0832500e-29 7.5915251e-27], sampled 0.017461361114794238
[2019-04-02 18:16:20,300] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 917732.9026359434 W.
[2019-04-02 18:16:24,310] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.15077466], dtype=float32), -0.12655354]
[2019-04-02 18:16:24,313] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [24.04316258, 94.38834373, 1.0, 2.0, 1.02, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.5273985626204, 6.9112, 121.9235369164284, 1485680.362890846, 1170138.108461352, 245992.3749257596]
[2019-04-02 18:16:24,314] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 18:16:24,318] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.0000000e+00 4.5968593e-34 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.6271890222352557
[2019-04-02 18:16:24,318] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1485680.362890846 W.
[2019-04-02 18:17:06,801] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.15077466], dtype=float32), -0.12655354]
[2019-04-02 18:17:06,805] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [27.2, 92.5, 1.0, 2.0, 0.9329145825257404, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 1063442.896513911, 1063442.89651391, 224938.3415403327]
[2019-04-02 18:17:06,808] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 18:17:06,811] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.0000000e+00 1.3716691e-38 2.8284499e-25 2.9176362e-38 1.7948899e-34], sampled 0.9968630554302211
[2019-04-02 18:17:06,812] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1063442.896513911 W.
[2019-04-02 18:17:45,018] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8561.6006 2258270367.4075 535.0000
[2019-04-02 18:17:45,979] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8355.3282 2339881944.4789 613.0000
[2019-04-02 18:17:46,010] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7822.7859 2530883164.5027 831.0000
[2019-04-02 18:17:46,162] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8628.6976 2219350885.8887 543.0000
[2019-04-02 18:17:46,243] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8399.3025 2293548318.5942 697.0000
[2019-04-02 18:17:47,255] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 325000, evaluation results [325000.0, 7822.785887018951, 2530883164.5027413, 831.0, 8561.600640183287, 2258270367.4075346, 535.0, 8628.697614584531, 2219350885.888656, 543.0, 8355.328230199319, 2339881944.4789057, 613.0, 8399.302464231718, 2293548318.5941586, 697.0]
[2019-04-02 18:17:49,600] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 2.9783695e-25 2.7670451e-38 4.4971273e-34 3.5866144e-33], sum to 1.0000
[2019-04-02 18:17:49,606] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5539
[2019-04-02 18:17:49,614] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1047199.503289468 W.
[2019-04-02 18:17:49,620] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.66666666666667, 53.5, 1.0, 2.0, 0.2939043218193763, 1.0, 1.0, 0.2939043218193763, 1.0, 2.0, 0.4721244286648891, 6.9112, 6.9112, 121.94756008, 1047199.503289468, 1047199.503289468, 253438.6361270608], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4263000.0000, 
sim time next is 4263600.0000, 
raw observation next is [27.53333333333333, 56.00000000000001, 1.0, 2.0, 0.6157093298247371, 1.0, 2.0, 0.6157093298247371, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 1455089.518880107, 1455089.518880107, 275838.4474484782], 
processed observation next is [1.0, 0.34782608695652173, 0.5753086419753086, 0.56, 1.0, 1.0, 0.5425111069342109, 1.0, 1.0, 0.5425111069342109, 0.0, 0.5, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.5196748281714668, 0.5196748281714668, 0.530458552785535], 
reward next is 0.4695, 
noisyNet noise sample is [array([-0.80868685], dtype=float32), -0.19859943]. 
=============================================
[2019-04-02 18:17:51,165] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 2.0880689e-24 5.9005901e-20 8.2824523e-31 1.2065091e-33], sum to 1.0000
[2019-04-02 18:17:51,171] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4282
[2019-04-02 18:17:51,178] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 705949.7138353636 W.
[2019-04-02 18:17:51,187] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 74.0, 1.0, 2.0, 0.2064816616032218, 1.0, 1.0, 0.2064816616032218, 1.0, 2.0, 0.3287255299105761, 6.9112, 6.9112, 121.94756008, 705949.7138353636, 705949.7138353636, 222723.5335083138], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4312800.0000, 
sim time next is 4313400.0000, 
raw observation next is [26.78333333333333, 74.0, 1.0, 2.0, 0.605861625737382, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 696399.5748209049, 696399.5748209049, 159378.6919590757], 
processed observation next is [1.0, 0.9565217391304348, 0.5475308641975308, 0.74, 1.0, 1.0, 0.5307876496873595, 0.0, 0.5, -0.1904761904761905, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2487141338646089, 0.2487141338646089, 0.306497484536684], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.839349], dtype=float32), -0.004567238]. 
=============================================
[2019-04-02 18:17:56,881] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 1.2970823e-21 3.3028443e-19 2.4965979e-29 4.4128757e-32], sum to 1.0000
[2019-04-02 18:17:56,896] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1757
[2019-04-02 18:17:56,910] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 754843.11965058 W.
[2019-04-02 18:17:56,914] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.8, 73.0, 1.0, 2.0, 0.2207753486536529, 1.0, 1.0, 0.2207753486536529, 1.0, 2.0, 0.3514815452077504, 6.9112, 6.9112, 121.94756008, 754843.11965058, 754843.11965058, 227501.2967989366], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4392000.0000, 
sim time next is 4392600.0000, 
raw observation next is [27.83333333333334, 74.0, 1.0, 2.0, 0.221311745568761, 1.0, 2.0, 0.221311745568761, 1.0, 2.0, 0.3523355065658305, 6.911200000000001, 6.9112, 121.94756008, 756677.9953986033, 756677.9953986028, 227682.8163965552], 
processed observation next is [1.0, 0.8695652173913043, 0.58641975308642, 0.74, 1.0, 1.0, 0.07299017329614403, 1.0, 1.0, 0.07299017329614403, 1.0, 1.0, 0.19041938320728813, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.27024214121378687, 0.2702421412137867, 0.4378515699933754], 
reward next is 0.5621, 
noisyNet noise sample is [array([-1.4069399], dtype=float32), 0.31377172]. 
=============================================
[2019-04-02 18:17:59,588] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 3.4788007e-25 7.1295319e-21 5.3792244e-31 7.3963316e-30], sum to 1.0000
[2019-04-02 18:17:59,599] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3913
[2019-04-02 18:17:59,608] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.06666666666667, 78.33333333333334, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8917156978927531, 6.911200000000001, 6.9112, 121.9260426156618, 654210.4289146803, 654210.4289146798, 174501.836491856], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4401600.0000, 
sim time next is 4402200.0000, 
raw observation next is [24.78333333333333, 78.16666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8639036145512629, 6.911200000000001, 6.9112, 121.9260426156618, 638057.4717540786, 638057.4717540783, 169730.7616389829], 
processed observation next is [1.0, 0.9565217391304348, 0.4734567901234567, 0.7816666666666666, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8298795181890786, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.22787766848359953, 0.2278776684835994, 0.3264053108441979], 
reward next is 0.6736, 
noisyNet noise sample is [array([1.8502162], dtype=float32), 2.9500022]. 
=============================================
[2019-04-02 18:18:01,253] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.000000e+00 1.730825e-32 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-02 18:18:01,264] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0941
[2019-04-02 18:18:01,270] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.13333333333333, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7909900076773129, 6.9112, 6.9112, 121.9260426156618, 588272.6634561362, 588272.6634561362, 158812.2977676054], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4426800.0000, 
sim time next is 4427400.0000, 
raw observation next is [22.06666666666667, 93.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7916271453909068, 6.911199999999999, 6.9112, 121.9260426156618, 588766.5316148774, 588766.5316148779, 158879.3369730608], 
processed observation next is [0.0, 0.21739130434782608, 0.3728395061728396, 0.935, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.7395339317386335, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.21027376129102765, 0.21027376129102782, 0.3055371864866554], 
reward next is 0.6945, 
noisyNet noise sample is [array([0.09334237], dtype=float32), -0.51160955]. 
=============================================
[2019-04-02 18:18:20,901] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 1.1868446e-24 0.0000000e+00 5.3441280e-38 0.0000000e+00], sum to 1.0000
[2019-04-02 18:18:20,909] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8389
[2019-04-02 18:18:20,919] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1170720.9386266 W.
[2019-04-02 18:18:20,924] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.2, 93.33333333333334, 1.0, 2.0, 0.9727392835028351, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.017243403194882, 6.9112, 121.9256659165402, 1170720.9386266, 1116417.405914113, 234633.9716987006], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4695600.0000, 
sim time next is 4696200.0000, 
raw observation next is [23.9, 95.0, 1.0, 2.0, 0.5601793518824147, 0.0, 2.0, 0.0, 1.0, 1.0, 0.8918237719646313, 6.911200000000001, 6.9112, 121.9259787582581, 1277291.296609551, 1277291.29660955, 278685.7335002204], 
processed observation next is [1.0, 0.34782608695652173, 0.4407407407407407, 0.95, 1.0, 1.0, 0.476403990336208, 0.0, 1.0, -0.1904761904761905, 1.0, 0.5, 0.864779714955789, 8.881784197001253e-17, 0.0, 0.8094617048733824, 0.45617546307483964, 0.4561754630748393, 0.5359341028850393], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.12842157], dtype=float32), 3.2292786]. 
=============================================
[2019-04-02 18:18:23,795] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 2.1071034e-25 2.0625017e-36 5.6959474e-35 2.2837600e-33], sum to 1.0000
[2019-04-02 18:18:23,810] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2648
[2019-04-02 18:18:23,819] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 769794.940768387 W.
[2019-04-02 18:18:23,825] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.33333333333333, 92.33333333333334, 1.0, 2.0, 0.6754386595063027, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 769794.940768387, 769794.940768387, 171545.1851873812], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4743600.0000, 
sim time next is 4744200.0000, 
raw observation next is [25.16666666666667, 93.16666666666667, 1.0, 2.0, 0.6759829207336929, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 770415.5450021918, 770415.5450021918, 171645.6638148567], 
processed observation next is [1.0, 0.9130434782608695, 0.4876543209876545, 0.9316666666666668, 1.0, 1.0, 0.6142653818258249, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2751484089293542, 0.2751484089293542, 0.3300878150285706], 
reward next is 0.6699, 
noisyNet noise sample is [array([0.38426113], dtype=float32), 1.147447]. 
=============================================
[2019-04-02 18:18:29,043] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 1.6484533e-27 0.0000000e+00 3.1435859e-35 5.7285839e-37], sum to 1.0000
[2019-04-02 18:18:29,062] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0910
[2019-04-02 18:18:29,070] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 905618.9457457593 W.
[2019-04-02 18:18:29,078] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.0, 94.0, 1.0, 2.0, 0.7945439056199906, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 905618.9457457593, 905618.9457457588, 194822.0582336037], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4819800.0000, 
sim time next is 4820400.0000, 
raw observation next is [27.0, 94.0, 1.0, 2.0, 0.3971396587404937, 0.0, 2.0, 0.0, 1.0, 1.0, 0.6322592706505862, 6.911199999999999, 6.9112, 121.9260426156618, 905317.190790436, 905317.1907904365, 223225.6274466083], 
processed observation next is [1.0, 0.8260869565217391, 0.5555555555555556, 0.94, 1.0, 1.0, 0.28230911754820676, 0.0, 1.0, -0.1904761904761905, 1.0, 0.5, 0.5403240883132326, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.3233275681394414, 0.3233275681394416, 0.42928005278193904], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5718341], dtype=float32), 0.10581638]. 
=============================================
[2019-04-02 18:18:33,767] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 8.3067907e-22 0.0000000e+00 2.6183297e-36 0.0000000e+00], sum to 1.0000
[2019-04-02 18:18:33,777] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9860
[2019-04-02 18:18:33,788] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1655263.80186275 W.
[2019-04-02 18:18:33,794] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.0, 89.0, 1.0, 2.0, 0.7257720937565222, 1.0, 1.0, 0.7257720937565222, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1655263.80186275, 1655263.80186275, 314039.5592293498], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4885200.0000, 
sim time next is 4885800.0000, 
raw observation next is [29.46666666666667, 87.0, 1.0, 2.0, 0.8795485998880098, 1.0, 2.0, 0.8795485998880098, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 2006374.807751252, 2006374.807751252, 377724.3289517512], 
processed observation next is [1.0, 0.5652173913043478, 0.6469135802469137, 0.87, 1.0, 1.0, 0.8566054760571545, 1.0, 1.0, 0.8566054760571545, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.7165624313397329, 0.7165624313397329, 0.7263929402918292], 
reward next is 0.2736, 
noisyNet noise sample is [array([-0.55250376], dtype=float32), -0.47705543]. 
=============================================
[2019-04-02 18:18:34,570] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 2.2078886e-25 0.0000000e+00 2.5156420e-36 0.0000000e+00], sum to 1.0000
[2019-04-02 18:18:34,582] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5874
[2019-04-02 18:18:34,591] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2002816.467143899 W.
[2019-04-02 18:18:34,597] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.95, 86.5, 1.0, 2.0, 0.877990452412384, 1.0, 2.0, 0.877990452412384, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 2002816.467143899, 2002816.467143899, 377036.5330358052], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4876200.0000, 
sim time next is 4876800.0000, 
raw observation next is [28.16666666666667, 85.66666666666667, 1.0, 2.0, 0.587282987304869, 1.0, 2.0, 0.587282987304869, 1.0, 1.0, 0.9349736422609595, 6.911199999999999, 6.9112, 121.94756008, 2009516.519502511, 2009516.519502511, 388490.2566613724], 
processed observation next is [1.0, 0.43478260869565216, 0.5987654320987656, 0.8566666666666667, 1.0, 1.0, 0.5086702229819869, 1.0, 1.0, 0.5086702229819869, 1.0, 0.5, 0.9187170528261992, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.7176844712508967, 0.7176844712508967, 0.7470966474257161], 
reward next is 0.2529, 
noisyNet noise sample is [array([-0.37387425], dtype=float32), 1.06523]. 
=============================================
[2019-04-02 18:18:40,311] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 1.9181440e-18 0.0000000e+00 1.1889139e-34 7.1486501e-36], sum to 1.0000
[2019-04-02 18:18:40,321] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4592
[2019-04-02 18:18:40,332] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1405080.135952911 W.
[2019-04-02 18:18:40,344] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.6161721143108696, 1.0, 1.0, 0.6161721143108696, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9256910125804, 1405080.135952911, 1405080.135952911, 273648.6811063022], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4971600.0000, 
sim time next is 4972200.0000, 
raw observation next is [25.7, 90.0, 1.0, 2.0, 0.588572978371729, 1.0, 2.0, 0.588572978371729, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260425084473, 1342089.658093665, 1342089.658093665, 264134.220854636], 
processed observation next is [1.0, 0.5652173913043478, 0.5074074074074074, 0.9, 1.0, 1.0, 0.5102059266330107, 1.0, 1.0, 0.5102059266330107, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621281083432, 0.4793177350334518, 0.4793177350334518, 0.5079504247204538], 
reward next is 0.4920, 
noisyNet noise sample is [array([-0.658917], dtype=float32), 0.9907492]. 
=============================================
[2019-04-02 18:18:49,906] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 6.0912927e-18 0.0000000e+00 4.4624709e-34 3.9252484e-34], sum to 1.0000
[2019-04-02 18:18:49,915] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5080
[2019-04-02 18:18:49,923] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 887788.2232040605 W.
[2019-04-02 18:18:49,931] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.83333333333334, 76.66666666666667, 1.0, 2.0, 0.3894545970957785, 0.0, 2.0, 0.0, 1.0, 2.0, 0.620024402227215, 6.911199999999999, 6.9112, 121.9260426156618, 887788.2232040605, 887788.2232040609, 220875.2894942868], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5138400.0000, 
sim time next is 5139000.0000, 
raw observation next is [29.95, 76.5, 1.0, 2.0, 0.4049261457667784, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6446556115846098, 6.9112, 6.9112, 121.9260426156618, 923077.9099025399, 923077.9099025399, 225632.520708408], 
processed observation next is [0.0, 0.4782608695652174, 0.6648148148148147, 0.765, 1.0, 1.0, 0.29157874496045044, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5558195144807623, 0.0, 0.0, 0.8094621288201359, 0.32967068210804995, 0.32967068210804995, 0.4339086936700154], 
reward next is 0.5661, 
noisyNet noise sample is [array([0.35350305], dtype=float32), -1.0393904]. 
=============================================
[2019-04-02 18:18:49,943] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[25.352274]
 [25.498392]
 [25.83253 ]
 [25.20287 ]
 [25.210608]], R is [[26.00798988]
 [26.32314873]
 [26.05991745]
 [25.79931831]
 [26.06540108]].
[2019-04-02 18:18:53,884] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 7.5119772e-11 8.2902344e-22 1.7082637e-21 6.2912012e-25], sum to 1.0000
[2019-04-02 18:18:53,892] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4577
[2019-04-02 18:18:53,902] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 893656.9118819162 W.
[2019-04-02 18:18:53,909] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.7, 69.83333333333333, 1.0, 2.0, 0.2613517409164953, 1.0, 1.0, 0.2613517409164953, 1.0, 1.0, 0.4160804831710341, 6.9112, 6.9112, 121.94756008, 893656.9118819162, 893656.9118819162, 241686.6514612802], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5159400.0000, 
sim time next is 5160000.0000, 
raw observation next is [30.5, 70.66666666666667, 1.0, 2.0, 0.2537106133296669, 1.0, 2.0, 0.2537106133296669, 1.0, 2.0, 0.4039155591986514, 6.9112, 6.9112, 121.94756008, 867514.3252030353, 867514.3252030353, 238945.0405093263], 
processed observation next is [0.0, 0.7391304347826086, 0.6851851851851852, 0.7066666666666667, 1.0, 1.0, 0.11156025396388915, 1.0, 1.0, 0.11156025396388915, 1.0, 1.0, 0.2548944489983142, 0.0, 0.0, 0.8096049824067558, 0.30982654471536974, 0.30982654471536974, 0.45950969328716595], 
reward next is 0.5405, 
noisyNet noise sample is [array([0.6549009], dtype=float32), 1.4682873]. 
=============================================
[2019-04-02 18:18:53,922] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[31.01044 ]
 [31.504677]
 [31.365437]
 [31.27106 ]
 [31.413757]], R is [[31.56137085]
 [31.78097534]
 [32.08902359]
 [32.39283752]
 [32.06890869]].
[2019-04-02 18:18:53,928] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-02 18:18:53,930] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 18:18:53,931] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:18:53,931] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 18:18:53,931] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 18:18:53,933] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 18:18:53,934] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:18:53,932] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 18:18:53,935] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:18:53,938] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:18:53,939] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:18:53,960] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run15
[2019-04-02 18:18:53,961] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run15
[2019-04-02 18:18:53,999] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run15
[2019-04-02 18:18:54,000] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run15
[2019-04-02 18:18:54,001] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run15
[2019-04-02 18:19:29,277] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.12970465], dtype=float32), -0.1518798]
[2019-04-02 18:19:29,278] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [24.98683801, 84.68081446333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9042388932722251, 6.911199999999999, 6.9112, 121.9260426156618, 660632.7626006637, 660632.7626006642, 176732.4104432662]
[2019-04-02 18:19:29,279] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 18:19:29,281] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.0000000e+00 1.5560596e-17 3.3348097e-35 2.8896507e-35 8.3184320e-36], sampled 0.42958536859611096
[2019-04-02 18:19:44,678] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.12970465], dtype=float32), -0.1518798]
[2019-04-02 18:19:44,679] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [21.6, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7361414960903186, 6.9112, 6.9112, 121.9260426156618, 549339.0287249684, 549339.0287249684, 150750.7546810003]
[2019-04-02 18:19:44,679] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 18:19:44,682] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.000000e+00 7.108608e-17 0.000000e+00 6.473454e-38 0.000000e+00], sampled 0.7193564552900736
[2019-04-02 18:19:49,404] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.12970465], dtype=float32), -0.1518798]
[2019-04-02 18:19:49,404] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [32.55558087, 66.3309314, 1.0, 2.0, 0.9814200053193421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.030190265931467, 6.9112, 121.9255830611569, 1179753.216817384, 1118819.79509774, 236286.428866494]
[2019-04-02 18:19:49,406] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 18:19:49,409] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.0000000e+00 1.6653096e-20 4.1256889e-29 1.1733066e-27 1.2682350e-28], sampled 0.21206192853071004
[2019-04-02 18:19:49,411] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1179753.216817384 W.
[2019-04-02 18:20:49,365] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.12970465], dtype=float32), -0.1518798]
[2019-04-02 18:20:49,365] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [24.0, 60.0, 1.0, 2.0, 0.797624329433316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 997062.691725878, 997062.691725878, 198887.9686360311]
[2019-04-02 18:20:49,365] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 18:20:49,367] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.0000000e+00 4.6499942e-21 8.0789581e-32 6.5459480e-28 1.3132336e-28], sampled 0.4872781145065729
[2019-04-02 18:20:49,368] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 997062.691725878 W.
[2019-04-02 18:20:51,021] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.12970465], dtype=float32), -0.1518798]
[2019-04-02 18:20:51,022] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [23.13333333333333, 94.0, 1.0, 2.0, 0.5436654231711456, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8667735341090319, 6.911199999999999, 6.9112, 121.9260426156618, 1257789.078537526, 1257789.078537527, 272290.7646375967]
[2019-04-02 18:20:51,023] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 18:20:51,027] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.0000000e+00 2.4319291e-27 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.44258336297760104
[2019-04-02 18:20:51,029] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1257789.078537526 W.
[2019-04-02 18:21:01,347] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8633.8375 2219139301.2698 543.0000
[2019-04-02 18:21:01,434] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8404.0051 2293101698.6415 697.0000
[2019-04-02 18:21:01,531] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8560.3296 2258226393.9236 536.0000
[2019-04-02 18:21:01,708] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7841.5838 2529739775.4178 831.0000
[2019-04-02 18:21:01,773] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8363.6586 2339443518.1415 616.0000
[2019-04-02 18:21:02,788] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 350000, evaluation results [350000.0, 7841.583789482413, 2529739775.4177794, 831.0, 8560.329577213746, 2258226393.9236007, 536.0, 8633.837517256845, 2219139301.2698236, 543.0, 8363.658596582882, 2339443518.141513, 616.0, 8404.005146647998, 2293101698.6415462, 697.0]
[2019-04-02 18:21:03,728] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 2.0077465e-13 4.1960648e-22 1.1112082e-26 1.8267711e-31], sum to 1.0000
[2019-04-02 18:21:03,735] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6816
[2019-04-02 18:21:03,745] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 774890.1576592176 W.
[2019-04-02 18:21:03,751] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.9, 86.66666666666667, 1.0, 2.0, 0.339953540183477, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5412171074859565, 6.911199999999997, 6.9112, 121.9260426156618, 774890.1576592176, 774890.157659219, 206294.1218056087], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5175600.0000, 
sim time next is 5176200.0000, 
raw observation next is [25.75, 86.83333333333333, 1.0, 2.0, 0.6728394217537006, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 766831.1173301353, 766831.1173301353, 171063.1994321154], 
processed observation next is [0.0, 0.9130434782608695, 0.5092592592592593, 0.8683333333333333, 1.0, 1.0, 0.6105231211353578, 0.0, 1.0, -0.1904761904761905, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288201359, 0.273868256189334, 0.273868256189334, 0.32896769121560654], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.258423], dtype=float32), 0.55605036]. 
=============================================
[2019-04-02 18:21:15,068] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.000000e+00 7.325169e-18 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-02 18:21:15,079] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0390
[2019-04-02 18:21:15,086] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 687437.7210783295 W.
[2019-04-02 18:21:15,091] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.26666666666667, 68.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9551035720216058, 6.9112, 6.9112, 121.9260426156618, 687437.7210783295, 687437.7210783295, 185153.0211419244], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5335800.0000, 
sim time next is 5336400.0000, 
raw observation next is [28.23333333333334, 68.66666666666667, 1.0, 1.0, 0.3032679094783778, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4828123886358576, 6.911199999999999, 6.9112, 121.9260426156618, 691231.2221128754, 691231.2221128759, 196126.437873473], 
processed observation next is [1.0, 0.782608695652174, 0.6012345679012349, 0.6866666666666668, 1.0, 0.5, 0.17055703509330689, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.353515485794822, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.24686829361174123, 0.2468682936117414, 0.3771662266797558], 
reward next is 0.6228, 
noisyNet noise sample is [array([0.622083], dtype=float32), -0.9291692]. 
=============================================
[2019-04-02 18:21:28,797] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.7338437e-04 9.9952662e-01 2.3365270e-17 6.1533340e-21 1.2805138e-21], sum to 1.0000
[2019-04-02 18:21:28,806] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6397
[2019-04-02 18:21:28,810] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 82.5, 1.0, 2.0, 0.6543756454713012, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 745777.8237105258, 745777.8237105262, 167685.3556043157], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5517000.0000, 
sim time next is 5517600.0000, 
raw observation next is [26.46666666666667, 83.0, 1.0, 2.0, 0.6587314595613419, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 750744.48217311, 750744.48217311, 168477.2370913509], 
processed observation next is [1.0, 0.8695652173913043, 0.5358024691358025, 0.83, 1.0, 1.0, 0.5937279280492166, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2681230293475393, 0.2681230293475393, 0.32399468671413634], 
reward next is 0.6760, 
noisyNet noise sample is [array([0.7661942], dtype=float32), 0.24898814]. 
=============================================
[2019-04-02 18:21:39,964] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.000000e+00 8.221097e-32 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-02 18:21:39,973] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5408
[2019-04-02 18:21:39,983] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 808829.6058145459 W.
[2019-04-02 18:21:39,989] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.48333333333333, 66.66666666666667, 1.0, 2.0, 0.7096706566424965, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 808829.6058145459, 808829.6058145459, 177985.9318262578], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5674200.0000, 
sim time next is 5674800.0000, 
raw observation next is [30.56666666666667, 66.33333333333334, 1.0, 2.0, 0.3540643441773492, 0.0, 2.0, 0.0, 1.0, 1.0, 0.5636819670010038, 6.911199999999999, 6.9112, 121.9260426156618, 807071.260281186, 807071.2602811864, 210352.2498530654], 
processed observation next is [0.0, 0.6956521739130435, 0.6876543209876544, 0.6633333333333334, 1.0, 1.0, 0.23102898116351098, 0.0, 1.0, -0.1904761904761905, 1.0, 0.5, 0.4546024587512547, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2882397358147093, 0.28823973581470946, 0.40452355740974116], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.211999], dtype=float32), -0.01800786]. 
=============================================
[2019-04-02 18:21:40,858] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:21:40,867] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9113
[2019-04-02 18:21:40,877] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.0, 94.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8780448671492036, 6.9112, 6.9112, 121.9260426156618, 646126.5283136271, 646126.5283136271, 172235.7406023592], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5703000.0000, 
sim time next is 5703600.0000, 
raw observation next is [22.9, 94.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.877029341257314, 6.9112, 6.9112, 121.9260426156618, 645859.8968338282, 645859.8968338282, 171975.5583515934], 
processed observation next is [0.0, 0.0, 0.4037037037037037, 0.9466666666666668, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8462866765716426, 0.0, 0.0, 0.8094621288201359, 0.23066424886922435, 0.23066424886922435, 0.3307222275992181], 
reward next is 0.6693, 
noisyNet noise sample is [array([0.38687754], dtype=float32), -0.115431376]. 
=============================================
[2019-04-02 18:21:42,284] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.000000e+00 6.076157e-27 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-02 18:21:42,293] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3728
[2019-04-02 18:21:42,299] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.36666666666667, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7075216614496765, 6.911200000000001, 6.9112, 121.9260426156618, 528712.0161820297, 528712.0161820293, 145904.5853112974], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5728800.0000, 
sim time next is 5729400.0000, 
raw observation next is [21.4, 88.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7057637116364961, 6.911200000000001, 6.9112, 121.9260426156618, 527405.0994779263, 527405.0994779258, 145634.5249535407], 
processed observation next is [0.0, 0.30434782608695654, 0.3481481481481481, 0.885, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6322046395456201, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.18835896409925937, 0.1883589640992592, 0.2800663941414244], 
reward next is 0.7199, 
noisyNet noise sample is [array([0.4463291], dtype=float32), -0.7365065]. 
=============================================
[2019-04-02 18:21:42,632] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 7.7642767e-16 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:21:42,644] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1215
[2019-04-02 18:21:42,649] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.26666666666667, 91.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7195369093866449, 6.9112, 6.9112, 121.9260426156618, 537615.466645261, 537615.466645261, 147643.2429877314], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5726400.0000, 
sim time next is 5727000.0000, 
raw observation next is [21.28333333333333, 90.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7159834757247074, 6.911200000000001, 6.9112, 121.9260426156618, 534989.0889191796, 534989.0889191792, 147128.8156225369], 
processed observation next is [0.0, 0.2608695652173913, 0.3438271604938271, 0.905, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6449793446558844, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.19106753175684987, 0.1910675317568497, 0.2829400300433402], 
reward next is 0.7171, 
noisyNet noise sample is [array([-0.47425324], dtype=float32), -0.019279256]. 
=============================================
[2019-04-02 18:21:42,664] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[60.83589 ]
 [60.870487]
 [60.903255]
 [60.947514]
 [60.98562 ]], R is [[60.91371155]
 [61.02064896]
 [61.12548447]
 [61.22834778]
 [61.32940292]].
[2019-04-02 18:21:48,157] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 7.5671899e-17 0.0000000e+00 1.3926229e-36 1.6508435e-36], sum to 1.0000
[2019-04-02 18:21:48,168] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2284
[2019-04-02 18:21:48,176] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1256861.556410781 W.
[2019-04-02 18:21:48,183] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.73333333333333, 45.83333333333334, 1.0, 2.0, 0.916870204205447, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.140718385839367, 6.9112, 121.9250515497104, 1256861.556410781, 1139328.584105483, 225259.0475070316], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5831400.0000, 
sim time next is 5832000.0000, 
raw observation next is [26.8, 46.0, 1.0, 2.0, 0.2927601234021744, 1.0, 1.0, 0.2927601234021744, 1.0, 1.0, 0.4787639218775569, 6.911200000000001, 6.9112, 121.94756008, 1073249.583544626, 1073249.583544625, 251997.5659444539], 
processed observation next is [1.0, 0.5217391304347826, 0.5481481481481482, 0.46, 1.0, 1.0, 0.1580477659549695, 1.0, 0.5, 0.1580477659549695, 1.0, 0.5, 0.3484549023469461, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.38330342269450923, 0.3833034226945089, 0.48461070373933446], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.27430594], dtype=float32), 0.15480332]. 
=============================================
[2019-04-02 18:21:48,200] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[48.152214]
 [49.104557]
 [49.094547]
 [49.86    ]
 [49.6449  ]], R is [[47.72100449]
 [47.24379349]
 [46.77135468]
 [46.33230972]
 [45.86898804]].
[2019-04-02 18:21:51,061] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 1.2962207e-15 2.5419126e-26 2.8744800e-19 2.8926982e-25], sum to 1.0000
[2019-04-02 18:21:51,072] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6109
[2019-04-02 18:21:51,080] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1124982.624964206 W.
[2019-04-02 18:21:51,087] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.6, 45.5, 1.0, 2.0, 0.3069025131713959, 1.0, 2.0, 0.3069025131713959, 1.0, 2.0, 0.501833593009561, 6.9112, 6.9112, 121.94756008, 1124982.624964206, 1124982.624964206, 257421.4335015976], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5830200.0000, 
sim time next is 5830800.0000, 
raw observation next is [26.66666666666667, 45.66666666666666, 1.0, 2.0, 0.4665675617151369, 0.0, 1.0, 0.0, 1.0, 2.0, 0.77019546613278, 6.911200000000001, 6.9112, 121.9260426156618, 1151438.007229189, 1151438.007229188, 241977.6263095943], 
processed observation next is [1.0, 0.4782608695652174, 0.5432098765432101, 0.45666666666666655, 1.0, 1.0, 0.3649613829942106, 0.0, 0.5, -0.1904761904761905, 1.0, 1.0, 0.7127443326659749, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.4112278597247103, 0.41122785972471, 0.46534158905691214], 
reward next is 0.5347, 
noisyNet noise sample is [array([0.6944598], dtype=float32), 0.6583145]. 
=============================================
[2019-04-02 18:21:51,671] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 6.4785038e-20 0.0000000e+00 8.7366649e-33 1.9702764e-36], sum to 1.0000
[2019-04-02 18:21:51,680] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1881
[2019-04-02 18:21:51,690] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1206112.10307685 W.
[2019-04-02 18:21:51,696] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.91666666666666, 44.66666666666666, 1.0, 2.0, 0.3336901555619701, 1.0, 1.0, 0.3336901555619701, 1.0, 2.0, 0.5400065775200381, 6.9112, 6.9112, 121.94756008, 1206112.10307685, 1206112.10307685, 268595.1574237939], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5843400.0000, 
sim time next is 5844000.0000, 
raw observation next is [27.93333333333333, 44.33333333333334, 1.0, 2.0, 0.9772424511271294, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.47853754538375, 6.9112, 121.9237214414091, 1492533.18538133, 1202011.185025522, 239286.179927308], 
processed observation next is [1.0, 0.6521739130434783, 0.5901234567901233, 0.4433333333333334, 1.0, 1.0, 0.9729076799132492, 0.0, 0.5, -0.1904761904761905, 0.0, 0.5, -0.25, 0.056733754538374995, 0.0, 0.8094467186369491, 0.5330475662076178, 0.4292897089376864, 0.46016573062943844], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.0789354], dtype=float32), 0.6818437]. 
=============================================
[2019-04-02 18:21:51,713] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[48.991108]
 [48.73029 ]
 [47.175735]
 [46.667027]
 [46.256145]], R is [[46.87764359]
 [46.8923378 ]
 [46.90761185]
 [46.89400482]
 [46.86800003]].
[2019-04-02 18:21:52,068] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.000000e+00 6.321649e-35 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-02 18:21:52,077] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8786
[2019-04-02 18:21:52,087] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1327962.983683402 W.
[2019-04-02 18:21:52,092] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.0, 43.0, 1.0, 2.0, 0.3661515261295717, 1.0, 1.0, 0.3661515261295717, 1.0, 1.0, 0.5938114757994011, 6.9112, 6.9112, 121.94756008, 1327962.983683402, 1327962.983683402, 281824.8013087836], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5846400.0000, 
sim time next is 5847000.0000, 
raw observation next is [28.03333333333333, 42.66666666666667, 1.0, 2.0, 0.5186840137097056, 1.0, 2.0, 0.5186840137097056, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260425279548, 1261487.319358694, 1261487.319358694, 244442.1501137524], 
processed observation next is [1.0, 0.6956521739130435, 0.5938271604938271, 0.4266666666666667, 1.0, 1.0, 0.42700477822584, 1.0, 1.0, 0.42700477822584, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.8094621282378527, 0.45053118548524784, 0.45053118548524784, 0.4700810579110623], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1191708], dtype=float32), 0.22479893]. 
=============================================
[2019-04-02 18:21:52,127] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[50.399258]
 [49.54711 ]
 [51.326736]
 [50.96354 ]
 [49.900955]], R is [[49.84175491]
 [49.34333801]
 [48.84990692]
 [48.83281326]
 [48.81272888]].
[2019-04-02 18:21:52,633] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.000000e+00 8.301874e-31 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-02 18:21:52,640] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9719
[2019-04-02 18:21:52,647] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.45, 77.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6776056197793742, 6.9112, 6.9112, 121.9260426156618, 506163.8990373791, 506163.8990373791, 141373.462701205], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5869800.0000, 
sim time next is 5870400.0000, 
raw observation next is [22.26666666666667, 78.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6757517559141747, 6.9112, 6.9112, 121.9260426156618, 504735.3473326365, 504735.3473326365, 141071.1623434738], 
processed observation next is [1.0, 0.9565217391304348, 0.38024691358024704, 0.78, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5946896948927183, 0.0, 0.0, 0.8094621288201359, 0.18026262404737017, 0.18026262404737017, 0.27129069681437273], 
reward next is 0.7287, 
noisyNet noise sample is [array([1.0748212], dtype=float32), -0.52159065]. 
=============================================
[2019-04-02 18:21:55,115] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 2.8194807e-37 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:21:55,126] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9870
[2019-04-02 18:21:55,136] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1212104.544189359 W.
[2019-04-02 18:21:55,145] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.1, 62.0, 1.0, 2.0, 0.5027756001804139, 1.0, 2.0, 0.5027756001804139, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1212104.544189359, 1212104.544189359, 239008.3149385212], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5911200.0000, 
sim time next is 5911800.0000, 
raw observation next is [25.35, 61.16666666666667, 1.0, 2.0, 0.3046565419479407, 1.0, 2.0, 0.3046565419479407, 1.0, 1.0, 0.490844489136731, 6.911200000000001, 6.9112, 121.94756008, 1092444.65656133, 1092444.65656133, 257399.1217643606], 
processed observation next is [1.0, 0.43478260869565216, 0.4944444444444445, 0.6116666666666667, 1.0, 1.0, 0.17221016898564367, 1.0, 1.0, 0.17221016898564367, 1.0, 0.5, 0.36355561142091375, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.3901588059147607, 0.3901588059147607, 0.49499831108530884], 
reward next is 0.5050, 
noisyNet noise sample is [array([1.1697091], dtype=float32), 1.5928771]. 
=============================================
[2019-04-02 18:21:58,503] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 2.5236101e-21 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:21:58,515] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1458
[2019-04-02 18:21:58,523] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.53333333333333, 52.33333333333334, 1.0, 2.0, 0.2286658260089507, 1.0, 2.0, 0.2286658260089507, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 542569.7214814376, 542569.721481438, 165267.3031207815], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5937600.0000, 
sim time next is 5938200.0000, 
raw observation next is [28.4, 53.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7189553836383163, 6.911199999999999, 6.9112, 121.9260426156618, 534113.6050922032, 534113.6050922037, 150506.5855478225], 
processed observation next is [1.0, 0.7391304347826086, 0.6074074074074074, 0.53, 0.0, 0.5, -0.1904761904761905, 0.0, 0.5, -0.1904761904761905, 1.0, 0.5, 0.6486942295478954, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.19075485896150116, 0.19075485896150132, 0.2894357414381202], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1956962], dtype=float32), -2.3936665]. 
=============================================
[2019-04-02 18:22:08,006] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.000000e+00 5.602949e-38 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-02 18:22:08,015] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0943
[2019-04-02 18:22:08,026] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 691595.0173262715 W.
[2019-04-02 18:22:08,031] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.76666666666667, 88.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9305065091579919, 6.9112, 6.9112, 121.9260378790227, 691595.0173262715, 691595.0173262715, 176660.1934626416], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6059400.0000, 
sim time next is 6060000.0000, 
raw observation next is [22.83333333333334, 88.66666666666667, 1.0, 1.0, 0.2705220151803632, 1.0, 1.0, 0.2705220151803632, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 640684.4310272778, 640684.4310272783, 174663.9252182654], 
processed observation next is [1.0, 0.13043478260869565, 0.4012345679012348, 0.8866666666666667, 1.0, 0.5, 0.13157382759567043, 1.0, 0.5, 0.13157382759567043, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2288158682240278, 0.22881586822402797, 0.33589216388127957], 
reward next is 0.6641, 
noisyNet noise sample is [array([0.63930565], dtype=float32), 0.37800783]. 
=============================================
[2019-04-02 18:22:08,045] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[46.703827]
 [46.46895 ]
 [45.94005 ]
 [44.200413]
 [45.06695 ]], R is [[44.72422028]
 [44.93724823]
 [45.15312958]
 [44.70159912]
 [44.25458527]].
[2019-04-02 18:22:09,399] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-02 18:22:09,404] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 18:22:09,404] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:22:09,406] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 18:22:09,408] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:22:09,411] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 18:22:09,413] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 18:22:09,414] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 18:22:09,415] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:22:09,415] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:22:09,416] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:22:09,431] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run16
[2019-04-02 18:22:09,454] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run16
[2019-04-02 18:22:09,455] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run16
[2019-04-02 18:22:09,455] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run16
[2019-04-02 18:22:09,456] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run16
[2019-04-02 18:22:39,707] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.26820746], dtype=float32), -0.10087245]
[2019-04-02 18:22:39,709] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [23.04696978166667, 81.23310643833334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8992048897863657, 6.911199999999999, 6.9112, 121.9260426156618, 670463.1469307904, 670463.1469307909, 171130.7149174568]
[2019-04-02 18:22:39,710] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 18:22:39,714] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.0000000e+00 3.3485993e-37 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.580089829775186
[2019-04-02 18:23:00,483] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.26820746], dtype=float32), -0.10087245]
[2019-04-02 18:23:00,486] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [34.86077758, 38.03214648, 1.0, 2.0, 0.796172175235016, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.9260426156618, 1622571.542335534, 1622571.542335535, 335470.1448388225]
[2019-04-02 18:23:00,486] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 18:23:00,488] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.000000e+00 6.695328e-36 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.40131841434371684
[2019-04-02 18:23:00,489] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1622571.542335534 W.
[2019-04-02 18:23:09,343] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.26820746], dtype=float32), -0.10087245]
[2019-04-02 18:23:09,344] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [26.074683375, 95.07739062, 1.0, 2.0, 0.7213974805982906, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 822202.1324843518, 822202.1324843522, 180237.6044023689]
[2019-04-02 18:23:09,346] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 18:23:09,351] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.0000000e+00 8.0835899e-30 5.1505773e-26 7.5745386e-36 7.5084867e-36], sampled 0.9053038620992893
[2019-04-02 18:23:09,351] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 822202.1324843518 W.
[2019-04-02 18:23:14,608] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.26820746], dtype=float32), -0.10087245]
[2019-04-02 18:23:14,610] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [27.25561299, 84.48844483, 1.0, 2.0, 0.7079832815862528, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000002, 6.9112, 121.9260426156618, 806905.449464638, 806905.4494646372, 177662.640490211]
[2019-04-02 18:23:14,612] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 18:23:14,613] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.0000000e+00 0.0000000e+00 2.4134119e-32 0.0000000e+00 0.0000000e+00], sampled 0.7872756378853122
[2019-04-02 18:23:14,614] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 806905.449464638 W.
[2019-04-02 18:23:22,681] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.26820746], dtype=float32), -0.10087245]
[2019-04-02 18:23:22,682] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [25.0, 78.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8302993770471083, 6.911200000000001, 6.9112, 121.9260426156618, 612873.7339423515, 612873.7339423511, 165599.5727115255]
[2019-04-02 18:23:22,683] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 18:23:22,686] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.000000e+00 2.710525e-31 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.7669115607021584
[2019-04-02 18:23:33,937] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.26820746], dtype=float32), -0.10087245]
[2019-04-02 18:23:33,939] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [27.13107278, 86.00811294, 1.0, 2.0, 0.7055972779472562, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 804184.6375594168, 804184.6375594165, 177208.7745293895]
[2019-04-02 18:23:33,940] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 18:23:33,942] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.000000e+00 0.000000e+00 1.108436e-32 0.000000e+00 0.000000e+00], sampled 0.04623900940354797
[2019-04-02 18:23:33,942] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 804184.6375594168 W.
[2019-04-02 18:23:41,868] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.26820746], dtype=float32), -0.10087245]
[2019-04-02 18:23:41,869] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [28.0, 79.0, 1.0, 2.0, 0.6795877662583345, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 774526.051476842, 774526.051476842, 172317.914087722]
[2019-04-02 18:23:41,870] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 18:23:41,875] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.0000000e+00 6.0773562e-29 1.9517538e-29 2.0616905e-37 1.9203876e-37], sampled 0.26964235961252714
[2019-04-02 18:23:41,876] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 774526.051476842 W.
[2019-04-02 18:23:58,574] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.26820746], dtype=float32), -0.10087245]
[2019-04-02 18:23:58,577] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [25.39914253666667, 67.30172205666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8067702989667495, 6.9112, 6.9112, 121.9260426156618, 600196.953371055, 600196.953371055, 160658.6042999371]
[2019-04-02 18:23:58,579] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 18:23:58,583] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.0000000e+00 8.5300949e-30 1.0605005e-33 0.0000000e+00 0.0000000e+00], sampled 0.17265343702057134
[2019-04-02 18:24:16,663] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8633.8375 2219139301.2698 543.0000
[2019-04-02 18:24:16,899] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8404.3352 2292930052.2689 697.0000
[2019-04-02 18:24:16,903] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8560.3296 2258226393.9236 536.0000
[2019-04-02 18:24:17,155] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7841.5838 2529739775.4178 831.0000
[2019-04-02 18:24:17,225] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8364.2563 2339423669.2034 616.0000
[2019-04-02 18:24:18,242] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 375000, evaluation results [375000.0, 7841.583789482413, 2529739775.4177794, 831.0, 8560.329577213746, 2258226393.9236007, 536.0, 8633.837517256845, 2219139301.2698236, 543.0, 8364.256289480296, 2339423669.203431, 616.0, 8404.335235826124, 2292930052.2689223, 697.0]
[2019-04-02 18:24:23,917] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:24:23,927] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4029
[2019-04-02 18:24:23,931] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.0, 67.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8659156464247921, 6.911200000000001, 6.9112, 121.9260426156618, 637197.4257888222, 637197.4257888218, 170667.085864107], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6210000.0000, 
sim time next is 6210600.0000, 
raw observation next is [26.86666666666667, 67.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.862746362046582, 6.911200000000001, 6.9112, 121.9260426156618, 635332.6529971688, 635332.6529971684, 170134.6075428972], 
processed observation next is [1.0, 0.9130434782608695, 0.5506172839506175, 0.675, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8284329525582274, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2269045189275603, 0.22690451892756014, 0.3271819375824946], 
reward next is 0.6728, 
noisyNet noise sample is [array([0.26314658], dtype=float32), -0.6319758]. 
=============================================
[2019-04-02 18:24:34,154] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 3.0965613e-20 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:24:34,163] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5736
[2019-04-02 18:24:34,170] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.26666666666667, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9151920428312048, 6.9112, 6.9112, 121.9260426156618, 669417.758575858, 669417.758575858, 178048.7443494075], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6315600.0000, 
sim time next is 6316200.0000, 
raw observation next is [24.25, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9147427908011198, 6.911199999999999, 6.9112, 121.9260426156618, 669256.8592061239, 669256.8592061243, 177954.8086184225], 
processed observation next is [0.0, 0.08695652173913043, 0.4537037037037037, 0.88, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8934284885013996, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.23902030685932996, 0.23902030685933012, 0.3422207858046587], 
reward next is 0.6578, 
noisyNet noise sample is [array([1.1995306], dtype=float32), -0.14247906]. 
=============================================
[2019-04-02 18:24:36,481] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 1.3766472e-33 1.4203774e-28 1.2894739e-30 5.5584981e-38], sum to 1.0000
[2019-04-02 18:24:36,490] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0604
[2019-04-02 18:24:36,498] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 715093.4287540189 W.
[2019-04-02 18:24:36,510] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.9, 83.0, 1.0, 2.0, 0.2091548398249409, 1.0, 2.0, 0.2091548398249409, 1.0, 2.0, 0.3329813166988896, 6.9112, 6.9112, 121.94756008, 715093.4287540189, 715093.4287540189, 223608.3704963762], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6337800.0000, 
sim time next is 6338400.0000, 
raw observation next is [26.13333333333333, 82.0, 1.0, 2.0, 0.6319270686225381, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 720181.6460073796, 720181.6460073796, 163655.2824505278], 
processed observation next is [0.0, 0.34782608695652173, 0.5234567901234567, 0.82, 1.0, 1.0, 0.5618179388363549, 0.0, 0.5, -0.1904761904761905, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2572077307169213, 0.2572077307169213, 0.31472169702024577], 
reward next is 0.6853, 
noisyNet noise sample is [array([0.83702445], dtype=float32), 1.3470832]. 
=============================================
[2019-04-02 18:24:36,928] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9064438e-06 1.3641254e-17 9.9999809e-01 6.1221797e-15 1.7270269e-20], sum to 1.0000
[2019-04-02 18:24:36,939] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5206
[2019-04-02 18:24:36,943] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.08333333333333, 58.66666666666667, 1.0, 2.0, 0.3369851007193216, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5364912551836971, 6.9112, 6.9112, 121.9260426156618, 768120.5059864046, 768120.5059864046, 205452.3719816368], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6372600.0000, 
sim time next is 6373200.0000, 
raw observation next is [30.96666666666667, 59.33333333333334, 1.0, 2.0, 0.3310940514879762, 0.0, 2.0, 0.0, 1.0, 2.0, 0.527112512949316, 6.9112, 6.9112, 121.9260426156618, 754685.8962624022, 754685.8962624022, 203790.4835332791], 
processed observation next is [0.0, 0.782608695652174, 0.7024691358024692, 0.5933333333333334, 1.0, 1.0, 0.20368339462854312, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.40889064118664503, 0.0, 0.0, 0.8094621288201359, 0.2695306772365722, 0.2695306772365722, 0.39190477602553675], 
reward next is 0.6081, 
noisyNet noise sample is [array([1.1286135], dtype=float32), 0.019570109]. 
=============================================
[2019-04-02 18:24:56,560] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 2.3472293e-33 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:24:56,575] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6906
[2019-04-02 18:24:56,580] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.86666666666667, 47.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5634844870900063, 6.911199999999999, 6.9112, 121.9260426156618, 413048.4228050101, 413048.4228050105, 124124.1996720578], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6645000.0000, 
sim time next is 6645600.0000, 
raw observation next is [24.9, 46.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5537895993433993, 6.911200000000001, 6.9112, 121.9260426156618, 404908.3077701267, 404908.3077701262, 122825.3564806379], 
processed observation next is [1.0, 0.9565217391304348, 0.47777777777777775, 0.46, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.44223699917924914, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.14461010991790238, 0.1446101099179022, 0.23620260861661135], 
reward next is 0.7638, 
noisyNet noise sample is [array([0.7934522], dtype=float32), 0.08070747]. 
=============================================
[2019-04-02 18:24:59,378] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 5.1383327e-33 0.0000000e+00 2.3049003e-37 0.0000000e+00], sum to 1.0000
[2019-04-02 18:24:59,393] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9972
[2019-04-02 18:24:59,406] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 856859.3873464904 W.
[2019-04-02 18:24:59,414] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.91666666666667, 41.16666666666666, 1.0, 2.0, 0.3391643264359644, 0.0, 2.0, 0.0, 1.0, 1.0, 0.5793200963957849, 6.911200000000001, 6.9112, 121.9260426156232, 856859.3873464904, 856859.3873464899, 200727.0835206835], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6689400.0000, 
sim time next is 6690000.0000, 
raw observation next is [26.13333333333334, 40.33333333333334, 1.0, 2.0, 0.2195684315616875, 1.0, 1.0, 0.2195684315616875, 1.0, 2.0, 0.3700364817849306, 6.911200000000002, 6.9112, 121.94756008, 825613.1484710418, 825613.1484710409, 224110.3728100206], 
processed observation next is [1.0, 0.43478260869565216, 0.523456790123457, 0.40333333333333343, 1.0, 1.0, 0.07091479947819941, 1.0, 0.5, 0.07091479947819941, 1.0, 1.0, 0.21254560223116323, 1.7763568394002506e-16, 0.0, 0.8096049824067558, 0.29486183873965777, 0.29486183873965743, 0.43098148617311655], 
reward next is 0.5690, 
noisyNet noise sample is [array([-0.7242157], dtype=float32), -1.4255402]. 
=============================================
[2019-04-02 18:24:59,429] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[52.426014]
 [52.494133]
 [55.09945 ]
 [57.6965  ]
 [54.867523]], R is [[51.50112534]
 [50.9861145 ]
 [50.47625351]
 [49.97149277]
 [49.47177887]].
[2019-04-02 18:25:24,778] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-02 18:25:24,782] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 18:25:24,783] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 18:25:24,784] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:25:24,784] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:25:24,786] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 18:25:24,788] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 18:25:24,787] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 18:25:24,790] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:25:24,792] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:25:24,791] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:25:24,808] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run17
[2019-04-02 18:25:24,827] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run17
[2019-04-02 18:25:24,849] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run17
[2019-04-02 18:25:24,863] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run17
[2019-04-02 18:25:24,881] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run17
[2019-04-02 18:26:12,512] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.29577136], dtype=float32), -0.025073785]
[2019-04-02 18:26:12,513] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [24.9, 92.66666666666667, 1.0, 2.0, 0.7136778445344939, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 813399.1226279062, 813399.1226279062, 178745.2402023711]
[2019-04-02 18:26:12,514] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 18:26:12,516] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7328812522927598
[2019-04-02 18:26:12,516] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 813399.1226279062 W.
[2019-04-02 18:26:30,546] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.29577136], dtype=float32), -0.025073785]
[2019-04-02 18:26:30,548] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [24.98096934333334, 87.01730928333333, 1.0, 2.0, 0.6065869242726358, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 699232.0502672163, 699232.0502672163, 159601.4637526351]
[2019-04-02 18:26:30,550] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 18:26:30,552] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.0000000e+00 0.0000000e+00 2.7243713e-33 0.0000000e+00 0.0000000e+00], sampled 0.2777731042728371
[2019-04-02 18:26:30,557] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 699232.0502672163 W.
[2019-04-02 18:26:49,551] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.29577136], dtype=float32), -0.025073785]
[2019-04-02 18:26:49,553] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [30.0, 66.66666666666667, 1.0, 2.0, 0.5151014556709399, 0.0, 2.0, 0.0, 1.0, 1.0, 0.8200582931113761, 6.9112, 6.9112, 121.9258690579419, 1174428.27657921, 1174428.27657921, 262284.7430241894]
[2019-04-02 18:26:49,554] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 18:26:49,557] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.3894067570937383
[2019-04-02 18:26:49,561] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1174428.27657921 W.
[2019-04-02 18:27:05,318] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.29577136], dtype=float32), -0.025073785]
[2019-04-02 18:27:05,320] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [28.96480245, 61.52249362666667, 1.0, 2.0, 0.5966525201191997, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 688303.8809681525, 688303.8809681525, 157907.3250345131]
[2019-04-02 18:27:05,321] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 18:27:05,323] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.0000000e+00 2.3765605e-37 3.5191627e-14 1.5730168e-30 2.1048936e-33], sampled 0.8430882564753824
[2019-04-02 18:27:05,325] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 688303.8809681525 W.
[2019-04-02 18:27:18,190] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.29577136], dtype=float32), -0.025073785]
[2019-04-02 18:27:18,191] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [24.2, 92.66666666666667, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9232400415297992, 6.911199999999999, 6.9112, 121.9260426156618, 668698.1545525899, 668698.1545525903, 180225.4424732025]
[2019-04-02 18:27:18,192] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 18:27:18,195] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6447108789832098
[2019-04-02 18:27:25,871] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.29577136], dtype=float32), -0.025073785]
[2019-04-02 18:27:25,872] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [24.630308845, 68.12952418500001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6876480244292111, 6.911200000000001, 6.9112, 121.9260426156618, 513819.6602532045, 513819.6602532041, 143951.0581301525]
[2019-04-02 18:27:25,874] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 18:27:25,877] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.788768762591029
[2019-04-02 18:27:29,423] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.29577136], dtype=float32), -0.025073785]
[2019-04-02 18:27:29,424] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [16.7, 77.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.364442756094236, 6.911199999999999, 6.9112, 121.9260426156618, 260199.2388787222, 260199.2388787227, 87495.65908945882]
[2019-04-02 18:27:29,424] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 18:27:29,426] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.029461392802316078
[2019-04-02 18:27:31,091] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8558.6876 2258253977.7077 536.0000
[2019-04-02 18:27:32,209] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7841.5105 2529777876.2104 831.0000
[2019-04-02 18:27:32,419] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8404.3352 2292930052.2689 697.0000
[2019-04-02 18:27:32,421] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8633.8375 2219139301.2698 543.0000
[2019-04-02 18:27:32,481] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8364.2563 2339423669.2034 616.0000
[2019-04-02 18:27:33,492] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 400000, evaluation results [400000.0, 7841.510518727289, 2529777876.2104435, 831.0, 8558.687622313513, 2258253977.70769, 536.0, 8633.837517256845, 2219139301.2698236, 543.0, 8364.256289480296, 2339423669.203431, 616.0, 8404.335235826124, 2292930052.2689223, 697.0]
[2019-04-02 18:27:37,464] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:27:37,475] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8846
[2019-04-02 18:27:37,483] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.46666666666667, 78.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7467677262612424, 6.911200000000001, 6.9112, 121.9260426156618, 557193.3175647946, 557193.3175647941, 152073.7990861096], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7086000.0000, 
sim time next is 7086600.0000, 
raw observation next is [23.45, 78.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7431920480030259, 6.9112, 6.9112, 121.9260426156618, 554658.3192336523, 554658.3192336523, 151501.8615953042], 
processed observation next is [1.0, 0.0, 0.42407407407407405, 0.78, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6789900600037824, 0.0, 0.0, 0.8094621288201359, 0.19809225686916154, 0.19809225686916154, 0.2913497338371234], 
reward next is 0.7087, 
noisyNet noise sample is [array([0.4269166], dtype=float32), 0.60029364]. 
=============================================
[2019-04-02 18:27:41,404] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:27:41,416] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3275
[2019-04-02 18:27:41,420] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.9, 80.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6162216919571631, 6.911200000000001, 6.9112, 121.9260426156618, 457800.4138376751, 457800.4138376746, 132218.9008820445], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7153200.0000, 
sim time next is 7153800.0000, 
raw observation next is [20.73333333333333, 81.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6166133304607745, 6.911200000000001, 6.9112, 121.9260426156618, 458077.5895931084, 458077.589593108, 132246.2104924227], 
processed observation next is [1.0, 0.8260869565217391, 0.3234567901234567, 0.8133333333333335, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5207666630759681, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.16359913914039587, 0.1635991391403957, 0.2543196355623513], 
reward next is 0.7457, 
noisyNet noise sample is [array([-1.9133174], dtype=float32), -1.8551941]. 
=============================================
[2019-04-02 18:27:44,442] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:27:44,449] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3105
[2019-04-02 18:27:44,458] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.68333333333334, 91.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.62478226087856, 6.9112, 6.9112, 121.9260426156618, 464836.3023063114, 464836.3023063114, 133581.4694076377], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7189800.0000, 
sim time next is 7190400.0000, 
raw observation next is [19.76666666666667, 91.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6127669245450887, 6.911199999999999, 6.9112, 121.9260426156618, 455962.9837958098, 455962.9837958103, 132472.1668789812], 
processed observation next is [1.0, 0.21739130434782608, 0.2876543209876544, 0.91, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5159586556813609, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.16284392278421778, 0.16284392278421797, 0.25475416707496384], 
reward next is 0.7452, 
noisyNet noise sample is [array([-0.25667006], dtype=float32), -0.7275504]. 
=============================================
[2019-04-02 18:27:50,311] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 2.7386047e-33 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:27:50,323] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3478
[2019-04-02 18:27:50,336] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 902103.8340315693 W.
[2019-04-02 18:27:50,343] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.8, 66.16666666666667, 1.0, 2.0, 0.3789467561639906, 0.0, 1.0, 0.0, 1.0, 2.0, 0.609345345991031, 6.911199999999999, 6.9112, 121.9260426156618, 902103.8340315693, 902103.8340315698, 216580.0101365768], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7305000.0000, 
sim time next is 7305600.0000, 
raw observation next is [26.0, 65.33333333333334, 1.0, 2.0, 0.544947773509092, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8752049299315333, 6.911199999999999, 6.9112, 121.9260426156618, 1293990.850858989, 1293990.850858989, 271837.4610945356], 
processed observation next is [1.0, 0.5652173913043478, 0.5185185185185185, 0.6533333333333334, 1.0, 1.0, 0.4582711589393952, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8440061624144165, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.46213958959249607, 0.46213958959249607, 0.5227643482587223], 
reward next is 0.4772, 
noisyNet noise sample is [array([0.87059516], dtype=float32), -1.8866739]. 
=============================================
[2019-04-02 18:28:03,361] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:28:03,372] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6739
[2019-04-02 18:28:03,378] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.66666666666666, 94.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6216507892445996, 6.911200000000001, 6.9112, 121.9260426156618, 463345.9992650067, 463345.9992650063, 134071.9721481413], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7454400.0000, 
sim time next is 7455000.0000, 
raw observation next is [19.73333333333333, 94.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6240254101275818, 6.911200000000001, 6.9112, 121.9260426156618, 465229.4147697355, 465229.4147697351, 134430.8404530893], 
processed observation next is [0.0, 0.2608695652173913, 0.28641975308641965, 0.9416666666666668, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5300317626594773, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.16615336241776268, 0.16615336241776255, 0.25852084702517175], 
reward next is 0.7415, 
noisyNet noise sample is [array([-1.5473526], dtype=float32), -0.6852091]. 
=============================================
[2019-04-02 18:28:03,394] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[76.17333]
 [76.16743]
 [76.1657 ]
 [76.18116]
 [76.20143]], R is [[76.16059113]
 [76.14115906]
 [76.12263489]
 [76.10499573]
 [76.08817291]].
[2019-04-02 18:28:06,832] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 1.5565286e-17 1.0408025e-25 4.4386017e-26 1.9179167e-26], sum to 1.0000
[2019-04-02 18:28:06,844] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2001
[2019-04-02 18:28:06,850] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.0, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7149339809226385, 6.9112, 6.9112, 121.9260426156618, 533813.3516528882, 533813.3516528882, 147913.4566358151], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7542000.0000, 
sim time next is 7542600.0000, 
raw observation next is [21.0, 95.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7153626115979705, 6.911199999999999, 6.9112, 121.9260426156618, 534143.9402890576, 534143.940289058, 147945.6979874428], 
processed observation next is [0.0, 0.30434782608695654, 0.3333333333333333, 0.9583333333333333, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6442032644974632, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1907656929603777, 0.19076569296037788, 0.28451095766815926], 
reward next is 0.7155, 
noisyNet noise sample is [array([0.5505866], dtype=float32), -0.028663447]. 
=============================================
[2019-04-02 18:28:15,397] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:28:15,408] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4584
[2019-04-02 18:28:15,415] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.68333333333334, 91.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6218753236585812, 6.911199999999999, 6.9112, 121.9260426156618, 462804.8397417176, 462804.839741718, 133411.2947694907], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7620600.0000, 
sim time next is 7621200.0000, 
raw observation next is [19.6, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6119087085526782, 6.911199999999999, 6.9112, 121.9260426156618, 455250.130227569, 455250.1302275694, 132325.7759132403], 
processed observation next is [1.0, 0.21739130434782608, 0.28148148148148155, 0.92, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5148858856908477, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1625893322241318, 0.1625893322241319, 0.2544726459870006], 
reward next is 0.7455, 
noisyNet noise sample is [array([0.39461136], dtype=float32), -0.34841308]. 
=============================================
[2019-04-02 18:28:16,933] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 2.5152955e-22 3.3965690e-17 2.9748781e-21 8.7628309e-26], sum to 1.0000
[2019-04-02 18:28:16,947] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2013
[2019-04-02 18:28:16,958] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1163866.292549968 W.
[2019-04-02 18:28:16,965] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.0, 82.0, 1.0, 2.0, 0.4986498923365183, 1.0, 2.0, 0.4986498923365183, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 1163866.292549968, 1163866.292549968, 236215.5033755885], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7639200.0000, 
sim time next is 7639800.0000, 
raw observation next is [24.26666666666667, 81.33333333333334, 1.0, 2.0, 0.535731840496908, 0.0, 1.0, 0.0, 1.0, 1.0, 0.856190848504593, 6.9112, 6.9112, 121.9260426156618, 1253809.268299633, 1253809.268299633, 269063.0584433852], 
processed observation next is [1.0, 0.43478260869565216, 0.4543209876543211, 0.8133333333333335, 1.0, 1.0, 0.4472998101153667, 0.0, 0.5, -0.1904761904761905, 1.0, 0.5, 0.8202385606307413, 0.0, 0.0, 0.8094621288201359, 0.44778902439272605, 0.44778902439272605, 0.5174289585449715], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.39706892], dtype=float32), 0.90350175]. 
=============================================
[2019-04-02 18:28:26,197] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:28:26,198] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:28:26,224] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res10/Eplus-env-sub_run3
[2019-04-02 18:28:33,241] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 1.8330088e-19 2.4715957e-10 1.6929095e-20 4.8162336e-20], sum to 1.0000
[2019-04-02 18:28:33,252] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7693
[2019-04-02 18:28:33,263] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1731714.960537851 W.
[2019-04-02 18:28:33,272] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.1, 49.66666666666667, 1.0, 2.0, 0.7512945003788098, 1.0, 1.0, 0.7512945003788098, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1731714.960537851, 1731714.960537851, 324977.3079907264], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7923000.0000, 
sim time next is 7923600.0000, 
raw observation next is [30.1, 50.0, 1.0, 2.0, 0.7510368473347052, 1.0, 2.0, 0.7510368473347052, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1727124.863583137, 1727124.863583137, 324672.4856609816], 
processed observation next is [1.0, 0.7391304347826086, 0.6703703703703704, 0.5, 1.0, 1.0, 0.7036152944460776, 1.0, 1.0, 0.7036152944460776, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.6168303084225489, 0.6168303084225489, 0.6243701647326569], 
reward next is 0.3756, 
noisyNet noise sample is [array([-1.3519961], dtype=float32), 0.73775893]. 
=============================================
[2019-04-02 18:28:36,002] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:28:36,002] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:28:36,040] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res4/Eplus-env-sub_run3
[2019-04-02 18:28:36,425] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:28:36,426] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:28:36,465] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res5/Eplus-env-sub_run3
[2019-04-02 18:28:37,556] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:28:37,558] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:28:37,580] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res14/Eplus-env-sub_run3
[2019-04-02 18:28:37,979] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:28:37,980] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:28:37,994] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res6/Eplus-env-sub_run3
[2019-04-02 18:28:38,039] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:28:38,039] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:28:38,056] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res11/Eplus-env-sub_run3
[2019-04-02 18:28:38,157] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:28:38,157] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:28:38,174] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res8/Eplus-env-sub_run3
[2019-04-02 18:28:38,241] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:28:38,241] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:28:38,251] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:28:38,252] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:28:38,257] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res2/Eplus-env-sub_run3
[2019-04-02 18:28:38,278] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:28:38,279] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:28:38,285] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:28:38,285] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:28:38,291] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res16/Eplus-env-sub_run3
[2019-04-02 18:28:38,293] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:28:38,294] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:28:38,323] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res9/Eplus-env-sub_run3
[2019-04-02 18:28:38,347] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res13/Eplus-env-sub_run3
[2019-04-02 18:28:38,367] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:28:38,368] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:28:38,369] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:28:38,370] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:28:38,374] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res7/Eplus-env-sub_run3
[2019-04-02 18:28:38,394] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res17/Eplus-env-sub_run3
[2019-04-02 18:28:38,410] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res12/Eplus-env-sub_run3
[2019-04-02 18:28:38,513] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:28:38,513] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:28:38,514] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res15/Eplus-env-sub_run3
[2019-04-02 18:28:38,569] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:28:38,569] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:28:38,570] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res3/Eplus-env-sub_run3
[2019-04-02 18:28:39,589] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-02 18:28:39,591] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 18:28:39,593] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 18:28:39,594] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:28:39,594] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:28:39,597] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 18:28:39,599] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:28:39,600] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run18
[2019-04-02 18:28:39,621] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 18:28:39,622] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:28:39,624] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run18
[2019-04-02 18:28:39,624] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run18
[2019-04-02 18:28:39,624] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 18:28:39,625] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:28:39,675] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run18
[2019-04-02 18:28:39,701] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run18
[2019-04-02 18:29:07,034] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.3318827], dtype=float32), -0.042638265]
[2019-04-02 18:29:07,035] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [23.140875205, 58.11855776500001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5425404667168042, 6.9112, 6.9112, 121.9260426156618, 398000.7283097793, 398000.7283097793, 122468.5648989014]
[2019-04-02 18:29:07,037] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 18:29:07,043] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.0000000e+00 1.7468525e-30 2.0755541e-23 1.0565261e-35 9.2890501e-37], sampled 0.5567038220305622
[2019-04-02 18:29:23,673] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.3318827], dtype=float32), -0.042638265]
[2019-04-02 18:29:23,674] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [29.6, 62.66666666666667, 1.0, 2.0, 0.6375810093268499, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 726628.2668478688, 726628.2668478688, 164662.4210612668]
[2019-04-02 18:29:23,674] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 18:29:23,680] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [9.9549270e-01 3.5352840e-30 4.5073358e-03 1.2069877e-24 8.5731000e-26], sampled 0.7287553396936434
[2019-04-02 18:29:23,680] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 726628.2668478688 W.
[2019-04-02 18:29:59,912] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.3318827], dtype=float32), -0.042638265]
[2019-04-02 18:29:59,912] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [26.532063585, 81.53814561499999, 1.0, 2.0, 0.6326728824670284, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 721032.0195387224, 721032.0195387224, 163789.2612403206]
[2019-04-02 18:29:59,913] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 18:29:59,914] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [9.9999857e-01 1.0946385e-26 1.4323496e-06 8.6514628e-23 4.2149346e-25], sampled 0.8161364844789756
[2019-04-02 18:29:59,915] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 721032.0195387224 W.
[2019-04-02 18:30:35,799] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.3318827], dtype=float32), -0.042638265]
[2019-04-02 18:30:35,800] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [31.0, 51.0, 1.0, 2.0, 0.6945114740890653, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 805670.8069003847, 805670.8069003847, 175801.959376167]
[2019-04-02 18:30:35,800] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 18:30:35,806] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 3.4032187e-08 8.4926266e-30 3.7828556e-29], sampled 0.035907100481986864
[2019-04-02 18:30:35,808] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 805670.8069003847 W.
[2019-04-02 18:30:46,958] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8105.4586 2359902680.9713 592.0000
[2019-04-02 18:30:47,347] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8398.2911 2241087905.1657 513.0000
[2019-04-02 18:30:47,404] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8282.6078 2277098222.4882 525.0000
[2019-04-02 18:30:47,405] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7606.0305 2550889381.1029 821.0000
[2019-04-02 18:30:47,463] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8183.1014 2314014488.6862 675.0000
[2019-04-02 18:30:48,478] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 425000, evaluation results [425000.0, 7606.030531105831, 2550889381.1029468, 821.0, 8282.607848373926, 2277098222.4882445, 525.0, 8398.291106876955, 2241087905.1656666, 513.0, 8105.458610889238, 2359902680.9713254, 592.0, 8183.101386279923, 2314014488.6862307, 675.0]
[2019-04-02 18:30:54,825] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.9999774e-01 1.6636076e-31 2.2724626e-06 1.8657800e-35 0.0000000e+00], sum to 1.0000
[2019-04-02 18:30:54,835] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6221
[2019-04-02 18:30:54,841] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [33.38333333333333, 13.83333333333333, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6794606407733028, 6.9112, 6.9112, 121.9260426156618, 488537.4932761782, 488537.4932761782, 131029.1406225668], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 226200.0000, 
sim time next is 226800.0000, 
raw observation next is [33.4, 14.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6836104542778266, 6.9112, 6.9112, 121.9260426156618, 489741.8559548255, 489741.8559548255, 130808.8557761969], 
processed observation next is [0.0, 0.6521739130434783, 0.7925925925925925, 0.14, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6045130678472832, 0.0, 0.0, 0.8094621288201359, 0.17490780569815198, 0.17490780569815198, 0.25155549187730175], 
reward next is 0.7484, 
noisyNet noise sample is [array([-0.15737139], dtype=float32), -0.16933766]. 
=============================================
[2019-04-02 18:30:55,015] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 1.3592362e-30 2.4399260e-30 4.3650649e-37 0.0000000e+00], sum to 1.0000
[2019-04-02 18:30:55,025] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6019
[2019-04-02 18:30:55,034] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.03333333333333, 74.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.682719627891424, 6.911200000000001, 6.9112, 121.9260426156618, 510096.2735931953, 510096.2735931948, 142255.7111307055], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 92400.0000, 
sim time next is 93000.0000, 
raw observation next is [22.91666666666666, 74.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6801465705868983, 6.911200000000001, 6.9112, 121.9260426156618, 508143.0141626775, 508143.0141626771, 141874.7849393004], 
processed observation next is [1.0, 0.043478260869565216, 0.4043209876543208, 0.745, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6001832132336229, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.18147964791524196, 0.18147964791524182, 0.27283612488327], 
reward next is 0.7272, 
noisyNet noise sample is [array([0.02964387], dtype=float32), 1.0194123]. 
=============================================
[2019-04-02 18:30:55,055] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[76.63349 ]
 [76.92191 ]
 [77.133606]
 [77.31548 ]
 [77.663635]], R is [[76.30575562]
 [76.26913452]
 [76.23258972]
 [76.19598389]
 [76.15901184]].
[2019-04-02 18:30:58,353] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.5564594e-06 1.0694253e-12 9.9999642e-01 5.4449067e-09 4.5765345e-13], sum to 1.0000
[2019-04-02 18:30:58,361] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1194
[2019-04-02 18:30:58,365] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [37.31666666666666, 9.5, 1.0, 2.0, 0.7036781341054795, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9606250290769011, 6.911199999999997, 6.9112, 121.9260426156618, 1597373.038440664, 1597373.038440666, 300220.0084115462], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 143400.0000, 
sim time next is 144000.0000, 
raw observation next is [37.3, 9.0, 1.0, 2.0, 0.7212625479510076, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9617800360497117, 6.911199999999999, 6.9112, 121.9260426156618, 1621215.499906843, 1621215.499906843, 303101.7515966049], 
processed observation next is [1.0, 0.6956521739130435, 0.9370370370370369, 0.09, 1.0, 1.0, 0.6681696999416757, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.9522250450621397, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.5790055356810154, 0.5790055356810154, 0.5828879838396248], 
reward next is 0.4171, 
noisyNet noise sample is [array([1.0369194], dtype=float32), -2.4640353]. 
=============================================
[2019-04-02 18:30:58,386] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[52.31944 ]
 [51.60912 ]
 [52.295357]
 [51.74886 ]
 [52.40222 ]], R is [[52.82620239]
 [52.7205925 ]
 [52.61405945]
 [52.52633667]
 [52.42424774]].
[2019-04-02 18:31:21,843] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 9.6371802e-38 0.0000000e+00 0.0000000e+00 1.6684054e-36], sum to 1.0000
[2019-04-02 18:31:21,854] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7767
[2019-04-02 18:31:21,863] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1268630.396492033 W.
[2019-04-02 18:31:21,870] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 37.0, 1.0, 2.0, 0.5203826438631972, 1.0, 2.0, 0.5203826438631972, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156438, 1268630.396492033, 1268630.396492034, 245082.8999205373], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 472800.0000, 
sim time next is 473400.0000, 
raw observation next is [29.5, 36.5, 1.0, 2.0, 0.9708321519072312, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.480331887266551, 6.9112, 121.9236722799268, 1493785.007025811, 1202344.278255809, 237973.4950881603], 
processed observation next is [1.0, 0.4782608695652174, 0.6481481481481481, 0.365, 1.0, 1.0, 0.9652763713181324, 0.0, 0.5, -0.1904761904761905, 0.0, 1.0, -0.25, 0.05691318872665514, 0.0, 0.809446392255831, 0.533494645366361, 0.4294086708056461, 0.45764133670800056], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.9305865], dtype=float32), -1.4295616]. 
=============================================
[2019-04-02 18:31:25,789] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 2.5743447e-37 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:31:25,801] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8239
[2019-04-02 18:31:25,808] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1415281.662913948 W.
[2019-04-02 18:31:25,813] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.06666666666667, 37.66666666666667, 1.0, 2.0, 0.5825132177548267, 0.0, 2.0, 0.0, 1.0, 2.0, 0.947916246083045, 6.911199999999999, 6.9112, 121.9260426156618, 1415281.662913948, 1415281.662913949, 284464.7321240976], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 566400.0000, 
sim time next is 567000.0000, 
raw observation next is [30.2, 37.0, 1.0, 2.0, 0.621717748794744, 1.0, 1.0, 0.621717748794744, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1500167.864333583, 1500167.864333583, 279150.6105410121], 
processed observation next is [1.0, 0.5652173913043478, 0.674074074074074, 0.37, 1.0, 1.0, 0.5496639866604095, 1.0, 0.5, 0.5496639866604095, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288201359, 0.5357742372619939, 0.5357742372619939, 0.536828097194254], 
reward next is 0.4632, 
noisyNet noise sample is [array([0.8340689], dtype=float32), 0.28326228]. 
=============================================
[2019-04-02 18:31:25,833] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[54.863655]
 [55.551605]
 [55.47185 ]
 [55.6149  ]
 [56.10809 ]], R is [[53.25556564]
 [53.17596436]
 [53.17889786]
 [52.64710999]
 [52.1206398 ]].
[2019-04-02 18:31:35,254] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:31:35,266] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6812
[2019-04-02 18:31:35,272] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.43333333333333, 46.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6871221776898827, 6.911200000000001, 6.9112, 121.9260426156618, 513477.1250751883, 513477.1250751878, 143360.8966228704], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 816000.0000, 
sim time next is 816600.0000, 
raw observation next is [28.71666666666667, 45.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6927365255464026, 6.911199999999999, 6.9112, 121.9260426156618, 517663.200393158, 517663.2003931585, 144245.5409290745], 
processed observation next is [0.0, 0.43478260869565216, 0.6191358024691359, 0.45666666666666655, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6159206569330032, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.18487971442612786, 0.18487971442612802, 0.27739527101745093], 
reward next is 0.7226, 
noisyNet noise sample is [array([3.392355], dtype=float32), -1.7672714]. 
=============================================
[2019-04-02 18:31:43,343] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:31:43,355] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2333
[2019-04-02 18:31:43,365] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.46666666666667, 47.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5249928578585215, 6.9112, 6.9112, 121.9260426156618, 382578.1213842431, 382578.1213842431, 119861.4001455968], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 782400.0000, 
sim time next is 783000.0000, 
raw observation next is [24.3, 48.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.525501766982375, 6.9112, 6.9112, 121.9260426156618, 382784.5362341505, 382784.5362341505, 119835.0332965287], 
processed observation next is [0.0, 0.043478260869565216, 0.4555555555555556, 0.48, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.4068772087279688, 0.0, 0.0, 0.8094621288201359, 0.13670876294076806, 0.13670876294076806, 0.23045198710870904], 
reward next is 0.7695, 
noisyNet noise sample is [array([-0.8514408], dtype=float32), -1.0238887]. 
=============================================
[2019-04-02 18:31:43,382] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[74.98382 ]
 [74.80169 ]
 [74.640785]
 [74.46434 ]
 [74.23178 ]], R is [[75.20636749]
 [75.22380829]
 [75.24140167]
 [75.25869751]
 [75.27574921]].
[2019-04-02 18:31:53,407] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:31:53,416] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0180
[2019-04-02 18:31:53,421] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.63333333333334, 50.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7091166391763422, 6.911199999999999, 6.9112, 121.9260426156618, 529888.6139624728, 529888.6139624732, 146197.3740204464], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 912000.0000, 
sim time next is 912600.0000, 
raw observation next is [27.85, 50.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7113942471390261, 6.9112, 6.9112, 121.9260426156618, 531559.5881168756, 531559.5881168756, 146613.0312858199], 
processed observation next is [0.0, 0.5652173913043478, 0.5870370370370371, 0.5, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6392428089237825, 0.0, 0.0, 0.8094621288201359, 0.1898427100417413, 0.1898427100417413, 0.28194813708811517], 
reward next is 0.7181, 
noisyNet noise sample is [array([-0.11010461], dtype=float32), -1.1374289]. 
=============================================
[2019-04-02 18:31:53,424] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:31:53,437] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5749
[2019-04-02 18:31:53,442] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.85, 50.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7113942471390261, 6.9112, 6.9112, 121.9260426156618, 531559.5881168756, 531559.5881168756, 146613.0312858199], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 912600.0000, 
sim time next is 913200.0000, 
raw observation next is [28.06666666666667, 49.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7142341576748145, 6.9112, 6.9112, 121.9260426156618, 533640.9973170536, 533640.9973170536, 147087.0481403944], 
processed observation next is [0.0, 0.5652173913043478, 0.5950617283950619, 0.4933333333333334, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6427926970935179, 0.0, 0.0, 0.8094621288201359, 0.1905860704703763, 0.1905860704703763, 0.2828597079622969], 
reward next is 0.7171, 
noisyNet noise sample is [array([-0.30244002], dtype=float32), 1.9782561]. 
=============================================
[2019-04-02 18:31:55,008] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-02 18:31:55,011] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 18:31:55,013] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 18:31:55,014] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:31:55,015] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:31:55,016] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 18:31:55,017] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:31:55,018] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 18:31:55,019] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 18:31:55,021] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:31:55,023] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:31:55,043] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run19
[2019-04-02 18:31:55,043] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run19
[2019-04-02 18:31:55,081] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run19
[2019-04-02 18:31:55,083] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run19
[2019-04-02 18:31:55,115] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run19
[2019-04-02 18:32:05,734] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.34011546], dtype=float32), -0.026514078]
[2019-04-02 18:32:05,737] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [16.0, 73.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4144725412164038, 6.9112, 6.9112, 121.9260426156618, 295925.6267698276, 295925.6267698276, 87347.87511508555]
[2019-04-02 18:32:05,738] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 18:32:05,743] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8940615934088335
[2019-04-02 18:32:16,008] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.34011546], dtype=float32), -0.026514078]
[2019-04-02 18:32:16,011] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [25.63333333333333, 59.0, 1.0, 2.0, 0.8955261269726971, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.932903453717838, 6.9112, 121.9259655174355, 1111882.400244323, 1100768.299135959, 220000.4435746497]
[2019-04-02 18:32:16,013] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 18:32:16,017] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.00000e+00 0.00000e+00 6.34664e-33 0.00000e+00 0.00000e+00], sampled 0.37615044410879406
[2019-04-02 18:32:16,019] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1111882.400244323 W.
[2019-04-02 18:32:44,912] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.34011546], dtype=float32), -0.026514078]
[2019-04-02 18:32:44,913] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [25.33333333333333, 89.33333333333334, 1.0, 2.0, 0.9610425362289566, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9259628362765, 1095529.282944568, 1095529.282944568, 231453.8038634853]
[2019-04-02 18:32:44,913] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 18:32:44,916] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.4535147470952756
[2019-04-02 18:32:44,916] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1095529.282944568 W.
[2019-04-02 18:32:52,739] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.34011546], dtype=float32), -0.026514078]
[2019-04-02 18:32:52,742] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [22.8, 89.66666666666666, 1.0, 2.0, 0.9599202670276326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.178964960771002, 6.9112, 121.9249271941546, 1283543.488857098, 1146425.150799068, 233827.3201296578]
[2019-04-02 18:32:52,745] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 18:32:52,748] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 4.2521666e-17 9.1491333e-34 1.0383961e-31], sampled 0.014472928179731781
[2019-04-02 18:32:52,750] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1283543.488857098 W.
[2019-04-02 18:33:27,859] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.34011546], dtype=float32), -0.026514078]
[2019-04-02 18:33:27,861] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [27.84239432, 63.69115088, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8357473894502152, 6.911200000000001, 6.9112, 121.9260426156618, 615405.8794664236, 615405.8794664232, 166712.2151528047]
[2019-04-02 18:33:27,861] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 18:33:27,865] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.34319753419996
[2019-04-02 18:33:58,916] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.34011546], dtype=float32), -0.026514078]
[2019-04-02 18:33:58,918] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [23.66666666666666, 40.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.490466915378084, 6.911200000000001, 6.9112, 121.9260426156618, 350196.5730943056, 350196.5730943052, 106194.5629459149]
[2019-04-02 18:33:58,919] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 18:33:58,921] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6904137435728164
[2019-04-02 18:34:02,517] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8558.9191 2258294760.5814 536.0000
[2019-04-02 18:34:02,546] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8358.3737 2339698740.4028 614.0000
[2019-04-02 18:34:02,612] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7832.0696 2530493246.6316 831.0000
[2019-04-02 18:34:02,751] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8390.6765 2293868587.1320 697.0000
[2019-04-02 18:34:02,778] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8624.2733 2219950513.5584 543.0000
[2019-04-02 18:34:03,791] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 450000, evaluation results [450000.0, 7832.069638770218, 2530493246.63165, 831.0, 8558.919093972647, 2258294760.581367, 536.0, 8624.273266976646, 2219950513.5584273, 543.0, 8358.373678427257, 2339698740.402799, 614.0, 8390.676491570413, 2293868587.132033, 697.0]
[2019-04-02 18:34:04,634] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.000000e+00 4.855504e-29 4.913635e-38 0.000000e+00 6.032450e-38], sum to 1.0000
[2019-04-02 18:34:04,652] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9351
[2019-04-02 18:34:04,660] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.88333333333334, 54.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4858177451706513, 6.9112, 6.9112, 121.9260426156618, 346978.9297327601, 346978.9297327601, 113816.4873830939], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 954600.0000, 
sim time next is 955200.0000, 
raw observation next is [21.76666666666667, 54.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4839306013952295, 6.9112, 6.9112, 121.9260426156618, 345536.5261901045, 345536.5261901045, 112951.2891824253], 
processed observation next is [1.0, 0.043478260869565216, 0.3617283950617285, 0.5466666666666667, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.3549132517440368, 0.0, 0.0, 0.8094621288201359, 0.12340590221075161, 0.12340590221075161, 0.2172140176585102], 
reward next is 0.7828, 
noisyNet noise sample is [array([1.5575665], dtype=float32), 0.31697518]. 
=============================================
[2019-04-02 18:34:04,864] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 2.7506865e-26 6.3522319e-34 0.0000000e+00 5.4004675e-37], sum to 1.0000
[2019-04-02 18:34:04,866] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4077
[2019-04-02 18:34:04,879] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.4, 53.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4960929236285073, 6.911199999999999, 6.9112, 121.9260426156618, 356135.8522003017, 356135.8522003021, 115478.1398931841], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 952200.0000, 
sim time next is 952800.0000, 
raw observation next is [22.26666666666667, 53.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4940844123275477, 6.9112, 6.9112, 121.9260426156618, 354216.057343427, 354216.057343427, 115159.2178765103], 
processed observation next is [1.0, 0.0, 0.38024691358024704, 0.5333333333333334, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.3676055154094346, 0.0, 0.0, 0.8094621288201359, 0.12650573476550964, 0.12650573476550964, 0.22146003437790443], 
reward next is 0.7785, 
noisyNet noise sample is [array([-1.8699596], dtype=float32), -0.06530302]. 
=============================================
[2019-04-02 18:34:14,447] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:34:14,454] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4945
[2019-04-02 18:34:14,461] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.26666666666667, 67.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5459851468133904, 6.9112, 6.9112, 121.9260426156618, 399236.7643035965, 399236.7643035965, 122178.4296125197], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1388400.0000, 
sim time next is 1389000.0000, 
raw observation next is [21.18333333333334, 67.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5423125007792439, 6.9112, 6.9112, 121.9260426156618, 396080.9093634714, 396080.9093634714, 121664.401962877], 
processed observation next is [0.0, 0.043478260869565216, 0.34012345679012373, 0.67, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.4278906259740548, 0.0, 0.0, 0.8094621288201359, 0.1414574676298112, 0.1414574676298112, 0.23397000377476346], 
reward next is 0.7660, 
noisyNet noise sample is [array([0.24791552], dtype=float32), 0.56534433]. 
=============================================
[2019-04-02 18:34:14,476] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[70.3743  ]
 [70.45888 ]
 [70.54083 ]
 [70.690155]
 [70.8624  ]], R is [[70.24789429]
 [70.31046295]
 [70.37151337]
 [70.43107605]
 [70.48914337]].
[2019-04-02 18:34:18,841] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 2.5354634e-33 3.7572984e-37 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:34:18,850] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6073
[2019-04-02 18:34:18,855] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.35, 27.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6301431864007885, 6.911200000000001, 6.9112, 121.9260426156618, 467163.2539017937, 467163.2539017932, 132880.3584337604], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1452600.0000, 
sim time next is 1453200.0000, 
raw observation next is [31.2, 27.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6284086101994706, 6.9112, 6.9112, 121.9260426156618, 465587.7458674674, 465587.7458674674, 132526.8146057346], 
processed observation next is [0.0, 0.8260869565217391, 0.7111111111111111, 0.2766666666666666, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5355107627493382, 0.0, 0.0, 0.8094621288201359, 0.1662813378098098, 0.1662813378098098, 0.2548592588571819], 
reward next is 0.7451, 
noisyNet noise sample is [array([1.5915705], dtype=float32), 1.3352737]. 
=============================================
[2019-04-02 18:34:22,151] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 3.5817663e-30 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:34:22,157] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8340
[2019-04-02 18:34:22,165] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.66666666666667, 86.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.595461989061399, 6.911200000000001, 6.9112, 121.9260426156618, 440654.1275842121, 440654.1275842116, 129091.6800928548], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1198200.0000, 
sim time next is 1198800.0000, 
raw observation next is [19.6, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5949785050898091, 6.911199999999999, 6.9112, 121.9260426156618, 440360.8727622697, 440360.8727622702, 129086.0431246647], 
processed observation next is [1.0, 0.9130434782608695, 0.28148148148148155, 0.87, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.4937231313622613, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.15727174027223917, 0.15727174027223936, 0.2482423906243552], 
reward next is 0.7518, 
noisyNet noise sample is [array([0.23728444], dtype=float32), 1.4695412]. 
=============================================
[2019-04-02 18:34:25,815] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 6.1716557e-34 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:34:25,823] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4820
[2019-04-02 18:34:25,834] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1234954.406490259 W.
[2019-04-02 18:34:25,841] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.81666666666667, 58.5, 1.0, 2.0, 0.9291009175660048, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.109316317278267, 6.9112, 121.9252009343169, 1234954.406490259, 1133501.844961677, 227492.553386808], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1259400.0000, 
sim time next is 1260000.0000, 
raw observation next is [26.0, 58.0, 1.0, 2.0, 0.493650279853756, 1.0, 1.0, 0.493650279853756, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9259233135751, 1190388.244664743, 1190388.244664744, 236148.0616133112], 
processed observation next is [1.0, 0.6086956521739131, 0.5185185185185185, 0.58, 1.0, 1.0, 0.39720271411161423, 1.0, 0.5, 0.39720271411161423, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094613367783456, 0.42513865880883683, 0.42513865880883717, 0.45413088771790616], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.32939693], dtype=float32), 0.3502792]. 
=============================================
[2019-04-02 18:34:25,856] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[51.931282]
 [53.278645]
 [54.900658]
 [55.95915 ]
 [58.60901 ]], R is [[52.19124222]
 [51.6693306 ]
 [51.65670013]
 [51.1401329 ]
 [51.2729187 ]].
[2019-04-02 18:34:35,099] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:34:35,109] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3132
[2019-04-02 18:34:35,114] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.75, 46.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6062472885624863, 6.911200000000001, 6.9112, 121.9260426156618, 450914.5900464834, 450914.590046483, 131677.7710123555], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1367400.0000, 
sim time next is 1368000.0000, 
raw observation next is [26.5, 48.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6108727730928725, 6.911200000000001, 6.9112, 121.9260426156618, 454464.1091072896, 454464.1091072892, 132212.9807632729], 
processed observation next is [1.0, 0.8695652173913043, 0.5370370370370371, 0.48, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5135909663660906, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.16230861039546057, 0.1623086103954604, 0.25425573223706327], 
reward next is 0.7457, 
noisyNet noise sample is [array([1.8034278], dtype=float32), 0.58573073]. 
=============================================
[2019-04-02 18:34:35,130] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[74.462585]
 [74.68462 ]
 [74.90278 ]
 [74.91811 ]
 [74.96613 ]], R is [[74.10357666]
 [74.10931396]
 [74.11592102]
 [74.12306976]
 [74.1306839 ]].
[2019-04-02 18:34:35,251] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 1.4865098e-30 4.4196920e-35 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:34:35,260] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2608
[2019-04-02 18:34:35,273] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.53333333333333, 66.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6835105469113889, 6.911200000000001, 6.9112, 121.9260426156618, 509926.3337581342, 509926.3337581337, 140884.5002189188], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1550400.0000, 
sim time next is 1551000.0000, 
raw observation next is [23.51666666666667, 65.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6746138268299956, 6.9112, 6.9112, 121.9260426156618, 502972.4453399439, 502972.4453399439, 139573.8064283091], 
processed observation next is [0.0, 0.9565217391304348, 0.4265432098765433, 0.6583333333333334, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5932672835374945, 0.0, 0.0, 0.8094621288201359, 0.1796330161928371, 0.1796330161928371, 0.2684111662082867], 
reward next is 0.7316, 
noisyNet noise sample is [array([0.782602], dtype=float32), 0.13976187]. 
=============================================
[2019-04-02 18:34:35,300] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[77.80982 ]
 [78.03972 ]
 [78.29231 ]
 [78.582306]
 [78.879265]], R is [[77.55781555]
 [77.51130676]
 [77.46294403]
 [77.41297913]
 [77.36217499]].
[2019-04-02 18:34:36,211] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 3.0639266e-23 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:34:36,221] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3880
[2019-04-02 18:34:36,227] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.51666666666667, 67.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5550502687804925, 6.911200000000001, 6.9112, 121.9260426156618, 407230.3023363766, 407230.3023363761, 123565.0433138931], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1386600.0000, 
sim time next is 1387200.0000, 
raw observation next is [21.43333333333334, 67.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5519638552735305, 6.911199999999999, 6.9112, 121.9260426156618, 404527.5561542539, 404527.5561542544, 123096.3801649519], 
processed observation next is [0.0, 0.043478260869565216, 0.349382716049383, 0.67, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.43995481909191314, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.14447412719794783, 0.144474127197948, 0.2367238080095229], 
reward next is 0.7633, 
noisyNet noise sample is [array([-1.3952018], dtype=float32), 0.121946335]. 
=============================================
[2019-04-02 18:34:44,432] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:34:44,445] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4419
[2019-04-02 18:34:44,450] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [34.61666666666667, 28.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7187154408898763, 6.911200000000001, 6.9112, 121.9260426156618, 536415.7118013024, 536415.7118013019, 148647.7793711236], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1523400.0000, 
sim time next is 1524000.0000, 
raw observation next is [34.43333333333334, 29.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7218251704469459, 6.911200000000001, 6.9112, 121.9260426156618, 538243.576751917, 538243.5767519166, 149524.3465176099], 
processed observation next is [0.0, 0.6521739130434783, 0.8308641975308644, 0.2966666666666667, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6522814630586823, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.19222984883997035, 0.19222984883997019, 0.2875468202261729], 
reward next is 0.7125, 
noisyNet noise sample is [array([-0.8997155], dtype=float32), 0.8637354]. 
=============================================
[2019-04-02 18:34:44,461] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[91.81489]
 [91.75795]
 [91.64073]
 [91.48463]
 [91.33162]], R is [[91.72634888]
 [91.52323151]
 [91.32949829]
 [91.14076233]
 [90.95454407]].
[2019-04-02 18:34:52,856] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 5.7028177e-20 0.0000000e+00 9.5736855e-37], sum to 1.0000
[2019-04-02 18:34:52,865] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5179
[2019-04-02 18:34:52,880] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1255979.379595111 W.
[2019-04-02 18:34:52,888] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [24.76666666666667, 67.83333333333334, 1.0, 2.0, 0.5242453944180568, 0.0, 1.0, 0.0, 1.0, 2.0, 0.8455489848279989, 6.9112, 6.9112, 121.9260426156618, 1255979.379595111, 1255979.379595111, 263907.9556364882], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1768200.0000, 
sim time next is 1768800.0000, 
raw observation next is [24.83333333333334, 67.66666666666667, 1.0, 2.0, 0.5106689309723214, 1.0, 1.0, 0.5106689309723214, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 1219336.434248747, 1219336.434248747, 241093.5453299254], 
processed observation next is [1.0, 0.4782608695652174, 0.47530864197530887, 0.6766666666666667, 1.0, 1.0, 0.4174630130622873, 1.0, 0.5, 0.4174630130622873, 0.0, 0.5, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.4354772979459811, 0.4354772979459811, 0.4636414333267796], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.77098125], dtype=float32), 0.48300958]. 
=============================================
[2019-04-02 18:34:54,732] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:34:54,745] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0564
[2019-04-02 18:34:54,756] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [18.11666666666667, 88.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5672524110698067, 6.9112, 6.9112, 121.9260426156618, 412680.0997343975, 412680.0997343975, 123115.5510352997], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1659000.0000, 
sim time next is 1659600.0000, 
raw observation next is [17.9, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5460933400450861, 6.9112, 6.9112, 121.9260426156618, 396984.9496668494, 396984.9496668494, 121215.906961715], 
processed observation next is [1.0, 0.21739130434782608, 0.21851851851851847, 0.9, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.43261667505635765, 0.0, 0.0, 0.8094621288201359, 0.14178033916673194, 0.14178033916673194, 0.23310751338791344], 
reward next is 0.7669, 
noisyNet noise sample is [array([-0.62112176], dtype=float32), 1.1203884]. 
=============================================
[2019-04-02 18:34:55,596] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 3.6084467e-34 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:34:55,603] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6813
[2019-04-02 18:34:55,613] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [18.86666666666667, 84.66666666666667, 1.0, 1.0, 0.264704599422334, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4535803059396417, 6.911199999999999, 6.9112, 121.9260426156618, 669759.7690642577, 669759.7690642581, 180921.959441829], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1672800.0000, 
sim time next is 1673400.0000, 
raw observation next is [18.93333333333333, 84.33333333333333, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.903646104510857, 6.911200000000001, 6.9112, 121.9260426156618, 663846.2155933793, 663846.2155933789, 158766.1998308634], 
processed observation next is [1.0, 0.34782608695652173, 0.25679012345679, 0.8433333333333333, 0.0, 0.5, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8795576306385713, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.23708793414049262, 0.23708793414049245, 0.3053196150593527], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.39361504], dtype=float32), -0.65019536]. 
=============================================
[2019-04-02 18:34:58,880] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 3.3153758e-21 4.6169987e-08 5.6622242e-25 1.6590297e-23], sum to 1.0000
[2019-04-02 18:34:58,889] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9857
[2019-04-02 18:34:58,899] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 924347.4126521314 W.
[2019-04-02 18:34:58,911] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.16666666666666, 72.66666666666667, 1.0, 2.0, 0.3798612387008564, 0.0, 1.0, 0.0, 1.0, 2.0, 0.6190316256290933, 6.911200000000001, 6.9112, 121.926042511536, 924347.4126521314, 924347.4126521309, 215717.2300797787], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1759200.0000, 
sim time next is 1759800.0000, 
raw observation next is [23.28333333333333, 72.33333333333333, 1.0, 2.0, 0.7573719042357896, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156301, 930107.5332750396, 930107.5332750396, 190091.7962726398], 
processed observation next is [1.0, 0.34782608695652173, 0.4179012345679012, 0.7233333333333333, 1.0, 1.0, 0.7111570288521305, 0.0, 1.0, -0.1904761904761905, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288199254, 0.3321812618839427, 0.3321812618839427, 0.3655611466781535], 
reward next is 0.6344, 
noisyNet noise sample is [array([0.6490926], dtype=float32), -1.2461187]. 
=============================================
[2019-04-02 18:35:05,522] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 9.2188228e-25 3.6143457e-25 9.8338639e-29 4.6771585e-29], sum to 1.0000
[2019-04-02 18:35:05,524] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9612
[2019-04-02 18:35:05,533] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [18.3, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5347929904750881, 6.911199999999999, 6.9112, 121.9260426156618, 388187.6273831076, 388187.6273831081, 120052.4290250093], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1800000.0000, 
sim time next is 1800600.0000, 
raw observation next is [18.31666666666667, 85.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5302457336329501, 6.911200000000001, 6.9112, 121.9260426156618, 384326.6800464141, 384326.6800464137, 119464.2318889498], 
processed observation next is [1.0, 0.8695652173913043, 0.23395061728395075, 0.8516666666666667, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.41280716704118764, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.13725952858800503, 0.1372595285880049, 0.22973890747874962], 
reward next is 0.7703, 
noisyNet noise sample is [array([0.7728037], dtype=float32), -0.479548]. 
=============================================
[2019-04-02 18:35:10,257] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-02 18:35:10,259] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 18:35:10,260] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:35:10,262] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 18:35:10,263] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:35:10,263] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 18:35:10,264] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 18:35:10,264] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:35:10,265] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 18:35:10,266] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:35:10,267] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:35:10,290] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run20
[2019-04-02 18:35:10,291] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run20
[2019-04-02 18:35:10,309] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run20
[2019-04-02 18:35:10,330] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run20
[2019-04-02 18:35:10,374] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run20
[2019-04-02 18:35:25,195] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.4154722], dtype=float32), -0.046166975]
[2019-04-02 18:35:25,196] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [20.0, 40.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4182240497522264, 6.9112, 6.9112, 121.9260426156618, 298604.654961104, 298604.654961104, 84767.40078264881]
[2019-04-02 18:35:25,198] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 18:35:25,202] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5229053156275242
[2019-04-02 18:35:50,907] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.4154722], dtype=float32), -0.046166975]
[2019-04-02 18:35:50,909] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [27.0, 62.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7951307988676213, 6.911199999999999, 6.9112, 121.9260426156618, 590380.055153895, 590380.0551538955, 159828.7261784995]
[2019-04-02 18:35:50,911] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 18:35:50,914] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.0000000e+00 1.1340926e-35 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.8154284894409767
[2019-04-02 18:35:56,528] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.4154722], dtype=float32), -0.046166975]
[2019-04-02 18:35:56,529] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [25.8, 67.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7708876043624371, 6.911200000000001, 6.9112, 121.9260426156618, 573857.9072322558, 573857.9072322554, 156028.824330549]
[2019-04-02 18:35:56,531] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 18:35:56,537] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5225265983247809
[2019-04-02 18:36:00,247] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.4154722], dtype=float32), -0.046166975]
[2019-04-02 18:36:00,248] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [26.75190035, 71.67687172999999, 1.0, 2.0, 1.02, 1.0, 2.0, 1.02, 1.0, 2.0, 0.9977734948820727, 150.7358650035034, 6.9112, 121.94756008, 76667685.26394945, 3003603.962646901, 512363.425667698]
[2019-04-02 18:36:00,250] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 18:36:00,253] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.8721843e-14], sampled 0.19768930004805985
[2019-04-02 18:36:00,254] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 0, 1] for the demand 76667685.26394945 W.
[2019-04-02 18:36:15,561] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.4154722], dtype=float32), -0.046166975]
[2019-04-02 18:36:15,561] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [31.46666666666667, 61.33333333333334, 1.0, 2.0, 1.02, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9977734948820727, 7.400693253697963, 6.9112, 121.9240683748533, 2129000.387294403, 1878340.149776224, 382379.778839099]
[2019-04-02 18:36:15,562] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 18:36:15,566] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.0000000e+00 2.4498327e-21 4.3886780e-35 3.9406073e-36 6.1129581e-35], sampled 0.9471537326610108
[2019-04-02 18:36:15,566] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2129000.387294403 W.
[2019-04-02 18:37:02,300] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.4154722], dtype=float32), -0.046166975]
[2019-04-02 18:37:02,302] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [27.3, 72.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.944632508204542, 6.911200000000001, 6.9112, 121.9260426156618, 682798.1649799709, 682798.1649799705, 183349.118570693]
[2019-04-02 18:37:02,304] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 18:37:02,309] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.000000e+00 0.000000e+00 3.452629e-37 0.000000e+00 0.000000e+00], sampled 0.7845239775490412
[2019-04-02 18:37:16,131] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.4154722], dtype=float32), -0.046166975]
[2019-04-02 18:37:16,132] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [27.3, 43.66666666666666, 1.0, 2.0, 0.8224885064813907, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1028097.784979238, 1028097.784979238, 204210.6210079155]
[2019-04-02 18:37:16,132] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 18:37:16,135] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.0000000e+00 1.4513535e-32 2.3720936e-12 5.6133931e-29 3.4070113e-27], sampled 0.4501297882422629
[2019-04-02 18:37:16,138] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1028097.784979238 W.
[2019-04-02 18:37:17,853] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8401.2956 2293189929.7756 697.0000
[2019-04-02 18:37:18,082] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8560.3296 2258226393.9236 536.0000
[2019-04-02 18:37:18,163] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7832.1551 2530178779.0627 831.0000
[2019-04-02 18:37:18,164] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8362.2985 2339615502.5310 613.0000
[2019-04-02 18:37:18,164] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8628.7390 2219313595.9621 543.0000
[2019-04-02 18:37:19,186] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 475000, evaluation results [475000.0, 7832.155124922041, 2530178779.062738, 831.0, 8560.329577213746, 2258226393.9236007, 536.0, 8628.739041280016, 2219313595.9620934, 543.0, 8362.2985105015, 2339615502.53096, 613.0, 8401.295568462769, 2293189929.7756047, 697.0]
[2019-04-02 18:37:21,961] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:37:21,968] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2582
[2019-04-02 18:37:21,975] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.7, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6326355633832683, 6.911199999999999, 6.9112, 121.9260426156618, 470915.3487308797, 470915.3487308802, 134555.7479927663], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1918800.0000, 
sim time next is 1919400.0000, 
raw observation next is [19.75, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.618574326923206, 6.9112, 6.9112, 121.9260426156618, 460554.7943707338, 460554.7943707338, 133275.3241273398], 
processed observation next is [1.0, 0.21739130434782608, 0.28703703703703703, 0.92, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5232179086540075, 0.0, 0.0, 0.8094621288201359, 0.16448385513240493, 0.16448385513240493, 0.2562987002448842], 
reward next is 0.7437, 
noisyNet noise sample is [array([1.2646638], dtype=float32), -0.15862575]. 
=============================================
[2019-04-02 18:37:23,016] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 2.2338277e-34 3.6359075e-31 3.0905680e-34 2.9431322e-32], sum to 1.0000
[2019-04-02 18:37:23,026] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7731
[2019-04-02 18:37:23,037] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 930823.2108392285 W.
[2019-04-02 18:37:23,041] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.66666666666666, 87.66666666666667, 1.0, 2.0, 0.3860054905988271, 1.0, 1.0, 0.3860054905988271, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 930823.2108392285, 930823.210839228, 204508.8677048616], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1935600.0000, 
sim time next is 1936200.0000, 
raw observation next is [21.78333333333333, 87.33333333333333, 1.0, 2.0, 0.7903050100943692, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 961489.429137445, 961489.429137445, 196621.575824404], 
processed observation next is [1.0, 0.391304347826087, 0.3623456790123456, 0.8733333333333333, 1.0, 1.0, 0.7503631072552014, 0.0, 0.5, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3433890818348018, 0.3433890818348018, 0.3781184150469307], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.5774413], dtype=float32), -1.0004336]. 
=============================================
[2019-04-02 18:37:24,571] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 2.7489073e-38 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:37:24,581] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9517
[2019-04-02 18:37:24,588] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.41666666666667, 90.16666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6027128934253462, 6.9112, 6.9112, 121.9260426156618, 446994.0100493766, 446994.0100493766, 130383.4005075818], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1993800.0000, 
sim time next is 1994400.0000, 
raw observation next is [19.4, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6001710864386196, 6.9112, 6.9112, 121.9260426156618, 444962.4106675597, 444962.4106675597, 130047.0686782315], 
processed observation next is [0.0, 0.08695652173913043, 0.274074074074074, 0.9, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5002138580482745, 0.0, 0.0, 0.8094621288201359, 0.1589151466669856, 0.1589151466669856, 0.25009051668890675], 
reward next is 0.7499, 
noisyNet noise sample is [array([0.2444657], dtype=float32), -0.18392858]. 
=============================================
[2019-04-02 18:37:39,536] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 1.4239257e-35 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:37:39,544] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8840
[2019-04-02 18:37:39,553] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.2, 82.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9064370932116963, 6.911200000000001, 6.9112, 121.9260426156618, 662149.905620034, 662149.9056200335, 177042.9876864033], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2160000.0000, 
sim time next is 2160600.0000, 
raw observation next is [25.13333333333333, 82.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9083174222723106, 6.911199999999999, 6.9112, 121.9260426156618, 663367.0101283867, 663367.010128387, 177324.1006229183], 
processed observation next is [1.0, 0.0, 0.4864197530864196, 0.8266666666666667, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8853967778403882, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.23691678933156668, 0.2369167893315668, 0.34100788581330443], 
reward next is 0.6590, 
noisyNet noise sample is [array([-0.5905108], dtype=float32), -1.9728334]. 
=============================================
[2019-04-02 18:37:45,219] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 3.6636462e-36 7.6142612e-35 8.6093392e-37 1.0401814e-36], sum to 1.0000
[2019-04-02 18:37:45,229] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9739
[2019-04-02 18:37:45,236] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.8, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8835485877659747, 6.911199999999999, 6.9112, 121.9260426156618, 648206.6378791508, 648206.6378791513, 173431.6089605248], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2246400.0000, 
sim time next is 2247000.0000, 
raw observation next is [22.83333333333333, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8849910770475354, 6.911200000000001, 6.9112, 121.9260426156618, 649008.1765999633, 649008.1765999629, 173678.281122062], 
processed observation next is [1.0, 0.0, 0.4012345679012344, 0.98, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8562388463094192, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2317886344999869, 0.23178863449998674, 0.33399669446550384], 
reward next is 0.6660, 
noisyNet noise sample is [array([1.1929431], dtype=float32), -1.1878932]. 
=============================================
[2019-04-02 18:37:45,247] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[73.51752 ]
 [74.07867 ]
 [74.05571 ]
 [74.032616]
 [74.00756 ]], R is [[73.15525818]
 [73.09018707]
 [73.02594757]
 [72.96264648]
 [72.90027618]].
[2019-04-02 18:37:45,850] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:37:45,860] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8913
[2019-04-02 18:37:45,868] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.93333333333333, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8912694451275494, 6.9112, 6.9112, 121.9260426156618, 652647.4863142942, 652647.4863142942, 174711.6289494786], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2248800.0000, 
sim time next is 2249400.0000, 
raw observation next is [22.96666666666667, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8938247845381783, 6.911200000000001, 6.9112, 121.9260426156618, 654186.4281930837, 654186.4281930834, 175117.8283594365], 
processed observation next is [1.0, 0.0, 0.4061728395061729, 0.98, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8672809806727227, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.23363801006895848, 0.23363801006895835, 0.3367650545373779], 
reward next is 0.6632, 
noisyNet noise sample is [array([0.5432615], dtype=float32), -0.4672796]. 
=============================================
[2019-04-02 18:37:55,780] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 5.1432850e-14 1.1307083e-31 5.3193998e-29 6.6141362e-31], sum to 1.0000
[2019-04-02 18:37:55,787] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4365
[2019-04-02 18:37:55,799] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1583137.092916016 W.
[2019-04-02 18:37:55,806] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.96666666666667, 37.0, 1.0, 2.0, 0.4475152428994602, 1.0, 2.0, 0.4475152428994602, 1.0, 2.0, 0.7168173247405097, 6.911200000000001, 6.9112, 121.94756008, 1583137.092916016, 1583137.092916015, 318524.8389341606], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2387400.0000, 
sim time next is 2388000.0000, 
raw observation next is [30.93333333333333, 37.0, 1.0, 2.0, 0.4012040817317404, 1.0, 2.0, 0.4012040817317404, 1.0, 2.0, 0.6427127896254419, 6.9112, 6.9112, 121.94756008, 1419640.86265082, 1419640.86265082, 297555.3408268607], 
processed observation next is [1.0, 0.6521739130434783, 0.7012345679012344, 0.37, 1.0, 1.0, 0.28714771634731, 1.0, 1.0, 0.28714771634731, 1.0, 1.0, 0.5533909870318023, 0.0, 0.0, 0.8096049824067558, 0.5070145938038643, 0.5070145938038643, 0.5722218092824244], 
reward next is 0.4278, 
noisyNet noise sample is [array([-0.1873105], dtype=float32), -1.9627275]. 
=============================================
[2019-04-02 18:37:55,817] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[51.92771 ]
 [51.463005]
 [51.887287]
 [51.103985]
 [50.85253 ]], R is [[52.58866882]
 [52.45023346]
 [52.35753632]
 [52.33179855]
 [51.80847931]].
[2019-04-02 18:37:57,618] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:37:57,631] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3228
[2019-04-02 18:37:57,637] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.7, 62.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5370624821150995, 6.9112, 6.9112, 121.9260426156618, 391537.9132002899, 391537.9132002899, 120924.9045623477], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2424000.0000, 
sim time next is 2424600.0000, 
raw observation next is [21.55, 62.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5304627577608253, 6.911199999999999, 6.9112, 121.9260426156618, 385695.9121220242, 385695.9121220246, 119956.9873854335], 
processed observation next is [1.0, 0.043478260869565216, 0.35370370370370374, 0.625, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.4130784472010316, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1377485400435801, 0.1377485400435802, 0.23068651420275674], 
reward next is 0.7693, 
noisyNet noise sample is [array([1.4468007], dtype=float32), 0.5125346]. 
=============================================
[2019-04-02 18:37:58,962] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:37:58,975] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4185
[2019-04-02 18:37:58,986] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.8, 73.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5052881691212245, 6.911200000000001, 6.9112, 121.9260426156618, 365203.2071224242, 365203.2071224237, 117071.5247877105], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2439000.0000, 
sim time next is 2439600.0000, 
raw observation next is [20.36666666666667, 70.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5087682404219916, 6.911200000000001, 6.9112, 121.9260426156618, 368995.9036151805, 368995.9036151801, 117832.3476135621], 
processed observation next is [1.0, 0.21739130434782608, 0.3098765432098767, 0.7033333333333333, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.38596030052748953, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1317842512911359, 0.13178425129113575, 0.22660066848761942], 
reward next is 0.7734, 
noisyNet noise sample is [array([-0.7083701], dtype=float32), -0.49832547]. 
=============================================
[2019-04-02 18:38:10,872] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 4.7032588e-23 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:38:10,881] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8932
[2019-04-02 18:38:10,890] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.96666666666667, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.773917825920548, 6.9112, 6.9112, 121.9260426156618, 576708.3994582038, 576708.3994582038, 155971.137337288], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2594400.0000, 
sim time next is 2595000.0000, 
raw observation next is [22.78333333333333, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7710884590378766, 6.9112, 6.9112, 121.9260426156618, 574744.1121849389, 574744.1121849389, 155514.46150746], 
processed observation next is [0.0, 0.0, 0.39938271604938264, 0.85, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.7138605737973458, 0.0, 0.0, 0.8094621288201359, 0.2052657543517639, 0.2052657543517639, 0.29906627212973075], 
reward next is 0.7009, 
noisyNet noise sample is [array([0.54877704], dtype=float32), 0.4016509]. 
=============================================
[2019-04-02 18:38:10,909] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[78.78097]
 [78.52682]
 [78.16575]
 [77.77347]
 [77.24125]], R is [[78.95935822]
 [78.86981964]
 [78.7802124 ]
 [78.69062805]
 [78.60121918]].
[2019-04-02 18:38:14,448] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 3.2714142e-38 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:38:14,458] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1691
[2019-04-02 18:38:14,466] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.66666666666666, 76.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8782767922664978, 6.911200000000001, 6.9112, 121.9260426156618, 645336.6819356454, 645336.6819356449, 172508.2021865121], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2641800.0000, 
sim time next is 2642400.0000, 
raw observation next is [25.8, 76.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8837949987583136, 6.911200000000001, 6.9112, 121.9260426156618, 648697.0099043174, 648697.009904317, 173393.0858942714], 
processed observation next is [0.0, 0.6086956521739131, 0.5111111111111112, 0.76, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.854743748447892, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.23167750353725622, 0.23167750353725605, 0.3334482421043681], 
reward next is 0.6666, 
noisyNet noise sample is [array([0.02135415], dtype=float32), -0.3807784]. 
=============================================
[2019-04-02 18:38:17,297] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:38:17,307] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0476
[2019-04-02 18:38:17,313] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.1, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7022019073442408, 6.911200000000001, 6.9112, 121.9260426156618, 524692.7872810373, 524692.7872810368, 144506.2777872302], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2698200.0000, 
sim time next is 2698800.0000, 
raw observation next is [21.06666666666667, 86.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.686170240976533, 6.9112, 6.9112, 121.9260426156618, 512500.1260645965, 512500.1260645965, 142132.3933032486], 
processed observation next is [0.0, 0.21739130434782608, 0.3358024691358026, 0.8633333333333333, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6077128012206662, 0.0, 0.0, 0.8094621288201359, 0.18303575930878446, 0.18303575930878446, 0.27333152558317036], 
reward next is 0.7267, 
noisyNet noise sample is [array([-0.81268144], dtype=float32), 0.2107095]. 
=============================================
[2019-04-02 18:38:19,357] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 9.6479280e-29 1.2920199e-22 2.3382133e-32 8.7115919e-36], sum to 1.0000
[2019-04-02 18:38:19,368] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1570
[2019-04-02 18:38:19,377] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 724161.9613191495 W.
[2019-04-02 18:38:19,384] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.7, 58.0, 1.0, 2.0, 0.6354179677333797, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 724161.9613191495, 724161.9613191495, 164277.4709426562], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2728800.0000, 
sim time next is 2729400.0000, 
raw observation next is [30.91666666666666, 56.5, 1.0, 2.0, 0.3166451321958775, 0.0, 2.0, 0.0, 1.0, 1.0, 0.5041093628678457, 6.911199999999999, 6.9112, 121.9260426156618, 721735.9562967469, 721735.9562967473, 199771.7496428483], 
processed observation next is [0.0, 0.6086956521739131, 0.7006172839506171, 0.565, 1.0, 1.0, 0.1864823002331875, 0.0, 1.0, -0.1904761904761905, 1.0, 0.5, 0.38013670358480706, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2577628415345524, 0.2577628415345526, 0.3841764416208621], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9387233], dtype=float32), -0.25346]. 
=============================================
[2019-04-02 18:38:25,815] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-02 18:38:25,816] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 18:38:25,817] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 18:38:25,818] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:38:25,818] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:38:25,819] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 18:38:25,819] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:38:25,820] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 18:38:25,821] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:38:25,822] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 18:38:25,824] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:38:25,831] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run21
[2019-04-02 18:38:25,853] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run21
[2019-04-02 18:38:25,855] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run21
[2019-04-02 18:38:25,855] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run21
[2019-04-02 18:38:25,906] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run21
[2019-04-02 18:38:47,595] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.40051958], dtype=float32), -0.10671441]
[2019-04-02 18:38:47,596] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [35.33333333333334, 18.5, 1.0, 2.0, 0.9212518749171427, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.205451584257047, 6.9112, 121.9245566665968, 1302021.93549661, 1151340.660868876, 226435.4647479281]
[2019-04-02 18:38:47,599] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 18:38:47,602] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.0000000e+00 3.9596748e-33 4.4570595e-27 0.0000000e+00 4.1623732e-32], sampled 0.914530010924123
[2019-04-02 18:38:47,603] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1302021.93549661 W.
[2019-04-02 18:39:12,894] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.40051958], dtype=float32), -0.10671441]
[2019-04-02 18:39:12,894] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [30.7666847, 68.6741709, 1.0, 2.0, 0.9465903548255657, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9258863373493, 1079043.086238097, 1079043.086238097, 228093.6168882414]
[2019-04-02 18:39:12,895] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 18:39:12,900] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.4087564171996284
[2019-04-02 18:39:12,902] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1079043.086238097 W.
[2019-04-02 18:39:36,421] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.40051958], dtype=float32), -0.10671441]
[2019-04-02 18:39:36,421] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [33.50115338666667, 28.71731913333333, 1.0, 2.0, 0.5979058657495245, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 730714.2604376319, 730714.2604376319, 159713.4889343899]
[2019-04-02 18:39:36,423] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 18:39:36,424] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.0000000e+00 6.1720308e-27 1.1229480e-11 1.6329396e-25 8.2650636e-23], sampled 0.08914384131514608
[2019-04-02 18:39:36,426] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 730714.2604376319 W.
[2019-04-02 18:40:14,501] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.40051958], dtype=float32), -0.10671441]
[2019-04-02 18:40:14,503] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [28.01666666666667, 73.5, 1.0, 2.0, 0.953709197768279, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260287523643, 1087163.797678306, 1087163.797678306, 229740.1150568599]
[2019-04-02 18:40:14,504] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 18:40:14,508] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.0000000e+00 0.0000000e+00 4.8099177e-35 0.0000000e+00 9.3412456e-38], sampled 0.6007908810103584
[2019-04-02 18:40:14,508] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1087163.797678306 W.
[2019-04-02 18:40:18,343] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.40051958], dtype=float32), -0.10671441]
[2019-04-02 18:40:18,343] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [22.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7903401280226967, 6.9112, 6.9112, 121.9260426156618, 587510.5849778845, 587510.5849778845, 158887.0169518359]
[2019-04-02 18:40:18,343] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 18:40:18,345] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.0000000e+00 1.4173406e-36 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.4650512568391325
[2019-04-02 18:40:22,093] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.40051958], dtype=float32), -0.10671441]
[2019-04-02 18:40:22,095] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [20.09544903500001, 70.45012589166667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7463519341274034, 6.911199999999999, 6.9112, 121.9260426156618, 538523.1821268738, 538523.1821268742, 137937.0950083016]
[2019-04-02 18:40:22,096] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 18:40:22,100] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.0000000e+00 1.7250004e-37 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.697256305270705
[2019-04-02 18:40:24,440] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.40051958], dtype=float32), -0.10671441]
[2019-04-02 18:40:24,444] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [21.46666666666667, 87.66666666666666, 1.0, 2.0, 0.6768675582072748, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 829833.2342077352, 829833.2342077352, 174203.4455338562]
[2019-04-02 18:40:24,445] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 18:40:24,446] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.000000e+00 6.688937e-31 6.782516e-31 5.245872e-35 3.854327e-32], sampled 0.12499060887055802
[2019-04-02 18:40:24,447] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 829833.2342077352 W.
[2019-04-02 18:40:34,735] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7840.8820 2529800801.7237 831.0000
[2019-04-02 18:40:34,925] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8633.8375 2219139301.2698 543.0000
[2019-04-02 18:40:35,017] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.40051958], dtype=float32), -0.10671441]
[2019-04-02 18:40:35,018] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [23.08333333333334, 77.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7622219484588272, 6.9112, 6.9112, 121.9260426156618, 569536.0095567645, 569536.0095567645, 152465.920060345]
[2019-04-02 18:40:35,018] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 18:40:35,020] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.0000000e+00 7.4241174e-38 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.6590935649347432
[2019-04-02 18:40:35,084] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8404.3352 2292930052.2689 697.0000
[2019-04-02 18:40:35,265] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8560.3296 2258226393.9236 536.0000
[2019-04-02 18:40:35,597] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8362.1958 2339518003.4437 616.0000
[2019-04-02 18:40:36,611] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 500000, evaluation results [500000.0, 7840.881980364831, 2529800801.7236648, 831.0, 8560.329577213746, 2258226393.9236007, 536.0, 8633.837517256845, 2219139301.2698236, 543.0, 8362.195759075801, 2339518003.4436603, 616.0, 8404.335235826124, 2292930052.2689223, 697.0]
[2019-04-02 18:40:39,016] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 1.1413370e-26 4.5317913e-35 0.0000000e+00 4.4939866e-35], sum to 1.0000
[2019-04-02 18:40:39,024] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5865
[2019-04-02 18:40:39,028] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.33333333333334, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8641272092670191, 6.9112, 6.9112, 121.9260426156618, 637912.5164393183, 637912.5164393183, 169857.5158699038], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2852400.0000, 
sim time next is 2853000.0000, 
raw observation next is [22.5, 97.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8656606093308782, 6.911200000000001, 6.9112, 121.9260426156618, 638698.3872514431, 638698.3872514426, 170160.0775216863], 
processed observation next is [1.0, 0.0, 0.3888888888888889, 0.97, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8320757616635976, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2281065668755154, 0.22810656687551523, 0.3272309183109352], 
reward next is 0.6728, 
noisyNet noise sample is [array([2.8699534], dtype=float32), -0.80767953]. 
=============================================
[2019-04-02 18:40:39,046] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[52.145744]
 [51.565357]
 [50.47783 ]
 [48.37157 ]
 [43.851967]], R is [[52.46223831]
 [52.61096954]
 [52.7585144 ]
 [52.90243912]
 [52.3734169 ]].
[2019-04-02 18:40:41,089] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 4.9495935e-35 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:40:41,102] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3158
[2019-04-02 18:40:41,110] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1734808.107697617 W.
[2019-04-02 18:40:41,116] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.58333333333334, 33.83333333333334, 1.0, 2.0, 0.8610187295586826, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9679297411973395, 6.9112, 6.9112, 121.9260426156618, 1734808.107697617, 1734808.107697617, 342911.6379650538], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3149400.0000, 
sim time next is 3150000.0000, 
raw observation next is [32.9, 32.0, 1.0, 2.0, 0.723150848885597, 1.0, 1.0, 0.723150848885597, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1713014.012810786, 1713014.012810787, 316046.6007910697], 
processed observation next is [1.0, 0.4782608695652174, 0.774074074074074, 0.32, 1.0, 1.0, 0.6704176772447583, 1.0, 0.5, 0.6704176772447583, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.611790718860995, 0.6117907188609953, 0.6077819245982109], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.07434008], dtype=float32), 1.3227175]. 
=============================================
[2019-04-02 18:40:41,136] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[41.64498 ]
 [41.84486 ]
 [41.734596]
 [41.095078]
 [41.839996]], R is [[41.46237946]
 [41.0477562 ]
 [40.97648621]
 [40.92263412]
 [40.51340866]].
[2019-04-02 18:40:45,659] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 6.7655109e-24 9.6715907e-28 0.0000000e+00 6.9222475e-33], sum to 1.0000
[2019-04-02 18:40:45,670] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5760
[2019-04-02 18:40:45,678] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 834590.7012702231 W.
[2019-04-02 18:40:45,707] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.4, 78.66666666666667, 1.0, 2.0, 0.2440871087071087, 1.0, 1.0, 0.2440871087071087, 1.0, 2.0, 0.3885946264238738, 6.9112, 6.9112, 121.94756008, 834590.7012702231, 834590.7012702231, 235538.5545167294], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3087600.0000, 
sim time next is 3088200.0000, 
raw observation next is [29.7, 76.83333333333333, 1.0, 2.0, 0.747004208088528, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 851403.2322861003, 851403.2322861003, 185241.2511414644], 
processed observation next is [1.0, 0.7391304347826086, 0.6555555555555556, 0.7683333333333333, 1.0, 1.0, 0.6988145334387238, 0.0, 0.5, -0.1904761904761905, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288201359, 0.30407258295932155, 0.30407258295932155, 0.3562331752720469], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0294809], dtype=float32), -1.9775989]. 
=============================================
[2019-04-02 18:40:51,976] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 7.5028152e-29 0.0000000e+00 1.6438554e-36], sum to 1.0000
[2019-04-02 18:40:51,984] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8367
[2019-04-02 18:40:51,995] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 898371.3849022567 W.
[2019-04-02 18:40:52,001] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.26666666666667, 96.33333333333334, 1.0, 2.0, 0.3940944949224653, 0.0, 2.0, 0.0, 1.0, 2.0, 0.627411270678223, 6.911199999999999, 6.9112, 121.9260426156618, 898371.3849022567, 898371.3849022571, 222286.7489602218], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3039600.0000, 
sim time next is 3040200.0000, 
raw observation next is [25.4, 94.5, 1.0, 2.0, 0.7942704914518192, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 905307.1249669809, 905307.1249669809, 194757.2043639309], 
processed observation next is [1.0, 0.17391304347826086, 0.49629629629629624, 0.945, 1.0, 1.0, 0.7550839183950228, 0.0, 1.0, -0.1904761904761905, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3233239732024932, 0.3233239732024932, 0.37453308531525176], 
reward next is 0.6255, 
noisyNet noise sample is [array([2.3246603], dtype=float32), 1.0357003]. 
=============================================
[2019-04-02 18:40:55,079] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 2.6790513e-29 1.3274574e-35 0.0000000e+00 9.6875587e-32], sum to 1.0000
[2019-04-02 18:40:55,088] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3515
[2019-04-02 18:40:55,097] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1837013.540140017 W.
[2019-04-02 18:40:55,105] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.21666666666667, 73.66666666666667, 1.0, 2.0, 0.9840322558862986, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.9260082759182, 1837013.540140017, 1837013.540140017, 375735.901086828], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3071400.0000, 
sim time next is 3072000.0000, 
raw observation next is [31.43333333333333, 72.33333333333334, 1.0, 2.0, 0.974260284572189, 1.0, 1.0, 0.974260284572189, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426051908, 2222694.465361439, 2222694.46536144, 421025.2434928329], 
processed observation next is [1.0, 0.5652173913043478, 0.719753086419753, 0.7233333333333334, 1.0, 1.0, 0.9693574816335584, 1.0, 0.5, 0.9693574816335584, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.8094621287506194, 0.7938194519147997, 0.7938194519148001, 0.8096639297939093], 
reward next is 0.1903, 
noisyNet noise sample is [array([1.3478467], dtype=float32), 0.9977856]. 
=============================================
[2019-04-02 18:40:55,119] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[24.777344]
 [25.076595]
 [27.106686]
 [27.055326]
 [26.297861]], R is [[23.37124252]
 [23.41496086]
 [23.18081093]
 [23.34392738]
 [23.50006676]].
[2019-04-02 18:40:57,654] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 7.7353034e-24 1.3382210e-20 4.8935864e-29 2.9419131e-29], sum to 1.0000
[2019-04-02 18:40:57,662] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1875
[2019-04-02 18:40:57,669] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 58.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8152399616845876, 6.9112, 6.9112, 121.9260426156618, 604274.2109454266, 604274.2109454266, 162802.5751146274], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3112200.0000, 
sim time next is 3112800.0000, 
raw observation next is [28.0, 58.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8140230372772916, 6.911200000000001, 6.9112, 121.9260426156618, 603372.2031876394, 603372.2031876389, 162648.2941721069], 
processed observation next is [1.0, 0.0, 0.5925925925925926, 0.58, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.7675287965966145, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.21549007256701405, 0.21549007256701389, 0.3127851811002056], 
reward next is 0.6872, 
noisyNet noise sample is [array([1.5143937], dtype=float32), -0.5774179]. 
=============================================
[2019-04-02 18:40:58,533] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 3.0499176e-33 1.1021099e-20 2.8331666e-34 2.1921140e-30], sum to 1.0000
[2019-04-02 18:40:58,546] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4032
[2019-04-02 18:40:58,556] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 815172.7436079461 W.
[2019-04-02 18:40:58,560] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666666, 60.83333333333334, 1.0, 2.0, 0.3447236345671795, 0.0, 1.0, 0.0, 1.0, 2.0, 0.5527956869546321, 6.911199999999999, 6.9112, 121.9260426156618, 815172.7436079461, 815172.7436079466, 206836.7445796807], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3124200.0000, 
sim time next is 3124800.0000, 
raw observation next is [27.0, 62.0, 1.0, 2.0, 0.6798793266815185, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 810960.8194052309, 810960.8194052309, 174005.3352916676], 
processed observation next is [1.0, 0.17391304347826086, 0.5555555555555556, 0.62, 1.0, 1.0, 0.6189039603351411, 0.0, 1.0, -0.1904761904761905, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288201359, 0.28962886407329674, 0.28962886407329674, 0.33462564479166845], 
reward next is 0.6654, 
noisyNet noise sample is [array([-1.5367227], dtype=float32), 1.1201535]. 
=============================================
[2019-04-02 18:41:00,020] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 3.3585263e-37 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:41:00,029] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7089
[2019-04-02 18:41:00,041] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1772555.168592451 W.
[2019-04-02 18:41:00,046] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 52.0, 1.0, 2.0, 0.7771490573890188, 1.0, 2.0, 0.7771490573890188, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1772555.168592451, 1772555.168592452, 334400.6459573995], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3142800.0000, 
sim time next is 3143400.0000, 
raw observation next is [30.16666666666666, 50.5, 1.0, 2.0, 0.6121989375024172, 1.0, 2.0, 0.6121989375024172, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 1406454.332782984, 1406454.332782983, 272779.2460936542], 
processed observation next is [1.0, 0.391304347826087, 0.6728395061728393, 0.505, 1.0, 1.0, 0.5383320684552585, 1.0, 1.0, 0.5383320684552585, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.5023051188510658, 0.5023051188510654, 0.5245754732570272], 
reward next is 0.4754, 
noisyNet noise sample is [array([0.03742155], dtype=float32), -0.78040403]. 
=============================================
[2019-04-02 18:41:01,428] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 1.5495903e-33 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:41:01,437] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1626
[2019-04-02 18:41:01,441] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.73333333333334, 85.66666666666667, 1.0, 1.0, 0.1996943974392545, 1.0, 1.0, 0.1996943974392545, 1.0, 2.0, 0.317923869684191, 6.911199999999999, 6.9112, 121.94756008, 682927.8803520818, 682927.8803520823, 220495.7485262666], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3205200.0000, 
sim time next is 3205800.0000, 
raw observation next is [24.55, 85.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9234105641401842, 6.911200000000001, 6.9112, 121.9260426156618, 671328.3080005508, 671328.3080005504, 179881.2292383208], 
processed observation next is [0.0, 0.08695652173913043, 0.46481481481481485, 0.85, 0.0, 0.5, -0.1904761904761905, 0.0, 0.5, -0.1904761904761905, 1.0, 1.0, 0.9042632051752303, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.23976011000019673, 0.23976011000019656, 0.34592544084292465], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.22610533], dtype=float32), -1.0937692]. 
=============================================
[2019-04-02 18:41:06,990] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:41:07,001] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2936
[2019-04-02 18:41:07,006] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.0, 55.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8915671465941915, 6.911200000000001, 6.9112, 121.9260426156618, 651780.4823276566, 651780.4823276561, 174969.2398258717], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3240000.0000, 
sim time next is 3240600.0000, 
raw observation next is [30.16666666666666, 53.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8941229453457449, 6.911199999999999, 6.9112, 121.9260426156618, 653559.5812661238, 653559.5812661243, 175324.9967784297], 
processed observation next is [0.0, 0.5217391304347826, 0.6728395061728393, 0.535, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8676536816821812, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.23341413616647277, 0.23341413616647294, 0.337163455343134], 
reward next is 0.6628, 
noisyNet noise sample is [array([0.46168363], dtype=float32), -0.53132147]. 
=============================================
[2019-04-02 18:41:07,682] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 1.2032197e-38 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:41:07,689] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8505
[2019-04-02 18:41:07,696] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [32.83333333333333, 43.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8951917005744593, 6.9112, 6.9112, 121.9260426156618, 653682.2336572811, 653682.2336572811, 175591.4474681838], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3253800.0000, 
sim time next is 3254400.0000, 
raw observation next is [33.0, 44.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.914837810173937, 6.9112, 6.9112, 121.9260426156618, 665325.7281161534, 665325.7281161534, 178682.7106105325], 
processed observation next is [0.0, 0.6956521739130435, 0.7777777777777778, 0.44, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8935472627174212, 0.0, 0.0, 0.8094621288201359, 0.2376163314700548, 0.2376163314700548, 0.3436205973279471], 
reward next is 0.6564, 
noisyNet noise sample is [array([-0.04135994], dtype=float32), 1.3481419]. 
=============================================
[2019-04-02 18:41:13,625] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:41:13,633] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3298
[2019-04-02 18:41:13,639] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.6, 81.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9378051877662198, 6.911200000000001, 6.9112, 121.9260426156618, 682332.5370616463, 682332.5370616459, 181762.5617440728], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3324000.0000, 
sim time next is 3324600.0000, 
raw observation next is [25.95, 81.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9611672620459951, 6.911199999999999, 6.9112, 121.9260015467587, 696299.8048860099, 696299.8048860104, 185447.0931817777], 
processed observation next is [0.0, 0.4782608695652174, 0.5166666666666666, 0.81, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.9514590775574938, -8.881784197001253e-17, 0.0, 0.809461856165328, 0.24867850174500353, 0.2486785017450037, 0.35662902534957247], 
reward next is 0.6434, 
noisyNet noise sample is [array([1.7696764], dtype=float32), -1.7335589]. 
=============================================
[2019-04-02 18:41:22,177] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 5.1817384e-32 6.4152626e-24 5.3177321e-38 1.9981932e-34], sum to 1.0000
[2019-04-02 18:41:22,189] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9194
[2019-04-02 18:41:22,197] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 751769.2437913677 W.
[2019-04-02 18:41:22,206] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 94.0, 1.0, 2.0, 0.3298150916287536, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5250763672006189, 6.911199999999999, 6.9112, 121.9260426156618, 751769.2437913677, 751769.2437913682, 203430.7405663783], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3456000.0000, 
sim time next is 3456600.0000, 
raw observation next is [25.0, 94.00000000000001, 1.0, 2.0, 0.6623386144733813, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 754857.516431737, 754857.516431737, 169135.6454176941], 
processed observation next is [1.0, 0.0, 0.48148148148148145, 0.9400000000000002, 1.0, 1.0, 0.5980221600873586, 0.0, 1.0, -0.1904761904761905, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2695919701541918, 0.2695919701541918, 0.32526085657248865], 
reward next is 0.6747, 
noisyNet noise sample is [array([0.29624414], dtype=float32), -2.036778]. 
=============================================
[2019-04-02 18:41:28,361] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 1.0571019e-29 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:41:28,375] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8442
[2019-04-02 18:41:28,383] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1949487.819733609 W.
[2019-04-02 18:41:28,392] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.8546378414290766, 1.0, 1.0, 0.8546378414290766, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1949487.819733609, 1949487.819733609, 366849.7094840425], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3685200.0000, 
sim time next is 3685800.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5804882543037824, 1.0, 2.0, 0.5804882543037824, 1.0, 1.0, 0.9241562060342247, 6.911199999999999, 6.9112, 121.94756008, 1986241.01124554, 1986241.01124554, 384847.1208018795], 
processed observation next is [1.0, 0.6521739130434783, 0.5555555555555556, 0.89, 1.0, 1.0, 0.5005812551235505, 1.0, 1.0, 0.5005812551235505, 1.0, 0.5, 0.9051952575427807, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.70937178973055, 0.70937178973055, 0.7400906169266913], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2241366], dtype=float32), -0.16304362]. 
=============================================
[2019-04-02 18:41:43,279] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-02 18:41:43,283] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 18:41:43,283] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:41:43,284] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 18:41:43,284] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:41:43,285] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 18:41:43,286] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 18:41:43,287] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 18:41:43,287] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:41:43,290] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:41:43,288] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:41:43,310] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run22
[2019-04-02 18:41:43,310] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run22
[2019-04-02 18:41:43,311] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run22
[2019-04-02 18:41:43,371] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run22
[2019-04-02 18:41:43,389] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run22
[2019-04-02 18:41:45,477] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.2748579], dtype=float32), -0.13781288]
[2019-04-02 18:41:45,478] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [22.0, 36.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4683093625027144, 6.911199999999999, 6.9112, 121.9260426156618, 334372.4872030866, 334372.4872030871, 92584.82964456714]
[2019-04-02 18:41:45,479] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 18:41:45,481] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.0000000e+00 3.2581805e-35 1.3980521e-36 0.0000000e+00 0.0000000e+00], sampled 0.19670216408454633
[2019-04-02 18:42:36,988] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.2748579], dtype=float32), -0.13781288]
[2019-04-02 18:42:36,990] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [34.4, 51.0, 1.0, 2.0, 0.7181932822613791, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 818548.2446513431, 818548.2446513436, 179622.2138292395]
[2019-04-02 18:42:36,990] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 18:42:36,995] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.0000000e+00 1.7444627e-29 9.7458469e-28 1.3262871e-29 3.2485174e-32], sampled 0.4481942832574213
[2019-04-02 18:42:36,997] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 818548.2446513431 W.
[2019-04-02 18:42:38,521] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.2748579], dtype=float32), -0.13781288]
[2019-04-02 18:42:38,524] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [23.93333333333333, 94.0, 1.0, 2.0, 0.6572234887431746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 759739.1805682823, 759739.1805682823, 168728.9813807404]
[2019-04-02 18:42:38,525] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 18:42:38,528] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8516633543112717
[2019-04-02 18:42:38,531] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 759739.1805682823 W.
[2019-04-02 18:42:57,793] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.2748579], dtype=float32), -0.13781288]
[2019-04-02 18:42:57,793] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [22.65349735, 90.52710735999999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8161662461074835, 6.9112, 6.9112, 121.9260426156618, 605565.7288478657, 605565.7288478657, 162663.2626302984]
[2019-04-02 18:42:57,794] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 18:42:57,797] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.18142705790799352
[2019-04-02 18:43:00,167] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.2748579], dtype=float32), -0.13781288]
[2019-04-02 18:43:00,168] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [26.458875405, 82.86225206, 1.0, 2.0, 0.899171293227521, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156427, 1024952.704473711, 1024952.704473711, 217277.0180058591]
[2019-04-02 18:43:00,170] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 18:43:00,171] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8362472601310242
[2019-04-02 18:43:00,173] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1024952.704473711 W.
[2019-04-02 18:43:11,558] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.2748579], dtype=float32), -0.13781288]
[2019-04-02 18:43:11,562] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [23.97077904, 94.01418573166666, 1.0, 2.0, 0.6248071911200845, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 718119.3863132952, 718119.3863132952, 162694.8870784983]
[2019-04-02 18:43:11,563] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 18:43:11,565] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.0000000e+00 0.0000000e+00 1.6872878e-27 0.0000000e+00 3.9605735e-37], sampled 0.05359825535692608
[2019-04-02 18:43:11,568] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 718119.3863132952 W.
[2019-04-02 18:43:11,774] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.2748579], dtype=float32), -0.13781288]
[2019-04-02 18:43:11,775] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [31.0, 75.0, 1.0, 2.0, 0.8570314066185531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 976887.4222695078, 976887.4222695078, 208017.5756505304]
[2019-04-02 18:43:11,776] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 18:43:11,779] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.0000000e+00 0.0000000e+00 1.0371588e-35 0.0000000e+00 0.0000000e+00], sampled 0.016022138014848086
[2019-04-02 18:43:11,781] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 976887.4222695078 W.
[2019-04-02 18:43:16,262] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.2748579], dtype=float32), -0.13781288]
[2019-04-02 18:43:16,263] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [27.4, 45.0, 1.0, 2.0, 0.9597832714387204, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.407254290108552, 6.9112, 121.9239513130825, 1442804.437490318, 1188784.661203786, 235311.3326801608]
[2019-04-02 18:43:16,264] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 18:43:16,270] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.0000000e+00 3.1264428e-31 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.7930632759151084
[2019-04-02 18:43:16,271] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1442804.437490318 W.
[2019-04-02 18:43:16,764] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.2748579], dtype=float32), -0.13781288]
[2019-04-02 18:43:16,765] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [26.00590713666667, 64.72128519666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.782553981804302, 6.911199999999999, 6.9112, 121.9260426156618, 582141.4363196399, 582141.4363196403, 157693.4370102271]
[2019-04-02 18:43:16,766] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 18:43:16,770] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.000000e+00 6.941983e-38 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.7135849047637803
[2019-04-02 18:43:26,965] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.2748579], dtype=float32), -0.13781288]
[2019-04-02 18:43:26,968] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [27.5, 70.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9321816576099091, 6.9112, 6.9112, 121.9260426156618, 675456.2369724088, 675456.2369724088, 181409.9874158647]
[2019-04-02 18:43:26,970] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 18:43:26,973] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.0000000e+00 2.6883085e-27 0.0000000e+00 9.0585925e-35 5.7691987e-37], sampled 0.66309417064434
[2019-04-02 18:43:42,878] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.2748579], dtype=float32), -0.13781288]
[2019-04-02 18:43:42,879] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [18.78210368666667, 103.9581247666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6329623324181344, 6.911199999999999, 6.9112, 121.9260426156618, 471082.9549545096, 471082.95495451, 134520.9590983329]
[2019-04-02 18:43:42,881] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 18:43:42,883] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.1603335369266483
[2019-04-02 18:43:50,403] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8560.3296 2258226393.9236 536.0000
[2019-04-02 18:43:50,432] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8362.6141 2339451215.1939 616.0000
[2019-04-02 18:43:50,767] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7841.5838 2529739775.4178 831.0000
[2019-04-02 18:43:50,919] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8633.8375 2219139301.2698 543.0000
[2019-04-02 18:43:51,009] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8404.3352 2292930052.2689 697.0000
[2019-04-02 18:43:52,025] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 525000, evaluation results [525000.0, 7841.583789482413, 2529739775.4177794, 831.0, 8560.329577213746, 2258226393.9236007, 536.0, 8633.837517256845, 2219139301.2698236, 543.0, 8362.614081096275, 2339451215.1938696, 616.0, 8404.335235826124, 2292930052.2689223, 697.0]
[2019-04-02 18:44:02,393] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.4962001e-01 8.1245884e-28 5.0379977e-02 3.6448166e-26 3.9975472e-22], sum to 1.0000
[2019-04-02 18:44:02,402] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8405
[2019-04-02 18:44:02,411] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 813448.8948747698 W.
[2019-04-02 18:44:02,419] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.4, 95.33333333333334, 1.0, 2.0, 0.3568607458157895, 0.0, 2.0, 0.0, 1.0, 2.0, 0.568133929481844, 6.911199999999999, 6.9112, 121.9260426156618, 813448.8948747698, 813448.8948747703, 211163.2787409718], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3904800.0000, 
sim time next is 3905400.0000, 
raw observation next is [25.25, 95.66666666666666, 1.0, 2.0, 0.2362427385264725, 1.0, 1.0, 0.2362427385264725, 1.0, 2.0, 0.3761061336229997, 6.9112, 6.9112, 121.94756008, 807754.8407945884, 807754.8407945884, 232800.1173930946], 
processed observation next is [0.0, 0.17391304347826086, 0.49074074074074076, 0.9566666666666666, 1.0, 1.0, 0.09076516491246728, 1.0, 0.5, 0.09076516491246728, 1.0, 1.0, 0.22013266702874962, 0.0, 0.0, 0.8096049824067558, 0.288483871712353, 0.288483871712353, 0.4476925334482589], 
reward next is 0.5523, 
noisyNet noise sample is [array([1.0693494], dtype=float32), -0.023687445]. 
=============================================
[2019-04-02 18:44:04,209] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.4417270e-03 0.0000000e+00 9.9255824e-01 3.7410490e-30 1.1037759e-27], sum to 1.0000
[2019-04-02 18:44:04,216] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9417
[2019-04-02 18:44:04,226] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.5, 84.0, 1.0, 2.0, 0.7428784981378975, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 846698.3283366073, 846698.3283366073, 184422.3954439443], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3922200.0000, 
sim time next is 3922800.0000, 
raw observation next is [27.66666666666666, 82.33333333333334, 1.0, 2.0, 0.369714562579499, 0.0, 2.0, 0.0, 1.0, 1.0, 0.5885976243892562, 6.911199999999999, 6.9112, 121.9260426156618, 842764.7321101808, 842764.7321101812, 214941.8000355128], 
processed observation next is [0.0, 0.391304347826087, 0.5802469135802467, 0.8233333333333335, 1.0, 1.0, 0.2496601935470226, 0.0, 1.0, -0.1904761904761905, 1.0, 0.5, 0.48574703048657025, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.30098740432506454, 0.3009874043250647, 0.4133496154529092], 
reward next is 0.5867, 
noisyNet noise sample is [array([-1.1442677], dtype=float32), -0.7930398]. 
=============================================
[2019-04-02 18:44:26,526] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:44:26,537] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6514
[2019-04-02 18:44:26,542] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.73333333333333, 59.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7060440595685167, 6.911199999999999, 6.9112, 121.9260426156618, 527622.3514421702, 527622.3514421707, 145488.3281418784], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4237800.0000, 
sim time next is 4238400.0000, 
raw observation next is [25.46666666666667, 61.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7022619605257998, 6.9112, 6.9112, 121.9260426156618, 524795.0768329051, 524795.0768329051, 145067.4784956645], 
processed observation next is [1.0, 0.043478260869565216, 0.4987654320987655, 0.61, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6278274506572498, 0.0, 0.0, 0.8094621288201359, 0.18742681315460896, 0.18742681315460896, 0.27897592018397016], 
reward next is 0.7210, 
noisyNet noise sample is [array([0.88708526], dtype=float32), 1.0832409]. 
=============================================
[2019-04-02 18:44:30,393] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 3.825408e-34 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-02 18:44:30,410] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9984
[2019-04-02 18:44:30,419] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1991264.986678213 W.
[2019-04-02 18:44:30,425] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.08333333333333, 44.0, 1.0, 2.0, 0.5819548998467432, 1.0, 2.0, 0.5819548998467432, 1.0, 1.0, 0.926491153503929, 6.9112, 6.9112, 121.94756008, 1991264.986678213, 1991264.986678213, 385631.3867275236], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4294200.0000, 
sim time next is 4294800.0000, 
raw observation next is [31.9, 44.0, 1.0, 2.0, 0.8136819769444721, 1.0, 2.0, 0.8136819769444721, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1855967.600526494, 1855967.600526494, 349438.5839425715], 
processed observation next is [1.0, 0.7391304347826086, 0.7370370370370369, 0.44, 1.0, 1.0, 0.7781928296958001, 1.0, 1.0, 0.7781928296958001, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288201359, 0.662845571616605, 0.662845571616605, 0.6719972768126374], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.8034773], dtype=float32), -1.5163497]. 
=============================================
[2019-04-02 18:44:34,728] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 2.1736496e-33 3.9112967e-37 2.6313065e-36 7.5215668e-34], sum to 1.0000
[2019-04-02 18:44:34,746] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7425
[2019-04-02 18:44:34,753] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1839008.568807855 W.
[2019-04-02 18:44:34,759] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.66666666666666, 79.0, 1.0, 2.0, 0.9857797724532709, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.9260426156618, 1839008.568807855, 1839008.568807855, 376131.5031035549], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4354800.0000, 
sim time next is 4355400.0000, 
raw observation next is [27.83333333333334, 79.0, 1.0, 2.0, 1.02, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 7.216007352165379, 6.9112, 121.9248769372693, 2034327.79446811, 1878240.679078287, 383041.2790070374], 
processed observation next is [1.0, 0.391304347826087, 0.58641975308642, 0.79, 1.0, 1.0, 1.0238095238095237, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.9972168686025908, 0.03048073521653789, 0.0, 0.8094543899278298, 0.7265456408814679, 0.6708002425279597, 0.7366178442443027], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0713013], dtype=float32), -0.8164012]. 
=============================================
[2019-04-02 18:44:39,962] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:44:39,972] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4759
[2019-04-02 18:44:39,978] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.83333333333334, 99.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8866316659594713, 6.911199999999999, 6.9112, 121.9260426156618, 650010.2332639744, 650010.2332639749, 173938.1145375005], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4431000.0000, 
sim time next is 4431600.0000, 
raw observation next is [23.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9109642540221892, 6.911200000000001, 6.9112, 121.9260426156618, 664869.4785638442, 664869.4785638438, 177758.5458032593], 
processed observation next is [0.0, 0.30434782608695654, 0.4074074074074074, 1.0, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8887053175277365, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.23745338520137294, 0.23745338520137277, 0.3418433573139602], 
reward next is 0.6582, 
noisyNet noise sample is [array([0.76134205], dtype=float32), 0.19937721]. 
=============================================
[2019-04-02 18:44:42,250] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 3.9991646e-30 0.0000000e+00 6.4839116e-37 3.5920566e-37], sum to 1.0000
[2019-04-02 18:44:42,263] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0683
[2019-04-02 18:44:42,271] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 751651.1553026389 W.
[2019-04-02 18:44:42,276] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.33333333333334, 66.66666666666667, 1.0, 2.0, 0.3297633094101342, 1.0, 1.0, 0.3297633094101342, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 751651.1553026389, 751651.1553026394, 187756.8154335274], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4458000.0000, 
sim time next is 4458600.0000, 
raw observation next is [29.5, 67.5, 1.0, 2.0, 0.338210982085585, 1.0, 2.0, 0.338210982085585, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 770916.1730125907, 770916.1730125912, 189883.6287019516], 
processed observation next is [0.0, 0.6086956521739131, 0.6481481481481481, 0.675, 1.0, 1.0, 0.21215593105426783, 1.0, 1.0, 0.21215593105426783, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.27532720464735383, 0.275327204647354, 0.36516082442682996], 
reward next is 0.6348, 
noisyNet noise sample is [array([0.8753229], dtype=float32), -1.9938068]. 
=============================================
[2019-04-02 18:44:45,050] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 4.2604872e-36 0.0000000e+00 0.0000000e+00 3.1872803e-38], sum to 1.0000
[2019-04-02 18:44:45,059] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4459
[2019-04-02 18:44:45,071] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 725483.9176268862 W.
[2019-04-02 18:44:45,080] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.06666666666667, 69.66666666666667, 1.0, 2.0, 0.2121924755101175, 1.0, 2.0, 0.2121924755101175, 1.0, 1.0, 0.3378173316385785, 6.9112, 6.9112, 121.94756008, 725483.9176268862, 725483.9176268862, 224618.6991745352], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4641600.0000, 
sim time next is 4642200.0000, 
raw observation next is [28.8, 72.0, 1.0, 2.0, 0.3176352604720868, 0.0, 1.0, 0.0, 1.0, 2.0, 0.5056856793297977, 6.9112, 6.9112, 121.9260426156618, 723993.8424038077, 723993.8424038077, 200046.3984774069], 
processed observation next is [1.0, 0.7391304347826086, 0.6222222222222222, 0.72, 1.0, 1.0, 0.1876610243715319, 0.0, 0.5, -0.1904761904761905, 1.0, 1.0, 0.38210709916224705, 0.0, 0.0, 0.8094621288201359, 0.2585692294299313, 0.2585692294299313, 0.38470461245655174], 
reward next is 0.6153, 
noisyNet noise sample is [array([0.439351], dtype=float32), -0.19787425]. 
=============================================
[2019-04-02 18:44:46,054] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 2.8507510e-34 1.8809853e-34 7.9570270e-36], sum to 1.0000
[2019-04-02 18:44:46,064] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2817
[2019-04-02 18:44:46,075] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 696792.9484091623 W.
[2019-04-02 18:44:46,082] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 94.0, 1.0, 2.0, 0.3046760050177069, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4851436598352342, 6.911199999999999, 6.9112, 121.9260426156618, 696792.9484091623, 696792.9484091628, 196467.7988381391], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4557600.0000, 
sim time next is 4558200.0000, 
raw observation next is [24.0, 94.00000000000001, 1.0, 2.0, 0.6056786797442228, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 697548.339781623, 697548.339781623, 159412.941523015], 
processed observation next is [0.0, 0.782608695652174, 0.4444444444444444, 0.9400000000000002, 1.0, 1.0, 0.5305698568383606, 0.0, 1.0, -0.1904761904761905, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288201359, 0.24912440706486536, 0.24912440706486536, 0.30656334908272115], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.741784], dtype=float32), 0.54900473]. 
=============================================
[2019-04-02 18:44:49,768] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.000000e+00 1.586962e-32 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-02 18:44:49,782] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9760
[2019-04-02 18:44:49,792] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 697144.6726107671 W.
[2019-04-02 18:44:49,797] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.75, 95.83333333333334, 1.0, 2.0, 0.6036532239367228, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 697144.6726107671, 697144.6726107671, 159153.3869948032], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4553400.0000, 
sim time next is 4554000.0000, 
raw observation next is [23.9, 95.0, 1.0, 2.0, 0.3061332227973791, 1.0, 1.0, 0.3061332227973791, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 698554.4250720482, 698554.4250720487, 181978.5285725931], 
processed observation next is [0.0, 0.7391304347826086, 0.4407407407407407, 0.95, 1.0, 1.0, 0.17396812237783224, 1.0, 0.5, 0.17396812237783224, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.24948372324001722, 0.24948372324001739, 0.34995870879344826], 
reward next is 0.6500, 
noisyNet noise sample is [array([-0.5498107], dtype=float32), 0.861208]. 
=============================================
[2019-04-02 18:44:49,813] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[50.40752 ]
 [52.066395]
 [56.58981 ]
 [56.91844 ]
 [56.70325 ]], R is [[49.15196609]
 [49.35438156]
 [49.55593491]
 [49.70801163]
 [49.86027527]].
[2019-04-02 18:44:55,617] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 1.4615623e-36 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:44:55,625] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1112
[2019-04-02 18:44:55,633] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1656845.245608443 W.
[2019-04-02 18:44:55,639] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.66666666666667, 88.16666666666666, 1.0, 2.0, 0.8262003882100295, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.9260426156618, 1656845.245608443, 1656845.245608443, 341496.4117317277], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4791000.0000, 
sim time next is 4791600.0000, 
raw observation next is [26.0, 87.0, 1.0, 2.0, 0.4986300394050555, 1.0, 1.0, 0.4986300394050555, 1.0, 2.0, 0.7938352619795109, 6.911199999999999, 6.9112, 121.94756008, 1705881.53813815, 1705881.53813815, 342917.1497636063], 
processed observation next is [1.0, 0.4782608695652174, 0.5185185185185185, 0.87, 1.0, 1.0, 0.4031309992917328, 1.0, 0.5, 0.4031309992917328, 1.0, 1.0, 0.7422940774743886, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.6092434064779108, 0.6092434064779108, 0.6594560572377044], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.18559733], dtype=float32), -0.12479071]. 
=============================================
[2019-04-02 18:44:58,516] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-02 18:44:58,518] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 18:44:58,519] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 18:44:58,521] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 18:44:58,521] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 18:44:58,520] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:44:58,522] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 18:44:58,522] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:44:58,523] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:44:58,524] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:44:58,527] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:44:58,549] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run23
[2019-04-02 18:44:58,550] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run23
[2019-04-02 18:44:58,569] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run23
[2019-04-02 18:44:58,589] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run23
[2019-04-02 18:44:58,630] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run23
[2019-04-02 18:45:05,712] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.26154503], dtype=float32), -0.14845933]
[2019-04-02 18:45:05,714] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [29.0, 31.0, 1.0, 2.0, 0.6780672264384291, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 860213.3872502315, 860213.3872502315, 175009.5190974664]
[2019-04-02 18:45:05,714] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 18:45:05,716] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.0000000e+00 3.2243483e-37 8.6856234e-33 2.4899572e-38 1.0985857e-32], sampled 0.9473323015095193
[2019-04-02 18:45:05,717] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 860213.3872502315 W.
[2019-04-02 18:45:28,681] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.26154503], dtype=float32), -0.14845933]
[2019-04-02 18:45:28,683] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [19.49414891, 74.080820615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5012568212737584, 6.9112, 6.9112, 121.9260426156618, 362986.5213899825, 362986.5213899825, 117019.5082396377]
[2019-04-02 18:45:28,683] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 18:45:28,686] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.024205021593245668
[2019-04-02 18:45:48,678] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.26154503], dtype=float32), -0.14845933]
[2019-04-02 18:45:48,679] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [28.8, 82.33333333333334, 1.0, 2.0, 0.7300982595247795, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 832124.1000003945, 832124.1000003945, 181928.3233994239]
[2019-04-02 18:45:48,682] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 18:45:48,686] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.0000000e+00 3.0507258e-22 0.0000000e+00 1.7308595e-38 7.2034918e-33], sampled 0.3691793545619845
[2019-04-02 18:45:48,687] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 832124.1000003945 W.
[2019-04-02 18:45:56,482] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.26154503], dtype=float32), -0.14845933]
[2019-04-02 18:45:56,487] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [33.25, 52.0, 1.0, 2.0, 0.9801389694719538, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.02229743250476, 6.9112, 121.9255790134176, 1174246.999532573, 1117355.400635376, 235969.303784569]
[2019-04-02 18:45:56,487] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 18:45:56,490] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6477650531181487
[2019-04-02 18:45:56,490] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1174246.999532573 W.
[2019-04-02 18:46:08,606] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.26154503], dtype=float32), -0.14845933]
[2019-04-02 18:46:08,610] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [25.06666666666667, 91.0, 1.0, 2.0, 0.9776512324221222, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.006969269801191, 6.9112, 121.9255343989501, 1163553.837705666, 1114511.616227735, 235372.4406358727]
[2019-04-02 18:46:08,610] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 18:46:08,613] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.0000000e+00 1.3936915e-28 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.9976345888016512
[2019-04-02 18:46:08,613] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1163553.837705666 W.
[2019-04-02 18:46:10,879] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.26154503], dtype=float32), -0.14845933]
[2019-04-02 18:46:10,883] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [27.1, 83.0, 1.0, 2.0, 0.7063758447674884, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 805072.4531963737, 805072.4531963737, 177354.4338556909]
[2019-04-02 18:46:10,884] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 18:46:10,886] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0523419e-37], sampled 0.9936327857635374
[2019-04-02 18:46:10,888] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 805072.4531963737 W.
[2019-04-02 18:46:11,745] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.26154503], dtype=float32), -0.14845933]
[2019-04-02 18:46:11,746] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [23.67391829, 92.80054074166668, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9203485370639156, 6.9112, 6.9112, 121.9260426156618, 671329.8969106623, 671329.8969106623, 179093.1250225544]
[2019-04-02 18:46:11,746] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 18:46:11,748] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.38635279954698554
[2019-04-02 18:46:20,273] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.26154503], dtype=float32), -0.14845933]
[2019-04-02 18:46:20,275] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [31.0, 73.33333333333334, 1.0, 2.0, 0.809659128287883, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 922857.6078973552, 922857.6078973549, 197954.845623841]
[2019-04-02 18:46:20,279] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 18:46:20,282] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.08484529621284775
[2019-04-02 18:46:20,283] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 922857.6078973552 W.
[2019-04-02 18:47:05,233] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.26154503], dtype=float32), -0.14845933]
[2019-04-02 18:47:05,234] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [21.2, 79.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6328530642710185, 6.911200000000001, 6.9112, 121.9260426156618, 470918.9867755907, 470918.9867755902, 134438.3205353153]
[2019-04-02 18:47:05,234] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 18:47:05,237] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6512147082773692
[2019-04-02 18:47:06,165] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8633.7982 2219159744.7191 543.0000
[2019-04-02 18:47:06,187] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7841.5838 2529739775.4178 831.0000
[2019-04-02 18:47:06,214] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8559.5091 2258259887.1738 536.0000
[2019-04-02 18:47:06,261] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8404.3352 2292930052.2689 697.0000
[2019-04-02 18:47:06,318] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.26154503], dtype=float32), -0.14845933]
[2019-04-02 18:47:06,318] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [23.1, 72.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.656888967491643, 6.911199999999999, 6.9112, 121.9260426156618, 490608.1941069754, 490608.1941069758, 139017.3931334233]
[2019-04-02 18:47:06,319] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 18:47:06,320] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.3953442847306806
[2019-04-02 18:47:06,348] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8364.2563 2339423669.2034 616.0000
[2019-04-02 18:47:07,360] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 550000, evaluation results [550000.0, 7841.583789482413, 2529739775.4177794, 831.0, 8559.509129880978, 2258259887.1737776, 536.0, 8633.798202931406, 2219159744.719052, 543.0, 8364.256289480296, 2339423669.203431, 616.0, 8404.335235826124, 2292930052.2689223, 697.0]
[2019-04-02 18:47:08,428] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 2.6049512e-35 0.0000000e+00 0.0000000e+00 2.6814239e-36], sum to 1.0000
[2019-04-02 18:47:08,438] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4554
[2019-04-02 18:47:08,451] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1802645.085508314 W.
[2019-04-02 18:47:08,457] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.9539270997848402, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.9260426156618, 1802645.085508314, 1802645.085508314, 368865.9508616039], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4708800.0000, 
sim time next is 4709400.0000, 
raw observation next is [26.16666666666667, 89.0, 1.0, 2.0, 0.4772779932976801, 1.0, 1.0, 0.4772779932976801, 1.0, 2.0, 0.7598421091889752, 6.911199999999999, 6.9112, 121.94756008, 1632766.52671464, 1632766.526714641, 332576.8579182694], 
processed observation next is [1.0, 0.5217391304347826, 0.5246913580246916, 0.89, 1.0, 1.0, 0.3777118967829526, 1.0, 0.5, 0.3777118967829526, 1.0, 1.0, 0.6998026364862189, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.5831309023980857, 0.5831309023980861, 0.6395708806120566], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.5293826], dtype=float32), 1.0156778]. 
=============================================
[2019-04-02 18:47:14,986] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 1.2041406e-34 0.0000000e+00 0.0000000e+00 1.5270566e-33], sum to 1.0000
[2019-04-02 18:47:14,991] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4718
[2019-04-02 18:47:15,003] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 786949.5562183494 W.
[2019-04-02 18:47:15,009] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.2287241385116058, 1.0, 1.0, 0.2287241385116058, 1.0, 1.0, 0.3642859655691357, 6.911200000000002, 6.9112, 121.94756008, 786949.5562183494, 786949.5562183484, 230221.9568623158], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4939200.0000, 
sim time next is 4939800.0000, 
raw observation next is [24.08333333333333, 87.33333333333334, 1.0, 2.0, 0.6510749770816195, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 758811.382408923, 758811.382408923, 167892.9554558184], 
processed observation next is [1.0, 0.17391304347826086, 0.4475308641975307, 0.8733333333333334, 1.0, 1.0, 0.584613067954309, 0.0, 0.5, -0.1904761904761905, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2710040651460439, 0.2710040651460439, 0.32287106818426614], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.41506436], dtype=float32), 1.2165194]. 
=============================================
[2019-04-02 18:47:18,562] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.8059289e-01 2.4645349e-30 7.1940714e-01 8.5711875e-29 1.3493966e-17], sum to 1.0000
[2019-04-02 18:47:18,570] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2821
[2019-04-02 18:47:18,575] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.0, 94.0, 1.0, 2.0, 0.3683997803616075, 0.0, 2.0, 0.0, 1.0, 1.0, 0.5865044482789055, 6.911199999999999, 6.9112, 121.9260426156618, 839766.0427753803, 839766.0427753808, 214549.4278840596], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4851000.0000, 
sim time next is 4851600.0000, 
raw observation next is [25.0, 94.0, 1.0, 2.0, 0.3697706498835289, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5886869172039078, 6.911199999999999, 6.9112, 121.9260426156618, 842892.6534808674, 842892.6534808679, 214955.3660434015], 
processed observation next is [1.0, 0.13043478260869565, 0.48148148148148145, 0.94, 1.0, 1.0, 0.24972696414705825, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.48585864650488464, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.3010330905288812, 0.3010330905288814, 0.41337570392961825], 
reward next is 0.5866, 
noisyNet noise sample is [array([1.0641587], dtype=float32), 0.24450281]. 
=============================================
[2019-04-02 18:47:23,785] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [6.5795094e-02 2.9955822e-25 9.3420494e-01 8.3083424e-27 1.6217018e-17], sum to 1.0000
[2019-04-02 18:47:23,791] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4982
[2019-04-02 18:47:23,799] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.01666666666667, 88.5, 1.0, 2.0, 0.4468769892776159, 0.0, 2.0, 0.0, 1.0, 1.0, 0.711442720697454, 6.911199999999999, 6.9112, 121.9260426156618, 1018773.478657095, 1018773.478657096, 239017.883835533], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4907400.0000, 
sim time next is 4908000.0000, 
raw observation next is [29.03333333333333, 88.0, 1.0, 2.0, 0.4452787111408559, 0.0, 2.0, 0.0, 1.0, 2.0, 0.708898209851447, 6.9112, 6.9112, 121.9260426156618, 1015127.370278928, 1015127.370278928, 238494.8199088119], 
processed observation next is [1.0, 0.8260869565217391, 0.6308641975308641, 0.88, 1.0, 1.0, 0.3396175132629237, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6361227623143086, 0.0, 0.0, 0.8094621288201359, 0.36254548938533143, 0.36254548938533143, 0.4586438844400229], 
reward next is 0.5414, 
noisyNet noise sample is [array([-0.9818531], dtype=float32), 2.752947]. 
=============================================
[2019-04-02 18:47:23,815] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[30.494587]
 [30.65339 ]
 [31.088821]
 [31.082344]
 [30.98472 ]], R is [[30.74129868]
 [30.97423553]
 [31.24889565]
 [31.5156002 ]
 [31.71375656]].
[2019-04-02 18:47:25,334] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 2.0787644e-37 2.5317436e-32 0.0000000e+00 1.6267095e-33], sum to 1.0000
[2019-04-02 18:47:25,347] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0986
[2019-04-02 18:47:25,356] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 909799.3814937389 W.
[2019-04-02 18:47:25,364] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.41666666666667, 79.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9642639932541084, 7.292881833107613, 6.9112, 121.9247120646142, 909799.3814937389, 714346.3015655916, 181798.4260595281], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4943400.0000, 
sim time next is 4944000.0000, 
raw observation next is [24.33333333333334, 80.33333333333334, 1.0, 1.0, 0.5801043611438075, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.925812773514, 690208.8344541029, 690208.8344541029, 155981.0732676266], 
processed observation next is [1.0, 0.21739130434782608, 0.4567901234567903, 0.8033333333333335, 1.0, 0.5, 0.5001242394569136, 0.0, 1.0, -0.1904761904761905, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094606029072935, 0.2465031551621796, 0.2465031551621796, 0.29996360243774345], 
reward next is 0.7000, 
noisyNet noise sample is [array([0.28489417], dtype=float32), -0.22060527]. 
=============================================
[2019-04-02 18:47:25,388] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[36.811474]
 [37.071968]
 [36.527126]
 [34.67608 ]
 [31.491575]], R is [[34.09707642]
 [33.75610733]
 [34.08555222]
 [34.4072876 ]
 [34.72019196]].
[2019-04-02 18:47:37,578] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 4.2309147e-28 3.1046990e-20 1.6868082e-31 2.9316766e-27], sum to 1.0000
[2019-04-02 18:47:37,584] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5529
[2019-04-02 18:47:37,593] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 812770.2870439558 W.
[2019-04-02 18:47:37,597] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 100.0, 1.0, 2.0, 0.2377088207103988, 1.0, 1.0, 0.2377088207103988, 1.0, 2.0, 0.3784401842067738, 6.9112, 6.9112, 121.94756008, 812770.2870439558, 812770.2870439558, 233309.3076496959], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5111400.0000, 
sim time next is 5112000.0000, 
raw observation next is [25.0, 100.0, 1.0, 2.0, 0.7129248651477424, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 812540.4755192936, 812540.4755192936, 178606.1482038229], 
processed observation next is [0.0, 0.17391304347826086, 0.48148148148148145, 1.0, 1.0, 1.0, 0.6582438870806457, 0.0, 0.5, -0.1904761904761905, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288201359, 0.29019302697117627, 0.29019302697117627, 0.34347336193042866], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.071365], dtype=float32), 1.8613757]. 
=============================================
[2019-04-02 18:47:37,615] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[33.282936]
 [34.111534]
 [33.634712]
 [33.309803]
 [33.509205]], R is [[32.78603745]
 [33.00950241]
 [33.27339935]
 [33.53451157]
 [33.19916534]].
[2019-04-02 18:47:41,740] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9073203e-06 2.8954386e-27 9.9999809e-01 2.8972923e-29 5.2794693e-27], sum to 1.0000
[2019-04-02 18:47:41,748] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1635
[2019-04-02 18:47:41,753] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.8, 88.0, 1.0, 2.0, 0.3104739989250281, 0.0, 2.0, 0.0, 1.0, 1.0, 0.4944041686333959, 6.911199999999999, 6.9112, 121.9260426156618, 710634.396380594, 710634.3963805945, 198031.6927613097], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5180400.0000, 
sim time next is 5181000.0000, 
raw observation next is [24.83333333333334, 89.00000000000001, 1.0, 2.0, 0.3127412676612131, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4979083088850129, 6.911199999999999, 6.9112, 121.9260426156618, 713286.4575619393, 713286.4575619397, 198692.4975296815], 
processed observation next is [0.0, 1.0, 0.47530864197530887, 0.8900000000000001, 1.0, 1.0, 0.18183484245382509, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.3723853861062661, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2547451634149783, 0.2547451634149785, 0.38210095678784906], 
reward next is 0.6179, 
noisyNet noise sample is [array([1.0798671], dtype=float32), -0.8179546]. 
=============================================
[2019-04-02 18:47:41,772] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[56.287674]
 [56.322887]
 [56.198402]
 [55.477325]
 [56.094822]], R is [[56.17120743]
 [56.22866821]
 [56.353508  ]
 [55.78997421]
 [55.23207474]].
[2019-04-02 18:47:42,287] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.6925645e-03 7.5115478e-14 9.9830747e-01 3.3485942e-17 9.8266716e-12], sum to 1.0000
[2019-04-02 18:47:42,297] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6900
[2019-04-02 18:47:42,304] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.26666666666667, 63.66666666666667, 1.0, 2.0, 0.9490057806912241, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.911199999999999, 6.9112, 121.9260426156618, 1797026.966706501, 1797026.966706502, 367754.7549059064], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5323200.0000, 
sim time next is 5323800.0000, 
raw observation next is [28.28333333333333, 63.83333333333334, 1.0, 2.0, 0.9648733246509158, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.9260426156618, 1815141.291126341, 1815141.291126341, 371338.515590235], 
processed observation next is [1.0, 0.6086956521739131, 0.6030864197530863, 0.6383333333333334, 1.0, 1.0, 0.9581825293463283, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.9972168686025908, 0.0, 0.0, 0.8094621288201359, 0.6482647468308361, 0.6482647468308361, 0.7141125299812212], 
reward next is 0.2859, 
noisyNet noise sample is [array([-0.9786056], dtype=float32), 0.9564778]. 
=============================================
[2019-04-02 18:47:51,493] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 5.6487878e-31 4.5074653e-27 4.0037317e-36 5.6855826e-31], sum to 1.0000
[2019-04-02 18:47:51,511] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7456
[2019-04-02 18:47:51,520] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 798530.6818425391 W.
[2019-04-02 18:47:51,525] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.36666666666667, 91.66666666666667, 1.0, 1.0, 0.3361621046451835, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5400499230739457, 6.911200000000001, 6.9112, 121.925751547869, 798530.6818425391, 798530.6818425386, 204244.8590364461], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5296800.0000, 
sim time next is 5297400.0000, 
raw observation next is [22.25, 92.0, 1.0, 2.0, 0.3135814863682417, 1.0, 1.0, 0.3135814863682417, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260425269067, 741539.5859359847, 741539.5859359847, 184971.6057885559], 
processed observation next is [1.0, 0.30434782608695654, 0.37962962962962965, 0.92, 1.0, 1.0, 0.18283510281933535, 1.0, 0.5, 0.18283510281933535, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621282308944, 0.2648355664057088, 0.2648355664057088, 0.35571462651645364], 
reward next is 0.6443, 
noisyNet noise sample is [array([-0.84953123], dtype=float32), -0.5915832]. 
=============================================
[2019-04-02 18:47:52,752] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 5.1352148e-19 1.6312470e-24 1.1451582e-32 3.7913851e-26], sum to 1.0000
[2019-04-02 18:47:52,766] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6908
[2019-04-02 18:47:52,771] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.36666666666667, 67.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9248495082583685, 6.9112, 6.9112, 121.9260426156618, 665724.5852588862, 665724.5852588862, 180954.0116404076], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5334000.0000, 
sim time next is 5334600.0000, 
raw observation next is [28.33333333333334, 67.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9420241617365464, 6.911200000000001, 6.9112, 121.9260426156618, 678167.7190721342, 678167.7190721338, 183314.4550123694], 
processed observation next is [1.0, 0.7391304347826086, 0.6049382716049385, 0.6783333333333335, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.9275302021706829, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.24220275681147652, 0.24220275681147635, 0.3525277981007104], 
reward next is 0.6475, 
noisyNet noise sample is [array([-1.7635111], dtype=float32), -0.26580405]. 
=============================================
[2019-04-02 18:48:05,372] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 2.3065435e-16 6.8759096e-12 3.2607999e-15 2.2729987e-08], sum to 1.0000
[2019-04-02 18:48:05,381] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0215
[2019-04-02 18:48:05,391] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 784576.1556248055 W.
[2019-04-02 18:48:05,395] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.3, 92.0, 1.0, 2.0, 0.3442007291627917, 1.0, 1.0, 0.3442007291627917, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 784576.1556248055, 784576.155624805, 191406.1304789273], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5548800.0000, 
sim time next is 5549400.0000, 
raw observation next is [25.3, 91.5, 1.0, 2.0, 0.6849143019420348, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 780599.7943653623, 780599.7943653618, 173305.4920607787], 
processed observation next is [1.0, 0.21739130434782608, 0.49259259259259264, 0.915, 1.0, 1.0, 0.6248979785024225, 0.0, 0.5, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.27878564084477225, 0.2787856408447721, 0.3332797924245744], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.5464647], dtype=float32), 0.38722917]. 
=============================================
[2019-04-02 18:48:07,042] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 1.5338992e-33 4.7081550e-10 1.7727336e-27 1.0464240e-17], sum to 1.0000
[2019-04-02 18:48:07,051] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4103
[2019-04-02 18:48:07,061] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 782486.3044003996 W.
[2019-04-02 18:48:07,068] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.76666666666667, 91.16666666666667, 1.0, 2.0, 0.686568720152628, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 782486.3044003996, 782486.3044003996, 173616.6075531187], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5530200.0000, 
sim time next is 5530800.0000, 
raw observation next is [25.73333333333333, 91.33333333333334, 1.0, 2.0, 0.6860379858818818, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 781881.1150001911, 781881.1150001911, 173517.3789222603], 
processed observation next is [1.0, 0.0, 0.5086419753086419, 0.9133333333333334, 1.0, 1.0, 0.6262356974784307, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2792432553572111, 0.2792432553572111, 0.3336872671581929], 
reward next is 0.6663, 
noisyNet noise sample is [array([1.4681894], dtype=float32), 1.1480583]. 
=============================================
[2019-04-02 18:48:12,737] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 3.1419690e-34 8.1104594e-12 1.6110378e-31 4.4589224e-23], sum to 1.0000
[2019-04-02 18:48:12,752] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3928
[2019-04-02 18:48:12,763] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 703271.485554846 W.
[2019-04-02 18:48:12,772] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.66666666666667, 98.0, 1.0, 2.0, 0.2056986716727773, 1.0, 2.0, 0.2056986716727773, 1.0, 2.0, 0.3274789844411066, 6.9112, 6.9112, 121.94756008, 703271.485554846, 703271.485554846, 222465.1167711642], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5613600.0000, 
sim time next is 5614200.0000, 
raw observation next is [23.65, 98.0, 1.0, 2.0, 0.3073062839557785, 1.0, 2.0, 0.3073062839557785, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 700439.996717607, 700439.9967176075, 182223.1579921301], 
processed observation next is [1.0, 1.0, 0.4314814814814814, 0.98, 1.0, 1.0, 0.1753646237568792, 1.0, 1.0, 0.1753646237568792, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.25015714168485964, 0.2501571416848598, 0.3504291499848656], 
reward next is 0.6496, 
noisyNet noise sample is [array([-1.5065032], dtype=float32), -0.44785118]. 
=============================================
[2019-04-02 18:48:13,902] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-02 18:48:13,905] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 18:48:13,907] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 18:48:13,907] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:48:13,909] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:48:13,910] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 18:48:13,908] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 18:48:13,912] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:48:13,913] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:48:13,913] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 18:48:13,915] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:48:13,935] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run24
[2019-04-02 18:48:13,935] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run24
[2019-04-02 18:48:13,979] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run24
[2019-04-02 18:48:13,999] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run24
[2019-04-02 18:48:14,033] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run24
[2019-04-02 18:48:19,837] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.37762862], dtype=float32), -0.2017111]
[2019-04-02 18:48:19,838] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [30.91666666666666, 25.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5905567774847038, 6.9112, 6.9112, 121.9260426156618, 433917.1370305573, 433917.1370305573, 126979.126423944]
[2019-04-02 18:48:19,838] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 18:48:19,841] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9811082479565755
[2019-04-02 18:48:38,931] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.37762862], dtype=float32), -0.2017111]
[2019-04-02 18:48:38,933] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [22.66666666666667, 69.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6190265451630821, 6.911200000000001, 6.9112, 121.9260426156618, 460425.3779187032, 460425.3779187027, 132913.656116983]
[2019-04-02 18:48:38,934] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 18:48:38,938] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.2136442778161407
[2019-04-02 18:48:47,070] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.37762862], dtype=float32), -0.2017111]
[2019-04-02 18:48:47,071] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [20.73333333333333, 82.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6075841883110827, 6.9112, 6.9112, 121.9260426156618, 451937.5025542168, 451937.5025542168, 131829.5676360938]
[2019-04-02 18:48:47,071] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 18:48:47,074] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8292908572442114
[2019-04-02 18:48:49,747] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.37762862], dtype=float32), -0.2017111]
[2019-04-02 18:48:49,748] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [27.63552876, 96.57759412, 1.0, 2.0, 0.7450037044477409, 1.0, 2.0, 0.6858665142003053, 1.0, 2.0, 0.9977734948820727, 6.911200000000001, 6.9112, 121.94756008, 2347284.838764932, 2347284.838764932, 441552.2612985806]
[2019-04-02 18:48:49,750] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 18:48:49,753] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9680033653357577
[2019-04-02 18:48:49,755] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2347284.838764932 W.
[2019-04-02 18:50:01,902] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.37762862], dtype=float32), -0.2017111]
[2019-04-02 18:50:01,902] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [27.66666666666667, 68.33333333333334, 1.0, 2.0, 0.604516997064384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 699406.5902686396, 699406.5902686391, 159362.1092824748]
[2019-04-02 18:50:01,903] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 18:50:01,905] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.000000e+00 0.000000e+00 2.225525e-20 7.745809e-33 6.301204e-29], sampled 0.18345195772263811
[2019-04-02 18:50:01,905] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 699406.5902686396 W.
[2019-04-02 18:50:16,735] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.37762862], dtype=float32), -0.2017111]
[2019-04-02 18:50:16,737] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [28.40112337, 65.12899529333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9009330000722189, 6.911199999999999, 6.9112, 121.9260426156618, 655352.5293856993, 655352.5293856997, 176789.8880423797]
[2019-04-02 18:50:16,740] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 18:50:16,746] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8275151015360669
[2019-04-02 18:50:20,743] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8559.9526 2258470246.9372 534.0000
[2019-04-02 18:50:21,229] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7837.4486 2531918341.6839 831.0000
[2019-04-02 18:50:21,266] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8404.1057 2293049392.0276 697.0000
[2019-04-02 18:50:21,423] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8355.4002 2340051811.0628 616.0000
[2019-04-02 18:50:21,554] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8633.7484 2219185658.3071 543.0000
[2019-04-02 18:50:22,571] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 575000, evaluation results [575000.0, 7837.4486307110365, 2531918341.683872, 831.0, 8559.952623937786, 2258470246.937164, 534.0, 8633.748369108142, 2219185658.307149, 543.0, 8355.400189098867, 2340051811.0628467, 616.0, 8404.105736290283, 2293049392.02756, 697.0]
[2019-04-02 18:50:24,461] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 5.7007225e-30 9.5581375e-30 3.8203156e-28 3.9944210e-31], sum to 1.0000
[2019-04-02 18:50:24,471] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1022
[2019-04-02 18:50:24,481] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 687174.5458160578 W.
[2019-04-02 18:50:24,488] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.76666666666667, 96.33333333333334, 1.0, 1.0, 0.3014888975610516, 1.0, 1.0, 0.3014888975610516, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 687174.5458160578, 687174.5458160583, 180818.3822021093], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5636400.0000, 
sim time next is 5637000.0000, 
raw observation next is [23.83333333333333, 96.16666666666666, 1.0, 2.0, 0.2013266018052924, 1.0, 2.0, 0.2013266018052924, 1.0, 1.0, 0.3205185068236957, 6.9112, 6.9112, 121.94756008, 688316.929865933, 688316.929865933, 221028.4740285739], 
processed observation next is [0.0, 0.21739130434782608, 0.43827160493827144, 0.9616666666666666, 1.0, 1.0, 0.04919833548249096, 1.0, 1.0, 0.04919833548249096, 1.0, 0.5, 0.15064813352961962, 0.0, 0.0, 0.8096049824067558, 0.24582747495211893, 0.24582747495211893, 0.4250547577472575], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.62975216], dtype=float32), -0.44867286]. 
=============================================
[2019-04-02 18:50:24,499] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[55.435642]
 [60.016994]
 [60.061287]
 [59.829327]
 [59.78699 ]], R is [[52.26982117]
 [52.39939499]
 [52.52292252]
 [52.64612961]
 [52.76948166]].
[2019-04-02 18:50:27,946] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 1.1398127e-21 5.7896333e-38 1.0055466e-30], sum to 1.0000
[2019-04-02 18:50:27,957] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8810
[2019-04-02 18:50:27,963] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.1, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8802239426472724, 6.911199999999999, 6.9112, 121.9260426156618, 647286.0818777919, 647286.0818777924, 172633.367025956], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5702400.0000, 
sim time next is 5703000.0000, 
raw observation next is [23.0, 94.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8780448678508483, 6.9112, 6.9112, 121.9260426156618, 646126.5283136271, 646126.5283136271, 172235.7408286679], 
processed observation next is [0.0, 0.0, 0.4074074074074074, 0.9433333333333335, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8475560848135605, 0.0, 0.0, 0.8094621288201359, 0.23075947439772396, 0.23075947439772396, 0.3312225785166691], 
reward next is 0.6688, 
noisyNet noise sample is [array([-0.02150424], dtype=float32), 1.1753938]. 
=============================================
[2019-04-02 18:50:27,975] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[73.12121 ]
 [73.90043 ]
 [73.43663 ]
 [73.045235]
 [71.69977 ]], R is [[72.49098969]
 [72.43409729]
 [72.37653351]
 [72.31851196]
 [72.26026154]].
[2019-04-02 18:50:29,277] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 1.7352994e-26 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:50:29,285] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7317
[2019-04-02 18:50:29,291] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.1, 96.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7564900088719032, 6.9112, 6.9112, 121.9260426156618, 564507.6823005414, 564507.6823005414, 153148.8294129007], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5719200.0000, 
sim time next is 5719800.0000, 
raw observation next is [21.05, 96.16666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7515979826759154, 6.9112, 6.9112, 121.9260426156618, 561004.0369393249, 561004.0369393249, 152394.9363536657], 
processed observation next is [0.0, 0.17391304347826086, 0.3351851851851852, 0.9616666666666666, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6894974783448943, 0.0, 0.0, 0.8094621288201359, 0.20035858462118747, 0.20035858462118747, 0.29306718529551096], 
reward next is 0.7069, 
noisyNet noise sample is [array([1.4845368], dtype=float32), -0.2020913]. 
=============================================
[2019-04-02 18:50:29,781] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 1.9935104e-27 2.6503792e-19 4.9996599e-25 2.2941407e-19], sum to 1.0000
[2019-04-02 18:50:29,792] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6498
[2019-04-02 18:50:29,800] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1782872.268726789 W.
[2019-04-02 18:50:29,807] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.0, 57.0, 1.0, 2.0, 0.7816679186993766, 1.0, 2.0, 0.7816679186993766, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1782872.268726789, 1782872.268726789, 336235.7286595846], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6020400.0000, 
sim time next is 6021000.0000, 
raw observation next is [29.0, 56.5, 1.0, 2.0, 0.9238253969314038, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9891590107177601, 6.911199999999999, 6.9112, 121.9260426156618, 1778245.183225471, 1778245.183225472, 360859.073709494], 
processed observation next is [1.0, 0.6956521739130435, 0.6296296296296297, 0.565, 1.0, 1.0, 0.9093159487278616, 0.0, 0.5, -0.1904761904761905, 1.0, 0.5, 0.9864487633972002, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.6350875654376682, 0.6350875654376685, 0.6939597571336423], 
reward next is 0.3060, 
noisyNet noise sample is [array([0.5106549], dtype=float32), 1.3540821]. 
=============================================
[2019-04-02 18:50:29,824] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[57.04555 ]
 [55.79001 ]
 [56.140602]
 [55.515648]
 [54.328796]], R is [[56.82961655]
 [56.61471558]
 [56.04856873]
 [55.82486725]
 [55.64888   ]].
[2019-04-02 18:50:32,991] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 1.3908603e-36 2.7155953e-24 2.6980753e-37 4.8249610e-38], sum to 1.0000
[2019-04-02 18:50:33,003] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8542
[2019-04-02 18:50:33,015] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1123463.658741567 W.
[2019-04-02 18:50:33,024] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.0, 72.0, 1.0, 2.0, 0.4678003485781063, 0.0, 1.0, 0.0, 1.0, 2.0, 0.7555545146484377, 6.911199999999999, 6.9112, 121.9260426156618, 1123463.658741567, 1123463.658741568, 244278.3480787435], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5819400.0000, 
sim time next is 5820000.0000, 
raw observation next is [24.16666666666666, 70.0, 1.0, 2.0, 0.4702812663560498, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7612866133493355, 6.9112, 6.9112, 121.9260426156618, 1133722.857041697, 1133722.857041697, 244895.3640396321], 
processed observation next is [1.0, 0.34782608695652173, 0.45061728395061706, 0.7, 1.0, 1.0, 0.3693824599476783, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.7016082666866693, 0.0, 0.0, 0.8094621288201359, 0.4049010203720346, 0.4049010203720346, 0.4709526231531387], 
reward next is 0.5290, 
noisyNet noise sample is [array([0.428381], dtype=float32), 0.82497823]. 
=============================================
[2019-04-02 18:50:33,042] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[60.221447]
 [62.45361 ]
 [68.25702 ]
 [68.52529 ]
 [68.56667 ]], R is [[59.15080261]
 [58.55929565]
 [58.49531174]
 [58.57125092]
 [58.67749786]].
[2019-04-02 18:50:35,653] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 1.6605011e-35 0.0000000e+00 3.7282904e-35 5.7903768e-33], sum to 1.0000
[2019-04-02 18:50:35,666] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1895
[2019-04-02 18:50:35,676] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1864586.202910862 W.
[2019-04-02 18:50:35,684] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.6, 51.0, 1.0, 2.0, 0.8174565520229619, 1.0, 2.0, 0.8174565520229619, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260425563673, 1864586.202910862, 1864586.202910862, 351018.9447219725], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6098400.0000, 
sim time next is 6099000.0000, 
raw observation next is [30.56666666666667, 51.33333333333333, 1.0, 2.0, 0.839972827973006, 1.0, 2.0, 0.839972827973006, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156437, 1916000.030265994, 1916000.030265994, 360547.0904668117], 
processed observation next is [1.0, 0.6086956521739131, 0.6876543209876544, 0.5133333333333333, 1.0, 1.0, 0.8094914618726262, 1.0, 1.0, 0.8094914618726262, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288200158, 0.6842857250949979, 0.6842857250949979, 0.6933597893592532], 
reward next is 0.3066, 
noisyNet noise sample is [array([0.42343065], dtype=float32), -0.5363992]. 
=============================================
[2019-04-02 18:50:35,698] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[67.38904]
 [66.83364]
 [67.51676]
 [68.41906]
 [68.67621]], R is [[67.90592194]
 [67.55182648]
 [66.87631226]
 [66.20755005]
 [65.83023071]].
[2019-04-02 18:50:41,605] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:50:41,617] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2766
[2019-04-02 18:50:41,624] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.4, 74.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.839241679941598, 6.9112, 6.9112, 121.9260426156618, 620756.4979903961, 620756.4979903961, 166297.5730581726], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6040800.0000, 
sim time next is 6041400.0000, 
raw observation next is [25.28333333333333, 74.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8399751620142728, 6.911199999999999, 6.9112, 121.9260426156618, 621396.820912825, 621396.8209128254, 166354.7863626111], 
processed observation next is [1.0, 0.9565217391304348, 0.49197530864197525, 0.7466666666666667, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.7999689525178411, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.22192743604029463, 0.2219274360402948, 0.31991305069732906], 
reward next is 0.6801, 
noisyNet noise sample is [array([-0.15785512], dtype=float32), 0.8670188]. 
=============================================
[2019-04-02 18:50:47,902] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 1.0457887e-26 1.3997665e-19 7.1614056e-26 6.6227778e-29], sum to 1.0000
[2019-04-02 18:50:47,912] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2802
[2019-04-02 18:50:47,921] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 737660.0597458855 W.
[2019-04-02 18:50:47,927] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.55, 63.5, 1.0, 2.0, 0.6472562164406247, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 737660.0597458855, 737660.0597458855, 166397.7650316935], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6273000.0000, 
sim time next is 6273600.0000, 
raw observation next is [29.6, 63.33333333333334, 1.0, 2.0, 0.3249457618378266, 0.0, 2.0, 0.0, 1.0, 1.0, 0.5173242355904633, 6.911199999999999, 6.9112, 121.9260426156618, 740664.8977279236, 740664.8977279239, 202069.5427644241], 
processed observation next is [0.0, 0.6086956521739131, 0.6518518518518519, 0.6333333333333334, 1.0, 1.0, 0.1963640021878888, 0.0, 1.0, -0.1904761904761905, 1.0, 0.5, 0.396655294488079, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2645231777599727, 0.26452317775997286, 0.38859527454696946], 
reward next is 0.6114, 
noisyNet noise sample is [array([-0.5634085], dtype=float32), -0.55842954]. 
=============================================
[2019-04-02 18:50:52,984] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 3.1430543e-30 3.1130567e-37 7.5526108e-36 8.1858773e-28], sum to 1.0000
[2019-04-02 18:50:52,995] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9966
[2019-04-02 18:50:53,002] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.9, 88.5, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8477696372379415, 6.911199999999999, 6.9112, 121.9260426156618, 628448.2191949001, 628448.2191949006, 166839.3256476665], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6060600.0000, 
sim time next is 6061200.0000, 
raw observation next is [22.96666666666667, 88.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8440333034112261, 6.9112, 6.9112, 121.9260426156618, 626560.0570599353, 626560.0570599353, 165981.2717263658], 
processed observation next is [1.0, 0.13043478260869565, 0.4061728395061729, 0.8833333333333334, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8050416292640324, 0.0, 0.0, 0.8094621288201359, 0.2237714489499769, 0.2237714489499769, 0.31919475331993424], 
reward next is 0.6808, 
noisyNet noise sample is [array([0.40412262], dtype=float32), 1.1967]. 
=============================================
[2019-04-02 18:50:53,018] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:50:53,035] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8284
[2019-04-02 18:50:53,041] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.31666666666666, 86.16666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8102115818987297, 6.9112, 6.9112, 121.9260426156618, 601111.1817791744, 601111.1817791744, 161927.2935096325], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6051000.0000, 
sim time next is 6051600.0000, 
raw observation next is [23.2, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8094703724735696, 6.911199999999999, 6.9112, 121.9260426156618, 600587.236346951, 600587.2363469515, 161822.4290058094], 
processed observation next is [1.0, 0.043478260869565216, 0.4148148148148148, 0.87, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.7618379655919618, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.21449544155248249, 0.21449544155248265, 0.3111969788573257], 
reward next is 0.6888, 
noisyNet noise sample is [array([-0.01480157], dtype=float32), -1.4371723]. 
=============================================
[2019-04-02 18:50:53,514] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:50:53,525] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3258
[2019-04-02 18:50:53,532] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.53333333333333, 85.33333333333334, 1.0, 1.0, 0.1910433232668182, 1.0, 1.0, 0.1910433232668182, 1.0, 2.0, 0.3053077373098125, 6.9112, 6.9112, 121.94756008, 670311.2733841727, 670311.2733841727, 217583.390750581], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6067200.0000, 
sim time next is 6067800.0000, 
raw observation next is [23.6, 85.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.8924451799718622, 6.911199999999999, 6.9112, 121.926042607149, 658426.3809994138, 658426.3809994143, 173637.7688719103], 
processed observation next is [1.0, 0.21739130434782608, 0.4296296296296297, 0.85, 0.0, 0.5, -0.1904761904761905, 0.0, 0.5, -0.1904761904761905, 1.0, 1.0, 0.8655564749648278, -8.881784197001253e-17, 0.0, 0.8094621287636198, 0.23515227892836207, 0.23515227892836224, 0.3339187862921352], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.44501847], dtype=float32), 0.43722865]. 
=============================================
[2019-04-02 18:50:54,409] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:50:54,416] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4386
[2019-04-02 18:50:54,425] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.93333333333334, 83.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8356950629105784, 6.911199999999999, 6.9112, 121.9260426156618, 618765.0352844838, 618765.0352844843, 165622.7260097814], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6070800.0000, 
sim time next is 6071400.0000, 
raw observation next is [24.0, 83.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8229813136144559, 6.911199999999999, 6.9112, 121.9260426156618, 609231.5549521815, 609231.554952182, 164089.3178183742], 
processed observation next is [1.0, 0.2608695652173913, 0.4444444444444444, 0.83, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.77872664201807, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.21758269819720769, 0.21758269819720785, 0.3155563804199504], 
reward next is 0.6844, 
noisyNet noise sample is [array([-0.5798725], dtype=float32), 0.957018]. 
=============================================
[2019-04-02 18:50:54,934] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.000000e+00 4.853546e-27 0.000000e+00 4.171393e-36 8.181134e-35], sum to 1.0000
[2019-04-02 18:50:54,945] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7933
[2019-04-02 18:50:54,953] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1631003.902723556 W.
[2019-04-02 18:50:54,959] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.9, 60.33333333333334, 1.0, 2.0, 0.4767632261527484, 1.0, 2.0, 0.4767632261527484, 1.0, 2.0, 0.7590225831294483, 6.9112, 6.9112, 121.94756008, 1631003.902723556, 1631003.902723556, 332330.624365596], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6085200.0000, 
sim time next is 6085800.0000, 
raw observation next is [28.7, 61.5, 1.0, 2.0, 0.4897119294159138, 1.0, 2.0, 0.4897119294159138, 1.0, 2.0, 0.7796373404342319, 6.911200000000001, 6.9112, 121.94756008, 1675342.86067485, 1675342.860674849, 338568.2238769588], 
processed observation next is [1.0, 0.43478260869565216, 0.6185185185185185, 0.615, 1.0, 1.0, 0.39251420168561163, 1.0, 1.0, 0.39251420168561163, 1.0, 1.0, 0.7245466755427898, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.5983367359553036, 0.5983367359553032, 0.6510927382249208], 
reward next is 0.3489, 
noisyNet noise sample is [array([-1.2642281], dtype=float32), -0.8823859]. 
=============================================
[2019-04-02 18:51:00,453] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.00000000e+00 6.64142829e-35 0.00000000e+00 2.17218638e-37
 1.13857215e-32], sum to 1.0000
[2019-04-02 18:51:00,461] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9332
[2019-04-02 18:51:00,474] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1466440.623454947 W.
[2019-04-02 18:51:00,481] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.3, 72.0, 1.0, 2.0, 0.6401054534046176, 1.0, 2.0, 0.6401054534046176, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 1466440.623454947, 1466440.623454946, 282450.6632812305], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6170400.0000, 
sim time next is 6171000.0000, 
raw observation next is [26.48333333333333, 71.16666666666667, 1.0, 2.0, 0.6514273942765318, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9883734937267581, 6.911200000000001, 6.9112, 121.9260426156618, 1465064.391816644, 1465064.391816644, 306954.273077532], 
processed observation next is [1.0, 0.43478260869565216, 0.5364197530864196, 0.7116666666666667, 1.0, 1.0, 0.5850326122339665, 0.0, 0.5, -0.1904761904761905, 1.0, 0.5, 0.9854668671584477, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.5232372827916586, 0.5232372827916586, 0.5902966789952538], 
reward next is 0.4097, 
noisyNet noise sample is [array([-0.56060386], dtype=float32), -1.1172746]. 
=============================================
[2019-04-02 18:51:00,502] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[41.761005]
 [40.781715]
 [40.558968]
 [40.613144]
 [40.809677]], R is [[42.52951813]
 [42.56105042]
 [42.59777832]
 [42.57990646]
 [42.55077744]].
[2019-04-02 18:51:06,664] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:51:06,677] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7718
[2019-04-02 18:51:06,684] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 703619.4203341577 W.
[2019-04-02 18:51:06,692] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.3, 73.0, 1.0, 2.0, 0.308700562781185, 1.0, 1.0, 0.308700562781185, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 703619.4203341577, 703619.420334158, 182561.69836821], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6262200.0000, 
sim time next is 6262800.0000, 
raw observation next is [27.43333333333333, 72.33333333333333, 1.0, 2.0, 0.2066165061357566, 1.0, 2.0, 0.2066165061357566, 1.0, 1.0, 0.3289402067979514, 6.911199999999999, 6.9112, 121.94756008, 706410.9524360928, 706410.9524360932, 222768.0720182616], 
processed observation next is [0.0, 0.4782608695652174, 0.5716049382716049, 0.7233333333333333, 1.0, 1.0, 0.05549584063780548, 1.0, 1.0, 0.05549584063780548, 1.0, 0.5, 0.16117525849743922, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.2522896258700331, 0.2522896258700333, 0.4284001384966569], 
reward next is 0.5716, 
noisyNet noise sample is [array([0.0662846], dtype=float32), 0.83953166]. 
=============================================
[2019-04-02 18:51:07,231] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 9.9595193e-26 0.0000000e+00 1.1143619e-34], sum to 1.0000
[2019-04-02 18:51:07,245] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2655
[2019-04-02 18:51:07,258] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1549266.390416208 W.
[2019-04-02 18:51:07,262] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.7, 80.0, 1.0, 2.0, 0.4528923859541141, 1.0, 2.0, 0.4528923859541141, 1.0, 1.0, 0.7210194281142317, 6.9112, 6.9112, 121.94756008, 1549266.390416208, 1549266.390416208, 321026.3573368291], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6526200.0000, 
sim time next is 6526800.0000, 
raw observation next is [27.6, 80.0, 1.0, 2.0, 0.910955308266441, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 121.9260426156618, 1753590.196889689, 1753590.196889689, 359343.8878812733], 
processed observation next is [1.0, 0.5652173913043478, 0.5777777777777778, 0.8, 1.0, 1.0, 0.893994414602906, 0.0, 0.5, -0.1904761904761905, 1.0, 1.0, 0.9972168686025908, 0.0, 0.0, 0.8094621288201359, 0.626282213174889, 0.626282213174889, 0.6910459382332179], 
reward next is 0.3090, 
noisyNet noise sample is [array([-2.1379447], dtype=float32), -0.06158129]. 
=============================================
[2019-04-02 18:51:11,301] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:51:11,308] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7880
[2019-04-02 18:51:11,321] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.05, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8989428366563316, 6.911200000000001, 6.9112, 121.9260426156618, 659604.4912601677, 659604.4912601673, 175435.6913497594], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6323400.0000, 
sim time next is 6324000.0000, 
raw observation next is [24.03333333333333, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8956193500838812, 6.9112, 6.9112, 121.9260426156618, 657317.4800206803, 657317.4800206803, 174961.4323734151], 
processed observation next is [0.0, 0.17391304347826086, 0.4456790123456789, 0.88, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8695241876048515, 0.0, 0.0, 0.8094621288201359, 0.2347562428645287, 0.2347562428645287, 0.3364642930257983], 
reward next is 0.6635, 
noisyNet noise sample is [array([-0.03370588], dtype=float32), 0.43244007]. 
=============================================
[2019-04-02 18:51:11,346] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[67.709   ]
 [67.69568 ]
 [67.701065]
 [67.77085 ]
 [67.6997  ]], R is [[67.70570374]
 [67.69126892]
 [67.67627716]
 [67.66124725]
 [67.6462326 ]].
[2019-04-02 18:51:17,484] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.1966307e-05 7.1723500e-33 9.9997807e-01 2.0442170e-33 3.9042673e-28], sum to 1.0000
[2019-04-02 18:51:17,492] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6544
[2019-04-02 18:51:17,497] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.06666666666667, 75.33333333333334, 1.0, 2.0, 0.33841191344278, 0.0, 1.0, 0.0, 1.0, 2.0, 0.5387627875074892, 6.9112, 6.9112, 121.9260426156618, 771374.4052507262, 771374.4052507262, 205857.1559665738], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6387600.0000, 
sim time next is 6388200.0000, 
raw observation next is [27.95, 76.0, 1.0, 2.0, 0.3451042052137331, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5494171339595734, 6.911199999999999, 6.9112, 121.9260426156618, 786636.6088536849, 786636.6088536852, 207765.855394827], 
processed observation next is [0.0, 0.9565217391304348, 0.5907407407407407, 0.76, 1.0, 1.0, 0.22036214906396795, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.4367714174494667, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.28094164601917315, 0.28094164601917326, 0.3995497219131288], 
reward next is 0.6005, 
noisyNet noise sample is [array([0.3167116], dtype=float32), -1.1229175]. 
=============================================
[2019-04-02 18:51:18,944] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 9.8575056e-18 3.6378110e-22 1.6044791e-19 1.3258478e-13], sum to 1.0000
[2019-04-02 18:51:18,951] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2694
[2019-04-02 18:51:18,963] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1013746.368611475 W.
[2019-04-02 18:51:18,969] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.25, 30.0, 1.0, 2.0, 0.4041619174014845, 0.0, 1.0, 0.0, 1.0, 2.0, 0.6816372841933536, 6.911199999999999, 6.9112, 121.9260426156618, 1013746.368611475, 1013746.368611476, 220517.7367744454], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6701400.0000, 
sim time next is 6702000.0000, 
raw observation next is [29.4, 29.66666666666666, 1.0, 2.0, 0.7677819307899825, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 970971.5261205495, 970971.5261205495, 192824.722691857], 
processed observation next is [1.0, 0.5652173913043478, 0.6444444444444444, 0.29666666666666663, 1.0, 1.0, 0.723549917607122, 0.0, 1.0, -0.1904761904761905, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288201359, 0.3467755450430534, 0.3467755450430534, 0.3708167744074173], 
reward next is 0.6292, 
noisyNet noise sample is [array([-0.6653757], dtype=float32), -0.03648062]. 
=============================================
[2019-04-02 18:51:18,984] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[29.669106]
 [29.366724]
 [29.950811]
 [29.086649]
 [29.905424]], R is [[29.38825607]
 [29.0943737 ]
 [28.80343056]
 [28.51539612]
 [28.23024178]].
[2019-04-02 18:51:21,860] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:51:21,869] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2949
[2019-04-02 18:51:21,876] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.58333333333333, 78.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5464815770047998, 6.9112, 6.9112, 121.9260426156618, 399249.4979097585, 399249.4979097585, 122068.2158703416], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6745800.0000, 
sim time next is 6746400.0000, 
raw observation next is [19.5, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5445925774765537, 6.9112, 6.9112, 121.9260426156618, 397627.6317390676, 397627.6317390676, 121805.2850100304], 
processed observation next is [1.0, 0.08695652173913043, 0.2777777777777778, 0.79, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.430740721845692, 0.0, 0.0, 0.8094621288201359, 0.14200986847823843, 0.14200986847823843, 0.23424093271159693], 
reward next is 0.7658, 
noisyNet noise sample is [array([2.1791012], dtype=float32), -0.32946232]. 
=============================================
[2019-04-02 18:51:26,495] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [9.9983323e-01 5.2849819e-06 3.8386775e-38 5.8799455e-13 1.6145500e-04], sum to 1.0000
[2019-04-02 18:51:26,503] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9454
[2019-04-02 18:51:26,517] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1738492.844286371 W.
[2019-04-02 18:51:26,522] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.15, 80.83333333333333, 1.0, 2.0, 0.5081530758184903, 1.0, 2.0, 0.5081530758184903, 1.0, 2.0, 0.808996246093343, 6.9112, 6.9112, 121.94756008, 1738492.844286371, 1738492.844286371, 347608.6965917116], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6531000.0000, 
sim time next is 6531600.0000, 
raw observation next is [27.2, 80.66666666666667, 1.0, 2.0, 0.5363978797400731, 1.0, 2.0, 0.5363978797400731, 1.0, 2.0, 0.8539628938055471, 6.911199999999999, 6.9112, 121.94756008, 1835223.192033582, 1835223.192033583, 361812.5460400559], 
processed observation next is [1.0, 0.6086956521739131, 0.5629629629629629, 0.8066666666666668, 1.0, 1.0, 0.44809271397627753, 1.0, 1.0, 0.44809271397627753, 1.0, 1.0, 0.8174536172569338, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.6554368542977078, 0.6554368542977083, 0.6957933577693383], 
reward next is 0.3042, 
noisyNet noise sample is [array([-0.5308909], dtype=float32), 0.8423844]. 
=============================================
[2019-04-02 18:51:28,852] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-02 18:51:28,853] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 18:51:28,854] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:51:28,855] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 18:51:28,856] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 18:51:28,857] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:51:28,858] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 18:51:28,859] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:51:28,857] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 18:51:28,861] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:51:28,864] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:51:28,889] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run25
[2019-04-02 18:51:28,911] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run25
[2019-04-02 18:51:28,913] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run25
[2019-04-02 18:51:28,914] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run25
[2019-04-02 18:51:28,974] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run25
[2019-04-02 18:51:35,266] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.2712372], dtype=float32), -0.205386]
[2019-04-02 18:51:35,270] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [20.82398780666667, 56.12506744500001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7692161478239197, 6.911199999999999, 6.9112, 121.9260426156618, 549305.8541952141, 549305.8541952146, 133263.0575645276]
[2019-04-02 18:51:35,271] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 18:51:35,273] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7566662338386668
[2019-04-02 18:51:42,733] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.2712372], dtype=float32), -0.205386]
[2019-04-02 18:51:42,734] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [33.70098902, 32.84006182, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6978636762854595, 6.9112, 6.9112, 121.9260426156618, 520386.6150130571, 520386.6150130571, 146784.8728676868]
[2019-04-02 18:51:42,737] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 18:51:42,739] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7032549992195559
[2019-04-02 18:52:36,091] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.2712372], dtype=float32), -0.205386]
[2019-04-02 18:52:36,094] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [33.42176077666667, 27.29756471, 1.0, 2.0, 0.6055149694214851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260425992819, 746948.445253456, 746948.445253456, 161245.4100705534]
[2019-04-02 18:52:36,094] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 18:52:36,096] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.0000000e+00 1.2315519e-37 0.0000000e+00 4.9419123e-36 2.3568164e-29], sampled 0.32190045675852397
[2019-04-02 18:52:36,098] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 746948.445253456 W.
[2019-04-02 18:52:53,405] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.2712372], dtype=float32), -0.205386]
[2019-04-02 18:52:53,406] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [24.76666666666667, 84.66666666666667, 1.0, 2.0, 0.6970417686936371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 812201.6480906843, 812201.6480906843, 176450.0751678433]
[2019-04-02 18:52:53,408] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 18:52:53,411] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 2.4503966e-26 1.9105837e-35 1.7051296e-24], sampled 0.9301330249847694
[2019-04-02 18:52:53,411] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 812201.6480906843 W.
[2019-04-02 18:53:35,628] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7830.3656 2531435576.8830 820.0000
[2019-04-02 18:53:35,644] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8357.5537 2340254129.0490 608.0000
[2019-04-02 18:53:35,692] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8632.9278 2219184152.7797 543.0000
[2019-04-02 18:53:35,878] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8557.6745 2258392610.2713 536.0000
[2019-04-02 18:53:36,001] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8404.8258 2293061381.0563 697.0000
[2019-04-02 18:53:37,016] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 600000, evaluation results [600000.0, 7830.365602939279, 2531435576.8830175, 820.0, 8557.674496776692, 2258392610.2712617, 536.0, 8632.92780011743, 2219184152.779748, 543.0, 8357.553736104297, 2340254129.048953, 608.0, 8404.825762645214, 2293061381.056288, 697.0]
[2019-04-02 18:53:38,381] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:53:38,394] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5371
[2019-04-02 18:53:38,402] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.95, 49.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.625682069175264, 6.911199999999999, 6.9112, 121.9260426156618, 460631.6444091235, 460631.6444091239, 130621.1129554673], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6589800.0000, 
sim time next is 6590400.0000, 
raw observation next is [24.86666666666667, 49.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6093021351811286, 6.9112, 6.9112, 121.9260426156618, 448491.2238487536, 448491.2238487536, 129065.5517129291], 
processed observation next is [1.0, 0.2608695652173913, 0.47654320987654336, 0.4933333333333333, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5116276689764108, 0.0, 0.0, 0.8094621288201359, 0.16017543708884058, 0.16017543708884058, 0.2482029840633252], 
reward next is 0.7518, 
noisyNet noise sample is [array([1.4729491], dtype=float32), -0.63135135]. 
=============================================
[2019-04-02 18:53:45,001] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:53:45,007] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5038
[2019-04-02 18:53:45,012] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.96666666666667, 44.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5509997241810838, 6.9112, 6.9112, 121.9260426156618, 393428.4792506882, 393428.4792506882, 112742.6056900198], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6668400.0000, 
sim time next is 6669000.0000, 
raw observation next is [22.95, 44.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5470310471236842, 6.911200000000001, 6.9112, 121.9260426156618, 390594.0164774082, 390594.0164774077, 112015.1726301024], 
processed observation next is [1.0, 0.17391304347826086, 0.4055555555555555, 0.445, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.43378880890460525, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.13949786302764577, 0.1394978630276456, 0.2154137935194277], 
reward next is 0.7846, 
noisyNet noise sample is [array([1.8035132], dtype=float32), -0.30118906]. 
=============================================
[2019-04-02 18:53:45,036] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[71.51621 ]
 [71.598915]
 [71.73234 ]
 [71.73544 ]
 [71.78721 ]], R is [[71.50708008]
 [71.57519531]
 [71.64027405]
 [71.70418549]
 [71.76493835]].
[2019-04-02 18:53:46,110] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:53:46,120] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2427
[2019-04-02 18:53:46,125] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.83333333333334, 75.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5851956354454416, 6.9112, 6.9112, 121.9260426156618, 432035.0809444831, 432035.0809444831, 127556.7196608411], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6736800.0000, 
sim time next is 6737400.0000, 
raw observation next is [20.75, 75.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5831435021028053, 6.911199999999999, 6.9112, 121.9260426156618, 430237.2518710215, 430237.251871022, 127216.1952115153], 
processed observation next is [1.0, 1.0, 0.32407407407407407, 0.755, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.47892937762850657, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.15365616138250768, 0.15365616138250784, 0.24464652925291402], 
reward next is 0.7554, 
noisyNet noise sample is [array([0.7080501], dtype=float32), -0.12686734]. 
=============================================
[2019-04-02 18:53:48,396] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:53:48,407] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5710
[2019-04-02 18:53:48,416] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1166179.184305535 W.
[2019-04-02 18:53:48,421] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.46666666666667, 82.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9559505988530839, 7.79332170378769, 6.9112, 121.922879413214, 1166179.184305535, 714465.7482070562, 174506.3266038535], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7028400.0000, 
sim time next is 7029000.0000, 
raw observation next is [22.65, 82.0, 1.0, 1.0, 0.4583898475242643, 1.0, 1.0, 0.4583898475242643, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9255114160557, 1090741.920194316, 1090741.920194317, 224808.2424793816], 
processed observation next is [1.0, 0.34782608695652173, 0.3944444444444444, 0.82, 1.0, 0.5, 0.35522600895745754, 1.0, 0.5, 0.35522600895745754, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.80945860220716, 0.3895506857836843, 0.3895506857836846, 0.43232354322958], 
reward next is 0.5677, 
noisyNet noise sample is [array([-0.72757673], dtype=float32), 0.43467703]. 
=============================================
[2019-04-02 18:53:48,432] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[70.819725]
 [71.80987 ]
 [71.68318 ]
 [71.566154]
 [71.366516]], R is [[62.13874817]
 [61.51736069]
 [61.60263824]
 [61.69290543]
 [61.78458405]].
[2019-04-02 18:53:49,147] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:53:49,158] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8379
[2019-04-02 18:53:49,164] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.3, 53.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5722094429541205, 6.911200000000001, 6.9112, 121.9260426156618, 421795.8533348636, 421795.8533348631, 126033.4202435027], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6728400.0000, 
sim time next is 6729000.0000, 
raw observation next is [24.03333333333333, 54.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5758230259914953, 6.911200000000001, 6.9112, 121.9260426156618, 424588.0373353301, 424588.0373353297, 126423.9710608486], 
processed observation next is [1.0, 0.9130434782608695, 0.4456790123456789, 0.5466666666666667, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.46977878248936905, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.15163858476261788, 0.15163858476261774, 0.24312302127086272], 
reward next is 0.7569, 
noisyNet noise sample is [array([0.70200104], dtype=float32), 2.2976387]. 
=============================================
[2019-04-02 18:53:49,173] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[67.0295 ]
 [66.59662]
 [66.37163]
 [66.23785]
 [66.28348]], R is [[67.37482452]
 [67.45870972]
 [67.54212189]
 [67.62506866]
 [67.70747375]].
[2019-04-02 18:53:53,342] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.9777573e-01 3.4091650e-24 2.2243033e-03 1.4201489e-24 5.2080835e-12], sum to 1.0000
[2019-04-02 18:53:53,353] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8350
[2019-04-02 18:53:53,362] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1073552.902815406 W.
[2019-04-02 18:53:53,368] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.0, 52.00000000000001, 1.0, 2.0, 0.4488681871834974, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7233700115915278, 6.911199999999999, 6.9112, 121.9260426156618, 1073552.902815406, 1073552.902815407, 238237.1293890442], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6794400.0000, 
sim time next is 6795000.0000, 
raw observation next is [28.0, 52.5, 1.0, 2.0, 0.2855560373042781, 1.0, 1.0, 0.2855560373042781, 1.0, 2.0, 0.4578163957519471, 6.911200000000001, 6.9112, 121.94756008, 1012431.764680414, 1012431.764680413, 250385.6160659546], 
processed observation next is [1.0, 0.6521739130434783, 0.5925925925925926, 0.525, 1.0, 1.0, 0.14947147298128344, 1.0, 0.5, 0.14947147298128344, 1.0, 1.0, 0.32227049468993385, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.36158277310014786, 0.36158277310014747, 0.4815108001268357], 
reward next is 0.5185, 
noisyNet noise sample is [array([0.84088916], dtype=float32), 1.0001522]. 
=============================================
[2019-04-02 18:53:53,385] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[54.38156 ]
 [52.5689  ]
 [51.359844]
 [50.30884 ]
 [49.708973]], R is [[54.77177429]
 [54.76590729]
 [54.68700027]
 [54.61617279]
 [54.54584122]].
[2019-04-02 18:53:55,303] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:53:55,311] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5712
[2019-04-02 18:53:55,318] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.43333333333333, 73.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7685534169143218, 6.9112, 6.9112, 121.9260426156618, 572851.3072298738, 572851.3072298738, 155212.9578271173], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6815400.0000, 
sim time next is 6816000.0000, 
raw observation next is [24.36666666666667, 74.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7665675096064849, 6.9112, 6.9112, 121.9260426156618, 571272.1156407197, 571272.1156407197, 155055.2979198241], 
processed observation next is [1.0, 0.9130434782608695, 0.4580246913580248, 0.7433333333333334, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.708209387008106, 0.0, 0.0, 0.8094621288201359, 0.2040257555859713, 0.2040257555859713, 0.298183265230431], 
reward next is 0.7018, 
noisyNet noise sample is [array([-1.3831542], dtype=float32), 0.688948]. 
=============================================
[2019-04-02 18:53:55,337] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[68.89145 ]
 [68.539406]
 [67.91517 ]
 [66.85766 ]
 [64.81088 ]], R is [[68.83764648]
 [68.8507843 ]
 [68.86369324]
 [68.87722778]
 [68.89125061]].
[2019-04-02 18:54:00,478] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3138133e-03 2.7682624e-31 9.9868613e-01 3.2648658e-35 1.9389061e-19], sum to 1.0000
[2019-04-02 18:54:00,490] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3311
[2019-04-02 18:54:00,496] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.58333333333333, 61.33333333333334, 1.0, 2.0, 0.5140512697910026, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8213590270129303, 6.911199999999999, 6.9112, 121.9260426156618, 1201997.798881274, 1201997.798881274, 261328.2392546964], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7045800.0000, 
sim time next is 7046400.0000, 
raw observation next is [27.76666666666667, 60.66666666666667, 1.0, 2.0, 0.5710460670944026, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9120455753652293, 6.911199999999999, 6.9112, 121.9260426156618, 1333128.738187994, 1333128.738187995, 282198.5952029911], 
processed observation next is [1.0, 0.5652173913043478, 0.5839506172839507, 0.6066666666666667, 1.0, 1.0, 0.489340556064765, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8900569692065365, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.47611740649571216, 0.4761174064957125, 0.5426896061595983], 
reward next is 0.4573, 
noisyNet noise sample is [array([-0.06185902], dtype=float32), 0.34167805]. 
=============================================
[2019-04-02 18:54:26,594] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 3.8001259e-21 0.0000000e+00 5.1861916e-38], sum to 1.0000
[2019-04-02 18:54:26,606] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0324
[2019-04-02 18:54:26,609] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.4935117e-16 0.0000000e+00 1.0000000e+00 0.0000000e+00 9.7982084e-38], sum to 1.0000
[2019-04-02 18:54:26,612] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.6, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6221482829931168, 6.9112, 6.9112, 121.9260426156618, 463253.4410386451, 463253.4410386451, 133658.9835736981], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7264800.0000, 
sim time next is 7265400.0000, 
raw observation next is [20.58333333333334, 85.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8460183685585787, 6.9112, 6.9112, 121.9260426156618, 630043.5549549238, 630043.5549549238, 157747.3934808918], 
processed observation next is [1.0, 0.08695652173913043, 0.3179012345679015, 0.8516666666666667, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8075229606982234, 0.0, 0.0, 0.8094621288201359, 0.22501555534104423, 0.22501555534104423, 0.3033603720786381], 
reward next is 0.6966, 
noisyNet noise sample is [array([-0.25632998], dtype=float32), 0.91644925]. 
=============================================
[2019-04-02 18:54:26,620] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3932
[2019-04-02 18:54:26,628] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [20.2, 90.0, 1.0, 2.0, 0.1927242297023952, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3191611441706799, 6.911199999999999, 6.9112, 121.9260426156618, 476775.060538201, 476775.0605382014, 165968.7016439325], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7422000.0000, 
sim time next is 7422600.0000, 
raw observation next is [20.2, 90.0, 1.0, 2.0, 0.1920329949080821, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3180267502845175, 6.911199999999999, 6.9112, 121.9260426156618, 475077.8731569721, 475077.8731569726, 165813.4780729842], 
processed observation next is [1.0, 0.9130434782608695, 0.3037037037037037, 0.9, 1.0, 1.0, 0.03813451774771678, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.14753343785564685, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1696706689846329, 0.16967066898463307, 0.3188720732172773], 
reward next is 0.6811, 
noisyNet noise sample is [array([-1.6650871], dtype=float32), 0.6116725]. 
=============================================
[2019-04-02 18:54:30,263] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.000000e+00 6.579165e-29 1.018394e-37 0.000000e+00 9.383548e-27], sum to 1.0000
[2019-04-02 18:54:30,273] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7375
[2019-04-02 18:54:30,283] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1604327.288548568 W.
[2019-04-02 18:54:30,290] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.83333333333334, 61.66666666666667, 1.0, 2.0, 0.7579751096322678, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9738603534415639, 6.911199999999999, 6.9112, 121.9260426156618, 1604327.288548568, 1604327.288548569, 323489.2956404057], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7312800.0000, 
sim time next is 7313400.0000, 
raw observation next is [26.85, 61.5, 1.0, 2.0, 0.6802688100043884, 1.0, 1.0, 0.6802688100043884, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1591750.062444587, 1591750.062444587, 298715.5580996104], 
processed observation next is [1.0, 0.6521739130434783, 0.55, 0.615, 1.0, 1.0, 0.6193676309576053, 1.0, 0.5, 0.6193676309576053, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288201359, 0.568482165158781, 0.568482165158781, 0.5744529963454046], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.94992316], dtype=float32), -0.007957631]. 
=============================================
[2019-04-02 18:54:31,379] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:54:31,388] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9152
[2019-04-02 18:54:31,393] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.85, 73.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7257400153174575, 6.9112, 6.9112, 121.9260426156618, 542010.5171956685, 542010.5171956685, 148929.3371337393], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7330200.0000, 
sim time next is 7330800.0000, 
raw observation next is [23.7, 74.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7264827995607799, 6.911199999999999, 6.9112, 121.9260426156618, 542597.4564559354, 542597.4564559358, 148953.6481641592], 
processed observation next is [1.0, 0.8695652173913043, 0.4333333333333333, 0.7466666666666667, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6581034994509749, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.19378480587711977, 0.19378480587711994, 0.28644932339261386], 
reward next is 0.7136, 
noisyNet noise sample is [array([-0.5198178], dtype=float32), 0.3985637]. 
=============================================
[2019-04-02 18:54:35,022] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 2.1659915e-27 0.0000000e+00 5.7244085e-38], sum to 1.0000
[2019-04-02 18:54:35,031] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4063
[2019-04-02 18:54:35,042] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 895639.279988687 W.
[2019-04-02 18:54:35,047] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [19.5, 95.0, 1.0, 2.0, 0.7217857182294682, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 895639.279988687, 895639.279988687, 183186.0904784887], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7380000.0000, 
sim time next is 7380600.0000, 
raw observation next is [19.53333333333333, 95.0, 1.0, 2.0, 0.3640463899242332, 1.0, 1.0, 0.3640463899242332, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 895245.7741307139, 895245.7741307144, 199084.8279408855], 
processed observation next is [1.0, 0.43478260869565216, 0.2790123456790123, 0.95, 1.0, 1.0, 0.2429123689574205, 1.0, 0.5, 0.2429123689574205, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.3197306336181121, 0.31973063361811227, 0.38285543834785674], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5392745], dtype=float32), 0.9009458]. 
=============================================
[2019-04-02 18:54:43,476] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-02 18:54:43,477] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 18:54:43,478] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:54:43,481] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 18:54:43,483] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:54:43,484] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 18:54:43,485] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 18:54:43,487] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:54:43,487] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 18:54:43,488] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:54:43,491] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:54:43,525] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run26
[2019-04-02 18:54:43,527] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run26
[2019-04-02 18:54:43,549] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run26
[2019-04-02 18:54:43,593] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run26
[2019-04-02 18:54:43,610] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run26
[2019-04-02 18:54:59,383] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.18697456], dtype=float32), -0.26011613]
[2019-04-02 18:54:59,384] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [24.92362735333334, 56.23870783666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6052142055998796, 6.9112, 6.9112, 121.9260426156618, 450114.2809108109, 450114.2809108109, 131552.5864955421]
[2019-04-02 18:54:59,384] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 18:54:59,388] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.0000000e+00 0.0000000e+00 1.5286155e-37 0.0000000e+00 0.0000000e+00], sampled 0.7043924629562667
[2019-04-02 18:55:30,298] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.18697456], dtype=float32), -0.26011613]
[2019-04-02 18:55:30,298] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [26.4, 83.0, 1.0, 2.0, 0.8769983665240284, 1.0, 1.0, 0.8769983665240284, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 121.9256007916992, 2000550.856606855, 2000550.856606855, 376598.08141211]
[2019-04-02 18:55:30,300] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 18:55:30,304] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.0000000e+00 7.9619783e-28 4.8858131e-12 7.1356013e-28 4.6521090e-12], sampled 0.45021004216149485
[2019-04-02 18:55:30,304] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2000550.856606855 W.
[2019-04-02 18:55:45,958] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.18697456], dtype=float32), -0.26011613]
[2019-04-02 18:55:45,958] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [25.0, 83.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8983629422364448, 6.9112, 6.9112, 121.9260426156618, 656824.5763965176, 656824.5763965176, 175855.7868228629]
[2019-04-02 18:55:45,959] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 18:55:45,962] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.38719414993463586
[2019-04-02 18:55:48,468] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.18697456], dtype=float32), -0.26011613]
[2019-04-02 18:55:48,470] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [28.6, 77.0, 1.0, 2.0, 0.3677574030770757, 0.0, 2.0, 0.0, 1.0, 1.0, 0.5854817627211634, 6.911199999999999, 6.9112, 121.9260426156618, 838300.9451520455, 838300.9451520459, 214362.710901561]
[2019-04-02 18:55:48,473] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 18:55:48,478] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [3.4958087e-02 0.0000000e+00 9.6504200e-01 5.5322831e-32 1.5038814e-17], sampled 0.6484109106725103
[2019-04-02 18:56:50,441] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7706.2834 2560284947.7991 739.0000
[2019-04-02 18:56:50,464] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8389.0814 2289110212.0583 477.0000
[2019-04-02 18:56:51,059] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8307.2390 2313372151.6779 636.0000
[2019-04-02 18:56:51,219] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8234.6243 2366482588.0433 546.0000
[2019-04-02 18:56:51,221] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8518.8076 2241677156.2163 486.0000
[2019-04-02 18:56:52,237] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 625000, evaluation results [625000.0, 7706.283448567724, 2560284947.799118, 739.0, 8389.081418371563, 2289110212.0583053, 477.0, 8518.807553234908, 2241677156.2163315, 486.0, 8234.624287684854, 2366482588.0432515, 546.0, 8307.238954895838, 2313372151.677943, 636.0]
[2019-04-02 18:57:04,143] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:57:04,144] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:57:04,183] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res10/Eplus-env-sub_run4
[2019-04-02 18:57:13,249] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:57:13,250] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:57:13,289] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res4/Eplus-env-sub_run4
[2019-04-02 18:57:14,092] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:57:14,093] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:57:14,121] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res5/Eplus-env-sub_run4
[2019-04-02 18:57:16,895] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:57:16,910] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4182
[2019-04-02 18:57:16,917] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.71666666666667, 69.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6859842231357415, 6.911199999999999, 6.9112, 121.9260426156618, 512544.1714841527, 512544.1714841531, 142636.1103007323], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7859400.0000, 
sim time next is 7860000.0000, 
raw observation next is [23.63333333333334, 70.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6866954338961815, 6.911200000000001, 6.9112, 121.9260426156618, 513086.948197257, 513086.9481972565, 142758.2883160672], 
processed observation next is [1.0, 1.0, 0.43086419753086447, 0.7033333333333335, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6083692923702269, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1832453386418775, 0.1832453386418773, 0.2745351698385908], 
reward next is 0.7255, 
noisyNet noise sample is [array([0.5290072], dtype=float32), 1.2579235]. 
=============================================
[2019-04-02 18:57:16,928] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[69.51607 ]
 [69.611374]
 [69.67709 ]
 [69.75863 ]
 [69.8599  ]], R is [[69.47225952]
 [69.50323486]
 [69.53353119]
 [69.5620575 ]
 [69.58912659]].
[2019-04-02 18:57:18,178] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:57:18,188] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5767
[2019-04-02 18:57:18,194] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.2, 79.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6328530642710185, 6.911200000000001, 6.9112, 121.9260426156618, 470918.9867755907, 470918.9867755902, 134438.3205353153], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7886400.0000, 
sim time next is 7887000.0000, 
raw observation next is [21.35, 78.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6345032704245321, 6.9112, 6.9112, 121.9260426156618, 472260.8182454483, 472260.8182454483, 134700.1199351659], 
processed observation next is [1.0, 0.2608695652173913, 0.3462962962962963, 0.7883333333333333, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5431290880306652, 0.0, 0.0, 0.8094621288201359, 0.16866457794480297, 0.16866457794480297, 0.25903869218301134], 
reward next is 0.7410, 
noisyNet noise sample is [array([0.58895075], dtype=float32), 0.862197]. 
=============================================
[2019-04-02 18:57:18,204] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[71.77892 ]
 [71.57199 ]
 [71.52814 ]
 [71.60139 ]
 [71.819115]], R is [[71.84645844]
 [71.86945343]
 [71.89224243]
 [71.91429138]
 [71.93006134]].
[2019-04-02 18:57:20,971] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:57:20,972] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:57:21,019] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res14/Eplus-env-sub_run4
[2019-04-02 18:57:23,141] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:57:23,141] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:57:23,174] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res11/Eplus-env-sub_run4
[2019-04-02 18:57:23,231] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:57:23,232] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:57:23,258] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res8/Eplus-env-sub_run4
[2019-04-02 18:57:23,303] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:57:23,304] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:57:23,327] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:57:23,328] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:57:23,332] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res12/Eplus-env-sub_run4
[2019-04-02 18:57:23,376] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res6/Eplus-env-sub_run4
[2019-04-02 18:57:23,467] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:57:23,468] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:57:23,477] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res3/Eplus-env-sub_run4
[2019-04-02 18:57:23,542] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:57:23,543] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:57:23,561] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res2/Eplus-env-sub_run4
[2019-04-02 18:57:23,601] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:57:23,602] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:57:23,618] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res9/Eplus-env-sub_run4
[2019-04-02 18:57:23,645] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:57:23,645] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:57:23,646] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:57:23,647] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:57:23,655] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:57:23,656] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:57:23,660] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res16/Eplus-env-sub_run4
[2019-04-02 18:57:23,694] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res7/Eplus-env-sub_run4
[2019-04-02 18:57:23,726] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res13/Eplus-env-sub_run4
[2019-04-02 18:57:23,751] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:57:23,752] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:57:23,756] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res15/Eplus-env-sub_run4
[2019-04-02 18:57:23,893] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 18:57:23,893] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:57:23,894] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res17/Eplus-env-sub_run4
[2019-04-02 18:57:24,666] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:57:24,668] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7287
[2019-04-02 18:57:24,673] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.1, 23.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6288048479411327, 6.911200000000001, 6.9112, 121.9260426156618, 459042.418703965, 459042.4187039646, 129137.3701949207], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 216600.0000, 
sim time next is 217200.0000, 
raw observation next is [31.3, 22.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6336340783273849, 6.911199999999999, 6.9112, 121.9260426156618, 461725.3201241422, 461725.3201241426, 129229.1547178717], 
processed observation next is [0.0, 0.5217391304347826, 0.7148148148148148, 0.22, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5420425979092311, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1649019000443365, 0.16490190004433666, 0.24851760522667635], 
reward next is 0.7515, 
noisyNet noise sample is [array([-0.28990322], dtype=float32), -0.12896699]. 
=============================================
[2019-04-02 18:57:25,426] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 2.2306643e-30 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:57:25,438] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2229
[2019-04-02 18:57:25,448] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.8, 57.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4611771275795505, 6.911199999999999, 6.9112, 121.9260426156618, 329278.9843430083, 329278.9843430087, 105226.6637521279], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 25200.0000, 
sim time next is 25800.0000, 
raw observation next is [21.13333333333334, 55.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5106813589190246, 6.911200000000001, 6.9112, 121.9260426156618, 364633.2486315679, 364633.2486315674, 109982.0363477341], 
processed observation next is [1.0, 0.30434782608695654, 0.33827160493827185, 0.555, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.38835169864878066, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.13022616022555997, 0.1302261602255598, 0.21150391605333482], 
reward next is 0.7885, 
noisyNet noise sample is [array([-0.4255233], dtype=float32), 1.3394576]. 
=============================================
[2019-04-02 18:57:27,839] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.3082915e-01 2.1668000e-31 7.6917082e-01 4.1044011e-25 1.2872025e-19], sum to 1.0000
[2019-04-02 18:57:27,847] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6079
[2019-04-02 18:57:27,859] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.0, 39.0, 1.0, 2.0, 0.9176909681383646, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.184570660806202, 6.9112, 121.9249255385042, 1287454.14922285, 1147465.219877377, 225615.2559224136], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 43200.0000, 
sim time next is 43800.0000, 
raw observation next is [28.18333333333334, 38.83333333333334, 1.0, 2.0, 0.493888477032878, 0.0, 2.0, 0.0, 1.0, 1.0, 0.8196366689676137, 6.911199999999999, 6.9112, 121.9258779967014, 1224609.995124694, 1224609.995124695, 250758.6704613994], 
processed observation next is [1.0, 0.5217391304347826, 0.599382716049383, 0.3883333333333334, 1.0, 1.0, 0.3974862821819977, 0.0, 1.0, -0.1904761904761905, 1.0, 0.5, 0.774545836209517, -8.881784197001253e-17, 0.0, 0.8094610359214306, 0.43736071254453357, 0.4373607125445339, 0.4822282124257681], 
reward next is 0.5178, 
noisyNet noise sample is [array([0.01298761], dtype=float32), 1.396262]. 
=============================================
[2019-04-02 18:57:31,484] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:57:31,497] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6468
[2019-04-02 18:57:31,502] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.2, 76.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6712426620479925, 6.9112, 6.9112, 121.9260426156618, 498021.3938532458, 498021.3938532458, 137167.5087388335], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 105000.0000, 
sim time next is 105600.0000, 
raw observation next is [21.3, 76.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.633932238904248, 6.911200000000001, 6.9112, 121.9260426156618, 470604.3662715729, 470604.3662715724, 133676.0248268062], 
processed observation next is [1.0, 0.21739130434782608, 0.3444444444444445, 0.7666666666666667, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5424152986303099, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.16807298795413317, 0.168072987954133, 0.25706927851308886], 
reward next is 0.7429, 
noisyNet noise sample is [array([0.48202473], dtype=float32), 2.3916948]. 
=============================================
[2019-04-02 18:57:33,316] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 1.3378817e-31 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 18:57:33,322] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1840
[2019-04-02 18:57:33,335] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1260001.617940655 W.
[2019-04-02 18:57:33,344] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.98333333333333, 63.83333333333334, 1.0, 2.0, 0.5202676009029622, 0.0, 2.0, 0.0, 1.0, 1.0, 0.8447221032296403, 6.911200000000001, 6.9112, 121.925872068412, 1260001.617940655, 1260001.617940655, 261842.9251101644], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 118200.0000, 
sim time next is 118800.0000, 
raw observation next is [25.4, 62.0, 1.0, 2.0, 0.5194211265821391, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8414600834515432, 6.911199999999999, 6.9112, 121.9260425636573, 1253752.629670786, 1253752.629670787, 261756.8347317879], 
processed observation next is [1.0, 0.391304347826087, 0.49629629629629624, 0.62, 1.0, 1.0, 0.4278822935501656, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8018251043144289, -8.881784197001253e-17, 0.0, 0.8094621284748802, 0.447768796310995, 0.4477687963109953, 0.5033785283303613], 
reward next is 0.4966, 
noisyNet noise sample is [array([-1.1706693], dtype=float32), 0.35803342]. 
=============================================
[2019-04-02 18:57:38,948] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 9.684793e-31 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-02 18:57:38,953] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4481
[2019-04-02 18:57:38,960] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.26666666666667, 65.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5577699437539203, 6.9112, 6.9112, 121.9260426156618, 410959.4815183475, 410959.4815183475, 124654.070675244], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 199200.0000, 
sim time next is 199800.0000, 
raw observation next is [22.45, 66.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5729080548943897, 6.911200000000001, 6.9112, 121.9260426156618, 423398.9442650486, 423398.9442650482, 126693.3706194505], 
processed observation next is [0.0, 0.30434782608695654, 0.387037037037037, 0.66, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.46613506861798704, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.15121390866608878, 0.15121390866608864, 0.24364109734509712], 
reward next is 0.7564, 
noisyNet noise sample is [array([-1.1856456], dtype=float32), -0.1874607]. 
=============================================
[2019-04-02 18:57:42,234] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.9999976e-01 4.9782285e-33 2.8617399e-07 5.3048614e-32 1.7300528e-24], sum to 1.0000
[2019-04-02 18:57:42,246] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8766
[2019-04-02 18:57:42,254] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1578942.805388757 W.
[2019-04-02 18:57:42,267] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.83333333333333, 28.5, 1.0, 2.0, 0.6507839856333755, 1.0, 2.0, 0.6507839856333755, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1578942.805388757, 1578942.805388757, 290006.094962002], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 481800.0000, 
sim time next is 482400.0000, 
raw observation next is [32.0, 28.0, 1.0, 2.0, 0.7159129134016642, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9560712335220106, 6.911200000000001, 6.9112, 121.9260426156618, 1589896.952664954, 1589896.952664953, 308086.5752344909], 
processed observation next is [1.0, 0.6086956521739131, 0.7407407407407407, 0.28, 1.0, 1.0, 0.6618010873829335, 0.0, 0.5, -0.1904761904761905, 1.0, 0.5, 0.9450890419025132, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.5678203402374835, 0.5678203402374832, 0.5924741831432517], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.64410347], dtype=float32), 1.0901139]. 
=============================================
[2019-04-02 18:57:47,508] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 2.3035842e-35 1.4079630e-15 0.0000000e+00 6.9476261e-23], sum to 1.0000
[2019-04-02 18:57:47,519] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2760
[2019-04-02 18:57:47,534] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1293061.264109333 W.
[2019-04-02 18:57:47,544] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.2, 44.66666666666667, 1.0, 2.0, 0.5312603460260331, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8657508457019493, 6.9112, 6.9112, 121.9260426156618, 1293061.264109333, 1293061.264109333, 265417.380643727], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 555600.0000, 
sim time next is 556200.0000, 
raw observation next is [27.9, 46.0, 1.0, 2.0, 0.5397740514419075, 1.0, 1.0, 0.5397740514419075, 0.0, 1.0, 0.0, 6.911199999999997, 6.9112, 121.9260426156618, 1306530.614298815, 1306530.614298817, 251121.8799374085], 
processed observation next is [1.0, 0.43478260869565216, 0.5888888888888888, 0.46, 1.0, 1.0, 0.45211196600227077, 1.0, 0.5, 0.45211196600227077, 0.0, 0.5, -0.25, -2.6645352591003756e-16, 0.0, 0.8094621288201359, 0.466618076535291, 0.4666180765352918, 0.48292669218732404], 
reward next is 0.5171, 
noisyNet noise sample is [array([-0.32672516], dtype=float32), -0.37962997]. 
=============================================
[2019-04-02 18:57:53,924] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 18:57:53,940] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4602
[2019-04-02 18:57:53,948] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.15, 30.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5913347178447989, 6.911199999999999, 6.9112, 121.9260426156618, 434223.5511164902, 434223.5511164907, 126922.3394492184], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 413400.0000, 
sim time next is 414000.0000, 
raw observation next is [29.0, 31.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5887311844400293, 6.911200000000001, 6.9112, 121.9260426156618, 432087.0220231767, 432087.0220231763, 126583.8503568919], 
processed observation next is [1.0, 0.8260869565217391, 0.6296296296296297, 0.31, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.48591398055003654, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.15431679357970596, 0.15431679357970582, 0.24343048145556134], 
reward next is 0.7566, 
noisyNet noise sample is [array([-0.59308136], dtype=float32), 0.43378225]. 
=============================================
[2019-04-02 18:57:53,966] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[71.254295]
 [72.1395  ]
 [72.01268 ]
 [70.35237 ]
 [66.61916 ]], R is [[70.78631592]
 [70.83437347]
 [70.88116455]
 [70.92662048]
 [70.97003174]].
[2019-04-02 18:57:54,757] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.00000000e+00 0.00000000e+00 1.21016596e-23 0.00000000e+00
 0.00000000e+00], sum to 1.0000
[2019-04-02 18:57:54,768] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9734
[2019-04-02 18:57:54,777] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.93333333333333, 56.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5013523056731244, 6.911200000000001, 6.9112, 121.9260426156618, 360887.1799558208, 360887.1799558203, 116225.041216842], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 436200.0000, 
sim time next is 436800.0000, 
raw observation next is [21.66666666666667, 58.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5021948410431626, 6.911200000000001, 6.9112, 121.9260426156618, 361484.5634428379, 361484.5634428374, 116286.9890566134], 
processed observation next is [1.0, 0.043478260869565216, 0.3580246913580249, 0.5833333333333335, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.3777435513039532, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.12910162980101353, 0.12910162980101336, 0.22362882510887191], 
reward next is 0.7764, 
noisyNet noise sample is [array([0.64000785], dtype=float32), -1.1312792]. 
=============================================
[2019-04-02 18:57:57,599] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-02 18:57:57,601] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 18:57:57,602] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:57:57,603] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 18:57:57,606] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:57:57,606] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 18:57:57,607] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:57:57,608] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 18:57:57,608] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:57:57,609] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 18:57:57,609] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 18:57:57,637] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run27
[2019-04-02 18:57:57,658] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run27
[2019-04-02 18:57:57,659] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run27
[2019-04-02 18:57:57,661] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run27
[2019-04-02 18:57:57,720] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run27
[2019-04-02 18:59:50,242] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.15981981], dtype=float32), -0.245094]
[2019-04-02 18:59:50,246] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [21.39374838666667, 84.329044735, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.709408168065651, 6.9112, 6.9112, 121.9260426156618, 530077.577299936, 530077.577299936, 145286.2262727059]
[2019-04-02 18:59:50,247] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 18:59:50,251] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.028263266687768573
[2019-04-02 19:00:04,582] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7743.2394 2553377835.4623 766.0000
[2019-04-02 19:00:04,926] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8224.4042 2361928993.3096 565.0000
[2019-04-02 19:00:05,018] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8293.9785 2316692078.3288 632.0000
[2019-04-02 19:00:05,238] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8376.6042 2282688329.7471 505.0000
[2019-04-02 19:00:05,291] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8489.9518 2245797846.0126 499.0000
[2019-04-02 19:00:06,306] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 650000, evaluation results [650000.0, 7743.239381251402, 2553377835.4622684, 766.0, 8376.604206685473, 2282688329.747141, 505.0, 8489.951842940573, 2245797846.012605, 499.0, 8224.404175586256, 2361928993.3096194, 565.0, 8293.978535358163, 2316692078.3288093, 632.0]
[2019-04-02 19:00:09,598] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:00:09,608] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5569
[2019-04-02 19:00:09,616] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.31666666666667, 24.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.612057798389518, 6.9112, 6.9112, 121.9260426156618, 450446.3090987082, 450446.3090987082, 129281.9555426783], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 755400.0000, 
sim time next is 756000.0000, 
raw observation next is [31.2, 25.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6143332268730726, 6.9112, 6.9112, 121.9260426156618, 451890.5089177069, 451890.5089177069, 129377.0451678636], 
processed observation next is [1.0, 0.782608695652174, 0.7111111111111111, 0.25, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5179165335913407, 0.0, 0.0, 0.8094621288201359, 0.16138946747060962, 0.16138946747060962, 0.24880200993819923], 
reward next is 0.7512, 
noisyNet noise sample is [array([1.7966862], dtype=float32), 1.859397]. 
=============================================
[2019-04-02 19:00:09,632] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[71.31788 ]
 [71.99177 ]
 [72.34847 ]
 [71.904335]
 [67.59376 ]], R is [[70.88043976]
 [70.92301941]
 [70.96516418]
 [71.00685883]
 [71.04411316]].
[2019-04-02 19:00:11,819] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 4.572525e-37 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-02 19:00:11,831] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1370
[2019-04-02 19:00:11,837] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.05, 69.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6100786772629319, 6.911200000000001, 6.9112, 121.9260426156618, 446445.5500425329, 446445.5500425324, 127912.4803791334], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 541800.0000, 
sim time next is 542400.0000, 
raw observation next is [21.23333333333333, 68.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5978311601861148, 6.911200000000001, 6.9112, 121.9260426156618, 437700.01137077, 437700.0113707695, 126912.7221703092], 
processed observation next is [1.0, 0.2608695652173913, 0.34197530864197523, 0.68, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.49728895023264347, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.15632143263241785, 0.15632143263241768, 0.2440629272505946], 
reward next is 0.7559, 
noisyNet noise sample is [array([-0.26345065], dtype=float32), -0.667177]. 
=============================================
[2019-04-02 19:00:12,506] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8127286e-04 0.0000000e+00 9.9981874e-01 2.1972947e-35 7.1689793e-29], sum to 1.0000
[2019-04-02 19:00:12,516] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5906
[2019-04-02 19:00:12,525] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.6, 35.0, 1.0, 2.0, 0.5818826863893578, 1.0, 2.0, 0.5818826863893578, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1405320.107990928, 1405320.107990928, 265205.9323777816], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 550800.0000, 
sim time next is 551400.0000, 
raw observation next is [30.3, 36.16666666666667, 1.0, 2.0, 0.5596802036133829, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9109448130890606, 6.911199999999999, 6.9112, 121.9260426156618, 1360122.765129562, 1360122.765129563, 275888.6561810544], 
processed observation next is [1.0, 0.391304347826087, 0.6777777777777778, 0.3616666666666667, 1.0, 1.0, 0.4758097662064082, 0.0, 0.5, -0.1904761904761905, 1.0, 0.5, 0.8886810163613257, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.485758130403415, 0.4857581304034153, 0.5305551080404892], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2793205], dtype=float32), -0.8836634]. 
=============================================
[2019-04-02 19:00:22,688] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:00:22,699] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8340
[2019-04-02 19:00:22,708] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.56666666666666, 33.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5864769049301373, 6.911199999999999, 6.9112, 121.9260426156618, 431662.0317630786, 431662.031763079, 126979.3178614227], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 686400.0000, 
sim time next is 687000.0000, 
raw observation next is [28.38333333333333, 34.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5830413914573057, 6.9112, 6.9112, 121.9260426156618, 429142.3934513604, 429142.3934513604, 126675.7743239828], 
processed observation next is [1.0, 0.9565217391304348, 0.60679012345679, 0.34333333333333343, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.4788017393216321, 0.0, 0.0, 0.8094621288201359, 0.153265140518343, 0.153265140518343, 0.24360725831535154], 
reward next is 0.7564, 
noisyNet noise sample is [array([0.42404205], dtype=float32), 0.8451636]. 
=============================================
[2019-04-02 19:00:22,724] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[64.52423 ]
 [64.619415]
 [64.72248 ]
 [64.84561 ]
 [64.98263 ]], R is [[64.54592133]
 [64.65627289]
 [64.7649231 ]
 [64.87193298]
 [64.97717285]].
[2019-04-02 19:00:29,357] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 3.0830145e-30 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 19:00:29,366] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3929
[2019-04-02 19:00:29,372] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.63333333333333, 46.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5221368294339426, 6.911200000000001, 6.9112, 121.9260426156618, 380646.3701127509, 380646.3701127504, 119689.9826653334], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 781800.0000, 
sim time next is 782400.0000, 
raw observation next is [24.46666666666667, 47.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5249928577530656, 6.9112, 6.9112, 121.9260426156618, 382578.1213842431, 382578.1213842431, 119861.4001688874], 
processed observation next is [0.0, 0.043478260869565216, 0.46172839506172847, 0.47333333333333344, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.40624107219133193, 0.0, 0.0, 0.8094621288201359, 0.13663504335151538, 0.13663504335151538, 0.23050269263247578], 
reward next is 0.7695, 
noisyNet noise sample is [array([-0.919199], dtype=float32), 0.8326592]. 
=============================================
[2019-04-02 19:00:32,709] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 1.3611591e-33 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 19:00:32,720] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0444
[2019-04-02 19:00:32,725] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.98333333333333, 60.16666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5124486977116692, 6.9112, 6.9112, 121.9260426156618, 366104.5798217494, 366104.5798217494, 116137.5662645925], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 971400.0000, 
sim time next is 972000.0000, 
raw observation next is [21.1, 60.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4898277039805555, 6.9112, 6.9112, 121.9260426156618, 350437.1718881117, 350437.1718881117, 114588.0768141119], 
processed observation next is [1.0, 0.2608695652173913, 0.3370370370370371, 0.6, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.36228462997569433, 0.0, 0.0, 0.8094621288201359, 0.12515613281718274, 0.12515613281718274, 0.22036168618098442], 
reward next is 0.7796, 
noisyNet noise sample is [array([1.3427283], dtype=float32), 1.113885]. 
=============================================
[2019-04-02 19:00:32,746] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[71.52671]
 [71.6001 ]
 [71.69123]
 [71.71981]
 [71.6931 ]], R is [[71.57598877]
 [71.6368866 ]
 [71.70085907]
 [71.77017975]
 [71.84088898]].
[2019-04-02 19:00:38,313] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:00:38,322] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7086
[2019-04-02 19:00:38,326] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.13333333333333, 49.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6275274740641958, 6.911199999999999, 6.9112, 121.9260426156618, 466969.4616661398, 466969.4616661402, 133927.2902225946], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 933000.0000, 
sim time next is 933600.0000, 
raw observation next is [25.96666666666667, 50.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6261768640899006, 6.911200000000001, 6.9112, 121.9260426156618, 465887.3678392748, 465887.3678392743, 133729.0506373946], 
processed observation next is [0.0, 0.8260869565217391, 0.517283950617284, 0.5033333333333334, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5327210801123757, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.16638834565688387, 0.16638834565688368, 0.25717125122575885], 
reward next is 0.7428, 
noisyNet noise sample is [array([-0.93048775], dtype=float32), -0.85467535]. 
=============================================
[2019-04-02 19:00:44,151] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:00:44,164] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2720
[2019-04-02 19:00:44,171] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [18.61666666666667, 89.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5615796230366993, 6.9112, 6.9112, 121.9260426156618, 412095.1839178958, 412095.1839178958, 124164.7156722767], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1234200.0000, 
sim time next is 1234800.0000, 
raw observation next is [18.7, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5538918556284094, 6.911199999999999, 6.9112, 121.9260426156618, 406696.9006072097, 406696.9006072102, 123614.6964759605], 
processed observation next is [1.0, 0.30434782608695654, 0.24814814814814812, 0.89, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.44236481953551166, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.14524889307400346, 0.14524889307400363, 0.2377205701460779], 
reward next is 0.7623, 
noisyNet noise sample is [array([-0.05175529], dtype=float32), 1.1143657]. 
=============================================
[2019-04-02 19:00:45,837] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:00:45,838] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0437
[2019-04-02 19:00:45,846] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.96666666666667, 65.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5234772824359202, 6.9112, 6.9112, 121.9260426156618, 379876.790836655, 379876.790836655, 119093.9116632429], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1392000.0000, 
sim time next is 1392600.0000, 
raw observation next is [20.93333333333333, 65.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.520560705567438, 6.9112, 6.9112, 121.9260426156618, 377238.5161715128, 377238.5161715128, 118656.2150745556], 
processed observation next is [0.0, 0.08695652173913043, 0.3308641975308641, 0.6533333333333334, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.40070088195929754, 0.0, 0.0, 0.8094621288201359, 0.13472804148982598, 0.13472804148982598, 0.22818502898953], 
reward next is 0.7718, 
noisyNet noise sample is [array([-0.6437383], dtype=float32), -0.441185]. 
=============================================
[2019-04-02 19:00:48,163] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:00:48,175] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8158
[2019-04-02 19:00:48,181] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.23333333333333, 65.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8137767500359171, 6.9112, 6.9112, 121.9260426156618, 592773.0574587536, 592773.0574587536, 146780.7677328203], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1044600.0000, 
sim time next is 1045200.0000, 
raw observation next is [21.16666666666667, 66.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6247656041571785, 6.911200000000001, 6.9112, 121.9260426156618, 455140.0409065691, 455140.0409065686, 128378.5577268429], 
processed observation next is [1.0, 0.08695652173913043, 0.33950617283950635, 0.66, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5309570051964732, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.16255001460948898, 0.16255001460948879, 0.2468818417823902], 
reward next is 0.7531, 
noisyNet noise sample is [array([-0.7483695], dtype=float32), 1.3713262]. 
=============================================
[2019-04-02 19:00:55,867] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:00:55,874] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4549
[2019-04-02 19:00:55,879] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.2, 67.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7407350646553656, 6.9112, 6.9112, 121.9260426156618, 533012.6622934369, 533012.6622934369, 136883.1641008448], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1153200.0000, 
sim time next is 1153800.0000, 
raw observation next is [20.25, 67.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8238864538547218, 6.9112, 6.9112, 121.9260426156618, 593235.101459866, 593235.101459866, 145254.3222581547], 
processed observation next is [1.0, 0.34782608695652173, 0.3055555555555556, 0.675, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.7798580673184021, 0.0, 0.0, 0.8094621288201359, 0.2118696790928093, 0.2118696790928093, 0.27933523511183594], 
reward next is 0.7207, 
noisyNet noise sample is [array([-0.7162061], dtype=float32), -0.67114955]. 
=============================================
[2019-04-02 19:00:56,018] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:00:56,028] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1033
[2019-04-02 19:00:56,034] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.8, 65.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6845629561915301, 6.9112, 6.9112, 121.9260426156618, 501938.778316556, 501938.778316556, 135277.910242467], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1170000.0000, 
sim time next is 1170600.0000, 
raw observation next is [21.86666666666667, 65.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7805123746526489, 6.911200000000001, 6.9112, 121.9260426156618, 572770.6287694976, 572770.6287694972, 145154.1945472138], 
processed observation next is [1.0, 0.5652173913043478, 0.36543209876543226, 0.65, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.725640468315811, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.20456093884624915, 0.20456093884624899, 0.279142681821565], 
reward next is 0.7209, 
noisyNet noise sample is [array([0.97217077], dtype=float32), -0.14938095]. 
=============================================
[2019-04-02 19:01:09,338] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:01:09,347] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2448
[2019-04-02 19:01:09,360] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.25, 44.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6002279011218267, 6.9112, 6.9112, 121.9260426156618, 446159.4689171146, 446159.4689171146, 130878.9668629919], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1366200.0000, 
sim time next is 1366800.0000, 
raw observation next is [27.0, 45.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6023447491727485, 6.911199999999999, 6.9112, 121.9260426156618, 447883.6856708567, 447883.6856708572, 131199.8826488688], 
processed observation next is [1.0, 0.8260869565217391, 0.5555555555555556, 0.4533333333333334, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5029309364659356, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1599584591681631, 0.1599584591681633, 0.25230746663244], 
reward next is 0.7477, 
noisyNet noise sample is [array([-1.260528], dtype=float32), 0.68050754]. 
=============================================
[2019-04-02 19:01:13,001] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-02 19:01:13,003] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 19:01:13,006] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 19:01:13,006] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:01:13,007] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 19:01:13,007] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:01:13,008] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:01:13,009] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 19:01:13,010] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 19:01:13,014] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:01:13,014] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:01:13,035] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run28
[2019-04-02 19:01:13,036] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run28
[2019-04-02 19:01:13,083] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run28
[2019-04-02 19:01:13,083] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run28
[2019-04-02 19:01:13,132] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run28
[2019-04-02 19:01:15,706] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.10514297], dtype=float32), -0.24495314]
[2019-04-02 19:01:15,708] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [19.65, 46.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4470267267790708, 6.911199999999999, 6.9112, 121.9260426156618, 319173.5414405849, 319173.5414405854, 89206.07223072452]
[2019-04-02 19:01:15,708] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 19:01:15,711] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.15639301170440723
[2019-04-02 19:01:17,310] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.10514297], dtype=float32), -0.24495314]
[2019-04-02 19:01:17,311] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [25.86071314333334, 21.20882787, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.540204855284437, 6.911200000000001, 6.9112, 121.9260426156618, 385718.8087390072, 385718.8087390068, 99241.11101566919]
[2019-04-02 19:01:17,313] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 19:01:17,316] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6331178840974695
[2019-04-02 19:01:56,970] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.10514297], dtype=float32), -0.24495314]
[2019-04-02 19:01:56,973] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [27.16666666666666, 69.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9049054334525626, 6.9112, 6.9112, 121.9260426156618, 661187.4947164053, 661187.4947164053, 176808.5198396512]
[2019-04-02 19:01:56,975] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 19:01:56,978] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8661914026344539
[2019-04-02 19:02:17,359] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.10514297], dtype=float32), -0.24495314]
[2019-04-02 19:02:17,361] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [31.0, 66.0, 1.0, 2.0, 0.7399813097657973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 843394.4310778711, 843394.4310778707, 183855.6419231664]
[2019-04-02 19:02:17,362] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 19:02:17,367] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6355597464667946
[2019-04-02 19:02:17,374] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 843394.4310778711 W.
[2019-04-02 19:02:34,447] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.10514297], dtype=float32), -0.24495314]
[2019-04-02 19:02:34,450] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [31.44777664, 68.71095318, 1.0, 2.0, 0.7933710099502846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 904281.2937914977, 904281.2937914972, 194583.1534994454]
[2019-04-02 19:02:34,453] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 19:02:34,456] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9346761978339228
[2019-04-02 19:02:34,458] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 904281.2937914977 W.
[2019-04-02 19:02:48,514] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.10514297], dtype=float32), -0.24495314]
[2019-04-02 19:02:48,515] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [21.27794777333333, 72.62369749, 1.0, 1.0, 0.5885107102454973, 1.0, 1.0, 0.5885107102454973, 0.0, 1.0, 0.0, 6.9112, 6.9112, 122.8883204034549, 1459922.285501014, 1459922.285501014, 268757.6712183663]
[2019-04-02 19:02:48,515] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 19:02:48,518] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9624148355592131
[2019-04-02 19:02:48,519] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1459922.285501014 W.
[2019-04-02 19:03:00,782] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.10514297], dtype=float32), -0.24495314]
[2019-04-02 19:03:00,783] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7011069476152286, 6.911200000000001, 6.9112, 121.9260426156618, 523929.4544209695, 523929.4544209691, 145032.0187054416]
[2019-04-02 19:03:00,786] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 19:03:00,789] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7664191737663416
[2019-04-02 19:03:19,623] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8633.7484 2219185658.3071 543.0000
[2019-04-02 19:03:20,209] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8362.6183 2339490860.4802 616.0000
[2019-04-02 19:03:20,519] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7841.4716 2529798096.8055 831.0000
[2019-04-02 19:03:20,560] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8558.6918 2258293591.0027 536.0000
[2019-04-02 19:03:20,573] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8404.2460 2292976464.9981 697.0000
[2019-04-02 19:03:21,588] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 675000, evaluation results [675000.0, 7841.471632967488, 2529798096.8055396, 831.0, 8558.691799887378, 2258293591.0026627, 536.0, 8633.748369108142, 2219185658.307149, 543.0, 8362.618258210465, 2339490860.480159, 616.0, 8404.245980577798, 2292976464.998051, 697.0]
[2019-04-02 19:03:36,019] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:03:36,028] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5879
[2019-04-02 19:03:36,034] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.08333333333334, 44.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6096602708699276, 6.9112, 6.9112, 121.9260426156618, 453092.8986665619, 453092.8986665619, 131717.7223708086], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1620600.0000, 
sim time next is 1621200.0000, 
raw observation next is [26.96666666666667, 44.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.614311855479358, 6.911199999999999, 6.9112, 121.9260426156618, 456436.8885596427, 456436.8885596432, 132077.2990878035], 
processed observation next is [1.0, 0.782608695652174, 0.554320987654321, 0.4466666666666667, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5178898193491974, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1630131744855867, 0.16301317448558686, 0.25399480593808366], 
reward next is 0.7460, 
noisyNet noise sample is [array([1.7315496], dtype=float32), 0.86154914]. 
=============================================
[2019-04-02 19:03:36,754] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:03:36,765] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6635
[2019-04-02 19:03:36,771] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.36666666666667, 46.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5995267598730595, 6.911200000000001, 6.9112, 121.9260426156618, 444846.6856187619, 444846.6856187614, 130231.1207460889], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1624200.0000, 
sim time next is 1624800.0000, 
raw observation next is [26.23333333333333, 47.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6011217910978669, 6.911200000000001, 6.9112, 121.9260426156618, 445948.3592934466, 445948.3592934461, 130324.9143442796], 
processed observation next is [1.0, 0.8260869565217391, 0.5271604938271603, 0.47, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5014022388723336, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.15926727117623093, 0.15926727117623077, 0.25062483527746077], 
reward next is 0.7494, 
noisyNet noise sample is [array([1.5563425], dtype=float32), -1.1437248]. 
=============================================
[2019-04-02 19:03:38,784] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:03:38,798] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4390
[2019-04-02 19:03:38,805] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [18.76666666666667, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5679026930189166, 6.911199999999999, 6.9112, 121.9260426156618, 413928.9163509767, 413928.9163509772, 123485.7860607681], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1657200.0000, 
sim time next is 1657800.0000, 
raw observation next is [18.55, 85.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5563359229845112, 6.9112, 6.9112, 121.9260426156618, 405264.0766751649, 405264.0766751649, 122406.464187503], 
processed observation next is [1.0, 0.17391304347826086, 0.2425925925925926, 0.855, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.44541990373063894, 0.0, 0.0, 0.8094621288201359, 0.14473717024113034, 0.14473717024113034, 0.23539704651442886], 
reward next is 0.7646, 
noisyNet noise sample is [array([0.03623971], dtype=float32), -0.59837043]. 
=============================================
[2019-04-02 19:03:56,053] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.00000000e+00 1.10882766e-35 1.38062881e-10 7.65083651e-26
 1.74782064e-13], sum to 1.0000
[2019-04-02 19:03:56,062] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4466
[2019-04-02 19:03:56,077] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 978291.5867509941 W.
[2019-04-02 19:03:56,082] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 88.0, 1.0, 2.0, 0.2723939812582466, 1.0, 2.0, 0.2723939812582466, 1.0, 1.0, 0.4392388777586857, 6.9112, 6.9112, 121.94756008, 978291.5867509941, 978291.5867509941, 245191.8889165983], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1875600.0000, 
sim time next is 1876200.0000, 
raw observation next is [21.46666666666667, 88.16666666666667, 1.0, 2.0, 0.5057412496647268, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 615841.2721221418, 615841.2721221418, 144222.441044053], 
processed observation next is [1.0, 0.7391304347826086, 0.35061728395061736, 0.8816666666666667, 1.0, 1.0, 0.4115967257913414, 0.0, 0.5, -0.1904761904761905, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2199433114721935, 0.2199433114721935, 0.2773508481616404], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7172204], dtype=float32), -0.3680203]. 
=============================================
[2019-04-02 19:04:09,982] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:04:09,989] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2618
[2019-04-02 19:04:09,996] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.26666666666667, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7212843189088213, 6.9112, 6.9112, 121.9260426156618, 538676.5422786593, 538676.5422786593, 148432.7408391726], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2082000.0000, 
sim time next is 2082600.0000, 
raw observation next is [22.15, 85.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7178953083974594, 6.911200000000001, 6.9112, 121.9260426156618, 536209.6142638904, 536209.61426389, 147924.3200265467], 
processed observation next is [0.0, 0.08695652173913043, 0.3759259259259259, 0.855, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6473691354968242, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.19150343366567515, 0.191503433665675, 0.28446984620489746], 
reward next is 0.7155, 
noisyNet noise sample is [array([-0.46677026], dtype=float32), -0.29512495]. 
=============================================
[2019-04-02 19:04:11,984] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:04:11,989] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2724
[2019-04-02 19:04:11,998] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.53333333333333, 73.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8936721554039492, 6.9112, 6.9112, 121.9260426156618, 652679.2293577581, 652679.2293577581, 175369.8581024993], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2112000.0000, 
sim time next is 2112600.0000, 
raw observation next is [26.76666666666667, 72.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9020667625475866, 6.911199999999999, 6.9112, 121.9260426156618, 657829.3039090307, 657829.3039090312, 176664.5949826682], 
processed observation next is [0.0, 0.43478260869565216, 0.5469135802469137, 0.7283333333333333, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.877583453184483, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.23493903711036812, 0.2349390371103683, 0.3397396057359004], 
reward next is 0.6603, 
noisyNet noise sample is [array([0.08753171], dtype=float32), -1.1109868]. 
=============================================
[2019-04-02 19:04:13,162] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 1.2119908e-23 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 19:04:13,171] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8286
[2019-04-02 19:04:13,182] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 719117.8817270868 W.
[2019-04-02 19:04:13,192] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.36666666666667, 74.0, 1.0, 2.0, 0.3154970504140056, 1.0, 2.0, 0.3154970504140056, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 719117.8817270868, 719117.8817270873, 184221.1824968825], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2150400.0000, 
sim time next is 2151000.0000, 
raw observation next is [27.2, 74.5, 1.0, 2.0, 0.6258941137045776, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 713763.0219588745, 713763.0219588745, 162610.4960775622], 
processed observation next is [0.0, 0.9130434782608695, 0.5629629629629629, 0.745, 1.0, 1.0, 0.5546358496483066, 0.0, 0.5, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2549153649853123, 0.2549153649853123, 0.31271249245685034], 
reward next is 0.6873, 
noisyNet noise sample is [array([0.24271446], dtype=float32), -2.8101196]. 
=============================================
[2019-04-02 19:04:13,219] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[67.25644]
 [67.38347]
 [67.68463]
 [67.06066]
 [68.21892]], R is [[67.4897995 ]
 [66.81490326]
 [66.71469116]
 [66.69057465]
 [66.66573334]].
[2019-04-02 19:04:13,433] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:04:13,444] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9609
[2019-04-02 19:04:13,449] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.63333333333333, 47.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8791000580358899, 6.911200000000001, 6.9112, 121.9260426156618, 643140.5893814862, 643140.5893814857, 173233.0340531631], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2136000.0000, 
sim time next is 2136600.0000, 
raw observation next is [31.45, 48.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8843591383909059, 6.9112, 6.9112, 121.9260426156618, 646125.8831583604, 646125.8831583604, 174091.5776732853], 
processed observation next is [0.0, 0.7391304347826086, 0.7203703703703703, 0.485, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8554489229886322, 0.0, 0.0, 0.8094621288201359, 0.2307592439851287, 0.2307592439851287, 0.33479149552554865], 
reward next is 0.6652, 
noisyNet noise sample is [array([-1.4805188], dtype=float32), 0.45555463]. 
=============================================
[2019-04-02 19:04:15,160] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:04:15,169] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8486
[2019-04-02 19:04:15,179] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1144403.494121909 W.
[2019-04-02 19:04:15,184] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.8, 85.0, 1.0, 2.0, 0.492811344524741, 1.0, 1.0, 0.492811344524741, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1144403.494121909, 1144403.494121909, 234145.7842276324], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2288400.0000, 
sim time next is 2289000.0000, 
raw observation next is [23.9, 84.5, 1.0, 2.0, 0.958255360473227, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.094740275046413, 6.9112, 121.9252370380783, 1224785.738637386, 1130797.334164901, 232813.8496474147], 
processed observation next is [1.0, 0.4782608695652174, 0.4407407407407407, 0.845, 1.0, 1.0, 0.9503040005633655, 0.0, 0.5, -0.1904761904761905, 0.0, 1.0, -0.25, 0.018354027504641303, 0.0, 0.8094567806227231, 0.43742347808478066, 0.4038561907731789, 0.44771894162964365], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.43708134], dtype=float32), 0.9433121]. 
=============================================
[2019-04-02 19:04:15,197] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[44.47503 ]
 [45.231308]
 [44.880684]
 [45.11437 ]
 [44.3948  ]], R is [[44.89983749]
 [45.00056076]
 [45.05516052]
 [45.09797668]
 [45.13981628]].
[2019-04-02 19:04:15,424] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 1.4287607e-38 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 19:04:15,430] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9257
[2019-04-02 19:04:15,435] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.61666666666667, 79.50000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9120096385861692, 6.9112, 6.9112, 121.9260426156618, 665720.6377520802, 665720.6377520802, 177882.8864661623], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2157000.0000, 
sim time next is 2157600.0000, 
raw observation next is [25.53333333333333, 80.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9108287471367923, 6.9112, 6.9112, 121.9260426156618, 664968.2585516499, 664968.2585516499, 177704.0984791613], 
processed observation next is [0.0, 1.0, 0.5012345679012346, 0.8, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8885359339209903, 0.0, 0.0, 0.8094621288201359, 0.23748866376844638, 0.23748866376844638, 0.34173865092146405], 
reward next is 0.6583, 
noisyNet noise sample is [array([0.79998803], dtype=float32), 0.120999485]. 
=============================================
[2019-04-02 19:04:26,907] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:04:26,914] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8425
[2019-04-02 19:04:26,925] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.93333333333333, 95.66666666666667, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8555319157002897, 6.911199999999999, 6.9112, 121.9260426156335, 634625.2473588826, 634625.247358883, 167634.1716512044], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2342400.0000, 
sim time next is 2343000.0000, 
raw observation next is [21.91666666666666, 95.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8496079452834672, 6.911200000000001, 6.9112, 121.9260426156618, 631210.9551064811, 631210.9551064806, 166425.2015856122], 
processed observation next is [1.0, 0.08695652173913043, 0.36728395061728375, 0.9583333333333333, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8120099316043339, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2254324839666004, 0.22543248396660023, 0.3200484645877158], 
reward next is 0.6800, 
noisyNet noise sample is [array([-0.34721157], dtype=float32), 1.3636805]. 
=============================================
[2019-04-02 19:04:26,952] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[57.011612]
 [53.720535]
 [54.89353 ]
 [59.774334]
 [60.203526]], R is [[58.01483917]
 [58.11231995]
 [57.53119659]
 [57.64322662]
 [57.06679535]].
[2019-04-02 19:04:28,047] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-02 19:04:28,049] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 19:04:28,050] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 19:04:28,051] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 19:04:28,052] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:04:28,053] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 19:04:28,054] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:04:28,050] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:04:28,052] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 19:04:28,057] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:04:28,062] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:04:28,082] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run29
[2019-04-02 19:04:28,105] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run29
[2019-04-02 19:04:28,127] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run29
[2019-04-02 19:04:28,130] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run29
[2019-04-02 19:04:28,150] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run29
[2019-04-02 19:05:04,998] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.07829843], dtype=float32), -0.31630248]
[2019-04-02 19:05:04,999] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [21.56666666666667, 93.50000000000001, 1.0, 2.0, 0.7557753372438523, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 914929.3006171582, 914929.3006171582, 189344.3218577057]
[2019-04-02 19:05:05,000] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 19:05:05,002] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 2.9055233e-35 0.0000000e+00 0.0000000e+00], sampled 0.7360659838684429
[2019-04-02 19:05:05,003] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 914929.3006171582 W.
[2019-04-02 19:05:26,612] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.07829843], dtype=float32), -0.31630248]
[2019-04-02 19:05:26,615] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [23.33333333333333, 96.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9131790264720923, 6.911200000000001, 6.9112, 121.9260426156618, 666760.5378339073, 666760.5378339068, 178005.8491617356]
[2019-04-02 19:05:26,617] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 19:05:26,620] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.4854118575127331
[2019-04-02 19:05:50,174] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.07829843], dtype=float32), -0.31630248]
[2019-04-02 19:05:50,175] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [28.33333333333334, 55.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7949856891921119, 6.9112, 6.9112, 121.9260426156618, 590137.2496229106, 590137.2496229106, 159874.0998507687]
[2019-04-02 19:05:50,175] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 19:05:50,177] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5475109093873183
[2019-04-02 19:05:59,478] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.07829843], dtype=float32), -0.31630248]
[2019-04-02 19:05:59,479] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [32.0, 49.0, 1.0, 2.0, 0.605926130966612, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 696183.3693783558, 696183.3693783553, 159377.0989006576]
[2019-04-02 19:05:59,479] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 19:05:59,482] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5064238649760706
[2019-04-02 19:05:59,484] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 696183.3693783558 W.
[2019-04-02 19:06:09,864] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.07829843], dtype=float32), -0.31630248]
[2019-04-02 19:06:09,867] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [22.28333333333333, 98.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8206851948483084, 6.911200000000001, 6.9112, 121.9260426156618, 606116.8088853565, 606116.8088853561, 164275.8872245156]
[2019-04-02 19:06:09,868] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 19:06:09,871] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.15911935224704532
[2019-04-02 19:06:35,698] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7841.4716 2529798096.8055 831.0000
[2019-04-02 19:06:35,704] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8404.2460 2292976464.9981 697.0000
[2019-04-02 19:06:35,828] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8362.6183 2339490860.4802 616.0000
[2019-04-02 19:06:35,859] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8558.6918 2258293591.0027 536.0000
[2019-04-02 19:06:35,962] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8633.7484 2219185658.3071 543.0000
[2019-04-02 19:06:36,976] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 700000, evaluation results [700000.0, 7841.471632967488, 2529798096.8055396, 831.0, 8558.691799887378, 2258293591.0026627, 536.0, 8633.748369108142, 2219185658.307149, 543.0, 8362.618258210465, 2339490860.480159, 616.0, 8404.245980577798, 2292976464.998051, 697.0]
[2019-04-02 19:06:40,537] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:06:40,546] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3415
[2019-04-02 19:06:40,556] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.93333333333333, 53.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6421889455666085, 6.9112, 6.9112, 121.9260426156618, 478884.467889408, 478884.467889408, 136370.6598891702], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2409600.0000, 
sim time next is 2410200.0000, 
raw observation next is [25.7, 54.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6461579107245247, 6.911199999999999, 6.9112, 121.9260426156618, 481776.8396765483, 481776.8396765488, 136692.5249695722], 
processed observation next is [1.0, 0.9130434782608695, 0.5074074074074074, 0.54, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5576973884056557, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.17206315702733868, 0.17206315702733885, 0.2628702403261004], 
reward next is 0.7371, 
noisyNet noise sample is [array([-1.9088483], dtype=float32), 0.8097621]. 
=============================================
[2019-04-02 19:06:45,219] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:06:45,227] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4819
[2019-04-02 19:06:45,233] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.53333333333333, 74.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8485986260130671, 6.9112, 6.9112, 121.9260426156618, 626322.7777543446, 626322.7777543446, 167919.8251575038], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2708400.0000, 
sim time next is 2709000.0000, 
raw observation next is [25.9, 72.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8512010994171478, 6.9112, 6.9112, 121.9260426156618, 628056.3840254659, 628056.3840254659, 168306.8530565362], 
processed observation next is [0.0, 0.34782608695652173, 0.5148148148148147, 0.725, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8140013742714347, 0.0, 0.0, 0.8094621288201359, 0.2243058514376664, 0.2243058514376664, 0.32366702510872347], 
reward next is 0.6763, 
noisyNet noise sample is [array([0.18147539], dtype=float32), 1.1497129]. 
=============================================
[2019-04-02 19:06:45,248] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[63.846725]
 [63.8281  ]
 [63.823303]
 [63.785557]
 [63.769783]], R is [[63.91757202]
 [63.95547485]
 [63.99457169]
 [64.036026  ]
 [64.08247375]].
[2019-04-02 19:06:47,573] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.000000e+00 1.233992e-38 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-02 19:06:47,584] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1915
[2019-04-02 19:06:47,590] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [32.2, 28.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6355357676184603, 6.911200000000001, 6.9112, 121.9260426156618, 473564.1161300126, 473564.1161300121, 135309.5709067177], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2485200.0000, 
sim time next is 2485800.0000, 
raw observation next is [32.0, 28.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6349863065162088, 6.9112, 6.9112, 121.9260426156618, 473103.8559341411, 473103.8559341411, 135203.2202984085], 
processed observation next is [1.0, 0.782608695652174, 0.7407407407407407, 0.285, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5437328831452609, 0.0, 0.0, 0.8094621288201359, 0.16896566283362183, 0.16896566283362183, 0.2600061928815548], 
reward next is 0.7400, 
noisyNet noise sample is [array([-1.0874935], dtype=float32), -0.008243908]. 
=============================================
[2019-04-02 19:06:52,574] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 4.9766509e-34 3.5515741e-38 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 19:06:52,583] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7127
[2019-04-02 19:06:52,591] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1749048.438934333 W.
[2019-04-02 19:06:52,597] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [33.73333333333333, 30.33333333333334, 1.0, 2.0, 0.502535805611915, 1.0, 2.0, 0.502535805611915, 1.0, 2.0, 0.8015540927276225, 6.9112, 6.9112, 121.94756008, 1749048.438934333, 1749048.438934333, 344958.1370206301], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2557200.0000, 
sim time next is 2557800.0000, 
raw observation next is [33.7, 30.5, 1.0, 2.0, 0.8534654926159367, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9679340846292636, 6.911199999999999, 6.9112, 121.9260426156618, 1725843.333177642, 1725843.333177643, 341312.3912741224], 
processed observation next is [1.0, 0.6086956521739131, 0.8037037037037038, 0.305, 1.0, 1.0, 0.8255541578761152, 0.0, 0.5, -0.1904761904761905, 1.0, 1.0, 0.9599176057865794, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.616372618992015, 0.6163726189920153, 0.6563699832194662], 
reward next is 0.3436, 
noisyNet noise sample is [array([0.30007792], dtype=float32), 1.7837882]. 
=============================================
[2019-04-02 19:06:54,802] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:06:54,813] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5205
[2019-04-02 19:06:54,819] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.4, 99.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7261191552316664, 6.9112, 6.9112, 121.9260426156618, 542487.2112780936, 542487.2112780936, 148537.9979952934], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2612400.0000, 
sim time next is 2613000.0000, 
raw observation next is [20.35, 99.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.726996932031257, 6.911199999999999, 6.9112, 121.9260426156618, 543136.8749322682, 543136.8749322685, 148655.4999963283], 
processed observation next is [0.0, 0.21739130434782608, 0.3092592592592593, 0.995, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6587461650390714, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1939774553329529, 0.19397745533295305, 0.28587596153140055], 
reward next is 0.7141, 
noisyNet noise sample is [array([0.5271066], dtype=float32), 0.4504221]. 
=============================================
[2019-04-02 19:06:54,830] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[62.28675 ]
 [62.309067]
 [62.341057]
 [62.387043]
 [62.456776]], R is [[62.36383438]
 [62.45454788]
 [62.54458237]
 [62.63381958]
 [62.72205353]].
[2019-04-02 19:06:54,950] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 6.4632885e-38 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 19:06:54,960] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5492
[2019-04-02 19:06:54,969] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 931826.6431437822 W.
[2019-04-02 19:06:54,976] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.0, 94.00000000000001, 1.0, 2.0, 0.8175232331481772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 931826.6431437822, 931826.6431437822, 199582.9181594568], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2949000.0000, 
sim time next is 2949600.0000, 
raw observation next is [25.0, 94.0, 1.0, 2.0, 0.2547316346133072, 1.0, 1.0, 0.2547316346133072, 1.0, 1.0, 0.4055410583345485, 6.9112, 6.9112, 121.94756008, 871007.4932942021, 871007.4932942021, 239309.4923687888], 
processed observation next is [1.0, 0.13043478260869565, 0.48148148148148145, 0.94, 1.0, 1.0, 0.1127757554920324, 1.0, 0.5, 0.1127757554920324, 1.0, 0.5, 0.2569263229181856, 0.0, 0.0, 0.8096049824067558, 0.31107410474792935, 0.31107410474792935, 0.46021056224767076], 
reward next is 0.5398, 
noisyNet noise sample is [array([1.5293179], dtype=float32), -0.19867977]. 
=============================================
[2019-04-02 19:06:57,459] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:06:57,470] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3520
[2019-04-02 19:06:57,475] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.75, 83.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8798402113660994, 6.911200000000001, 6.9112, 121.9260426156618, 645869.8404517189, 645869.8404517184, 172858.3568637419], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2634600.0000, 
sim time next is 2635200.0000, 
raw observation next is [25.0, 83.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8968239180719121, 6.9112, 6.9112, 121.9260426156618, 656460.3237094629, 656460.3237094629, 175499.092310223], 
processed observation next is [0.0, 0.5217391304347826, 0.48148148148148145, 0.83, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.87102989758989, 0.0, 0.0, 0.8094621288201359, 0.23445011561052245, 0.23445011561052245, 0.33749825444273657], 
reward next is 0.6625, 
noisyNet noise sample is [array([-0.44015533], dtype=float32), 0.113152936]. 
=============================================
[2019-04-02 19:07:04,259] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 1.3724633e-25 9.1999319e-38 0.0000000e+00], sum to 1.0000
[2019-04-02 19:07:04,273] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7226
[2019-04-02 19:07:04,282] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 722174.6886609289 W.
[2019-04-02 19:07:04,289] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.0, 49.0, 1.0, 2.0, 0.3168375253522712, 1.0, 1.0, 0.3168375253522712, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 722174.6886609289, 722174.6886609293, 184550.1377973028], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2733000.0000, 
sim time next is 2733600.0000, 
raw observation next is [32.0, 49.0, 1.0, 2.0, 0.3040036468872777, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4839837065521208, 6.9112, 6.9112, 121.9260426156618, 692908.9285079209, 692908.9285079209, 196324.1788728306], 
processed observation next is [0.0, 0.6521739130434783, 0.7407407407407407, 0.49, 1.0, 1.0, 0.1714329129610449, 0.0, 0.5, -0.1904761904761905, 1.0, 0.5, 0.35497963319015097, 0.0, 0.0, 0.8094621288201359, 0.24746747446711462, 0.24746747446711462, 0.37754649783236655], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.95410746], dtype=float32), -0.9026969]. 
=============================================
[2019-04-02 19:07:04,522] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:07:04,535] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9612
[2019-04-02 19:07:04,544] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 701707.316049283 W.
[2019-04-02 19:07:04,550] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.0, 54.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9714557697361385, 6.9112, 6.9112, 121.9259664681445, 701707.316049283, 701707.316049283, 187160.1560418538], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2745000.0000, 
sim time next is 2745600.0000, 
raw observation next is [30.66666666666667, 56.66666666666666, 1.0, 1.0, 0.3107911179457228, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4947895815296655, 6.911199999999999, 6.9112, 121.9260426156618, 708386.6123708332, 708386.6123708336, 198167.5503380758], 
processed observation next is [0.0, 0.782608695652174, 0.6913580246913582, 0.5666666666666665, 1.0, 0.5, 0.17951323564966998, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.3684869769120819, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.252995218703869, 0.25299521870386915, 0.38109144295783803], 
reward next is 0.6189, 
noisyNet noise sample is [array([0.84415185], dtype=float32), -0.6927927]. 
=============================================
[2019-04-02 19:07:06,900] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.9128552e-07 0.0000000e+00 9.9999976e-01 1.1812865e-35 6.1777724e-36], sum to 1.0000
[2019-04-02 19:07:06,907] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3323
[2019-04-02 19:07:06,914] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.0, 62.0, 1.0, 2.0, 0.3063734178925341, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4878293802686679, 6.9112, 6.9112, 121.9260426156618, 700300.4287062406, 700300.4287062406, 196933.573720887], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3101400.0000, 
sim time next is 3102000.0000, 
raw observation next is [29.0, 60.66666666666667, 1.0, 2.0, 0.2978299735427664, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4744279876528148, 6.911199999999999, 6.9112, 121.9260426156618, 684264.2380425144, 684264.2380425149, 194565.9387211272], 
processed observation next is [1.0, 0.9130434782608695, 0.6296296296296297, 0.6066666666666667, 1.0, 1.0, 0.16408330183662667, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.34303498456601844, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2443800850151837, 0.24438008501518388, 0.37416526677139844], 
reward next is 0.6258, 
noisyNet noise sample is [array([0.40542153], dtype=float32), -0.656299]. 
=============================================
[2019-04-02 19:07:06,930] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[30.019156]
 [29.880653]
 [29.627935]
 [29.025911]
 [28.369757]], R is [[31.0658989 ]
 [31.37652016]
 [31.68010139]
 [31.97749138]
 [31.65771675]].
[2019-04-02 19:07:08,366] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.5086636e-03 2.1028625e-26 9.9149138e-01 8.5485941e-23 1.7237136e-19], sum to 1.0000
[2019-04-02 19:07:08,373] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5424
[2019-04-02 19:07:08,378] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.33333333333334, 59.66666666666666, 1.0, 2.0, 0.3544972963954325, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5690471255549036, 6.911200000000001, 6.9112, 121.9260426156618, 840483.6181604272, 840483.6181604267, 209550.9429446467], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3123600.0000, 
sim time next is 3124200.0000, 
raw observation next is [27.16666666666666, 60.83333333333334, 1.0, 2.0, 0.3440294565750835, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5521181974330525, 6.911199999999999, 6.9112, 121.9260426156618, 815190.5655976014, 815190.5655976018, 206566.6528170229], 
processed observation next is [1.0, 0.13043478260869565, 0.5617283950617282, 0.6083333333333334, 1.0, 1.0, 0.21908268639890893, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.4401477467913155, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.29113948771342907, 0.29113948771342923, 0.3972435631096594], 
reward next is 0.6028, 
noisyNet noise sample is [array([-1.3677064], dtype=float32), -0.21096091]. 
=============================================
[2019-04-02 19:07:09,776] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.4118329e-11 4.4267887e-38 1.0000000e+00 3.2363571e-33 5.2726873e-23], sum to 1.0000
[2019-04-02 19:07:09,788] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6272
[2019-04-02 19:07:09,795] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [23.98333333333333, 99.33333333333334, 1.0, 2.0, 0.3251632039008596, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5176704104364053, 6.9112, 6.9112, 121.9260426156618, 741160.7637530809, 741160.7637530809, 202129.7002068244], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3017400.0000, 
sim time next is 3018000.0000, 
raw observation next is [23.96666666666667, 98.66666666666667, 1.0, 2.0, 0.323023627685248, 0.0, 2.0, 0.0, 1.0, 2.0, 0.514264135420019, 6.9112, 6.9112, 121.9260426156618, 736281.5784411214, 736281.5784411214, 201534.3798933507], 
processed observation next is [1.0, 0.9565217391304348, 0.4432098765432099, 0.9866666666666667, 1.0, 1.0, 0.19407574724434287, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.39283016927502373, 0.0, 0.0, 0.8094621288201359, 0.2629577065861148, 0.2629577065861148, 0.3875661151795206], 
reward next is 0.6124, 
noisyNet noise sample is [array([0.7253355], dtype=float32), -0.8495897]. 
=============================================
[2019-04-02 19:07:09,808] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[28.622065]
 [28.642593]
 [29.538744]
 [28.864464]
 [28.639854]], R is [[29.19417   ]
 [28.90222931]
 [29.2532196 ]
 [29.56957245]
 [29.88155556]].
[2019-04-02 19:07:13,346] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.7300205e-03 2.5854061e-25 9.9626994e-01 3.1566464e-31 6.0169643e-13], sum to 1.0000
[2019-04-02 19:07:13,357] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7081
[2019-04-02 19:07:13,366] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.55, 79.5, 1.0, 2.0, 0.5717400686474631, 1.0, 2.0, 0.5717400686474631, 1.0, 1.0, 0.9102288095608481, 6.9112, 6.9112, 121.94756008, 1956274.800813246, 1956274.800813246, 380193.2594535471], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3079800.0000, 
sim time next is 3080400.0000, 
raw observation next is [27.03333333333333, 81.0, 1.0, 2.0, 1.02, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9977734948820727, 7.571755078387488, 6.9112, 121.923423314629, 2216688.263425536, 1878432.2903571, 381762.1949644524], 
processed observation next is [1.0, 0.6521739130434783, 0.55679012345679, 0.81, 1.0, 1.0, 1.0238095238095237, 0.0, 0.5, -0.1904761904761905, 1.0, 1.0, 0.9972168686025908, 0.0660555078387488, 0.0, 0.8094447393851593, 0.7916743797948342, 0.6708686751275358, 0.7341580672393315], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.29486576], dtype=float32), 1.030739]. 
=============================================
[2019-04-02 19:07:19,826] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.9809092e-01 1.1765477e-34 1.9090198e-03 1.4369132e-27 2.0494073e-26], sum to 1.0000
[2019-04-02 19:07:19,834] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5406
[2019-04-02 19:07:19,843] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 955917.1677712046 W.
[2019-04-02 19:07:19,848] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.33333333333334, 88.66666666666666, 1.0, 2.0, 0.4193227483047534, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6675754716896776, 6.911199999999999, 6.9112, 121.9260426156618, 955917.1677712046, 955917.167771205, 230137.4258033701], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2965200.0000, 
sim time next is 2965800.0000, 
raw observation next is [25.41666666666666, 87.33333333333333, 1.0, 2.0, 0.8190852415188928, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 933608.1308842349, 933608.1308842344, 199908.2334074221], 
processed observation next is [1.0, 0.30434782608695654, 0.49691358024691334, 0.8733333333333333, 1.0, 1.0, 0.7846252875224915, 0.0, 1.0, -0.1904761904761905, 0.0, 0.5, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.3334314753157982, 0.33343147531579803, 0.38443891039888867], 
reward next is 0.6156, 
noisyNet noise sample is [array([0.6827153], dtype=float32), 2.0502214]. 
=============================================
[2019-04-02 19:07:33,919] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:07:33,931] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1877
[2019-04-02 19:07:33,938] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 719818.1563288034 W.
[2019-04-02 19:07:33,944] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.1, 89.0, 1.0, 2.0, 0.3158041362119263, 1.0, 1.0, 0.3158041362119263, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 719818.1563288034, 719818.1563288039, 184296.5004075969], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3367800.0000, 
sim time next is 3368400.0000, 
raw observation next is [25.06666666666666, 90.66666666666667, 1.0, 2.0, 0.6445027952951419, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 734520.5581464361, 734520.5581464361, 165901.7460786368], 
processed observation next is [0.0, 1.0, 0.48395061728395034, 0.9066666666666667, 1.0, 1.0, 0.576789042018026, 0.0, 0.5, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.26232877076658434, 0.26232877076658434, 0.31904181938199383], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.29197836], dtype=float32), 1.4079113]. 
=============================================
[2019-04-02 19:07:43,684] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-02 19:07:43,685] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 19:07:43,685] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:07:43,686] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 19:07:43,688] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 19:07:43,690] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 19:07:43,694] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:07:43,692] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 19:07:43,696] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:07:43,694] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:07:43,699] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:07:43,724] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run30
[2019-04-02 19:07:43,745] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run30
[2019-04-02 19:07:43,781] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run30
[2019-04-02 19:07:43,781] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run30
[2019-04-02 19:07:43,782] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run30
[2019-04-02 19:07:51,315] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.1551133], dtype=float32), -0.295921]
[2019-04-02 19:07:51,316] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [17.05, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.547065617276422, 6.911199999999999, 6.9112, 121.9260426156618, 390618.7067281082, 390618.7067281086, 116649.2667018429]
[2019-04-02 19:07:51,317] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 19:07:51,319] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.066634489727314
[2019-04-02 19:08:08,269] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.1551133], dtype=float32), -0.295921]
[2019-04-02 19:08:08,271] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [31.13333333333333, 35.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6594818029382168, 6.9112, 6.9112, 121.9260426156618, 492814.3811725732, 492814.3811725732, 140368.5244212824]
[2019-04-02 19:08:08,271] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 19:08:08,273] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.12400991757592317
[2019-04-02 19:08:09,600] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.1551133], dtype=float32), -0.295921]
[2019-04-02 19:08:09,603] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [28.0, 51.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6993050426170975, 6.9112, 6.9112, 121.9260426156618, 522227.4340424818, 522227.4340424818, 146015.3808211631]
[2019-04-02 19:08:09,604] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 19:08:09,608] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.2573760989973294
[2019-04-02 19:08:47,669] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.1551133], dtype=float32), -0.295921]
[2019-04-02 19:08:47,673] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [30.33333333333333, 69.66666666666666, 1.0, 2.0, 0.7475456678430423, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 852020.7078269114, 852020.7078269114, 185344.9131271234]
[2019-04-02 19:08:47,675] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 19:08:47,681] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.000000e+00 0.000000e+00 3.505027e-26 0.000000e+00 0.000000e+00], sampled 0.33750950613520403
[2019-04-02 19:08:47,683] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 852020.7078269114 W.
[2019-04-02 19:08:49,416] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.1551133], dtype=float32), -0.295921]
[2019-04-02 19:08:49,417] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [32.9, 55.0, 1.0, 2.0, 0.8288271650684451, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9977734948820727, 6.9112, 6.9112, 121.9260425744836, 1659843.467605279, 1659843.467605279, 342034.6306187785]
[2019-04-02 19:08:49,417] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 19:08:49,420] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [4.4400534e-01 6.3047796e-34 5.5599469e-01 1.6188707e-23 2.9023224e-15], sampled 0.4019948684245237
[2019-04-02 19:08:49,421] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1659843.467605279 W.
[2019-04-02 19:09:06,612] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.1551133], dtype=float32), -0.295921]
[2019-04-02 19:09:06,613] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [24.89149301833334, 111.6767336833333, 1.0, 2.0, 0.7612603492916421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 867660.965654233, 867660.965654233, 188065.2258558968]
[2019-04-02 19:09:06,618] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 19:09:06,623] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.000000e+00 0.000000e+00 1.319026e-30 0.000000e+00 0.000000e+00], sampled 0.22199527210088932
[2019-04-02 19:09:06,626] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 867660.965654233 W.
[2019-04-02 19:09:18,167] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.1551133], dtype=float32), -0.295921]
[2019-04-02 19:09:18,169] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [26.33333333333334, 73.0, 1.0, 1.0, 0.8978160105283057, 1.0, 1.0, 0.7622726672405876, 1.0, 2.0, 0.9977734948820727, 6.9112, 6.9112, 124.016047559946, 2609093.034142621, 2609093.034142621, 487254.4888559289]
[2019-04-02 19:09:18,170] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 19:09:18,174] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.11967288102862761
[2019-04-02 19:09:18,175] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 0, 0, 1] for the demand 2609093.034142621 W.
[2019-04-02 19:09:19,326] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.1551133], dtype=float32), -0.295921]
[2019-04-02 19:09:19,327] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [27.0, 75.66666666666667, 1.0, 2.0, 0.6936828865059956, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 792931.828936699, 792931.828936699, 175066.3687268313]
[2019-04-02 19:09:19,330] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 19:09:19,332] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.0000000e+00 0.0000000e+00 1.7791722e-24 0.0000000e+00 0.0000000e+00], sampled 0.48403339823993174
[2019-04-02 19:09:19,333] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 792931.828936699 W.
[2019-04-02 19:09:28,531] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.1551133], dtype=float32), -0.295921]
[2019-04-02 19:09:28,532] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [24.25, 82.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8240118829216863, 6.9112, 6.9112, 121.9260426156618, 609167.957768948, 609167.957768948, 164511.0706015757]
[2019-04-02 19:09:28,533] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 19:09:28,536] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.35392529260510386
[2019-04-02 19:09:42,636] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.1551133], dtype=float32), -0.295921]
[2019-04-02 19:09:42,639] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [19.26666666666667, 71.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.487911270521725, 6.911199999999999, 6.9112, 121.9260426156618, 348380.5230565235, 348380.5230565239, 113758.1932189066]
[2019-04-02 19:09:42,639] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 19:09:42,644] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8865188707141467
[2019-04-02 19:09:51,081] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8559.2892 2258326566.4970 536.0000
[2019-04-02 19:09:51,650] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8633.7484 2219185658.3071 543.0000
[2019-04-02 19:09:51,732] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8362.3547 2339666400.6670 615.0000
[2019-04-02 19:09:51,743] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7839.9754 2530289318.3878 831.0000
[2019-04-02 19:09:51,785] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8404.2460 2292976464.9981 697.0000
[2019-04-02 19:09:52,801] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 725000, evaluation results [725000.0, 7839.975421745612, 2530289318.3878093, 831.0, 8559.289222897212, 2258326566.4970026, 536.0, 8633.748369108142, 2219185658.307149, 543.0, 8362.354739013106, 2339666400.667019, 615.0, 8404.245980577798, 2292976464.998051, 697.0]
[2019-04-02 19:09:58,704] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.9999380e-01 6.5225289e-35 6.1799510e-06 1.0364937e-24 6.3393030e-15], sum to 1.0000
[2019-04-02 19:09:58,712] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3658
[2019-04-02 19:09:58,722] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 750244.7021354008 W.
[2019-04-02 19:09:58,730] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [24.76666666666667, 93.66666666666667, 1.0, 2.0, 0.6582931482492995, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 750244.7021354008, 750244.7021354008, 168396.3730724604], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3370800.0000, 
sim time next is 3371400.0000, 
raw observation next is [24.65, 93.5, 1.0, 2.0, 0.3264885609160157, 1.0, 1.0, 0.3264885609160157, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 744183.1827555321, 744183.1827555321, 186938.7417137322], 
processed observation next is [1.0, 0.0, 0.46851851851851845, 0.935, 1.0, 1.0, 0.19820066775716155, 1.0, 0.5, 0.19820066775716155, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.26577970812697577, 0.26577970812697577, 0.35949758021871575], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.10162423], dtype=float32), 0.7593289]. 
=============================================
[2019-04-02 19:10:13,625] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.8800200e-12 3.1467564e-18 0.0000000e+00 5.4063640e-14 1.0000000e+00], sum to 1.0000
[2019-04-02 19:10:13,636] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4801
[2019-04-02 19:10:13,645] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.1, 79.33333333333333, 1.0, 2.0, 0.3942918033226305, 1.0, 2.0, 0.3942918033226305, 1.0, 2.0, 0.627725392077172, 6.911199999999999, 6.9112, 121.94756008, 1348627.473480047, 1348627.473480048, 294590.6564314394], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3595200.0000, 
sim time next is 3595800.0000, 
raw observation next is [25.05, 81.16666666666667, 1.0, 2.0, 0.4024730786854949, 1.0, 2.0, 0.4024730786854949, 1.0, 2.0, 0.6407502489003891, 6.911200000000001, 6.9112, 121.94756008, 1376635.680894116, 1376635.680894116, 298167.1744601318], 
processed observation next is [1.0, 0.6086956521739131, 0.48333333333333334, 0.8116666666666668, 1.0, 1.0, 0.2886584270065416, 1.0, 1.0, 0.2886584270065416, 1.0, 1.0, 0.5509378111254863, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.49165560031932715, 0.49165560031932715, 0.5733984124233303], 
reward next is 0.4266, 
noisyNet noise sample is [array([-0.59738094], dtype=float32), 0.5212189]. 
=============================================
[2019-04-02 19:10:15,271] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.4824324e-15 1.9901725e-29 9.9999988e-01 2.2603415e-14 6.3756694e-08], sum to 1.0000
[2019-04-02 19:10:15,285] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5203
[2019-04-02 19:10:15,293] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.0, 83.0, 1.0, 2.0, 0.434887699322391, 1.0, 2.0, 0.434887699322391, 1.0, 2.0, 0.6923553806248217, 6.911199999999999, 6.9112, 121.94756008, 1487615.66203356, 1487615.662033561, 312701.9271479188], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3597000.0000, 
sim time next is 3597600.0000, 
raw observation next is [25.0, 83.0, 1.0, 2.0, 0.6545165850533536, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9962182368248236, 6.911199999999999, 6.9112, 121.9260426156618, 1462114.651133266, 1462114.651133266, 308892.9336835798], 
processed observation next is [1.0, 0.6521739130434783, 0.48148148148148145, 0.83, 1.0, 1.0, 0.5887102203016115, 0.0, 0.5, -0.1904761904761905, 1.0, 1.0, 0.9952727960310295, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.5221838039761665, 0.5221838039761665, 0.5940248724684226], 
reward next is 0.4060, 
noisyNet noise sample is [array([-0.73331034], dtype=float32), 1.043073]. 
=============================================
[2019-04-02 19:10:18,331] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:10:18,336] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2699
[2019-04-02 19:10:18,347] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.13333333333333, 96.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8982417194848433, 6.911200000000001, 6.9112, 121.9260426156618, 656966.5603917178, 656966.5603917174, 175794.2994736894], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3634800.0000, 
sim time next is 3635400.0000, 
raw observation next is [23.16666666666666, 95.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8922125006494834, 6.911199999999999, 6.9112, 121.9260426156618, 653252.4559505237, 653252.4559505242, 174853.9624991521], 
processed observation next is [1.0, 0.043478260869565216, 0.41358024691358003, 0.9583333333333335, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8652656258118543, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.23330444855375848, 0.23330444855375865, 0.3362576201906771], 
reward next is 0.6637, 
noisyNet noise sample is [array([0.31343123], dtype=float32), -1.3120999]. 
=============================================
[2019-04-02 19:10:21,672] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 4.7416528e-36 4.2282028e-21], sum to 1.0000
[2019-04-02 19:10:21,682] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0881
[2019-04-02 19:10:21,694] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 847598.1076451079 W.
[2019-04-02 19:10:21,699] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.33333333333334, 92.33333333333334, 1.0, 2.0, 0.3718337563427875, 1.0, 1.0, 0.3718337563427875, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 847598.1076451079, 847598.1076451074, 198593.389404853], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3901200.0000, 
sim time next is 3901800.0000, 
raw observation next is [26.16666666666667, 93.16666666666667, 1.0, 2.0, 0.3701207411953183, 1.0, 2.0, 0.3701207411953183, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 843691.126295877, 843691.1262958775, 198140.1194663878], 
processed observation next is [0.0, 0.13043478260869565, 0.5246913580246916, 0.9316666666666668, 1.0, 1.0, 0.25014373951823604, 1.0, 1.0, 0.25014373951823604, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.3013182593913847, 0.30131825939138485, 0.381038691281515], 
reward next is 0.6190, 
noisyNet noise sample is [array([0.6244113], dtype=float32), 0.06841333]. 
=============================================
[2019-04-02 19:10:23,178] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:10:23,192] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4005
[2019-04-02 19:10:23,198] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.5, 62.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.912312707008552, 6.911200000000001, 6.9112, 121.9260426156618, 666405.4145350732, 666405.4145350727, 177837.579856209], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3828600.0000, 
sim time next is 3829200.0000, 
raw observation next is [28.66666666666666, 60.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9060168029581515, 6.911199999999999, 6.9112, 121.9260426156618, 662623.5769445844, 662623.5769445848, 176835.947862473], 
processed observation next is [0.0, 0.30434782608695654, 0.6172839506172837, 0.6066666666666667, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8825210036976894, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2366512774802087, 0.23665127748020887, 0.34006913050475573], 
reward next is 0.6599, 
noisyNet noise sample is [array([0.22665878], dtype=float32), -1.5233207]. 
=============================================
[2019-04-02 19:10:26,965] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 1.9386269e-37 0.0000000e+00 5.9493225e-34], sum to 1.0000
[2019-04-02 19:10:26,972] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4731
[2019-04-02 19:10:26,982] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 893824.1039252477 W.
[2019-04-02 19:10:26,988] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.33333333333333, 84.0, 1.0, 2.0, 0.3921008717842107, 0.0, 2.0, 0.0, 1.0, 1.0, 0.624237357714349, 6.9112, 6.9112, 121.9260426156618, 893824.1039252477, 893824.1039252477, 221681.1940410416], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3879600.0000, 
sim time next is 3880200.0000, 
raw observation next is [28.16666666666667, 86.5, 1.0, 2.0, 0.4003573666513287, 1.0, 1.0, 0.4003573666513287, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 912656.6261011969, 912656.6261011973, 206289.6375583168], 
processed observation next is [0.0, 0.9130434782608695, 0.5987654320987656, 0.865, 1.0, 1.0, 0.2861397222039628, 1.0, 0.5, 0.2861397222039628, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.3259487950361417, 0.3259487950361419, 0.39671084145830154], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.01415864], dtype=float32), 0.47739735]. 
=============================================
[2019-04-02 19:10:27,416] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:10:27,425] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6318
[2019-04-02 19:10:27,437] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 826386.7857937005 W.
[2019-04-02 19:10:27,442] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.46666666666667, 55.0, 1.0, 2.0, 0.2416890574904339, 1.0, 1.0, 0.2416890574904339, 1.0, 2.0, 0.3847768507878518, 6.9112, 6.9112, 121.94756008, 826386.7857937005, 826386.7857937005, 234697.7555215946], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3778800.0000, 
sim time next is 3779400.0000, 
raw observation next is [33.73333333333333, 51.0, 1.0, 2.0, 0.7003968510515377, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 798254.5109956822, 798254.5109956822, 176221.7921146203], 
processed observation next is [1.0, 0.7391304347826086, 0.804938271604938, 0.51, 1.0, 1.0, 0.6433295845851639, 0.0, 0.5, -0.1904761904761905, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2850908967841722, 0.2850908967841722, 0.3388880617588852], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6289747], dtype=float32), 0.47737324]. 
=============================================
[2019-04-02 19:10:29,483] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:10:29,493] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8790
[2019-04-02 19:10:29,501] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 825403.4045741934 W.
[2019-04-02 19:10:29,506] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.43333333333334, 91.66666666666667, 1.0, 2.0, 0.2414016078369333, 1.0, 2.0, 0.2414016078369333, 1.0, 2.0, 0.3843192215778972, 6.911199999999999, 6.9112, 121.94756008, 825403.4045741934, 825403.4045741939, 234597.1862819515], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3914400.0000, 
sim time next is 3915000.0000, 
raw observation next is [26.65, 90.5, 1.0, 2.0, 0.364144391826782, 0.0, 1.0, 0.0, 1.0, 2.0, 0.5797297311431341, 6.911199999999999, 6.9112, 121.9260426156618, 830060.6464857913, 830060.6464857918, 213297.2832676907], 
processed observation next is [0.0, 0.30434782608695654, 0.5425925925925925, 0.905, 1.0, 1.0, 0.2430290378890262, 0.0, 0.5, -0.1904761904761905, 1.0, 1.0, 0.4746621639289176, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2964502308877826, 0.2964502308877828, 0.4101870832070975], 
reward next is 0.5898, 
noisyNet noise sample is [array([-0.10526925], dtype=float32), -0.19174536]. 
=============================================
[2019-04-02 19:10:29,520] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[42.945766]
 [42.45801 ]
 [42.732918]
 [42.95163 ]
 [42.803295]], R is [[43.24661636]
 [43.36300278]
 [42.92937469]
 [43.14826965]
 [43.30464172]].
[2019-04-02 19:10:30,578] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 3.5361853e-33 1.6676898e-17 7.0744886e-19 1.3598015e-15], sum to 1.0000
[2019-04-02 19:10:30,588] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0828
[2019-04-02 19:10:30,597] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 739548.098164931 W.
[2019-04-02 19:10:30,601] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.25, 74.5, 1.0, 2.0, 0.6489120677577349, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 739548.098164931, 739548.0981649306, 166694.6323133875], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3801000.0000, 
sim time next is 3801600.0000, 
raw observation next is [27.0, 74.0, 1.0, 2.0, 0.3163163378888698, 1.0, 1.0, 0.3163163378888698, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 720986.1760315938, 720986.1760315943, 184422.0534544258], 
processed observation next is [0.0, 0.0, 0.5555555555555556, 0.74, 1.0, 1.0, 0.18609087843913075, 1.0, 0.5, 0.18609087843913075, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2574950628684264, 0.25749506286842655, 0.354657795104665], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.016507], dtype=float32), -0.13390617]. 
=============================================
[2019-04-02 19:10:45,665] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.2870394e-13 0.0000000e+00 1.0000000e+00 2.9740506e-28 6.2572320e-27], sum to 1.0000
[2019-04-02 19:10:45,672] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9272
[2019-04-02 19:10:45,678] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.53333333333333, 87.0, 1.0, 2.0, 0.3260437299933996, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5190722366524618, 6.911199999999999, 6.9112, 121.9260426156618, 743168.7638638788, 743168.7638638793, 202374.9590454974], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4045200.0000, 
sim time next is 4045800.0000, 
raw observation next is [25.41666666666667, 86.5, 1.0, 2.0, 0.3214566189667651, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5117694065062832, 6.911199999999999, 6.9112, 121.9260426156618, 732708.1213645973, 732708.1213645978, 201099.3209289427], 
processed observation next is [1.0, 0.8260869565217391, 0.49691358024691373, 0.865, 1.0, 1.0, 0.19221026067472033, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.38971175813285397, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2616814719159276, 0.2616814719159278, 0.38672946332488983], 
reward next is 0.6133, 
noisyNet noise sample is [array([0.46702632], dtype=float32), -0.0394224]. 
=============================================
[2019-04-02 19:10:47,264] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.9814522e-01 1.3811024e-28 5.5205838e-27 1.7138827e-17 1.8547996e-03], sum to 1.0000
[2019-04-02 19:10:47,274] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0783
[2019-04-02 19:10:47,284] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1539901.498088144 W.
[2019-04-02 19:10:47,294] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.5, 56.0, 1.0, 2.0, 0.7067707994108264, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9775515285144386, 6.911200000000001, 6.9112, 121.9260426156618, 1539901.498088144, 1539901.498088143, 314657.4496958081], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4267800.0000, 
sim time next is 4268400.0000, 
raw observation next is [29.0, 52.66666666666667, 1.0, 2.0, 0.456437594600397, 1.0, 1.0, 0.456437594600397, 1.0, 2.0, 0.7270262796185916, 6.9112, 6.9112, 121.94756008, 1572708.827928026, 1572708.827928026, 322760.473762722], 
processed observation next is [1.0, 0.391304347826087, 0.6296296296296297, 0.5266666666666667, 1.0, 1.0, 0.3529018983338059, 1.0, 0.5, 0.3529018983338059, 1.0, 1.0, 0.6587828495232395, 0.0, 0.0, 0.8096049824067558, 0.5616817242600093, 0.5616817242600093, 0.6206932187744654], 
reward next is 0.3793, 
noisyNet noise sample is [array([-0.8878649], dtype=float32), -0.79329574]. 
=============================================
[2019-04-02 19:10:48,930] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 6.2102812e-15 1.3789104e-36 6.8588613e-29], sum to 1.0000
[2019-04-02 19:10:48,943] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0903
[2019-04-02 19:10:48,954] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2027288.155501097 W.
[2019-04-02 19:10:48,961] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.66666666666667, 37.33333333333334, 1.0, 2.0, 0.8873481949182841, 1.0, 2.0, 0.8873481949182841, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 2027288.155501097, 2027288.155501097, 381340.8293606893], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4288800.0000, 
sim time next is 4289400.0000, 
raw observation next is [33.5, 39.0, 1.0, 2.0, 0.5909194380075026, 1.0, 2.0, 0.5909194380075026, 1.0, 1.0, 0.9407629902104834, 6.9112, 6.9112, 121.94756008, 2021973.511030818, 2021973.511030818, 390450.2363444224], 
processed observation next is [1.0, 0.6521739130434783, 0.7962962962962963, 0.39, 1.0, 1.0, 0.5129993309613126, 1.0, 1.0, 0.5129993309613126, 1.0, 0.5, 0.9259537377631041, 0.0, 0.0, 0.8096049824067558, 0.7221333967967207, 0.7221333967967207, 0.7508658391238892], 
reward next is 0.2491, 
noisyNet noise sample is [array([-0.4434214], dtype=float32), 0.25600532]. 
=============================================
[2019-04-02 19:10:59,494] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-02 19:10:59,496] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 19:10:59,497] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 19:10:59,497] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:10:59,497] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 19:10:59,499] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 19:10:59,500] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:10:59,499] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:10:59,501] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 19:10:59,502] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:10:59,506] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:10:59,528] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run31
[2019-04-02 19:10:59,550] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run31
[2019-04-02 19:10:59,550] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run31
[2019-04-02 19:10:59,551] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run31
[2019-04-02 19:10:59,571] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run31
[2019-04-02 19:11:22,336] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.03804047], dtype=float32), -0.26305625]
[2019-04-02 19:11:22,337] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [21.06666666666667, 66.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5344567247349234, 6.911200000000001, 6.9112, 121.9260426156618, 389379.700589987, 389379.7005899866, 120601.5072428894]
[2019-04-02 19:11:22,337] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 19:11:22,338] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.3428972751206445
[2019-04-02 19:11:22,571] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.03804047], dtype=float32), -0.26305625]
[2019-04-02 19:11:22,574] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [22.96666666666667, 53.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5032843822352485, 6.9112, 6.9112, 121.9260426156618, 364014.9683480734, 364014.9683480734, 117012.0077609248]
[2019-04-02 19:11:22,577] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 19:11:22,578] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6263385357175286
[2019-04-02 19:12:28,476] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.03804047], dtype=float32), -0.26305625]
[2019-04-02 19:12:28,477] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [32.92509937, 79.55197807333334, 1.0, 2.0, 0.9509523869975804, 1.0, 2.0, 0.9509523869975804, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260425868384, 2169454.815903644, 2169454.815903643, 410083.3045211857]
[2019-04-02 19:12:28,477] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 19:12:28,479] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.0000000e+00 0.0000000e+00 2.2084979e-30 2.6043604e-31 3.9483926e-33], sampled 0.04108766972722422
[2019-04-02 19:12:28,480] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2169454.815903644 W.
[2019-04-02 19:12:35,541] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.03804047], dtype=float32), -0.26305625]
[2019-04-02 19:12:35,541] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [27.15, 65.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9135457623359174, 6.9112, 6.9112, 121.9260426156618, 674191.67476642, 674191.67476642, 176354.9511952101]
[2019-04-02 19:12:35,542] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 19:12:35,544] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.24750925772213028
[2019-04-02 19:12:50,965] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.03804047], dtype=float32), -0.26305625]
[2019-04-02 19:12:50,966] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [24.97977425333334, 59.00590971333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6359298000559191, 6.9112, 6.9112, 121.9260426156618, 473904.9350092739, 473904.9350092739, 135397.6660975922]
[2019-04-02 19:12:50,966] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 19:12:50,968] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8110710702886678
[2019-04-02 19:13:06,397] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8633.7484 2219185658.3071 543.0000
[2019-04-02 19:13:06,576] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8557.8765 2258327642.4849 536.0000
[2019-04-02 19:13:06,723] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8361.7757 2339539473.4650 616.0000
[2019-04-02 19:13:06,792] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7841.4107 2529829775.6138 831.0000
[2019-04-02 19:13:06,874] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8404.1885 2293006379.4818 697.0000
[2019-04-02 19:13:07,892] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 750000, evaluation results [750000.0, 7841.4107121824145, 2529829775.613778, 831.0, 8557.876498474936, 2258327642.484933, 536.0, 8633.748369108142, 2219185658.307149, 543.0, 8361.775716815082, 2339539473.464993, 616.0, 8404.188452724367, 2293006379.4818354, 697.0]
[2019-04-02 19:13:09,238] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.9018574e-01 1.7511072e-36 9.8143211e-03 2.0607888e-31 1.7778104e-26], sum to 1.0000
[2019-04-02 19:13:09,250] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4522
[2019-04-02 19:13:09,260] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 716911.2798343408 W.
[2019-04-02 19:13:09,268] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333334, 75.66666666666666, 1.0, 2.0, 0.3145294051701781, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5007410565385135, 6.911199999999999, 6.9112, 121.9260426156618, 716911.2798343408, 716911.2798343413, 199189.1443165046], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4445400.0000, 
sim time next is 4446000.0000, 
raw observation next is [26.8, 75.0, 1.0, 2.0, 0.6170205138642776, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 708803.6344677388, 708803.6344677388, 161306.0684577996], 
processed observation next is [0.0, 0.4782608695652174, 0.5481481481481482, 0.75, 1.0, 1.0, 0.5440720403146162, 0.0, 1.0, -0.1904761904761905, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2531441551670496, 0.2531441551670496, 0.3102039778034608], 
reward next is 0.6898, 
noisyNet noise sample is [array([0.07704052], dtype=float32), -0.4369104]. 
=============================================
[2019-04-02 19:13:09,282] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[48.32898 ]
 [47.662266]
 [46.987705]
 [46.372948]
 [45.851734]], R is [[48.91067505]
 [49.03851318]
 [49.16348648]
 [48.67185211]
 [48.18513489]].
[2019-04-02 19:13:12,792] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:13:12,804] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6263
[2019-04-02 19:13:12,815] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 703440.2113786127 W.
[2019-04-02 19:13:12,820] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 74.0, 1.0, 2.0, 0.3078042481952489, 0.0, 2.0, 0.0, 1.0, 1.0, 0.4901018843789517, 6.911199999999999, 6.9112, 121.9260426156618, 703440.2113786127, 703440.2113786131, 197324.2144081687], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4311000.0000, 
sim time next is 4311600.0000, 
raw observation next is [27.0, 74.0, 1.0, 2.0, 0.6145458084730885, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 706066.4528876122, 706066.4528876122, 160877.7662738368], 
processed observation next is [1.0, 0.9130434782608695, 0.5555555555555556, 0.74, 1.0, 1.0, 0.5411259624679625, 0.0, 1.0, -0.1904761904761905, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288201359, 0.25216659031700434, 0.25216659031700434, 0.30938031975737845], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0868361], dtype=float32), 0.64949226]. 
=============================================
[2019-04-02 19:13:12,914] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:13:12,923] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0374
[2019-04-02 19:13:12,930] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 687334.7762708185 W.
[2019-04-02 19:13:12,938] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.98333333333333, 93.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9453945854927385, 6.9112, 6.9112, 121.9260426156618, 687334.7762708185, 687334.7762708185, 182889.4582202184], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4503000.0000, 
sim time next is 4503600.0000, 
raw observation next is [24.0, 94.0, 1.0, 1.0, 0.3039670419187792, 1.0, 1.0, 0.3039670419187792, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 693680.6173211023, 693680.6173211023, 181458.3282376975], 
processed observation next is [0.0, 0.13043478260869565, 0.4444444444444444, 0.94, 1.0, 0.5, 0.1713893356175943, 1.0, 0.5, 0.1713893356175943, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2477430776146794, 0.2477430776146794, 0.34895832353403367], 
reward next is 0.6510, 
noisyNet noise sample is [array([1.9738382], dtype=float32), -2.6193027]. 
=============================================
[2019-04-02 19:13:16,048] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 4.6910504e-33 1.8947650e-31], sum to 1.0000
[2019-04-02 19:13:16,060] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4454
[2019-04-02 19:13:16,072] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 808556.3412768769 W.
[2019-04-02 19:13:16,078] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.6, 87.5, 1.0, 2.0, 0.236477028507598, 1.0, 1.0, 0.236477028507598, 1.0, 1.0, 0.3764791309032435, 6.911199999999999, 6.9112, 121.94756008, 808556.3412768769, 808556.3412768773, 232881.4088341586], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4347000.0000, 
sim time next is 4347600.0000, 
raw observation next is [24.8, 87.0, 1.0, 2.0, 0.4217524949697395, 0.0, 1.0, 0.0, 1.0, 2.0, 0.6715346052219874, 6.911199999999999, 6.9112, 121.9260426156618, 963978.614563593, 963978.6145635934, 230870.9554153767], 
processed observation next is [1.0, 0.30434782608695654, 0.4740740740740741, 0.87, 1.0, 1.0, 0.3116101130592137, 0.0, 0.5, -0.1904761904761905, 1.0, 1.0, 0.5894182565274841, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.3442780766298546, 0.34427807662985477, 0.4439826065680321], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.4329429], dtype=float32), -0.09416236]. 
=============================================
[2019-04-02 19:13:18,441] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 2.8550106e-31 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 19:13:18,449] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5324
[2019-04-02 19:13:18,457] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2309620.307428805 W.
[2019-04-02 19:13:18,467] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.8, 49.0, 1.0, 2.0, 0.7230213087425855, 1.0, 2.0, 0.6748753163477276, 1.0, 2.0, 0.9977734948820727, 6.911200000000001, 6.9112, 121.94756008, 2309620.307428805, 2309620.307428805, 435483.5778335813], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4379400.0000, 
sim time next is 4380000.0000, 
raw observation next is [32.73333333333333, 49.0, 1.0, 2.0, 1.01714150460938, 1.0, 2.0, 1.01714150460938, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.925902275967, 2320651.585656285, 2320651.585656285, 441649.5598844623], 
processed observation next is [1.0, 0.6956521739130435, 0.767901234567901, 0.49, 1.0, 1.0, 1.0204065531064046, 1.0, 1.0, 1.0204065531064046, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094611971105055, 0.8288041377343874, 0.8288041377343874, 0.849326076700889], 
reward next is 0.1507, 
noisyNet noise sample is [array([-0.6085195], dtype=float32), -0.3647891]. 
=============================================
[2019-04-02 19:13:18,476] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[15.709127]
 [15.921478]
 [16.516216]
 [17.08505 ]
 [15.82325 ]], R is [[15.99524021]
 [15.9978199 ]
 [16.00578499]
 [15.84572697]
 [15.9349823 ]].
[2019-04-02 19:13:35,408] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:13:35,416] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5754
[2019-04-02 19:13:35,425] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 758828.0958117247 W.
[2019-04-02 19:13:35,430] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.08333333333333, 87.33333333333334, 1.0, 2.0, 0.3280529441318974, 1.0, 2.0, 0.3280529441318974, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 758828.0958117247, 758828.0958117251, 187860.643016566], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4939800.0000, 
sim time next is 4940400.0000, 
raw observation next is [24.16666666666666, 85.66666666666667, 1.0, 2.0, 0.5930281547991041, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 694801.2615501422, 694801.2615501422, 157765.2343314048], 
processed observation next is [1.0, 0.17391304347826086, 0.45061728395061706, 0.8566666666666667, 1.0, 1.0, 0.5155097080941715, 0.0, 0.5, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.24814330769647935, 0.24814330769647935, 0.3033946814065477], 
reward next is 0.6966, 
noisyNet noise sample is [array([0.4923736], dtype=float32), 0.40130317]. 
=============================================
[2019-04-02 19:13:36,379] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 8.1601290e-27 0.0000000e+00 4.4009264e-38], sum to 1.0000
[2019-04-02 19:13:36,390] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1408
[2019-04-02 19:13:36,401] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 816069.7284871606 W.
[2019-04-02 19:13:36,408] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.9, 86.66666666666667, 1.0, 2.0, 0.3580098960656137, 1.0, 2.0, 0.3580098960656137, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 816069.7284871606, 816069.7284871611, 194965.0525837483], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4735200.0000, 
sim time next is 4735800.0000, 
raw observation next is [26.85, 85.5, 1.0, 2.0, 0.3538314010023581, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5633111138799884, 6.911199999999999, 6.9112, 121.9260426156618, 806539.9990577968, 806539.9990577972, 210283.0506275525], 
processed observation next is [1.0, 0.8260869565217391, 0.55, 0.855, 1.0, 1.0, 0.23075166785995013, 0.0, 0.5, -0.1904761904761905, 1.0, 0.5, 0.4541388923499855, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.28804999966349887, 0.28804999966349903, 0.4043904819760625], 
reward next is 0.5956, 
noisyNet noise sample is [array([0.64891976], dtype=float32), 1.8044628]. 
=============================================
[2019-04-02 19:13:39,214] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.8928037e-36], sum to 1.0000
[2019-04-02 19:13:39,221] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0364
[2019-04-02 19:13:39,231] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 760019.1086480346 W.
[2019-04-02 19:13:39,236] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.32682164879118, 1.0, 2.0, 0.32682164879118, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 760019.1086480346, 760019.1086480346, 187736.1862831249], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4678800.0000, 
sim time next is 4679400.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.2201046342460977, 1.0, 2.0, 0.2201046342460977, 1.0, 1.0, 0.3508477189201437, 6.9112, 6.9112, 121.94756008, 762608.4915664509, 762608.4915664509, 227275.9239874415], 
processed observation next is [1.0, 0.13043478260869565, 0.4074074074074074, 0.94, 1.0, 1.0, 0.07155313600725915, 1.0, 1.0, 0.07155313600725915, 1.0, 0.5, 0.18855964865017963, 0.0, 0.0, 0.8096049824067558, 0.27236017555944675, 0.27236017555944675, 0.4370690845912336], 
reward next is 0.5629, 
noisyNet noise sample is [array([-0.50152993], dtype=float32), 0.49045068]. 
=============================================
[2019-04-02 19:13:48,084] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:13:48,093] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4478
[2019-04-02 19:13:48,100] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1105514.8948262 W.
[2019-04-02 19:13:48,105] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.16666666666667, 69.83333333333333, 1.0, 2.0, 0.4848980017924293, 1.0, 2.0, 0.4848980017924293, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 121.9260426155773, 1105514.8948262, 1105514.894826199, 230754.15333672], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4900200.0000, 
sim time next is 4900800.0000, 
raw observation next is [30.73333333333333, 73.66666666666667, 1.0, 2.0, 0.2447899241557641, 1.0, 2.0, 0.2447899241557641, 1.0, 1.0, 0.3897135315072344, 6.9112, 6.9112, 121.94756008, 836995.1029742497, 836995.1029742497, 235785.582837999], 
processed observation next is [1.0, 0.7391304347826086, 0.6938271604938271, 0.7366666666666667, 1.0, 1.0, 0.10094038589971915, 1.0, 1.0, 0.10094038589971915, 1.0, 0.5, 0.237141914384043, 0.0, 0.0, 0.8096049824067558, 0.2989268224908035, 0.2989268224908035, 0.4534338131499981], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.32860982], dtype=float32), -0.4961937]. 
=============================================
[2019-04-02 19:13:48,663] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 3.5239193e-32 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 19:13:48,671] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0884
[2019-04-02 19:13:48,681] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 717204.918378797 W.
[2019-04-02 19:13:48,687] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.0, 96.0, 1.0, 2.0, 0.6274436434445587, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 717204.918378797, 717204.918378797, 162967.9268831561], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5002800.0000, 
sim time next is 5003400.0000, 
raw observation next is [24.0, 95.0, 1.0, 2.0, 0.207395357562484, 1.0, 1.0, 0.207395357562484, 1.0, 1.0, 0.3301801636347215, 6.911200000000001, 6.9112, 121.94756008, 709075.0357894428, 709075.0357894424, 223025.5221080862], 
processed observation next is [1.0, 0.9130434782608695, 0.4444444444444444, 0.95, 1.0, 1.0, 0.05642304471724286, 1.0, 0.5, 0.05642304471724286, 1.0, 0.5, 0.1627252045434019, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.2532410842105153, 0.2532410842105151, 0.4288952348232427], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.933455], dtype=float32), -0.1762638]. 
=============================================
[2019-04-02 19:13:49,896] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 2.869974e-36 2.400343e-35], sum to 1.0000
[2019-04-02 19:13:49,903] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2483
[2019-04-02 19:13:49,912] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 809434.6731554978 W.
[2019-04-02 19:13:49,917] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.91666666666667, 93.33333333333334, 1.0, 2.0, 0.3551006326074606, 1.0, 2.0, 0.3551006326074606, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 809434.6731554978, 809434.6731554982, 194209.8811028361], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4835400.0000, 
sim time next is 4836000.0000, 
raw observation next is [25.73333333333333, 94.66666666666667, 1.0, 2.0, 0.2366910681313431, 1.0, 2.0, 0.2366910681313431, 1.0, 1.0, 0.3768198889550295, 6.9112, 6.9112, 121.94756008, 809288.5664981469, 809288.5664981469, 232955.7008397048], 
processed observation next is [1.0, 1.0, 0.5086419753086419, 0.9466666666666668, 1.0, 1.0, 0.09129889063255132, 1.0, 1.0, 0.09129889063255132, 1.0, 0.5, 0.22102486119378684, 0.0, 0.0, 0.8096049824067558, 0.2890316308921953, 0.2890316308921953, 0.4479917323840477], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.14141001], dtype=float32), -0.8500212]. 
=============================================
[2019-04-02 19:13:49,930] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[32.014107]
 [32.032383]
 [32.20584 ]
 [31.965668]
 [32.88583 ]], R is [[31.91235924]
 [32.21975327]
 [32.44958115]
 [32.12508392]
 [32.46133804]].
[2019-04-02 19:13:54,558] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 5.4912966e-35 0.0000000e+00 0.0000000e+00 1.0302961e-30], sum to 1.0000
[2019-04-02 19:13:54,567] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6110
[2019-04-02 19:13:54,577] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2225498.064905623 W.
[2019-04-02 19:13:54,585] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.65, 91.5, 1.0, 2.0, 1.02, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9977734948820727, 7.588941126567609, 6.9112, 121.9234025671107, 2225498.064905623, 1878441.547220306, 381703.6864629331], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4872600.0000, 
sim time next is 4873200.0000, 
raw observation next is [26.86666666666667, 90.66666666666666, 1.0, 2.0, 0.6138124371667901, 1.0, 1.0, 0.6138124371667901, 1.0, 2.0, 0.9772093904450008, 6.911200000000002, 6.9112, 121.94756008, 2100399.544974034, 2100399.544974033, 402952.6680012824], 
processed observation next is [1.0, 0.391304347826087, 0.5506172839506175, 0.9066666666666666, 1.0, 1.0, 0.5402529013890358, 1.0, 0.5, 0.5402529013890358, 1.0, 1.0, 0.971511738056251, 1.7763568394002506e-16, 0.0, 0.8096049824067558, 0.7501426946335836, 0.7501426946335832, 0.7749089769255431], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.517267], dtype=float32), -0.7508326]. 
=============================================
[2019-04-02 19:14:03,936] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:14:03,956] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0218
[2019-04-02 19:14:03,963] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.86666666666667, 99.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9165455218841353, 6.911200000000001, 6.9112, 121.9260426156618, 669586.4831630648, 669586.4831630645, 178390.1795290564], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5016000.0000, 
sim time next is 5016600.0000, 
raw observation next is [22.8, 99.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9069091366204906, 6.9112, 6.9112, 121.9260426156618, 663659.1030791916, 663659.1030791916, 176878.4599093659], 
processed observation next is [0.0, 0.043478260869565216, 0.4, 0.99, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8836364207756131, 0.0, 0.0, 0.8094621288201359, 0.23702110824256845, 0.23702110824256845, 0.34015088444108826], 
reward next is 0.6598, 
noisyNet noise sample is [array([0.751268], dtype=float32), 0.8260849]. 
=============================================
[2019-04-02 19:14:04,432] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:14:04,441] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6846
[2019-04-02 19:14:04,448] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.66666666666667, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.902866929873804, 6.911199999999999, 6.9112, 121.9260426156618, 659237.3427507356, 659237.342750736, 176622.7901730663], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5035200.0000, 
sim time next is 5035800.0000, 
raw observation next is [23.83333333333333, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9170804723434951, 6.9112, 6.9112, 121.9260426156618, 667774.7509243641, 667774.7509243641, 178853.1462009445], 
processed observation next is [0.0, 0.2608695652173913, 0.43827160493827144, 0.94, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8963505904293689, 0.0, 0.0, 0.8094621288201359, 0.23849098247298717, 0.23849098247298717, 0.3439483580787394], 
reward next is 0.6561, 
noisyNet noise sample is [array([-0.67029333], dtype=float32), 0.962706]. 
=============================================
[2019-04-02 19:14:09,436] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.9997294e-01 2.2034374e-32 2.7068501e-05 6.0093438e-30 3.6538674e-25], sum to 1.0000
[2019-04-02 19:14:09,440] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4654
[2019-04-02 19:14:09,447] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 798550.430782345 W.
[2019-04-02 19:14:09,453] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [22.36666666666667, 91.66666666666667, 1.0, 1.0, 0.2266154287576375, 1.0, 1.0, 0.2266154287576375, 1.0, 2.0, 0.3625886486587513, 6.9112, 6.9112, 121.94756008, 798550.430782345, 798550.430782345, 229342.1665564369], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5296800.0000, 
sim time next is 5297400.0000, 
raw observation next is [22.25, 92.0, 1.0, 2.0, 0.3126546936087564, 0.0, 1.0, 0.0, 1.0, 2.0, 0.5019612473631979, 6.9112, 6.9112, 121.9260425270351, 741522.9464548834, 741522.9464548834, 197787.7982284928], 
processed observation next is [1.0, 0.30434782608695654, 0.37962962962962965, 0.92, 1.0, 1.0, 0.1817317781056624, 0.0, 0.5, -0.1904761904761905, 1.0, 1.0, 0.37745155920399737, 0.0, 0.0, 0.8094621282317468, 0.26482962373388697, 0.26482962373388697, 0.3803611504394092], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.31423756], dtype=float32), 1.8410629]. 
=============================================
[2019-04-02 19:14:10,237] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [9.225304e-08 0.000000e+00 9.999999e-01 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-02 19:14:10,245] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5844
[2019-04-02 19:14:10,249] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.9, 94.0, 1.0, 2.0, 0.3616870867577317, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5758176214443671, 6.9112, 6.9112, 121.9260426156618, 824456.2507619836, 824456.2507619836, 212574.7417499133], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5104800.0000, 
sim time next is 5105400.0000, 
raw observation next is [25.75, 95.0, 1.0, 2.0, 0.3600063595773819, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5731418490357872, 6.911199999999999, 6.9112, 121.9260426156618, 820623.026166194, 820623.0261661945, 212082.4484989831], 
processed observation next is [0.0, 0.08695652173913043, 0.5092592592592593, 0.95, 1.0, 1.0, 0.23810280902069275, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.466427311294734, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.29307965220221216, 0.2930796522022123, 0.4078508624980444], 
reward next is 0.5921, 
noisyNet noise sample is [array([1.3217651], dtype=float32), 0.40872556]. 
=============================================
[2019-04-02 19:14:14,097] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-02 19:14:14,100] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 19:14:14,102] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 19:14:14,103] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:14:14,104] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 19:14:14,105] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:14:14,107] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 19:14:14,105] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 19:14:14,108] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:14:14,115] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:14:14,115] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:14:14,139] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run32
[2019-04-02 19:14:14,165] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run32
[2019-04-02 19:14:14,165] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run32
[2019-04-02 19:14:14,200] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run32
[2019-04-02 19:14:14,219] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run32
[2019-04-02 19:15:03,206] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00097987], dtype=float32), -0.21690637]
[2019-04-02 19:15:03,208] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [28.0, 72.66666666666667, 1.0, 2.0, 0.3214828217496039, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5118111221899274, 6.911199999999999, 6.9112, 121.9260426156618, 732767.8748963128, 732767.8748963132, 201107.7808792478]
[2019-04-02 19:15:03,209] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 19:15:03,212] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [7.553725e-07 0.000000e+00 9.999993e-01 0.000000e+00 0.000000e+00], sampled 0.3665987810426512
[2019-04-02 19:15:19,822] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00097987], dtype=float32), -0.21690637]
[2019-04-02 19:15:19,850] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [27.83333333333334, 84.83333333333333, 1.0, 2.0, 0.3730271376028755, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5938713517095084, 6.911199999999999, 6.9112, 121.9260426156618, 850319.9390868862, 850319.9390868867, 215927.5402846327]
[2019-04-02 19:15:19,850] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 19:15:19,853] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [7.74043e-04 0.00000e+00 9.99226e-01 0.00000e+00 0.00000e+00], sampled 0.5246863698103431
[2019-04-02 19:15:45,741] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00097987], dtype=float32), -0.21690637]
[2019-04-02 19:15:45,743] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [32.73333333333333, 50.33333333333333, 1.0, 2.0, 0.3193417905970432, 0.0, 2.0, 0.0, 1.0, 1.0, 0.5084025308665342, 6.911199999999999, 6.9112, 121.9260426156618, 727885.4251496441, 727885.4251496445, 200516.084268959]
[2019-04-02 19:15:45,745] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 19:15:45,748] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [3.7324948e-09 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.33942910162470663
[2019-04-02 19:16:20,980] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7563.2508 2652240011.2594 582.0000
[2019-04-02 19:16:21,827] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8059.4892 2398264166.6888 293.0000
[2019-04-02 19:16:21,838] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 7864.4689 2454430422.0429 417.0000
[2019-04-02 19:16:21,929] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 7554.6291 2487442172.3065 410.0000
[2019-04-02 19:16:21,972] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 7683.2191 2428742724.1461 336.0000
[2019-04-02 19:16:22,989] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 775000, evaluation results [775000.0, 7563.250755811762, 2652240011.259434, 582.0, 7683.219069430809, 2428742724.1460776, 336.0, 8059.489189715789, 2398264166.6888423, 293.0, 7554.629054872657, 2487442172.3064513, 410.0, 7864.468869305893, 2454430422.042937, 417.0]
[2019-04-02 19:16:27,127] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 4.3955287e-21 1.1313535e-08 1.7558292e-10 2.6160132e-10], sum to 1.0000
[2019-04-02 19:16:27,135] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2854
[2019-04-02 19:16:27,141] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1638264.77425928 W.
[2019-04-02 19:16:27,147] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.16666666666667, 88.16666666666667, 1.0, 2.0, 0.7183254751975108, 1.0, 1.0, 0.7183254751975108, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 121.9260426055642, 1638264.77425928, 1638264.77425928, 311160.9937847884], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5217000.0000, 
sim time next is 5217600.0000, 
raw observation next is [26.33333333333334, 87.33333333333334, 1.0, 2.0, 0.4736309662474419, 1.0, 2.0, 0.4736309662474419, 1.0, 1.0, 0.7540359233496171, 6.9112, 6.9112, 121.94756008, 1620278.746833352, 1620278.746833352, 330835.4434320768], 
processed observation next is [1.0, 0.391304347826087, 0.5308641975308644, 0.8733333333333334, 1.0, 1.0, 0.37337019791362136, 1.0, 1.0, 0.37337019791362136, 1.0, 0.5, 0.6925449041870213, 0.0, 0.0, 0.8096049824067558, 0.5786709810119114, 0.5786709810119114, 0.6362220066001476], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.73114765], dtype=float32), -0.65866905]. 
=============================================
[2019-04-02 19:16:39,336] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 7.4653374e-27 0.0000000e+00 1.3804504e-24 1.4067558e-14], sum to 1.0000
[2019-04-02 19:16:39,350] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1099
[2019-04-02 19:16:39,360] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1868478.258473204 W.
[2019-04-02 19:16:39,365] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.16666666666666, 82.66666666666666, 1.0, 2.0, 0.5461074954102069, 1.0, 1.0, 0.5461074954102069, 1.0, 2.0, 0.8694209181725059, 6.9112, 6.9112, 121.94756008, 1868478.258473204, 1868478.258473204, 366795.069350212], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5399400.0000, 
sim time next is 5400000.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.827176277605508, 1.0, 2.0, 0.827176277605508, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426094319, 1886779.931556326, 1886779.931556326, 355112.2377331718], 
processed observation next is [1.0, 0.5217391304347826, 0.5555555555555556, 0.84, 1.0, 1.0, 0.7942574733398905, 1.0, 1.0, 0.7942574733398905, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.8094621287787759, 0.6738499755558307, 0.6738499755558307, 0.6829081494868688], 
reward next is 0.3171, 
noisyNet noise sample is [array([-0.06842203], dtype=float32), -0.1527015]. 
=============================================
[2019-04-02 19:16:39,379] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[29.909365]
 [28.965088]
 [29.417082]
 [28.917637]
 [28.968143]], R is [[29.66837692]
 [29.37169266]
 [29.16960907]
 [29.16934013]
 [29.16674805]].
[2019-04-02 19:16:44,099] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 4.3963770e-24 0.0000000e+00 1.8098523e-25 9.2041514e-14], sum to 1.0000
[2019-04-02 19:16:44,110] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9567
[2019-04-02 19:16:44,120] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1067843.385239459 W.
[2019-04-02 19:16:44,127] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.18333333333333, 94.16666666666667, 1.0, 2.0, 0.3122574588992171, 1.0, 1.0, 0.3122574588992171, 1.0, 2.0, 0.4971240440830191, 6.9112, 6.9112, 121.94756008, 1067843.385239459, 1067843.385239459, 260782.76513504], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5458200.0000, 
sim time next is 5458800.0000, 
raw observation next is [26.16666666666667, 94.33333333333334, 1.0, 2.0, 0.4176397799486409, 0.0, 1.0, 0.0, 1.0, 2.0, 0.6648961312562937, 6.9112, 6.9112, 121.9260426156618, 952078.1732975881, 952078.1732975881, 229610.1886218512], 
processed observation next is [1.0, 0.17391304347826086, 0.5246913580246916, 0.9433333333333335, 1.0, 1.0, 0.30671402374838197, 0.0, 0.5, -0.1904761904761905, 1.0, 1.0, 0.581120164070367, 0.0, 0.0, 0.8094621288201359, 0.3400279190348529, 0.3400279190348529, 0.44155805504202156], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.37297738], dtype=float32), -0.37875068]. 
=============================================
[2019-04-02 19:16:51,596] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:16:51,605] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4999
[2019-04-02 19:16:51,610] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 783596.0413554864 W.
[2019-04-02 19:16:51,616] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.4, 85.0, 1.0, 2.0, 0.343770963753197, 1.0, 1.0, 0.343770963753197, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 783596.0413554864, 783596.0413554864, 191295.7587977727], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5558400.0000, 
sim time next is 5559000.0000, 
raw observation next is [25.48333333333333, 84.66666666666667, 1.0, 2.0, 0.2229980069193535, 1.0, 2.0, 0.2229980069193535, 1.0, 1.0, 0.355020089553672, 6.911199999999999, 6.9112, 121.94756008, 762446.289943918, 762446.2899439185, 228254.5044665111], 
processed observation next is [1.0, 0.34782608695652173, 0.4993827160493826, 0.8466666666666667, 1.0, 1.0, 0.07499762728494465, 1.0, 1.0, 0.07499762728494465, 1.0, 0.5, 0.19377511194208996, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.27230224640854217, 0.27230224640854234, 0.43895097012790596], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7110386], dtype=float32), 0.55635476]. 
=============================================
[2019-04-02 19:16:51,632] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[44.271564]
 [45.00596 ]
 [44.20475 ]
 [43.716057]
 [44.70461 ]], R is [[43.8598671 ]
 [44.0533905 ]
 [44.20393753]
 [43.76189804]
 [43.32427979]].
[2019-04-02 19:16:51,862] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:16:51,876] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9526
[2019-04-02 19:16:51,885] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1595609.57748221 W.
[2019-04-02 19:16:51,890] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.96666666666667, 82.66666666666667, 1.0, 2.0, 0.7725491846473033, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9977734948820727, 6.911200000000001, 6.9112, 121.9260426156618, 1595609.57748221, 1595609.57748221, 330841.426116582], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5562600.0000, 
sim time next is 5563200.0000, 
raw observation next is [26.03333333333333, 82.33333333333334, 1.0, 2.0, 0.4544236515342243, 1.0, 1.0, 0.4544236515342243, 1.0, 2.0, 0.7234572527876055, 6.9112, 6.9112, 121.94756008, 1554509.90079403, 1554509.90079403, 321742.6014074113], 
processed observation next is [1.0, 0.391304347826087, 0.519753086419753, 0.8233333333333335, 1.0, 1.0, 0.3505043470645528, 1.0, 0.5, 0.3505043470645528, 1.0, 1.0, 0.654321565984507, 0.0, 0.0, 0.8096049824067558, 0.5551821074264393, 0.5551821074264393, 0.6187357719373294], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.500814], dtype=float32), -0.5690156]. 
=============================================
[2019-04-02 19:16:52,621] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 1.5647204e-32 1.0481144e-16 5.5211369e-15 7.0447379e-17], sum to 1.0000
[2019-04-02 19:16:52,629] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4318
[2019-04-02 19:16:52,641] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2431570.160266736 W.
[2019-04-02 19:16:52,648] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.5, 74.0, 1.0, 2.0, 1.02, 1.0, 2.0, 1.02, 0.0, 1.0, 0.0, 7.114783662894184, 6.9112, 121.9252841086892, 2431570.160266736, 2327317.777326039, 443049.6075003074], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5580000.0000, 
sim time next is 5580600.0000, 
raw observation next is [30.2, 74.83333333333334, 1.0, 2.0, 0.8446443508075303, 1.0, 2.0, 0.7356868373801999, 1.0, 1.0, 0.9977734948820727, 6.911200000000002, 6.9112, 122.9354078259037, 2518000.03468674, 2518000.03468674, 470659.2884304823], 
processed observation next is [1.0, 0.6086956521739131, 0.674074074074074, 0.7483333333333334, 1.0, 1.0, 0.8150527985803933, 1.0, 1.0, 0.6853414730716666, 1.0, 0.5, 0.9972168686025908, 1.7763568394002506e-16, 0.0, 0.816163264150304, 0.8992857266738358, 0.8992857266738358, 0.905114016212466], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.78514016], dtype=float32), -0.16513096]. 
=============================================
[2019-04-02 19:17:01,433] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:17:01,445] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8053
[2019-04-02 19:17:01,449] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.43333333333333, 97.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.788454766029778, 6.911200000000001, 6.9112, 121.9260426156618, 587135.824157319, 587135.8241573187, 158031.3584144553], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5715600.0000, 
sim time next is 5716200.0000, 
raw observation next is [21.36666666666667, 97.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.782490936733374, 6.911200000000001, 6.9112, 121.9260426156618, 582928.3042787963, 582928.3042787958, 157136.6224311382], 
processed observation next is [0.0, 0.13043478260869565, 0.3469135802469137, 0.97, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.7281136709167174, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.20818868009957012, 0.20818868009956995, 0.30218581236757347], 
reward next is 0.6978, 
noisyNet noise sample is [array([1.5534033], dtype=float32), 0.16160175]. 
=============================================
[2019-04-02 19:17:02,970] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:17:02,979] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2065
[2019-04-02 19:17:02,984] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.0, 75.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7509251428149261, 6.9112, 6.9112, 121.9260426156618, 560300.4314810024, 560300.4314810024, 152555.7354258617], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5738400.0000, 
sim time next is 5739000.0000, 
raw observation next is [24.21666666666667, 74.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7530486582425117, 6.9112, 6.9112, 121.9260426156618, 561712.9439515518, 561712.9439515518, 152986.4682278824], 
processed observation next is [0.0, 0.43478260869565216, 0.4524691358024692, 0.7416666666666667, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6913108228031395, 0.0, 0.0, 0.8094621288201359, 0.20061176569698277, 0.20061176569698277, 0.29420474659208157], 
reward next is 0.7058, 
noisyNet noise sample is [array([0.5529252], dtype=float32), -0.6532455]. 
=============================================
[2019-04-02 19:17:03,007] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[69.80544 ]
 [69.752235]
 [69.74718 ]
 [69.74374 ]
 [69.73924 ]], R is [[69.8200531 ]
 [69.82848358]
 [69.83755493]
 [69.84765625]
 [69.85926819]].
[2019-04-02 19:17:09,582] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:17:09,591] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0831
[2019-04-02 19:17:09,596] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.46666666666667, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7516477045855149, 6.911199999999999, 6.9112, 121.9260426156618, 561033.7989933798, 561033.7989933803, 152410.1731021016], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5812800.0000, 
sim time next is 5813400.0000, 
raw observation next is [22.58333333333334, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7512318470533713, 6.911200000000001, 6.9112, 121.9260426156618, 560754.3700962423, 560754.3700962418, 152321.4297997086], 
processed observation next is [1.0, 0.2608695652173913, 0.39197530864197555, 0.84, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.689039808816714, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2002694178915151, 0.20026941789151495, 0.29292582653790117], 
reward next is 0.7071, 
noisyNet noise sample is [array([-0.6736253], dtype=float32), -0.3076669]. 
=============================================
[2019-04-02 19:17:13,251] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:17:13,262] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6858
[2019-04-02 19:17:13,267] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.2, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7020920984098361, 6.911199999999999, 6.9112, 121.9260426156618, 524656.1705694987, 524656.1705694991, 145277.4832912645], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5860800.0000, 
sim time next is 5861400.0000, 
raw observation next is [25.01666666666667, 63.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7063416738058961, 6.911200000000001, 6.9112, 121.9260426156618, 527833.1393263801, 527833.1393263796, 145743.4697623083], 
processed observation next is [1.0, 0.8695652173913043, 0.48209876543209884, 0.6383333333333333, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6329270922573701, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.18851183547370717, 0.188511835473707, 0.2802759033890544], 
reward next is 0.7197, 
noisyNet noise sample is [array([-0.95868176], dtype=float32), -0.9783569]. 
=============================================
[2019-04-02 19:17:15,779] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:17:15,790] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7310
[2019-04-02 19:17:15,795] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.35, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6038177389030417, 6.9112, 6.9112, 121.9260426156618, 444610.9388424202, 444610.9388424202, 128642.7690224224], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5889000.0000, 
sim time next is 5889600.0000, 
raw observation next is [19.3, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5966151862797048, 6.9112, 6.9112, 121.9260426156618, 439029.5655287554, 439029.5655287554, 127846.8311049478], 
processed observation next is [1.0, 0.17391304347826086, 0.27037037037037037, 0.85, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.49576898284963095, 0.0, 0.0, 0.8094621288201359, 0.15679627340312693, 0.15679627340312693, 0.2458592905864381], 
reward next is 0.7541, 
noisyNet noise sample is [array([-0.35776535], dtype=float32), 2.1459901]. 
=============================================
[2019-04-02 19:17:29,820] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-02 19:17:29,824] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 19:17:29,825] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 19:17:29,826] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:17:29,827] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:17:29,828] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 19:17:29,830] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 19:17:29,831] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 19:17:29,832] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:17:29,834] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:17:29,835] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:17:29,865] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run33
[2019-04-02 19:17:29,866] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run33
[2019-04-02 19:17:29,867] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run33
[2019-04-02 19:17:29,867] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run33
[2019-04-02 19:17:29,956] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run33
[2019-04-02 19:19:16,531] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.14449964], dtype=float32), -0.2425223]
[2019-04-02 19:19:16,531] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [22.33333333333334, 88.33333333333334, 1.0, 2.0, 0.8167873571834212, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 985314.7465942196, 985314.7465942191, 201931.5403683031]
[2019-04-02 19:19:16,533] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 19:19:16,538] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.28000927709756196
[2019-04-02 19:19:16,539] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 985314.7465942196 W.
[2019-04-02 19:19:31,284] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.14449964], dtype=float32), -0.2425223]
[2019-04-02 19:19:31,286] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [24.42863036333333, 79.95328822, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8271929555063803, 6.911200000000001, 6.9112, 121.9260426156618, 611592.5698550944, 611592.5698550941, 164882.2651841169]
[2019-04-02 19:19:31,287] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 19:19:31,291] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.2774101829394231
[2019-04-02 19:19:37,024] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8381.6251 2295524096.7308 688.0000
[2019-04-02 19:19:37,054] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8540.5926 2259464565.4143 536.0000
[2019-04-02 19:19:37,116] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8607.3061 2222165403.1570 537.0000
[2019-04-02 19:19:37,219] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8336.4532 2341274696.6648 612.0000
[2019-04-02 19:19:37,340] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7829.4070 2532263595.7419 826.0000
[2019-04-02 19:19:38,358] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 800000, evaluation results [800000.0, 7829.407034454973, 2532263595.7418985, 826.0, 8540.592622088576, 2259464565.414288, 536.0, 8607.306055150771, 2222165403.1569896, 537.0, 8336.453229690347, 2341274696.664785, 612.0, 8381.625106777936, 2295524096.7308373, 688.0]
[2019-04-02 19:19:45,563] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:19:45,576] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6891
[2019-04-02 19:19:45,580] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.1, 73.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8046114638365084, 6.911199999999999, 6.9112, 121.9260426156618, 597537.3870743546, 597537.387074355, 160954.6327734767], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6219000.0000, 
sim time next is 6219600.0000, 
raw observation next is [25.03333333333333, 73.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8045137333765956, 6.9112, 6.9112, 121.9260426156618, 597530.7690800518, 597530.7690800518, 160910.2565781096], 
processed observation next is [1.0, 1.0, 0.482716049382716, 0.7333333333333333, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.7556421667207444, 0.0, 0.0, 0.8094621288201359, 0.2134038461000185, 0.2134038461000185, 0.30944280111174927], 
reward next is 0.6906, 
noisyNet noise sample is [array([0.17194204], dtype=float32), 0.040190004]. 
=============================================
[2019-04-02 19:19:55,078] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 1.4532341e-36 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 19:19:55,088] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4052
[2019-04-02 19:19:55,098] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 687328.6446847823 W.
[2019-04-02 19:19:55,102] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.86666666666667, 86.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9460613434673872, 6.911200000000001, 6.9112, 121.9260426156618, 687328.6446847823, 687328.6446847818, 183057.735525361], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6333000.0000, 
sim time next is 6333600.0000, 
raw observation next is [24.93333333333333, 86.66666666666667, 1.0, 1.0, 0.3010479315819295, 0.0, 2.0, 0.0, 1.0, 2.0, 0.479449270864836, 6.911199999999999, 6.9112, 121.9260426156618, 690054.7523995407, 690054.7523995412, 195461.4454837559], 
processed observation next is [0.0, 0.30434782608695654, 0.47901234567901224, 0.8666666666666667, 1.0, 0.5, 0.16791420426420178, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.349311588581045, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.24644812585697884, 0.246448125856979, 0.37588739516106906], 
reward next is 0.6241, 
noisyNet noise sample is [array([1.205024], dtype=float32), -0.93811977]. 
=============================================
[2019-04-02 19:20:06,774] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 2.6910431e-23 0.0000000e+00 1.7043942e-32], sum to 1.0000
[2019-04-02 19:20:06,787] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0020
[2019-04-02 19:20:06,801] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1020388.475663696 W.
[2019-04-02 19:20:06,807] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.33333333333334, 85.66666666666667, 1.0, 2.0, 0.8951698473543127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 1020388.475663696, 1020388.475663696, 216384.2524224122], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6497400.0000, 
sim time next is 6498000.0000, 
raw observation next is [26.3, 86.0, 1.0, 2.0, 0.4143916742198007, 0.0, 2.0, 0.0, 1.0, 1.0, 0.6597250411525615, 6.911199999999999, 6.9112, 121.9260426156618, 944669.0212570757, 944669.0212570761, 228583.9259621353], 
processed observation next is [1.0, 0.21739130434782608, 0.5296296296296297, 0.86, 1.0, 1.0, 0.30284723121404844, 0.0, 1.0, -0.1904761904761905, 1.0, 0.5, 0.5746563014407018, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.33738179330609847, 0.3373817933060986, 0.43958447300410636], 
reward next is 0.5604, 
noisyNet noise sample is [array([1.0203376], dtype=float32), 0.9331365]. 
=============================================
[2019-04-02 19:20:06,823] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[31.285439]
 [31.37405 ]
 [31.777456]
 [31.775211]
 [31.37066 ]], R is [[32.07876968]
 [32.34185791]
 [32.59009552]
 [32.76962662]
 [32.94809341]].
[2019-04-02 19:20:10,533] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.3573512e-01 0.0000000e+00 2.6426488e-01 6.4873729e-37 2.1080134e-38], sum to 1.0000
[2019-04-02 19:20:10,544] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7475
[2019-04-02 19:20:10,555] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.5, 81.0, 1.0, 2.0, 0.3313118317121932, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5274592261588763, 6.9112, 6.9112, 121.9260426156618, 755182.5425322284, 755182.5425322284, 203852.7675482999], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6544800.0000, 
sim time next is 6545400.0000, 
raw observation next is [27.45, 81.66666666666667, 1.0, 2.0, 0.3352652907849946, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5337532617579491, 6.911199999999999, 6.9112, 121.9260426156618, 764198.4340892116, 764198.4340892121, 204967.053552966], 
processed observation next is [1.0, 0.782608695652174, 0.5722222222222222, 0.8166666666666668, 1.0, 1.0, 0.20864915569642217, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.4171915771974364, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.27292801217471846, 0.2729280121747186, 0.3941674106787808], 
reward next is 0.6058, 
noisyNet noise sample is [array([-1.8216852], dtype=float32), -0.8421837]. 
=============================================
[2019-04-02 19:20:21,166] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:20:21,171] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1314
[2019-04-02 19:20:21,177] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.25, 62.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7096876053874992, 6.9112, 6.9112, 121.9260426156618, 530333.20189525, 530333.20189525, 146125.0715145734], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6901800.0000, 
sim time next is 6902400.0000, 
raw observation next is [25.1, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7066999311397091, 6.911200000000001, 6.9112, 121.9260426156618, 528110.4046949175, 528110.404694917, 145649.1846605656], 
processed observation next is [0.0, 0.9130434782608695, 0.4851851851851852, 0.63, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6333749139246364, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1886108588196134, 0.18861085881961323, 0.2800945858857031], 
reward next is 0.7199, 
noisyNet noise sample is [array([-0.69934696], dtype=float32), -1.3992174]. 
=============================================
[2019-04-02 19:20:24,565] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 1.3611103e-21 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 19:20:24,578] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2255
[2019-04-02 19:20:24,585] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.75, 75.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5831435021028053, 6.911199999999999, 6.9112, 121.9260426156618, 430237.2518710215, 430237.251871022, 127216.1952115153], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6737400.0000, 
sim time next is 6738000.0000, 
raw observation next is [20.66666666666667, 75.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5800070909515546, 6.911199999999999, 6.9112, 121.9260426156618, 427632.8479199976, 427632.8479199981, 126777.945634995], 
processed observation next is [1.0, 1.0, 0.3209876543209878, 0.7566666666666667, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.4750088636894432, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.15272601711428485, 0.15272601711428505, 0.24380374160575963], 
reward next is 0.7562, 
noisyNet noise sample is [array([-1.3749429], dtype=float32), -0.0036058659]. 
=============================================
[2019-04-02 19:20:24,611] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[62.15047 ]
 [62.344948]
 [62.536736]
 [62.731785]
 [62.84936 ]], R is [[62.08265305]
 [62.21717834]
 [62.34970856]
 [62.48048782]
 [62.60957718]].
[2019-04-02 19:20:37,800] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:20:37,811] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0617
[2019-04-02 19:20:37,818] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.26666666666667, 74.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7583693337713865, 6.911200000000001, 6.9112, 121.9260426156618, 565338.9664780254, 565338.966478025, 153931.2141556345], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6943200.0000, 
sim time next is 6943800.0000, 
raw observation next is [24.53333333333333, 73.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7629468323883005, 6.911199999999999, 6.9112, 121.9260426156618, 568570.0467943298, 568570.0467943302, 154625.249435484], 
processed observation next is [0.0, 0.34782608695652173, 0.46419753086419746, 0.7333333333333333, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.7036835404853755, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.20306073099797492, 0.2030607309979751, 0.29735624891439233], 
reward next is 0.7026, 
noisyNet noise sample is [array([-0.9137566], dtype=float32), 0.38604587]. 
=============================================
[2019-04-02 19:20:44,970] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-02 19:20:44,971] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 19:20:44,973] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 19:20:44,973] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:20:44,974] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:20:44,974] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 19:20:44,976] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:20:44,977] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 19:20:44,978] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 19:20:44,979] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:20:44,981] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:20:45,014] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run34
[2019-04-02 19:20:45,014] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run34
[2019-04-02 19:20:45,014] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run34
[2019-04-02 19:20:45,078] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run34
[2019-04-02 19:20:45,079] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run34
[2019-04-02 19:20:48,329] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.17531972], dtype=float32), -0.25707233]
[2019-04-02 19:20:48,330] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [37.3, 8.5, 1.0, 2.0, 0.7841746489677861, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9626736505917552, 6.9112, 6.9112, 121.9260426156618, 1701702.595587456, 1701702.595587456, 315194.0082625035]
[2019-04-02 19:20:48,330] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 19:20:48,331] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.962541002590848
[2019-04-02 19:20:48,333] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1701702.595587456 W.
[2019-04-02 19:20:59,399] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.17531972], dtype=float32), -0.25707233]
[2019-04-02 19:20:59,400] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [20.02893728, 73.16136219833334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5456683771439143, 6.9112, 6.9112, 121.9260426156618, 396939.6282214986, 396939.6282214986, 121286.0376689061]
[2019-04-02 19:20:59,403] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 19:20:59,407] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.13320455759703853
[2019-04-02 19:21:04,923] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.17531972], dtype=float32), -0.25707233]
[2019-04-02 19:21:04,926] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [21.86666666666667, 65.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.780512683256855, 6.911200000000001, 6.9112, 121.9260426156618, 572770.6287694976, 572770.6287694972, 145154.1218205529]
[2019-04-02 19:21:04,929] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 19:21:04,931] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.747029559983878
[2019-04-02 19:21:32,005] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.17531972], dtype=float32), -0.25707233]
[2019-04-02 19:21:32,005] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [27.911195905, 95.68098149000001, 1.0, 2.0, 0.8245028275765153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 939786.9795075449, 939786.9795075444, 201067.8746977368]
[2019-04-02 19:21:32,005] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 19:21:32,007] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [9.999690e-01 0.000000e+00 3.102377e-05 0.000000e+00 0.000000e+00], sampled 0.12634856215020918
[2019-04-02 19:21:32,008] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 939786.9795075449 W.
[2019-04-02 19:21:52,850] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.17531972], dtype=float32), -0.25707233]
[2019-04-02 19:21:52,851] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [24.843542345, 62.35481063666666, 1.0, 2.0, 0.5846082156345181, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9490808457325057, 6.911199999999999, 6.9112, 121.9260426156618, 1415739.475194561, 1415739.475194561, 285494.9705847456]
[2019-04-02 19:21:52,855] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 19:21:52,860] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5131935415432174
[2019-04-02 19:21:52,861] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1415739.475194561 W.
[2019-04-02 19:22:44,515] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.17531972], dtype=float32), -0.25707233]
[2019-04-02 19:22:44,516] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [18.73991084, 100.281371995, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.602228933518611, 6.9112, 6.9112, 121.9260426156618, 447971.5942752498, 447971.5942752498, 131331.6820739458]
[2019-04-02 19:22:44,518] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 19:22:44,524] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5142525798943831
[2019-04-02 19:22:51,758] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8633.6908 2219215572.7071 543.0000
[2019-04-02 19:22:52,197] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8557.8765 2258327642.4849 536.0000
[2019-04-02 19:22:52,480] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8360.4344 2339582304.1711 616.0000
[2019-04-02 19:22:52,744] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8404.1885 2293006379.4818 697.0000
[2019-04-02 19:22:52,792] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7838.5282 2530303215.1269 831.0000
[2019-04-02 19:22:53,809] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 825000, evaluation results [825000.0, 7838.528180464375, 2530303215.1268845, 831.0, 8557.876498474936, 2258327642.484933, 536.0, 8633.690841416023, 2219215572.7070513, 543.0, 8360.434374098988, 2339582304.1710963, 616.0, 8404.188452724367, 2293006379.4818354, 697.0]
[2019-04-02 19:22:59,985] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:22:59,995] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1782
[2019-04-02 19:23:00,001] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.93333333333334, 78.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7227811406027234, 6.9112, 6.9112, 121.9260426156618, 540003.7077979653, 540003.7077979653, 148127.6795076983], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7333800.0000, 
sim time next is 7334400.0000, 
raw observation next is [22.76666666666667, 79.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.720905217967135, 6.9112, 6.9112, 121.9260426156618, 538653.6978664428, 538653.6978664428, 147739.1398895468], 
processed observation next is [1.0, 0.9130434782608695, 0.3987654320987655, 0.7933333333333334, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6511315224589187, 0.0, 0.0, 0.8094621288201359, 0.19237632066658672, 0.19237632066658672, 0.28411373055682076], 
reward next is 0.7159, 
noisyNet noise sample is [array([0.6706203], dtype=float32), -0.21597007]. 
=============================================
[2019-04-02 19:23:00,211] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 1.3150426e-29 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 19:23:00,222] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2530
[2019-04-02 19:23:00,235] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.5, 65.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7923253071414607, 6.9112, 6.9112, 121.9260426156618, 590268.7102833192, 590268.7102833192, 151866.2959767417], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7131600.0000, 
sim time next is 7132200.0000, 
raw observation next is [23.58333333333334, 64.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9589709020879721, 7.069966516995076, 6.9112, 121.9255336839492, 795595.4159764154, 714293.1082501963, 171185.3891776987], 
processed observation next is [1.0, 0.5652173913043478, 0.4290123456790126, 0.6416666666666667, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.948713627609965, 0.015876651699507603, 0.0, 0.809458750042816, 0.28414121999157693, 0.25510468151792726, 0.3292026714955744], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.2518741], dtype=float32), -0.25009543]. 
=============================================
[2019-04-02 19:23:03,464] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:23:03,473] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7088
[2019-04-02 19:23:03,478] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.76666666666667, 94.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6872069977865966, 6.911199999999999, 6.9112, 121.9260426156618, 512650.2314944922, 512650.2314944926, 141228.2136394888], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7179600.0000, 
sim time next is 7180200.0000, 
raw observation next is [19.75, 94.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6593365718457866, 6.911200000000001, 6.9112, 121.9260426156618, 491774.0370530526, 491774.0370530521, 138231.8502910465], 
processed observation next is [1.0, 0.08695652173913043, 0.28703703703703703, 0.945, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5741707148072333, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1756335846618045, 0.17563358466180432, 0.26583048132893555], 
reward next is 0.7342, 
noisyNet noise sample is [array([-0.57571065], dtype=float32), 0.1460906]. 
=============================================
[2019-04-02 19:23:06,525] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 3.0191179e-16 7.9919312e-33 2.5856657e-35], sum to 1.0000
[2019-04-02 19:23:06,533] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9380
[2019-04-02 19:23:06,542] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 968241.3636454884 W.
[2019-04-02 19:23:06,550] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.9, 73.5, 1.0, 2.0, 0.7843634938057819, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 968241.3636454884, 968241.3636454884, 195804.1339677129], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7213800.0000, 
sim time next is 7214400.0000, 
raw observation next is [23.0, 73.0, 1.0, 2.0, 0.3860604142735306, 1.0, 1.0, 0.3860604142735306, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 940703.5775303991, 940703.5775303995, 204835.9565987433], 
processed observation next is [1.0, 0.5217391304347826, 0.4074074074074074, 0.73, 1.0, 1.0, 0.26911954080182215, 1.0, 0.5, 0.26911954080182215, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.335965563403714, 0.33596556340371414, 0.3939153011514294], 
reward next is 0.6061, 
noisyNet noise sample is [array([0.09519842], dtype=float32), 0.21951598]. 
=============================================
[2019-04-02 19:23:09,320] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:23:09,331] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0214
[2019-04-02 19:23:09,338] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.53333333333333, 94.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6167150599485849, 6.911199999999999, 6.9112, 121.9260426156618, 459425.5974099464, 459425.5974099468, 133337.8297458536], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7453200.0000, 
sim time next is 7453800.0000, 
raw observation next is [19.6, 94.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6191120395117421, 6.911199999999999, 6.9112, 121.9260426156618, 461335.3546405227, 461335.3546405231, 133697.5798466527], 
processed observation next is [0.0, 0.2608695652173913, 0.28148148148148155, 0.945, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5238900493896775, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.16476262665732952, 0.16476262665732969, 0.2571107304743321], 
reward next is 0.7429, 
noisyNet noise sample is [array([-0.3948143], dtype=float32), -1.0584443]. 
=============================================
[2019-04-02 19:23:25,161] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 1.0326414e-37 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 19:23:25,172] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2963
[2019-04-02 19:23:25,180] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.9, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6949848249440989, 6.911199999999999, 6.9112, 121.9260426156618, 519203.1123151541, 519203.1123151546, 145103.4766372804], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7465200.0000, 
sim time next is 7465800.0000, 
raw observation next is [22.05, 86.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7035787312687214, 6.9112, 6.9112, 121.9260426156618, 525537.898875966, 525537.898875966, 146265.5574077557], 
processed observation next is [0.0, 0.391304347826087, 0.37222222222222223, 0.865, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6294734140859017, 0.0, 0.0, 0.8094621288201359, 0.18769210674141643, 0.18769210674141643, 0.2812799180918379], 
reward next is 0.7187, 
noisyNet noise sample is [array([-1.9702722], dtype=float32), 1.8664262]. 
=============================================
[2019-04-02 19:23:32,854] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 1.7157785e-31 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 19:23:32,865] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4339
[2019-04-02 19:23:32,873] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.13333333333333, 70.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8267624914245801, 6.911200000000001, 6.9112, 121.9260426156618, 611102.6098314914, 611102.6098314909, 164887.1781415451], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7582200.0000, 
sim time next is 7582800.0000, 
raw observation next is [25.96666666666667, 71.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.82698929396764, 6.9112, 6.9112, 121.9260426156618, 611271.2314245914, 611271.2314245914, 164915.050354669], 
processed observation next is [0.0, 0.782608695652174, 0.517283950617284, 0.71, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.78373661745955, 0.0, 0.0, 0.8094621288201359, 0.2183111540802112, 0.2183111540802112, 0.31714432760513267], 
reward next is 0.6829, 
noisyNet noise sample is [array([0.34132352], dtype=float32), 0.49521112]. 
=============================================
[2019-04-02 19:23:34,638] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 19:23:34,638] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:23:34,678] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res10/Eplus-env-sub_run5
[2019-04-02 19:23:44,286] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 19:23:44,286] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:23:44,313] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res4/Eplus-env-sub_run5
[2019-04-02 19:23:45,110] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 19:23:45,110] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:23:45,149] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res5/Eplus-env-sub_run5
[2019-04-02 19:23:50,144] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 19:23:50,145] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:23:50,181] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res14/Eplus-env-sub_run5
[2019-04-02 19:23:50,758] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 4.9951433e-32 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 19:23:50,768] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2636
[2019-04-02 19:23:50,774] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.46666666666667, 78.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6879668279910288, 6.911200000000001, 6.9112, 121.9260426156618, 514069.6578362684, 514069.6578362679, 143052.412657456], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7868400.0000, 
sim time next is 7869000.0000, 
raw observation next is [22.38333333333334, 79.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6867854574953617, 6.911200000000001, 6.9112, 121.9260426156618, 513190.7817546593, 513190.7817546589, 142949.5111601084], 
processed observation next is [1.0, 0.043478260869565216, 0.38456790123456813, 0.7933333333333334, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6084818218692021, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.18328242205523546, 0.18328242205523532, 0.2749029060771315], 
reward next is 0.7251, 
noisyNet noise sample is [array([-0.04461384], dtype=float32), -0.5528686]. 
=============================================
[2019-04-02 19:23:50,786] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[73.552704]
 [73.6985  ]
 [73.894775]
 [74.117165]
 [74.40502 ]], R is [[73.38676453]
 [73.37779999]
 [73.36862183]
 [73.35959625]
 [73.35136414]].
[2019-04-02 19:23:57,145] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 19:23:57,146] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:23:57,150] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 19:23:57,151] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:23:57,184] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res8/Eplus-env-sub_run5
[2019-04-02 19:23:57,185] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res6/Eplus-env-sub_run5
[2019-04-02 19:23:57,588] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 19:23:57,589] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:23:57,615] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res13/Eplus-env-sub_run5
[2019-04-02 19:23:57,685] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 19:23:57,686] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:23:57,709] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res11/Eplus-env-sub_run5
[2019-04-02 19:23:58,010] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:23:58,011] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8212
[2019-04-02 19:23:58,012] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1175914.43316398 W.
[2019-04-02 19:23:58,017] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.33333333333333, 26.83333333333333, 1.0, 2.0, 0.4673657354179911, 1.0, 1.0, 0.4673657354179911, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1175914.43316398, 1175914.433163981, 229282.0522768422], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 389400.0000, 
sim time next is 390000.0000, 
raw observation next is [29.46666666666667, 26.66666666666667, 1.0, 2.0, 0.2704987409446123, 1.0, 2.0, 0.2704987409446123, 1.0, 1.0, 0.4540599436625653, 6.911199999999999, 6.9112, 121.94756008, 1014584.068460175, 1014584.068460175, 242209.4169290263], 
processed observation next is [1.0, 0.5217391304347826, 0.6469135802469137, 0.2666666666666667, 1.0, 1.0, 0.1315461201721575, 1.0, 1.0, 0.1315461201721575, 1.0, 0.5, 0.3175749295782066, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.3623514530214911, 0.3623514530214911, 0.4657873402481275], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.1992812], dtype=float32), -0.05996484]. 
=============================================
[2019-04-02 19:23:58,025] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[54.83183 ]
 [55.385933]
 [54.980793]
 [55.485744]
 [55.847996]], R is [[54.65259933]
 [54.66514969]
 [54.67553711]
 [54.12878036]
 [54.15540695]].
[2019-04-02 19:23:58,235] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 19:23:58,236] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:23:58,242] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 19:23:58,242] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:23:58,263] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res16/Eplus-env-sub_run5
[2019-04-02 19:23:58,281] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 19:23:58,282] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:23:58,294] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res12/Eplus-env-sub_run5
[2019-04-02 19:23:58,312] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 19:23:58,313] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:23:58,338] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res2/Eplus-env-sub_run5
[2019-04-02 19:23:58,369] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res9/Eplus-env-sub_run5
[2019-04-02 19:23:58,389] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 19:23:58,390] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:23:58,414] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res3/Eplus-env-sub_run5
[2019-04-02 19:23:58,431] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 19:23:58,433] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 19:23:58,433] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:23:58,435] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:23:58,451] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res17/Eplus-env-sub_run5
[2019-04-02 19:23:58,477] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-02 19:23:58,481] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:23:58,485] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res15/Eplus-env-sub_run5
[2019-04-02 19:23:58,516] EPLUS_ENV_Part3-NA-Bej-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res7/Eplus-env-sub_run5
[2019-04-02 19:23:58,924] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:23:58,926] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1311
[2019-04-02 19:23:58,928] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.56666666666667, 35.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5082529843100639, 6.9112, 6.9112, 121.9260426156618, 362898.9467908815, 362898.9467908815, 101579.7427261008], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 287400.0000, 
sim time next is 288000.0000, 
raw observation next is [23.8, 35.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5122149395936186, 6.911199999999999, 6.9112, 121.9260426156618, 365728.5066216229, 365728.5066216233, 102140.5825179861], 
processed observation next is [0.0, 0.34782608695652173, 0.43703703703703706, 0.35, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.39026867449202324, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.13061732379343674, 0.13061732379343688, 0.19642419714997328], 
reward next is 0.8036, 
noisyNet noise sample is [array([-1.6520432], dtype=float32), 2.067858]. 
=============================================
[2019-04-02 19:23:58,941] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[74.12172]
 [74.08348]
 [74.06029]
 [74.0555 ]
 [74.0727 ]], R is [[74.21914673]
 [74.28161621]
 [74.34457397]
 [74.40817261]
 [74.47245026]].
[2019-04-02 19:23:58,972] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-02 19:23:58,975] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 19:23:58,975] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:23:58,975] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 19:23:58,976] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:23:58,976] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 19:23:58,976] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:23:58,977] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 19:23:58,977] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 19:23:58,977] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:23:58,977] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:23:58,980] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run35
[2019-04-02 19:23:59,005] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run35
[2019-04-02 19:23:59,025] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run35
[2019-04-02 19:23:59,050] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run35
[2019-04-02 19:23:59,073] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run35
[2019-04-02 19:25:21,388] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.31344816], dtype=float32), -0.21346025]
[2019-04-02 19:25:21,389] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [23.731415475, 103.089306735, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9869926346833527, 6.911199999999999, 6.9112, 121.9259251104856, 710434.2443547697, 710434.2443547702, 189657.9300344288]
[2019-04-02 19:25:21,390] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 19:25:21,395] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.009480902836034866
[2019-04-02 19:25:21,396] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 710434.2443547697 W.
[2019-04-02 19:25:45,741] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.31344816], dtype=float32), -0.21346025]
[2019-04-02 19:25:45,742] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [27.0, 79.0, 1.0, 2.0, 0.6787152949437109, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 773531.194345824, 773531.194345824, 172151.1826762753]
[2019-04-02 19:25:45,743] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 19:25:45,745] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.000000e+00 0.000000e+00 5.457147e-15 0.000000e+00 0.000000e+00], sampled 0.7314442144698283
[2019-04-02 19:25:45,745] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 773531.194345824 W.
[2019-04-02 19:26:01,727] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.31344816], dtype=float32), -0.21346025]
[2019-04-02 19:26:01,729] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [22.93333333333333, 84.66666666666667, 1.0, 2.0, 0.8884462618887818, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1071342.973196152, 1071342.973196152, 217673.5948317891]
[2019-04-02 19:26:01,730] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 19:26:01,735] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 5.1626316e-20 0.0000000e+00 0.0000000e+00], sampled 0.922122816473692
[2019-04-02 19:26:01,737] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1071342.973196152 W.
[2019-04-02 19:26:06,215] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8633.6908 2219215572.7071 543.0000
[2019-04-02 19:26:06,550] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7841.4107 2529829775.6138 831.0000
[2019-04-02 19:26:06,745] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8361.8030 2339524911.9634 616.0000
[2019-04-02 19:26:07,020] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8557.8765 2258327642.4849 536.0000
[2019-04-02 19:26:07,113] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8404.1885 2293006379.4818 697.0000
[2019-04-02 19:26:08,128] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 850000, evaluation results [850000.0, 7841.4107121824145, 2529829775.613778, 831.0, 8557.876498474936, 2258327642.484933, 536.0, 8633.690841416023, 2219215572.7070513, 543.0, 8361.802956777869, 2339524911.9634447, 616.0, 8404.188452724367, 2293006379.4818354, 697.0]
[2019-04-02 19:26:10,737] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 1.9104477e-25 1.7772923e-10 7.3043417e-21 5.3455992e-22], sum to 1.0000
[2019-04-02 19:26:10,746] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6863
[2019-04-02 19:26:10,752] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 928400.6766222557 W.
[2019-04-02 19:26:10,757] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.96666666666667, 42.33333333333334, 1.0, 2.0, 0.3706436937124924, 1.0, 2.0, 0.3706436937124924, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 928400.6766222557, 928400.6766222562, 201241.0374972268], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 37200.0000, 
sim time next is 37800.0000, 
raw observation next is [26.2, 42.0, 1.0, 2.0, 0.7500332775111789, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 121.9260426156618, 948227.8031357019, 948227.8031357015, 189176.9903991207], 
processed observation next is [1.0, 0.43478260869565216, 0.5259259259259259, 0.42, 1.0, 1.0, 0.7024205684656891, 0.0, 0.5, -0.1904761904761905, 0.0, 1.0, -0.25, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.33865278683417926, 0.3386527868341791, 0.36380190461369366], 
reward next is 0.6362, 
noisyNet noise sample is [array([-1.307058], dtype=float32), 0.92985266]. 
=============================================
[2019-04-02 19:26:10,873] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 2.9340910e-36 1.4253724e-26 4.7981732e-34 3.1034351e-33], sum to 1.0000
[2019-04-02 19:26:10,879] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8041
[2019-04-02 19:26:10,887] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1016641.950800668 W.
[2019-04-02 19:26:10,894] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.9, 41.0, 1.0, 2.0, 0.805138827525674, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 1016641.950800668, 1016641.950800668, 200658.7212623789], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 39600.0000, 
sim time next is 40200.0000, 
raw observation next is [27.08333333333334, 40.66666666666667, 1.0, 2.0, 0.8779072728279721, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.96832970398833, 6.9112, 121.9258058301803, 1136597.133898681, 1107341.67607709, 216650.9092613499], 
processed observation next is [1.0, 0.4782608695652174, 0.5586419753086422, 0.40666666666666673, 1.0, 1.0, 0.8546515152713954, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.005712970398832962, 0.0, 0.8094605568107783, 0.4059275478209575, 0.39547917002753213, 0.41663636396413445], 
reward next is 0.2977, 
noisyNet noise sample is [array([-0.15326923], dtype=float32), 1.1835334]. 
=============================================
[2019-04-02 19:26:16,159] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:26:16,167] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2787
[2019-04-02 19:26:16,173] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.5, 73.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6881839992793007, 6.9112, 6.9112, 121.9260426156618, 513295.1017620845, 513295.1017620845, 141225.4192745326], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 112800.0000, 
sim time next is 113400.0000, 
raw observation next is [22.6, 73.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6946790536821178, 6.911199999999999, 6.9112, 121.9260426156618, 518298.9091919371, 518298.9091919376, 142111.9704732226], 
processed observation next is [1.0, 0.30434782608695654, 0.39259259259259266, 0.735, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6183488171026472, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.18510675328283469, 0.18510675328283485, 0.2732922509100435], 
reward next is 0.7267, 
noisyNet noise sample is [array([-0.51879317], dtype=float32), -0.2009911]. 
=============================================
[2019-04-02 19:26:16,280] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 5.6777686e-35 0.0000000e+00], sum to 1.0000
[2019-04-02 19:26:16,289] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0140
[2019-04-02 19:26:16,303] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1714641.260415463 W.
[2019-04-02 19:26:16,310] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [37.4, 14.5, 1.0, 2.0, 0.8144727904606118, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9557174088059993, 6.9112, 6.9112, 121.9260426156618, 1714641.260415463, 1714641.260415463, 326788.6060670077], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 137400.0000, 
sim time next is 138000.0000, 
raw observation next is [37.4, 14.0, 1.0, 2.0, 0.6427474716624999, 1.0, 1.0, 0.6427474716624999, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1572564.80613065, 1572564.80613065, 287465.8769494771], 
processed observation next is [1.0, 0.6086956521739131, 0.9407407407407407, 0.14, 1.0, 1.0, 0.5746993710267856, 1.0, 0.5, 0.5746993710267856, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.5616302879038035, 0.5616302879038035, 0.5528189941336099], 
reward next is 0.4472, 
noisyNet noise sample is [array([0.485412], dtype=float32), 2.0181258]. 
=============================================
[2019-04-02 19:26:16,324] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[67.28955 ]
 [67.81775 ]
 [68.82154 ]
 [67.717926]
 [68.58977 ]], R is [[67.00966644]
 [66.71112823]
 [66.45788574]
 [66.25737   ]
 [65.59479523]].
[2019-04-02 19:26:18,797] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:26:18,807] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2536
[2019-04-02 19:26:18,813] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [36.73333333333333, 6.666666666666666, 1.0, 2.0, 0.2083158055078482, 1.0, 2.0, 0.2083158055078482, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 533319.6457521173, 533319.6457521177, 161938.1132490633], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 148800.0000, 
sim time next is 149400.0000, 
raw observation next is [36.45, 7.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7199099403081162, 6.911200000000001, 6.9112, 121.9260426156618, 516915.0513753616, 516915.0513753611, 130235.6630463853], 
processed observation next is [1.0, 0.7391304347826086, 0.9055555555555557, 0.07, 0.0, 0.5, -0.1904761904761905, 0.0, 0.5, -0.1904761904761905, 1.0, 0.5, 0.6498874253851452, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.18461251834834344, 0.18461251834834325, 0.2504531981661256], 
reward next is 0.7495, 
noisyNet noise sample is [array([-0.09605174], dtype=float32), -0.30072132]. 
=============================================
[2019-04-02 19:26:19,467] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:26:19,479] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0034
[2019-04-02 19:26:19,485] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.1, 46.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5914682146641194, 6.911200000000001, 6.9112, 121.9260426156618, 437666.8967051255, 437666.896705125, 128703.69511926], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 601200.0000, 
sim time next is 601800.0000, 
raw observation next is [25.96666666666667, 46.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5886950364312198, 6.9112, 6.9112, 121.9260426156618, 435453.5254505347, 435453.5254505347, 128352.0395623692], 
processed observation next is [1.0, 1.0, 0.517283950617284, 0.465, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.4858687955390247, 0.0, 0.0, 0.8094621288201359, 0.15551911623233383, 0.15551911623233383, 0.24683084531224847], 
reward next is 0.7532, 
noisyNet noise sample is [array([-0.7262964], dtype=float32), 0.58281094]. 
=============================================
[2019-04-02 19:26:23,377] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:26:23,388] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1501
[2019-04-02 19:26:23,395] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.9, 29.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.604248237046477, 6.9112, 6.9112, 121.9260426156618, 444736.4876184454, 444736.4876184454, 128585.6261417211], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 213000.0000, 
sim time next is 213600.0000, 
raw observation next is [30.1, 28.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6079551476798267, 6.9112, 6.9112, 121.9260426156618, 446980.8524579622, 446980.8524579622, 128685.6463736377], 
processed observation next is [0.0, 0.4782608695652174, 0.6703703703703704, 0.28, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5099439345997834, 0.0, 0.0, 0.8094621288201359, 0.15963601873498648, 0.15963601873498648, 0.2474723968723802], 
reward next is 0.7525, 
noisyNet noise sample is [array([-4.2543063], dtype=float32), 1.0372648]. 
=============================================
[2019-04-02 19:26:42,775] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:26:42,785] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2798
[2019-04-02 19:26:42,795] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.96666666666667, 43.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6476762567665227, 6.911200000000001, 6.9112, 121.9260426156618, 482812.4040356977, 482812.4040356972, 136737.5516171776], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 926400.0000, 
sim time next is 927000.0000, 
raw observation next is [27.8, 43.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6443897987793092, 6.911200000000001, 6.9112, 121.9260426156618, 480264.8425212653, 480264.8425212648, 136301.184081176], 
processed observation next is [0.0, 0.7391304347826086, 0.5851851851851853, 0.435, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5554872484741366, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.17152315804330903, 0.17152315804330887, 0.26211766169456924], 
reward next is 0.7379, 
noisyNet noise sample is [array([0.09820739], dtype=float32), -1.8333998]. 
=============================================
[2019-04-02 19:26:42,812] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[72.75743 ]
 [72.693596]
 [72.64506 ]
 [72.54644 ]
 [72.447426]], R is [[72.84992981]
 [72.85847473]
 [72.86686707]
 [72.87483215]
 [72.88135529]].
[2019-04-02 19:26:51,752] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 4.5051828e-33 2.8709908e-37 9.0350764e-38], sum to 1.0000
[2019-04-02 19:26:51,762] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1665
[2019-04-02 19:26:51,768] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.05, 57.5, 1.0, 2.0, 0.16, 1.0, 1.0, 0.16, 1.0, 2.0, 0.2579700709605, 6.911200000000001, 6.9112, 121.94756008, 575271.6413362314, 575271.6413362309, 201646.7363767817], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 613800.0000, 
sim time next is 614400.0000, 
raw observation next is [22.86666666666667, 58.33333333333334, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.698230603529718, 6.911200000000001, 6.9112, 121.9260426156391, 515049.8706026625, 515049.8706026621, 138117.6953274033], 
processed observation next is [1.0, 0.08695652173913043, 0.4024691358024693, 0.5833333333333335, 0.0, 0.5, -0.1904761904761905, 0.0, 0.5, -0.1904761904761905, 1.0, 1.0, 0.6227882544121475, 8.881784197001253e-17, 0.0, 0.8094621288199852, 0.18394638235809374, 0.1839463823580936, 0.2656109525526986], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.3805419], dtype=float32), 0.6413854]. 
=============================================
[2019-04-02 19:26:53,787] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.000000e+00 6.191970e-38 0.000000e+00 3.907142e-34 7.988719e-36], sum to 1.0000
[2019-04-02 19:26:53,795] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2230
[2019-04-02 19:26:53,804] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1280524.492194185 W.
[2019-04-02 19:26:53,810] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.05, 47.16666666666667, 1.0, 2.0, 0.921989322818211, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.174636891869526, 6.9112, 121.9247718089709, 1280524.492194185, 1145622.663899708, 226453.1542206201], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 636600.0000, 
sim time next is 637200.0000, 
raw observation next is [27.4, 46.0, 1.0, 2.0, 0.4905982906321911, 0.0, 2.0, 0.0, 1.0, 1.0, 0.8055365520583447, 6.9112, 6.9112, 121.925883947783, 1204428.797261614, 1204428.797261614, 250511.4024734234], 
processed observation next is [1.0, 0.391304347826087, 0.5703703703703703, 0.46, 1.0, 1.0, 0.3935693936097514, 0.0, 1.0, -0.1904761904761905, 1.0, 0.5, 0.7569206900729308, 0.0, 0.0, 0.8094610754304237, 0.4301531418791478, 0.4301531418791478, 0.48175269706427576], 
reward next is 0.5182, 
noisyNet noise sample is [array([-0.3795396], dtype=float32), -0.8914345]. 
=============================================
[2019-04-02 19:26:57,217] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 1.9226345e-33 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 19:26:57,228] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6858
[2019-04-02 19:26:57,234] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.38333333333333, 34.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5830413914573057, 6.9112, 6.9112, 121.9260426156618, 429142.3934513604, 429142.3934513604, 126675.7743239828], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 687000.0000, 
sim time next is 687600.0000, 
raw observation next is [28.2, 35.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5798705942692681, 6.9112, 6.9112, 121.9260426156618, 426805.2562799744, 426805.2562799744, 126390.6280639236], 
processed observation next is [1.0, 1.0, 0.6, 0.35, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.4748382428365851, 0.0, 0.0, 0.8094621288201359, 0.15243044867141944, 0.15243044867141944, 0.24305890012293002], 
reward next is 0.7569, 
noisyNet noise sample is [array([0.18608697], dtype=float32), 0.024545558]. 
=============================================
[2019-04-02 19:27:01,052] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:27:01,063] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6517
[2019-04-02 19:27:01,077] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1247459.532283867 W.
[2019-04-02 19:27:01,085] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.16666666666667, 23.0, 1.0, 2.0, 0.5005001777841251, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8361186493594468, 6.911199999999999, 6.9112, 121.9260426156618, 1247459.532283867, 1247459.532283867, 252507.4261100831], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 742800.0000, 
sim time next is 743400.0000, 
raw observation next is [32.15000000000001, 23.0, 1.0, 2.0, 0.4961049270344449, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8292748721951997, 6.9112, 6.9112, 121.9260426156618, 1237041.904895655, 1237041.904895655, 250950.818538756], 
processed observation next is [1.0, 0.6086956521739131, 0.7462962962962968, 0.23, 1.0, 1.0, 0.400124913136244, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.7865935902439997, 0.0, 0.0, 0.8094621288201359, 0.44180068031987685, 0.44180068031987685, 0.4825977279591461], 
reward next is 0.5174, 
noisyNet noise sample is [array([-0.75116426], dtype=float32), 1.9366121]. 
=============================================
[2019-04-02 19:27:08,018] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:27:08,029] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8578
[2019-04-02 19:27:08,039] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.4, 37.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7168122669205949, 6.9112, 6.9112, 121.9260426156618, 535318.7071283328, 535318.7071283328, 147956.9298486078], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 838800.0000, 
sim time next is 839400.0000, 
raw observation next is [31.33333333333334, 36.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7146772631124689, 6.911200000000001, 6.9112, 121.9260426156618, 533816.5471119583, 533816.5471119578, 147538.1818624658], 
processed observation next is [0.0, 0.7391304347826086, 0.7160493827160496, 0.3683333333333334, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6433465788905862, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.19064876682569937, 0.1906487668256992, 0.28372727281243426], 
reward next is 0.7163, 
noisyNet noise sample is [array([-1.6686926], dtype=float32), -0.30922908]. 
=============================================
[2019-04-02 19:27:08,646] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:27:08,655] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8711
[2019-04-02 19:27:08,660] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.61666666666667, 40.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6478270667110725, 6.911199999999999, 6.9112, 121.9260426156618, 483111.1005097752, 483111.1005097757, 136965.5313895537], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 849000.0000, 
sim time next is 849600.0000, 
raw observation next is [28.4, 41.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6420806588725622, 6.9112, 6.9112, 121.9260426156618, 478574.355408374, 478574.355408374, 136102.1831880801], 
processed observation next is [0.0, 0.8695652173913043, 0.6074074074074074, 0.41, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5526008235907026, 0.0, 0.0, 0.8094621288201359, 0.17091941264584784, 0.17091941264584784, 0.2617349676693848], 
reward next is 0.7383, 
noisyNet noise sample is [array([2.162333], dtype=float32), -0.2645229]. 
=============================================
[2019-04-02 19:27:10,740] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 1.2979824e-22 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 19:27:10,750] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6578
[2019-04-02 19:27:10,759] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.5, 65.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5722863309450227, 6.9112, 6.9112, 121.9260426156618, 418988.8651334851, 418988.8651334851, 124659.2267922776], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 879000.0000, 
sim time next is 879600.0000, 
raw observation next is [21.4, 65.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5670044032273566, 6.911199999999999, 6.9112, 121.9260426156618, 414404.7320090333, 414404.7320090338, 123885.8967230845], 
processed observation next is [0.0, 0.17391304347826086, 0.3481481481481481, 0.6566666666666667, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.4587555040341957, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1480016900032262, 0.14800169000322635, 0.2382421090828548], 
reward next is 0.7618, 
noisyNet noise sample is [array([-0.2429591], dtype=float32), 0.121956505]. 
=============================================
[2019-04-02 19:27:14,980] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-02 19:27:14,983] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 19:27:14,984] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 19:27:14,985] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 19:27:14,987] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 19:27:14,988] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:27:14,989] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 19:27:14,987] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:27:14,993] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:27:14,990] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:27:14,995] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:27:15,012] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run36
[2019-04-02 19:27:15,036] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run36
[2019-04-02 19:27:15,057] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run36
[2019-04-02 19:27:15,077] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run36
[2019-04-02 19:27:15,096] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run36
[2019-04-02 19:27:33,320] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.43669248], dtype=float32), -0.14955404]
[2019-04-02 19:27:33,321] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [28.61720681, 53.51567109, 1.0, 2.0, 0.4275330388080604, 0.0, 2.0, 0.0, 1.0, 1.0, 0.6862014611261702, 6.911199999999999, 6.9112, 121.9260426156618, 1013451.445271759, 1013451.44527176, 231751.6393902803]
[2019-04-02 19:27:33,322] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 19:27:33,325] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [7.011276e-04 0.000000e+00 9.992988e-01 0.000000e+00 0.000000e+00], sampled 0.3120134853010993
[2019-04-02 19:27:50,697] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.43669248], dtype=float32), -0.14955404]
[2019-04-02 19:27:50,699] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [33.91666666666667, 32.16666666666666, 1.0, 2.0, 0.3412691370817403, 0.0, 2.0, 0.0, 1.0, 1.0, 0.5489521210713074, 6.911199999999999, 6.9112, 121.9260426156618, 812982.8933882149, 812982.8933882153, 205576.8824530166]
[2019-04-02 19:27:50,701] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 19:27:50,704] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.1966656e-04 0.0000000e+00 9.9988031e-01 0.0000000e+00 0.0000000e+00], sampled 0.02206071491916406
[2019-04-02 19:28:25,728] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.43669248], dtype=float32), -0.14955404]
[2019-04-02 19:28:25,729] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [26.58718295, 92.414475605, 1.0, 2.0, 0.7477023449085684, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 852199.3809512013, 852199.3809512013, 185374.988590763]
[2019-04-02 19:28:25,730] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 19:28:25,733] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0.9245393  0.         0.07546068 0.         0.        ], sampled 0.01235376576486058
[2019-04-02 19:28:25,733] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 852199.3809512013 W.
[2019-04-02 19:28:47,706] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.43669248], dtype=float32), -0.14955404]
[2019-04-02 19:28:47,707] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [24.0404822, 89.265759065, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8777604458660514, 6.911200000000001, 6.9112, 121.9260426156618, 643069.3195973495, 643069.319597349, 172870.0884688904]
[2019-04-02 19:28:47,708] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 19:28:47,713] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.025837797788678896
[2019-04-02 19:29:04,954] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.43669248], dtype=float32), -0.14955404]
[2019-04-02 19:29:04,958] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [25.67893762, 56.14812226833334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6796921568847663, 6.9112, 6.9112, 121.9260426156618, 506888.1558714727, 506888.1558714727, 140249.7621970162]
[2019-04-02 19:29:04,959] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 19:29:04,961] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.000000e+00 0.000000e+00 9.051357e-31 0.000000e+00 0.000000e+00], sampled 0.44637793764626343
[2019-04-02 19:29:14,203] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.43669248], dtype=float32), -0.14955404]
[2019-04-02 19:29:14,207] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [20.0, 73.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4891557417928767, 6.911199999999999, 6.9112, 121.9260426156618, 354022.4800422074, 354022.4800422079, 115996.6982472576]
[2019-04-02 19:29:14,208] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 19:29:14,211] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.0000000e+00 0.0000000e+00 3.0026943e-23 0.0000000e+00 0.0000000e+00], sampled 0.974472375898032
[2019-04-02 19:29:22,228] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8410.2189 2300527307.6241 491.0000
[2019-04-02 19:29:22,300] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7759.8590 2566930561.6228 742.0000
[2019-04-02 19:29:22,805] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8272.9696 2376589676.1824 533.0000
[2019-04-02 19:29:22,893] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8510.7668 2268642063.0026 467.0000
[2019-04-02 19:29:22,939] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8322.9066 2336527096.0071 587.0000
[2019-04-02 19:29:23,956] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 875000, evaluation results [875000.0, 7759.85904827282, 2566930561.622772, 742.0, 8410.218930773995, 2300527307.6240964, 491.0, 8510.766750733726, 2268642063.002619, 467.0, 8272.969573816863, 2376589676.1823645, 533.0, 8322.906605506252, 2336527096.0071287, 587.0]
[2019-04-02 19:29:28,755] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:29:28,767] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9950
[2019-04-02 19:29:28,773] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [18.6, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5836463883554721, 6.9112, 6.9112, 121.9260426156618, 431231.7268789453, 431231.7268789453, 127606.8513327741], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1209600.0000, 
sim time next is 1210200.0000, 
raw observation next is [18.55, 94.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5819892009612205, 6.9112, 6.9112, 121.9260426156618, 429795.7249121654, 429795.7249121654, 127337.1472895147], 
processed observation next is [1.0, 0.0, 0.2425925925925926, 0.9400000000000002, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.47748650120152564, 0.0, 0.0, 0.8094621288201359, 0.1534984731829162, 0.1534984731829162, 0.2448791294029129], 
reward next is 0.7551, 
noisyNet noise sample is [array([-2.147923], dtype=float32), 0.16529082]. 
=============================================
[2019-04-02 19:29:35,198] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 9.8536288e-19 4.1900293e-32 1.8127330e-33], sum to 1.0000
[2019-04-02 19:29:35,207] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9539
[2019-04-02 19:29:35,218] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1086775.169905622 W.
[2019-04-02 19:29:35,223] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.43333333333333, 44.66666666666667, 1.0, 2.0, 0.4375493765520134, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7277476863194109, 6.911199999999999, 6.9112, 121.9260425153403, 1086775.169905622, 1086775.169905622, 231945.0249498112], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1095600.0000, 
sim time next is 1096200.0000, 
raw observation next is [26.4, 45.0, 1.0, 2.0, 0.8789266988487805, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.937943040412955, 6.9112, 121.9258529477782, 1115398.4990319, 1101703.693378281, 216762.963769857], 
processed observation next is [1.0, 0.6956521739130435, 0.5333333333333333, 0.45, 1.0, 1.0, 0.8558651176771196, 0.0, 1.0, -0.1904761904761905, 0.0, 0.5, -0.25, 0.0026743040412955388, 0.0, 0.8094608696226295, 0.3983566067971071, 0.39346560477795756, 0.4168518534035711], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.05076417], dtype=float32), -2.4722126]. 
=============================================
[2019-04-02 19:29:42,684] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.3048463e-08 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 19:29:42,699] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5313
[2019-04-02 19:29:42,706] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [19.6, 87.0, 1.0, 2.0, 0.1757184263709051, 0.0, 2.0, 0.0, 1.0, 2.0, 0.296120499309381, 6.911199999999999, 6.9112, 121.9260426156618, 440360.8727622697, 440360.8727622702, 161251.2019391832], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1198800.0000, 
sim time next is 1199400.0000, 
raw observation next is [19.53333333333333, 87.5, 1.0, 2.0, 0.1751817090255948, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2952266686566354, 6.911199999999999, 6.9112, 121.9260426156618, 439025.5540117465, 439025.5540117469, 161133.4302453107], 
processed observation next is [1.0, 0.9130434782608695, 0.2790123456790123, 0.875, 1.0, 1.0, 0.018073463125708088, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.11903333582079421, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.15679484071848088, 0.15679484071848104, 0.30987198124098214], 
reward next is 0.6901, 
noisyNet noise sample is [array([1.9126086], dtype=float32), -0.18603528]. 
=============================================
[2019-04-02 19:29:46,813] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 2.1681508e-18 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 19:29:46,823] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6404
[2019-04-02 19:29:46,832] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.75, 70.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6599760779690731, 6.9112, 6.9112, 121.9260426156441, 493184.9756654122, 493184.9756654122, 140510.4453647742], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1704600.0000, 
sim time next is 1705200.0000, 
raw observation next is [23.7, 70.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6673298395352704, 6.911200000000001, 6.9112, 121.9260426156618, 498661.613249061, 498661.6132490606, 140962.0460210547], 
processed observation next is [1.0, 0.7391304347826086, 0.4333333333333333, 0.7066666666666667, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5841622994190879, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.17809343330323607, 0.17809343330323593, 0.2710808577327975], 
reward next is 0.7289, 
noisyNet noise sample is [array([-0.47702956], dtype=float32), 2.1389296]. 
=============================================
[2019-04-02 19:30:17,718] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.7266734e-07 0.0000000e+00 9.9999976e-01 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 19:30:17,722] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6166
[2019-04-02 19:30:17,729] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [23.46666666666667, 69.0, 1.0, 2.0, 0.5052470970072716, 0.0, 2.0, 0.0, 1.0, 1.0, 0.8259810941261565, 6.911199999999999, 6.9112, 121.9258496231757, 1234471.123605939, 1234471.12360594, 255939.9857400866], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1696800.0000, 
sim time next is 1697400.0000, 
raw observation next is [23.5, 69.0, 1.0, 2.0, 0.4938083941817775, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8059612400985008, 6.911199999999997, 6.9112, 121.9260425568131, 1204148.883447025, 1204148.883447027, 252130.1610790125], 
processed observation next is [1.0, 0.6521739130434783, 0.42592592592592593, 0.69, 1.0, 1.0, 0.3973909454544971, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.757451550123126, -2.6645352591003756e-16, 0.0, 0.8094621284294418, 0.43005317265965176, 0.43005317265965254, 0.4848656943827163], 
reward next is 0.5151, 
noisyNet noise sample is [array([-0.19483313], dtype=float32), -0.44502485]. 
=============================================
[2019-04-02 19:30:19,782] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:30:19,791] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3184
[2019-04-02 19:30:19,798] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.51666666666667, 90.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6125229048356146, 6.9112, 6.9112, 121.9260426156618, 454900.6736348627, 454900.6736348627, 131751.8257811827], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1986600.0000, 
sim time next is 1987200.0000, 
raw observation next is [19.5, 91.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6125538666154928, 6.911199999999999, 6.9112, 121.9260426156618, 454934.7871474361, 454934.7871474365, 131762.9280738588], 
processed observation next is [0.0, 0.0, 0.2777777777777778, 0.91, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.515692333269366, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1624767096955129, 0.16247670969551303, 0.25339024629588236], 
reward next is 0.7466, 
noisyNet noise sample is [array([-1.2439039], dtype=float32), -0.47505093]. 
=============================================
[2019-04-02 19:30:21,345] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:30:21,353] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3127
[2019-04-02 19:30:21,364] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.2, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6160631733621726, 6.911200000000001, 6.9112, 121.9260426156618, 458310.6505138577, 458310.6505138573, 132701.4806605205], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1742400.0000, 
sim time next is 1743000.0000, 
raw observation next is [20.13333333333333, 87.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6586659273737349, 6.9112, 6.9112, 121.9260426156618, 489935.8632362562, 489935.8632362562, 136845.3812813262], 
processed observation next is [1.0, 0.17391304347826086, 0.30123456790123443, 0.8733333333333333, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5733324092171685, 0.0, 0.0, 0.8094621288201359, 0.17497709401294864, 0.17497709401294864, 0.26316419477178116], 
reward next is 0.7368, 
noisyNet noise sample is [array([-1.6886255], dtype=float32), 0.9726]. 
=============================================
[2019-04-02 19:30:21,380] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[73.27931 ]
 [73.26247 ]
 [73.30659 ]
 [73.33653 ]
 [73.302895]], R is [[73.09729004]
 [73.11112213]
 [73.12438202]
 [73.13700104]
 [73.14884186]].
[2019-04-02 19:30:25,933] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:30:25,944] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5196
[2019-04-02 19:30:25,961] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.0, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8966448609230174, 6.911199999999999, 6.9112, 121.9260426156618, 655913.0320072607, 655913.0320072612, 175559.515135272], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2250000.0000, 
sim time next is 2250600.0000, 
raw observation next is [23.01666666666667, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8991855591360788, 6.911200000000001, 6.9112, 121.9260426156618, 657539.078513331, 657539.0785133305, 175943.0136948491], 
processed observation next is [1.0, 0.043478260869565216, 0.40802469135802477, 0.98, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8739819489200984, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2348353851833325, 0.23483538518333233, 0.3383519494131713], 
reward next is 0.6616, 
noisyNet noise sample is [array([0.6736078], dtype=float32), -0.91886526]. 
=============================================
[2019-04-02 19:30:27,314] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 3.0220453e-23 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 19:30:27,324] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0955
[2019-04-02 19:30:27,338] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 990493.8328985197 W.
[2019-04-02 19:30:27,356] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [21.93333333333333, 79.33333333333333, 1.0, 2.0, 0.2725890748011445, 1.0, 1.0, 0.2725890748011445, 1.0, 2.0, 0.4427154951001093, 6.911200000000001, 6.9112, 121.94756008, 990493.8328985197, 990493.8328985192, 244861.4992833613], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1856400.0000, 
sim time next is 1857000.0000, 
raw observation next is [21.91666666666666, 79.66666666666667, 1.0, 2.0, 0.2735773764049089, 1.0, 2.0, 0.2735773764049089, 1.0, 2.0, 0.4434198402455762, 6.911200000000001, 6.9112, 121.94756008, 991126.3476955105, 991126.3476955101, 245340.6517503636], 
processed observation next is [1.0, 0.4782608695652174, 0.36728395061728375, 0.7966666666666667, 1.0, 1.0, 0.13521116238679634, 1.0, 1.0, 0.13521116238679634, 1.0, 1.0, 0.3042748003069702, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.3539736956055395, 0.3539736956055393, 0.4718089456737762], 
reward next is 0.5282, 
noisyNet noise sample is [array([-0.7051914], dtype=float32), 0.367273]. 
=============================================
[2019-04-02 19:30:27,370] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[59.793285]
 [59.705894]
 [59.53339 ]
 [60.096695]
 [60.173355]], R is [[60.42822647]
 [60.35305786]
 [60.32256317]
 [59.71933746]
 [59.74914169]].
[2019-04-02 19:30:28,378] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:30:28,389] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6643
[2019-04-02 19:30:28,398] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 842032.637537648 W.
[2019-04-02 19:30:28,405] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [21.5, 76.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9605867312921249, 7.160607412463321, 6.9112, 121.9251011155354, 842032.637537648, 714314.7368067193, 170290.1245026319], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1845000.0000, 
sim time next is 1845600.0000, 
raw observation next is [21.53333333333333, 76.66666666666666, 1.0, 1.0, 0.2138574751998552, 1.0, 1.0, 0.2138574751998552, 1.0, 2.0, 0.3521748159235278, 6.9112, 6.9112, 121.94756008, 789658.3752257539, 789658.3752257539, 223457.1968965825], 
processed observation next is [1.0, 0.34782608695652173, 0.35308641975308636, 0.7666666666666666, 1.0, 0.5, 0.06411604190458953, 1.0, 0.5, 0.06411604190458953, 1.0, 1.0, 0.19021851990440972, 0.0, 0.0, 0.8096049824067558, 0.2820208482949121, 0.2820208482949121, 0.429725378647274], 
reward next is 0.5703, 
noisyNet noise sample is [array([0.09658682], dtype=float32), 0.33023307]. 
=============================================
[2019-04-02 19:30:30,763] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-02 19:30:30,765] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 19:30:30,765] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 19:30:30,766] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:30:30,766] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 19:30:30,766] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:30:30,768] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:30:30,770] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 19:30:30,770] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 19:30:30,771] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:30:30,772] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:30:30,797] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run37
[2019-04-02 19:30:30,797] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run37
[2019-04-02 19:30:30,839] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run37
[2019-04-02 19:30:30,862] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run37
[2019-04-02 19:30:30,879] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run37
[2019-04-02 19:30:35,462] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.42971063], dtype=float32), -0.13576293]
[2019-04-02 19:30:35,464] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [27.5, 25.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5518156420363212, 6.9112, 6.9112, 121.9260426156618, 394011.2159929035, 394011.2159929035, 111609.6267340077]
[2019-04-02 19:30:35,467] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 19:30:35,471] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.4988862489486303
[2019-04-02 19:31:50,785] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.42971063], dtype=float32), -0.13576293]
[2019-04-02 19:31:50,787] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [26.46268363, 85.40138141, 1.0, 2.0, 0.7197129412761156, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 820281.1762748513, 820281.1762748518, 179907.7276311777]
[2019-04-02 19:31:50,787] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 19:31:50,790] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6541932932820311
[2019-04-02 19:31:50,791] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 820281.1762748513 W.
[2019-04-02 19:32:38,578] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8633.6908 2219215572.7071 543.0000
[2019-04-02 19:32:38,939] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7841.4107 2529829775.6138 831.0000
[2019-04-02 19:32:38,996] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8361.8030 2339524911.9634 616.0000
[2019-04-02 19:32:39,059] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8404.1885 2293006379.4818 697.0000
[2019-04-02 19:32:39,124] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8557.8765 2258327642.4849 536.0000
[2019-04-02 19:32:40,142] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 900000, evaluation results [900000.0, 7841.4107121824145, 2529829775.613778, 831.0, 8557.876498474936, 2258327642.484933, 536.0, 8633.690841416023, 2219215572.7070513, 543.0, 8361.802956777869, 2339524911.9634447, 616.0, 8404.188452724367, 2293006379.4818354, 697.0]
[2019-04-02 19:32:44,320] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:32:44,331] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6376
[2019-04-02 19:32:44,342] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2031911.915134679 W.
[2019-04-02 19:32:44,350] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.65, 83.0, 1.0, 2.0, 0.8907307590990021, 1.0, 2.0, 0.8907307590990021, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 2031911.915134679, 2031911.91513468, 382671.8811368114], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2212200.0000, 
sim time next is 2212800.0000, 
raw observation next is [26.43333333333333, 83.66666666666667, 1.0, 2.0, 0.860267126641266, 1.0, 2.0, 0.860267126641266, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1962342.714799709, 1962342.714799709, 369286.7277722666], 
processed observation next is [1.0, 0.6086956521739131, 0.5345679012345678, 0.8366666666666667, 1.0, 1.0, 0.8336513412396024, 1.0, 1.0, 0.8336513412396024, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.7008366838570389, 0.7008366838570389, 0.7101667841774357], 
reward next is 0.2898, 
noisyNet noise sample is [array([0.969193], dtype=float32), 0.21618012]. 
=============================================
[2019-04-02 19:32:53,247] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:32:53,255] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3383
[2019-04-02 19:32:53,265] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.93333333333333, 41.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6243977780183678, 6.9112, 6.9112, 121.9260426156618, 464610.7189233045, 464610.7189233045, 133595.601863884], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2499000.0000, 
sim time next is 2499600.0000, 
raw observation next is [27.76666666666667, 42.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.627476894559512, 6.911199999999999, 6.9112, 121.9260426156618, 466986.9778884696, 466986.97788847, 133970.3500885312], 
processed observation next is [1.0, 0.9565217391304348, 0.5839506172839507, 0.4266666666666667, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.53434611819939, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.16678106353159627, 0.1667810635315964, 0.2576352886317908], 
reward next is 0.7424, 
noisyNet noise sample is [array([-1.0623149], dtype=float32), -0.14642133]. 
=============================================
[2019-04-02 19:33:05,040] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:33:05,056] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7097
[2019-04-02 19:33:05,064] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.45, 97.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7658271892765999, 6.9112, 6.9112, 121.9260426156618, 570004.9978336083, 570004.9978336083, 155474.1506916983], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2260200.0000, 
sim time next is 2260800.0000, 
raw observation next is [21.3, 97.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7543195012492397, 6.911199999999999, 6.9112, 121.9260426156618, 562026.5994681667, 562026.5994681672, 153687.2258562921], 
processed observation next is [1.0, 0.17391304347826086, 0.3444444444444445, 0.97, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6928993765615495, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.20072378552434525, 0.20072378552434542, 0.2955523574159463], 
reward next is 0.7044, 
noisyNet noise sample is [array([-0.5994678], dtype=float32), -0.6155656]. 
=============================================
[2019-04-02 19:33:06,169] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:33:06,178] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1384
[2019-04-02 19:33:06,183] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.75, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8812565815377316, 6.911200000000001, 6.9112, 121.9260426156618, 646986.1148935963, 646986.1148935958, 173025.5269905592], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2244600.0000, 
sim time next is 2245200.0000, 
raw observation next is [22.76666666666667, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.882201001946534, 6.9112, 6.9112, 121.9260426156618, 647526.7534626354, 647526.7534626354, 173184.5034046108], 
processed observation next is [1.0, 1.0, 0.3987654320987655, 0.98, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8527512524331673, 0.0, 0.0, 0.8094621288201359, 0.23125955480808408, 0.23125955480808408, 0.3330471219319438], 
reward next is 0.6670, 
noisyNet noise sample is [array([-1.2142599], dtype=float32), 0.7987095]. 
=============================================
[2019-04-02 19:33:16,289] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:33:16,298] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8381
[2019-04-02 19:33:16,302] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.36666666666667, 41.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6670895946127553, 6.911199999999999, 6.9112, 121.9260426156618, 498394.2374046109, 498394.2374046114, 140522.1048456069], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2398800.0000, 
sim time next is 2399400.0000, 
raw observation next is [29.2, 41.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6633963101203028, 6.911200000000001, 6.9112, 121.9260426156618, 495609.8301924938, 495609.8301924934, 140054.5608216129], 
processed observation next is [1.0, 0.782608695652174, 0.637037037037037, 0.415, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.5792453876503784, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.1770035107830335, 0.17700351078303336, 0.2693356938877171], 
reward next is 0.7307, 
noisyNet noise sample is [array([0.72571546], dtype=float32), 0.70913345]. 
=============================================
[2019-04-02 19:33:31,289] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:33:31,300] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9373
[2019-04-02 19:33:31,307] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.0, 91.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8297941327173189, 6.9112, 6.9112, 121.9260426156618, 613969.3986013862, 613969.3986013862, 165046.3909467121], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2626200.0000, 
sim time next is 2626800.0000, 
raw observation next is [23.33333333333334, 90.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8458885369268598, 6.911200000000001, 6.9112, 121.9260426156618, 624329.4816665691, 624329.4816665687, 167574.8088787159], 
processed observation next is [0.0, 0.391304347826087, 0.4197530864197533, 0.9066666666666666, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8073606711585748, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.22297481488091755, 0.2229748148809174, 0.3222592478436844], 
reward next is 0.6777, 
noisyNet noise sample is [array([-0.29563814], dtype=float32), -1.4095211]. 
=============================================
[2019-04-02 19:33:41,780] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4792015e-15 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 19:33:41,791] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2971
[2019-04-02 19:33:41,796] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.66666666666667, 80.66666666666667, 1.0, 2.0, 0.3283693521512242, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5227747028679446, 6.911199999999999, 6.9112, 121.9260426156618, 748472.2657834737, 748472.2657834742, 203025.2113624017], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2755200.0000, 
sim time next is 2755800.0000, 
raw observation next is [26.5, 81.5, 1.0, 2.0, 0.3287620829775375, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5233999431337854, 6.9112, 6.9112, 121.9260426156618, 749367.8786440929, 749367.8786440929, 203135.038530219], 
processed observation next is [0.0, 0.9130434782608695, 0.5370370370370371, 0.815, 1.0, 1.0, 0.2009072416399256, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.40424992891723166, 0.0, 0.0, 0.8094621288201359, 0.2676313852300332, 0.2676313852300332, 0.39064430486580576], 
reward next is 0.6094, 
noisyNet noise sample is [array([0.16747274], dtype=float32), 0.4466378]. 
=============================================
[2019-04-02 19:33:45,667] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 1.5200328e-37 3.0564509e-11 1.0709963e-28 4.0378763e-20], sum to 1.0000
[2019-04-02 19:33:45,674] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7753
[2019-04-02 19:33:45,685] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 718441.2239902444 W.
[2019-04-02 19:33:45,695] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.34999999999999, 53.5, 1.0, 2.0, 0.3152003208636959, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5018091761727924, 6.911199999999999, 6.9112, 121.9260426156618, 718441.2239902444, 718441.2239902448, 199376.3848523117], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2827800.0000, 
sim time next is 2828400.0000, 
raw observation next is [32.2, 54.0, 1.0, 2.0, 0.6315777933175654, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 719783.4042278827, 719783.4042278832, 163597.5332560718], 
processed observation next is [1.0, 0.7391304347826086, 0.7481481481481482, 0.54, 1.0, 1.0, 0.5614021349018635, 0.0, 1.0, -0.1904761904761905, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2570655015099581, 0.25706550150995827, 0.31461064087706114], 
reward next is 0.6854, 
noisyNet noise sample is [array([-0.21476471], dtype=float32), 1.7581608]. 
=============================================
[2019-04-02 19:33:46,846] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-02 19:33:46,848] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 19:33:46,849] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:33:46,849] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 19:33:46,849] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:33:46,851] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 19:33:46,852] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:33:46,853] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 19:33:46,854] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:33:46,854] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 19:33:46,856] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:33:46,892] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run38
[2019-04-02 19:33:46,893] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run38
[2019-04-02 19:33:46,893] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run38
[2019-04-02 19:33:46,957] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run38
[2019-04-02 19:33:46,958] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run38
[2019-04-02 19:34:09,409] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.43690056], dtype=float32), -0.1555032]
[2019-04-02 19:34:09,410] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [24.35, 61.33333333333333, 1.0, 2.0, 0.884726559562964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.922956422363411, 6.9112, 121.9260008528098, 1104942.986418446, 1098922.649453396, 217857.8135328389]
[2019-04-02 19:34:09,412] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 19:34:09,415] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [3.3736613e-03 0.0000000e+00 9.9662638e-01 5.4975634e-36 3.6947407e-33], sampled 0.5226667021297476
[2019-04-02 19:34:21,409] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.43690056], dtype=float32), -0.1555032]
[2019-04-02 19:34:21,409] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [24.0, 78.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7814799082638074, 6.9112, 6.9112, 121.9260426156618, 581349.8269153845, 581349.8269153845, 157557.8801348548]
[2019-04-02 19:34:21,410] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 19:34:21,411] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.18187792507495326
[2019-04-02 19:34:32,535] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.43690056], dtype=float32), -0.1555032]
[2019-04-02 19:34:32,538] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [32.22251553333334, 67.30608682, 1.0, 2.0, 1.02, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.184817628737564, 6.9112, 121.9210974439915, 1815437.599185859, 1163257.991303127, 245598.4445680939]
[2019-04-02 19:34:32,539] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 19:34:32,544] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.0000000e+00 0.0000000e+00 1.9654290e-19 0.0000000e+00 2.8167407e-36], sampled 0.8461425904508131
[2019-04-02 19:34:32,545] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1815437.599185859 W.
[2019-04-02 19:35:13,170] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.43690056], dtype=float32), -0.1555032]
[2019-04-02 19:35:13,170] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [26.7, 87.66666666666667, 1.0, 2.0, 0.7154332503669486, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 815400.8732488728, 815400.8732488728, 179087.401297078]
[2019-04-02 19:35:13,172] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 19:35:13,175] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [9.999999e-01 0.000000e+00 7.513521e-08 0.000000e+00 0.000000e+00], sampled 0.05624611312579508
[2019-04-02 19:35:13,176] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 815400.8732488728 W.
[2019-04-02 19:35:24,006] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.43690056], dtype=float32), -0.1555032]
[2019-04-02 19:35:24,007] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [24.93949625833334, 68.89204070666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7239937029636833, 6.911200000000001, 6.9112, 121.9260426156618, 540222.1428933819, 540222.1428933814, 149408.6913622061]
[2019-04-02 19:35:24,008] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 19:35:24,010] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.4819613492085095
[2019-04-02 19:35:26,251] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.43690056], dtype=float32), -0.1555032]
[2019-04-02 19:35:26,252] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [26.37501768, 49.72846759, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.880480295529403, 6.9112, 6.9112, 121.9260426156618, 657477.9128221448, 657477.9128221448, 163874.7803116648]
[2019-04-02 19:35:26,252] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 19:35:26,255] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.0000000e+00 0.0000000e+00 5.5952184e-15 0.0000000e+00 0.0000000e+00], sampled 0.30349549290901
[2019-04-02 19:35:26,532] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.43690056], dtype=float32), -0.1555032]
[2019-04-02 19:35:26,533] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [28.5, 78.16666666666667, 1.0, 2.0, 0.8642393578712163, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 985108.6879629003, 985108.6879629003, 209571.8374396236]
[2019-04-02 19:35:26,533] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 19:35:26,537] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 1.2943772e-08 0.0000000e+00 0.0000000e+00], sampled 0.6102644657049517
[2019-04-02 19:35:26,537] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 985108.6879629003 W.
[2019-04-02 19:35:33,677] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.43690056], dtype=float32), -0.1555032]
[2019-04-02 19:35:33,678] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [22.66666666666667, 88.66666666666667, 1.0, 2.0, 0.6035241110434593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 723700.0241549114, 723700.0241549114, 160246.4381002744]
[2019-04-02 19:35:33,678] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 19:35:33,681] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 4.1794338e-15 0.0000000e+00 0.0000000e+00], sampled 0.6674948761231172
[2019-04-02 19:35:33,683] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 723700.0241549114 W.
[2019-04-02 19:35:54,355] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8574.2142 2232332408.4537 525.0000
[2019-04-02 19:35:54,844] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8508.8865 2267454198.0715 528.0000
[2019-04-02 19:35:54,870] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8327.6991 2348166272.5467 588.0000
[2019-04-02 19:35:54,999] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8356.1291 2304494590.3555 656.0000
[2019-04-02 19:35:55,193] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7832.6815 2537873608.7205 794.0000
[2019-04-02 19:35:56,210] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 925000, evaluation results [925000.0, 7832.681537153534, 2537873608.720506, 794.0, 8508.886460311358, 2267454198.071523, 528.0, 8574.214154484409, 2232332408.4536786, 525.0, 8327.699056383137, 2348166272.546742, 588.0, 8356.12906389155, 2304494590.355544, 656.0]
[2019-04-02 19:36:06,287] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:36:06,296] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1204
[2019-04-02 19:36:06,305] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.95, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7893085789666894, 6.911200000000001, 6.9112, 121.9260426156618, 587745.946772943, 587745.9467729426, 158153.5014299202], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3213000.0000, 
sim time next is 3213600.0000, 
raw observation next is [23.26666666666667, 82.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7830700549989593, 6.9112, 6.9112, 121.9260426156618, 583398.8979315119, 583398.8979315119, 157177.8383461837], 
processed observation next is [0.0, 0.17391304347826086, 0.41728395061728407, 0.82, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.7288375687486992, 0.0, 0.0, 0.8094621288201359, 0.20835674926125425, 0.20835674926125425, 0.302265073742661], 
reward next is 0.6977, 
noisyNet noise sample is [array([1.1179991], dtype=float32), 2.3452442]. 
=============================================
[2019-04-02 19:36:07,489] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 6.4151746e-21 0.0000000e+00 7.3321787e-35], sum to 1.0000
[2019-04-02 19:36:07,502] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5291
[2019-04-02 19:36:07,508] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2284949.541358612 W.
[2019-04-02 19:36:07,513] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.4, 80.0, 1.0, 2.0, 1.02, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9977734948820727, 7.704920327118487, 6.9112, 121.922973336969, 2284949.541358612, 1878504.023781446, 381286.355916071], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2977200.0000, 
sim time next is 2977800.0000, 
raw observation next is [28.33333333333334, 80.66666666666667, 1.0, 2.0, 0.8890104987369674, 1.0, 1.0, 0.8890104987369674, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 121.9255646501606, 2027983.254906508, 2027983.254906508, 381908.5480261248], 
processed observation next is [1.0, 0.4782608695652174, 0.6049382716049385, 0.8066666666666668, 1.0, 1.0, 0.8678696413535326, 1.0, 0.5, 0.8678696413535326, 0.0, 0.5, -0.25, 8.881784197001253e-17, 0.0, 0.809458955626257, 0.7242797338951814, 0.7242797338951814, 0.7344395154348553], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.20627029], dtype=float32), -0.6608105]. 
=============================================
[2019-04-02 19:36:08,038] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.99667597 0.         0.00332406 0.         0.        ], sum to 1.0000
[2019-04-02 19:36:08,047] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9309
[2019-04-02 19:36:08,056] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1708081.146493701 W.
[2019-04-02 19:36:08,061] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.83333333333334, 89.83333333333333, 1.0, 2.0, 0.7489084318818127, 1.0, 2.0, 0.7489084318818127, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1708081.146493701, 1708081.146493701, 323094.731663899], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2994600.0000, 
sim time next is 2995200.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5028807039473685, 1.0, 2.0, 0.5028807039473685, 1.0, 1.0, 0.8006024583653527, 6.9112, 6.9112, 121.94756008, 1720437.643575218, 1720437.643575218, 345005.1738277043], 
processed observation next is [1.0, 0.6956521739130435, 0.5185185185185185, 0.89, 1.0, 1.0, 0.4081913142230577, 1.0, 1.0, 0.4081913142230577, 1.0, 0.5, 0.7507530729566908, 0.0, 0.0, 0.8096049824067558, 0.6144420155625778, 0.6144420155625778, 0.6634714881302005], 
reward next is 0.3365, 
noisyNet noise sample is [array([0.74732876], dtype=float32), -0.14099333]. 
=============================================
[2019-04-02 19:36:12,217] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:36:12,228] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8258
[2019-04-02 19:36:12,233] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8543043752849812, 6.911200000000001, 6.9112, 121.9260426156618, 631231.9948262519, 631231.9948262514, 168421.6837468536], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3308400.0000, 
sim time next is 3309000.0000, 
raw observation next is [22.18333333333334, 98.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8535768398601552, 6.9112, 6.9112, 121.9260426156618, 630756.4488575602, 630756.4488575602, 168308.8914037368], 
processed observation next is [0.0, 0.30434782608695654, 0.37716049382716077, 0.9833333333333334, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8169710498251939, 0.0, 0.0, 0.8094621288201359, 0.22527016030627148, 0.22527016030627148, 0.3236709450071862], 
reward next is 0.6763, 
noisyNet noise sample is [array([0.4242354], dtype=float32), -0.4391238]. 
=============================================
[2019-04-02 19:36:12,250] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[65.05831 ]
 [65.02297 ]
 [65.02133 ]
 [65.020676]
 [65.02751 ]], R is [[65.04691315]
 [65.07255554]
 [65.09758759]
 [65.12204742]
 [65.14631653]].
[2019-04-02 19:36:25,753] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:36:25,763] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2558
[2019-04-02 19:36:25,774] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.0, 49.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9074933601742488, 6.911200000000001, 6.9112, 121.9260426156618, 662021.345743069, 662021.3457430685, 177349.4103981504], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3261600.0000, 
sim time next is 3262200.0000, 
raw observation next is [30.56666666666667, 51.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8977469969669883, 6.911199999999999, 6.9112, 121.9260426156618, 657859.371571572, 657859.3715715724, 175469.616751209], 
processed observation next is [0.0, 0.782608695652174, 0.6876543209876544, 0.51, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8721837462087353, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.23494977556127572, 0.23494977556127586, 0.3374415706754019], 
reward next is 0.6626, 
noisyNet noise sample is [array([-0.19996357], dtype=float32), -0.6148264]. 
=============================================
[2019-04-02 19:36:26,098] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:36:26,110] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9985
[2019-04-02 19:36:26,117] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.05, 52.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8116679170440895, 6.9112, 6.9112, 121.9260426156618, 602331.4786813528, 602331.4786813528, 162048.5927993107], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3234600.0000, 
sim time next is 3235200.0000, 
raw observation next is [29.4, 50.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8061632178445161, 6.911200000000001, 6.9112, 121.9260426156618, 598617.4392451, 598617.4392450996, 161183.7516955457], 
processed observation next is [0.0, 0.43478260869565216, 0.6444444444444444, 0.5, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.7577040223056449, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.21379194258753573, 0.21379194258753556, 0.3099687532606648], 
reward next is 0.6900, 
noisyNet noise sample is [array([0.60466933], dtype=float32), -0.25606468]. 
=============================================
[2019-04-02 19:36:26,603] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:36:26,613] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3599
[2019-04-02 19:36:26,618] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.16666666666666, 53.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8941229453457449, 6.911199999999999, 6.9112, 121.9260426156618, 653559.5812661238, 653559.5812661243, 175324.9967784297], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3240600.0000, 
sim time next is 3241200.0000, 
raw observation next is [30.33333333333334, 52.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8871526954014121, 6.9112, 6.9112, 121.9260426156618, 649738.6266990401, 649738.6266990401, 174146.6911573643], 
processed observation next is [0.0, 0.5217391304347826, 0.6790123456790126, 0.52, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8589408692517649, 0.0, 0.0, 0.8094621288201359, 0.23204950953537146, 0.23204950953537146, 0.33489748299493133], 
reward next is 0.6651, 
noisyNet noise sample is [array([0.796607], dtype=float32), -0.9295684]. 
=============================================
[2019-04-02 19:36:47,313] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.5455092e-04 3.9863194e-18 2.1336607e-08 3.7671697e-09 9.9964547e-01], sum to 1.0000
[2019-04-02 19:36:47,322] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2211
[2019-04-02 19:36:47,326] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [22.25, 83.16666666666667, 1.0, 2.0, 0.16, 1.0, 2.0, 0.16, 1.0, 2.0, 0.2427257478000619, 6.9112, 6.9112, 121.94756008, 542150.8812260194, 542150.8812260194, 202339.0808454465], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3539400.0000, 
sim time next is 3540000.0000, 
raw observation next is [22.4, 84.33333333333334, 1.0, 2.0, 0.16, 1.0, 2.0, 0.16, 1.0, 2.0, 0.2460536408519099, 6.911200000000001, 6.9112, 121.94756008, 548725.0050754552, 548725.0050754547, 203623.2890253873], 
processed observation next is [1.0, 1.0, 0.38518518518518513, 0.8433333333333334, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.05756705106488735, 8.881784197001253e-17, 0.0, 0.8096049824067558, 0.19597321609837684, 0.19597321609837667, 0.3915832481257448], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.84114665], dtype=float32), -0.7938787]. 
=============================================
[2019-04-02 19:36:47,340] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[58.698704]
 [58.539116]
 [58.552727]
 [58.55764 ]
 [58.28997 ]], R is [[58.30538177]
 [57.72232819]
 [57.14510727]
 [57.17290115]
 [57.19277191]].
[2019-04-02 19:36:48,562] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.5742622e-34], sum to 1.0000
[2019-04-02 19:36:48,571] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4558
[2019-04-02 19:36:48,575] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.26666666666667, 96.16666666666667, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.8875662578478118, 6.9112, 6.9112, 121.9260426156618, 655141.5981802508, 655141.5981802508, 172905.097882518], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3552600.0000, 
sim time next is 3553200.0000, 
raw observation next is [22.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8911531798413088, 6.911200000000001, 6.9112, 121.9260426156618, 658815.4587493628, 658815.4587493624, 173038.4575318434], 
processed observation next is [1.0, 0.13043478260869565, 0.37037037037037035, 1.0, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8639414748016361, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2352912352676296, 0.23529123526762943, 0.33276626448431423], 
reward next is 0.6672, 
noisyNet noise sample is [array([-0.917875], dtype=float32), -0.7300274]. 
=============================================
[2019-04-02 19:36:54,251] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.8090577e-36], sum to 1.0000
[2019-04-02 19:36:54,264] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4849
[2019-04-02 19:36:54,274] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 732804.0812430524 W.
[2019-04-02 19:36:54,282] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [23.03333333333333, 99.16666666666667, 1.0, 2.0, 0.2143324839031672, 1.0, 2.0, 0.2143324839031672, 1.0, 1.0, 0.3412242946955213, 6.911199999999999, 6.9112, 121.94756008, 732804.0812430524, 732804.0812430528, 225333.5738886163], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3639000.0000, 
sim time next is 3639600.0000, 
raw observation next is [23.0, 100.0, 1.0, 2.0, 0.3118348016808185, 0.0, 1.0, 0.0, 1.0, 2.0, 0.4967155824999516, 6.911199999999999, 6.9112, 121.9260426156618, 716151.5433801253, 716151.5433801258, 198356.7789323553], 
processed observation next is [1.0, 0.13043478260869565, 0.4074074074074074, 1.0, 1.0, 1.0, 0.1807557162866887, 0.0, 0.5, -0.1904761904761905, 1.0, 1.0, 0.3708944781249395, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2557684083500448, 0.25576840835004494, 0.38145534410068327], 
reward next is 0.6185, 
noisyNet noise sample is [array([-0.7421438], dtype=float32), -0.022890786]. 
=============================================
[2019-04-02 19:36:57,368] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:36:57,380] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6397
[2019-04-02 19:36:57,390] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 836392.8885341728 W.
[2019-04-02 19:36:57,396] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.15, 88.0, 1.0, 2.0, 0.3669208069987138, 0.0, 1.0, 0.0, 1.0, 2.0, 0.584149874518379, 6.911200000000001, 6.9112, 121.9260426156618, 836392.8885341728, 836392.8885341723, 214116.0952657593], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3918600.0000, 
sim time next is 3919200.0000, 
raw observation next is [27.1, 88.33333333333333, 1.0, 2.0, 0.37474758113312, 1.0, 1.0, 0.37474758113312, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 854243.8967971733, 854243.8967971733, 199366.7364229062], 
processed observation next is [0.0, 0.34782608695652173, 0.5592592592592593, 0.8833333333333333, 1.0, 1.0, 0.25565188230133334, 1.0, 0.5, 0.25565188230133334, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288201359, 0.30508710599899047, 0.30508710599899047, 0.3833975700440504], 
reward next is 0.0000, 
noisyNet noise sample is [array([3.372456], dtype=float32), 0.82177967]. 
=============================================
[2019-04-02 19:37:00,839] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.3728071e-35], sum to 1.0000
[2019-04-02 19:37:00,849] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2575
[2019-04-02 19:37:00,858] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 924013.1925869095 W.
[2019-04-02 19:37:00,862] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.33333333333333, 98.33333333333334, 1.0, 2.0, 0.4053361785818995, 1.0, 2.0, 0.4053361785818995, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 924013.1925869095, 924013.1925869095, 207659.6552066976], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3733800.0000, 
sim time next is 3734400.0000, 
raw observation next is [24.26666666666667, 98.66666666666667, 1.0, 2.0, 0.7338425944663457, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 836394.0066292302, 836394.0066292307, 182645.702041024], 
processed observation next is [1.0, 0.21739130434782608, 0.4543209876543211, 0.9866666666666667, 1.0, 1.0, 0.6831459457932687, 0.0, 0.5, -0.1904761904761905, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.29871214522472506, 0.2987121452247252, 0.3512417346942769], 
reward next is 0.6488, 
noisyNet noise sample is [array([0.6165913], dtype=float32), -0.033698395]. 
=============================================
[2019-04-02 19:37:01,497] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.4157267e-29], sum to 1.0000
[2019-04-02 19:37:01,509] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3445
[2019-04-02 19:37:01,517] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 903769.0258198188 W.
[2019-04-02 19:37:01,523] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.4, 94.0, 1.0, 2.0, 0.7929218365992009, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 903769.0258198188, 903769.0258198188, 194479.7360188178], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3744000.0000, 
sim time next is 3744600.0000, 
raw observation next is [25.5, 94.00000000000001, 1.0, 2.0, 0.5439063174773812, 1.0, 1.0, 0.5439063174773812, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 122.5299911778874, 1240151.401026045, 1240151.401026045, 249383.6397404978], 
processed observation next is [1.0, 0.34782608695652173, 0.5, 0.9400000000000002, 1.0, 1.0, 0.45703133033021565, 1.0, 0.5, 0.45703133033021565, 0.0, 1.0, -0.25, -8.881784197001253e-17, 0.0, 0.8134717192110751, 0.44291121465215894, 0.44291121465215894, 0.4795839225778804], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.260233], dtype=float32), -1.200133]. 
=============================================
[2019-04-02 19:37:02,909] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-02 19:37:02,910] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 19:37:02,910] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 19:37:02,911] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 19:37:02,912] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 19:37:02,912] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:37:02,913] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:37:02,914] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 19:37:02,914] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:37:02,916] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:37:02,911] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:37:02,940] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run39
[2019-04-02 19:37:02,963] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run39
[2019-04-02 19:37:02,986] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run39
[2019-04-02 19:37:02,987] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run39
[2019-04-02 19:37:03,034] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run39
[2019-04-02 19:37:14,517] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.53019875], dtype=float32), -0.11453207]
[2019-04-02 19:37:14,517] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [34.71665429, 20.01193727166667, 1.0, 2.0, 0.9337207292961011, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.273043126025854, 6.9112, 121.9245140036823, 1349175.236628534, 1163881.538026177, 229295.7124767925]
[2019-04-02 19:37:14,517] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 19:37:14,521] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8246886898039006
[2019-04-02 19:37:14,521] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1349175.236628534 W.
[2019-04-02 19:37:28,523] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.53019875], dtype=float32), -0.11453207]
[2019-04-02 19:37:28,524] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [24.0, 69.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7187780050264224, 6.911199999999999, 6.9112, 121.9260426156618, 537141.3181358713, 537141.3181358718, 146926.6361333483]
[2019-04-02 19:37:28,525] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 19:37:28,526] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.07462453417869874
[2019-04-02 19:37:32,767] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.53019875], dtype=float32), -0.11453207]
[2019-04-02 19:37:32,768] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [20.3, 91.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6510716954874959, 6.9112, 6.9112, 121.9260426156618, 485838.1891180755, 485838.1891180755, 137689.9527166161]
[2019-04-02 19:37:32,769] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 19:37:32,775] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.40894472420419226
[2019-04-02 19:38:06,305] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.53019875], dtype=float32), -0.11453207]
[2019-04-02 19:38:06,306] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [27.0, 82.0, 1.0, 2.0, 0.6785590479151671, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 773353.029919097, 773353.029919097, 172124.3591417266]
[2019-04-02 19:38:06,306] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 19:38:06,311] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.0000000e+00 0.0000000e+00 6.2369774e-34 0.0000000e+00 0.0000000e+00], sampled 0.8611706045450838
[2019-04-02 19:38:06,313] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 773353.029919097 W.
[2019-04-02 19:38:10,850] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.53019875], dtype=float32), -0.11453207]
[2019-04-02 19:38:10,852] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [21.73573171333334, 89.17947655666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6711297820101603, 6.9112, 6.9112, 121.9260426156618, 501522.4689765255, 501522.4689765255, 141711.6562228986]
[2019-04-02 19:38:10,854] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 19:38:10,858] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6740501812282992
[2019-04-02 19:38:14,291] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.53019875], dtype=float32), -0.11453207]
[2019-04-02 19:38:14,292] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [21.7, 95.0, 1.0, 2.0, 0.7749479818372869, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 931646.0333752536, 931646.0333752536, 193031.8444242187]
[2019-04-02 19:38:14,293] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 19:38:14,300] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8308078297712664
[2019-04-02 19:38:14,301] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 931646.0333752536 W.
[2019-04-02 19:38:21,854] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.53019875], dtype=float32), -0.11453207]
[2019-04-02 19:38:21,856] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [25.33333333333334, 95.83333333333334, 1.0, 2.0, 0.6979603191189947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 795476.1120403366, 795476.1120403371, 175758.5047773202]
[2019-04-02 19:38:21,856] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 19:38:21,862] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.37118390667984946
[2019-04-02 19:38:21,862] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 795476.1120403366 W.
[2019-04-02 19:38:40,245] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.53019875], dtype=float32), -0.11453207]
[2019-04-02 19:38:40,246] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [22.75, 75.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.8299178936758201, 6.9112, 6.9112, 121.9260426156618, 620041.5570686285, 620041.5570686285, 158688.3386300254]
[2019-04-02 19:38:40,247] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 19:38:40,250] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8430485740522271
[2019-04-02 19:39:10,347] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8557.8765 2258327642.4849 536.0000
[2019-04-02 19:39:11,068] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.53019875], dtype=float32), -0.11453207]
[2019-04-02 19:39:11,069] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [28.01666666666667, 57.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8012992946270835, 6.9112, 6.9112, 121.9260426156618, 594465.5973739398, 594465.5973739398, 160822.7114798421]
[2019-04-02 19:39:11,069] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 19:39:11,071] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6297372074271433
[2019-04-02 19:39:11,291] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7841.4107 2529829775.6138 831.0000
[2019-04-02 19:39:11,304] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8633.7484 2219185658.3071 543.0000
[2019-04-02 19:39:11,403] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8404.2460 2292976464.9981 697.0000
[2019-04-02 19:39:11,416] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8361.8030 2339524911.9634 616.0000
[2019-04-02 19:39:12,433] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 950000, evaluation results [950000.0, 7841.4107121824145, 2529829775.613778, 831.0, 8557.876498474936, 2258327642.484933, 536.0, 8633.748369108142, 2219185658.307149, 543.0, 8361.802956777869, 2339524911.9634447, 616.0, 8404.245980577798, 2292976464.998051, 697.0]
[2019-04-02 19:39:16,629] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:39:16,642] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8183
[2019-04-02 19:39:16,657] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.5, 68.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8484915142001885, 6.9112, 6.9112, 121.9260426156618, 626999.7950989611, 626999.7950989611, 167664.8341722007], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3820200.0000, 
sim time next is 3820800.0000, 
raw observation next is [26.40000000000001, 69.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8513810740412168, 6.911199999999999, 6.9112, 121.9260426156618, 628555.7402801255, 628555.740280126, 168216.915409802], 
processed observation next is [0.0, 0.21739130434782608, 0.5333333333333337, 0.6933333333333335, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8142263425515208, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2244841929571877, 0.22448419295718786, 0.3234940680957731], 
reward next is 0.6765, 
noisyNet noise sample is [array([0.5500731], dtype=float32), 1.3424432]. 
=============================================
[2019-04-02 19:39:25,145] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 2.8943908e-30 0.0000000e+00 2.4983069e-26], sum to 1.0000
[2019-04-02 19:39:25,156] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7447
[2019-04-02 19:39:25,165] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1550699.489821897 W.
[2019-04-02 19:39:25,171] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.33333333333333, 72.0, 1.0, 2.0, 0.4533108959014101, 1.0, 2.0, 0.4533108959014101, 1.0, 2.0, 0.7216857095802444, 6.9112, 6.9112, 121.94756008, 1550699.489821897, 1550699.489821897, 321221.9852443062], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4180200.0000, 
sim time next is 4180800.0000, 
raw observation next is [27.66666666666667, 70.0, 1.0, 2.0, 0.4612293490640028, 1.0, 2.0, 0.4612293490640028, 1.0, 2.0, 0.7342921449011075, 6.9112, 6.9112, 121.94756008, 1577815.074776769, 1577815.074776769, 324941.6133023439], 
processed observation next is [1.0, 0.391304347826087, 0.580246913580247, 0.7, 1.0, 1.0, 0.35860636793333667, 1.0, 1.0, 0.35860636793333667, 1.0, 1.0, 0.6678651811263844, 0.0, 0.0, 0.8096049824067558, 0.5635053838488461, 0.5635053838488461, 0.6248877178891228], 
reward next is 0.3751, 
noisyNet noise sample is [array([-0.40159315], dtype=float32), -1.4196112]. 
=============================================
[2019-04-02 19:39:25,549] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 6.1689127e-38 3.3315648e-29 0.0000000e+00 1.8912054e-23], sum to 1.0000
[2019-04-02 19:39:25,561] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8472
[2019-04-02 19:39:25,570] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1668364.071663876 W.
[2019-04-02 19:39:25,574] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.5, 79.0, 1.0, 2.0, 0.7315107201742502, 1.0, 2.0, 0.7315107201742502, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 1668364.071663876, 1668364.071663877, 316266.2388056736], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4354200.0000, 
sim time next is 4354800.0000, 
raw observation next is [27.66666666666666, 79.0, 1.0, 2.0, 0.5375157674362006, 1.0, 2.0, 0.5375157674362006, 1.0, 1.0, 0.8557426074248425, 6.911199999999999, 6.9112, 121.94756008, 1839051.852054754, 1839051.852054755, 362383.5970376629], 
processed observation next is [1.0, 0.391304347826087, 0.5802469135802467, 0.79, 1.0, 1.0, 0.4494235326621435, 1.0, 1.0, 0.4494235326621435, 1.0, 0.5, 0.8196782592810531, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.6568042328766979, 0.6568042328766982, 0.6968915327647364], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.00851088], dtype=float32), 1.4310353]. 
=============================================
[2019-04-02 19:39:40,355] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:39:40,364] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3137
[2019-04-02 19:39:40,372] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.95, 94.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7227488034136073, 6.911200000000001, 6.9112, 121.9260426156618, 539944.9223391169, 539944.9223391166, 148221.7747471858], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4151400.0000, 
sim time next is 4152000.0000, 
raw observation next is [20.9, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7224509833467168, 6.9112, 6.9112, 121.9260426156618, 539714.4385544419, 539714.4385544419, 148208.9265051533], 
processed observation next is [1.0, 0.043478260869565216, 0.32962962962962955, 0.95, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.653063729183396, 0.0, 0.0, 0.8094621288201359, 0.1927551566265864, 0.1927551566265864, 0.28501716635606406], 
reward next is 0.7150, 
noisyNet noise sample is [array([-0.8931299], dtype=float32), 0.8759533]. 
=============================================
[2019-04-02 19:39:40,389] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[77.50511 ]
 [77.630745]
 [77.69402 ]
 [77.903114]
 [78.17895 ]], R is [[77.31156921]
 [77.25341034]
 [77.19559479]
 [77.13751221]
 [77.07910156]].
[2019-04-02 19:39:40,792] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 1.1035190e-28 0.0000000e+00 6.2052923e-35 4.2241757e-27], sum to 1.0000
[2019-04-02 19:39:40,802] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8690
[2019-04-02 19:39:40,810] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.11666666666667, 99.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7184924553763993, 6.9112, 6.9112, 121.9260426156618, 536910.3986606745, 536910.3986606745, 147147.2764770337], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4157400.0000, 
sim time next is 4158000.0000, 
raw observation next is [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7174651407933154, 6.911200000000001, 6.9112, 121.9260426156618, 536156.9162806877, 536156.9162806872, 146864.3939193285], 
processed observation next is [1.0, 0.13043478260869565, 0.2962962962962963, 1.0, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.6468314259916441, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.19148461295738844, 0.19148461295738828, 0.2824315267679394], 
reward next is 0.7176, 
noisyNet noise sample is [array([2.216888], dtype=float32), -1.0416901]. 
=============================================
[2019-04-02 19:39:40,825] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[59.194878]
 [59.39122 ]
 [59.74063 ]
 [57.9301  ]
 [61.147778]], R is [[59.36966705]
 [59.49299622]
 [59.61257553]
 [59.01644897]
 [59.09619141]].
[2019-04-02 19:39:41,488] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:39:41,501] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7424
[2019-04-02 19:39:41,508] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.96666666666667, 99.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8457584172809947, 6.9112, 6.9112, 121.9260426156618, 632071.5619682093, 632071.5619682093, 161544.0903143726], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4162800.0000, 
sim time next is 4163400.0000, 
raw observation next is [19.95, 99.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8462876200837642, 6.911199999999999, 6.9112, 121.9260426156618, 632461.3365437427, 632461.3365437431, 161491.1728585135], 
processed observation next is [1.0, 0.17391304347826086, 0.2944444444444444, 0.995, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8078595251047052, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.2258790487656224, 0.22587904876562256, 0.3105599478048337], 
reward next is 0.6894, 
noisyNet noise sample is [array([1.7566692], dtype=float32), 0.0026793675]. 
=============================================
[2019-04-02 19:39:49,710] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:39:49,722] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0659
[2019-04-02 19:39:49,734] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 703010.5228182639 W.
[2019-04-02 19:39:49,738] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.3, 86.0, 1.0, 2.0, 0.3084335422163809, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4910362441876718, 6.911199999999999, 6.9112, 121.9260426156618, 703010.5228182639, 703010.5228182643, 197524.5944525859], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4541400.0000, 
sim time next is 4542000.0000, 
raw observation next is [25.73333333333333, 83.33333333333334, 1.0, 2.0, 0.3095556126674577, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4928226168890695, 6.9112, 6.9112, 121.9260426156618, 705569.2275372175, 705569.2275372175, 197830.0090902073], 
processed observation next is [0.0, 0.5652173913043478, 0.5086419753086419, 0.8333333333333335, 1.0, 1.0, 0.1780423960326877, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.3660282711113368, 0.0, 0.0, 0.8094621288201359, 0.25198900983472056, 0.25198900983472056, 0.3804423251734756], 
reward next is 0.6196, 
noisyNet noise sample is [array([0.21162301], dtype=float32), -0.63229567]. 
=============================================
[2019-04-02 19:39:49,758] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[50.865532]
 [51.831078]
 [52.76578 ]
 [56.424625]
 [56.508327]], R is [[50.91590881]
 [51.02689362]
 [51.13763046]
 [51.2490387 ]
 [51.38480759]].
[2019-04-02 19:39:51,625] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:39:51,635] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5007
[2019-04-02 19:39:51,643] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 706066.448089996 W.
[2019-04-02 19:39:51,650] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 74.0, 1.0, 2.0, 0.6144740389374893, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 706066.448089996, 706066.448089996, 160869.2214899459], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4311600.0000, 
sim time next is 4312200.0000, 
raw observation next is [27.0, 74.0, 1.0, 2.0, 0.6138140370220025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 706835.9890671264, 706835.9890671264, 160827.6905928761], 
processed observation next is [1.0, 0.9130434782608695, 0.5555555555555556, 0.74, 1.0, 1.0, 0.5402548059785744, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.25244142466683084, 0.25244142466683084, 0.3092840203709156], 
reward next is 0.6907, 
noisyNet noise sample is [array([-1.1953773], dtype=float32), 1.1235316]. 
=============================================
[2019-04-02 19:39:51,910] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:39:51,919] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2085
[2019-04-02 19:39:51,925] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.91666666666667, 74.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8907779455965491, 6.911200000000001, 6.9112, 121.9260426156618, 654599.9762918707, 654599.9762918702, 174124.69135124], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4315800.0000, 
sim time next is 4316400.0000, 
raw observation next is [25.7, 74.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8751554021621922, 6.9112, 6.9112, 121.9260426156618, 644830.6159256756, 644830.6159256756, 171635.8357302023], 
processed observation next is [1.0, 1.0, 0.5074074074074074, 0.74, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8439442527027402, 0.0, 0.0, 0.8094621288201359, 0.23029664854488416, 0.23029664854488416, 0.33006891486577367], 
reward next is 0.6699, 
noisyNet noise sample is [array([0.40508825], dtype=float32), -0.52251637]. 
=============================================
[2019-04-02 19:39:52,410] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:39:52,431] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5854
[2019-04-02 19:39:52,440] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1437999.909004004 W.
[2019-04-02 19:39:52,444] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.13333333333334, 86.0, 1.0, 1.0, 0.4203966949574064, 1.0, 1.0, 0.4203966949574064, 1.0, 2.0, 0.6692852297367018, 6.9112, 6.9112, 121.94756008, 1437999.909004004, 1437999.909004004, 306132.2532109633], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4328400.0000, 
sim time next is 4329000.0000, 
raw observation next is [24.1, 88.0, 1.0, 2.0, 0.3031837291412167, 1.0, 2.0, 0.3031837291412167, 1.0, 2.0, 0.482678370797535, 6.911199999999999, 6.9112, 121.94756008, 1036792.480772625, 1036792.480772625, 257273.1771443609], 
processed observation next is [1.0, 0.08695652173913043, 0.4481481481481482, 0.88, 1.0, 1.0, 0.17045682040621038, 1.0, 1.0, 0.17045682040621038, 1.0, 1.0, 0.3533479634969187, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.3702830288473661, 0.3702830288473661, 0.4947561098930017], 
reward next is 0.5052, 
noisyNet noise sample is [array([-0.10768501], dtype=float32), 1.3905637]. 
=============================================
[2019-04-02 19:39:52,460] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[35.407436]
 [35.54054 ]
 [38.12402 ]
 [38.075977]
 [38.101124]], R is [[36.07597733]
 [36.12650299]
 [35.76523972]
 [36.08481216]
 [36.39807129]].
[2019-04-02 19:39:52,500] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:39:52,510] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6332
[2019-04-02 19:39:52,514] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.6, 82.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8835850493508748, 6.911200000000001, 6.9112, 121.9260426156618, 649775.0505569557, 649775.0505569553, 173067.3747403423], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4325400.0000, 
sim time next is 4326000.0000, 
raw observation next is [24.46666666666667, 82.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.871460809300094, 6.9112, 6.9112, 121.9260426156618, 642110.5865007341, 642110.5865007341, 171157.6859004461], 
processed observation next is [1.0, 0.043478260869565216, 0.46172839506172847, 0.8233333333333335, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.8393260116251174, 0.0, 0.0, 0.8094621288201359, 0.2293252094645479, 0.2293252094645479, 0.32914939596239634], 
reward next is 0.6709, 
noisyNet noise sample is [array([2.2222004], dtype=float32), 1.4179928]. 
=============================================
[2019-04-02 19:39:52,529] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[43.83279 ]
 [43.610325]
 [43.32995 ]
 [42.730225]
 [39.58352 ]], R is [[44.01024246]
 [44.23731995]
 [44.45824051]
 [44.67267609]
 [44.87820053]].
[2019-04-02 19:39:56,391] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-02 19:39:56,401] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7346
[2019-04-02 19:39:56,410] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2254751.09707898 W.
[2019-04-02 19:39:56,418] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.0, 49.66666666666667, 1.0, 2.0, 0.9882937540978061, 1.0, 2.0, 0.9882937540978061, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 2254751.09707898, 2254751.09707898, 427703.1047304858], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4377000.0000, 
sim time next is 4377600.0000, 
raw observation next is [33.0, 49.0, 1.0, 2.0, 0.6065252608853211, 1.0, 2.0, 0.6065252608853211, 1.0, 1.0, 0.9656079684781392, 6.9112, 6.9112, 121.94756008, 2075434.644758897, 2075434.644758897, 398942.3555027678], 
processed observation next is [1.0, 0.6956521739130435, 0.7777777777777778, 0.49, 1.0, 1.0, 0.5315776915301441, 1.0, 1.0, 0.5315776915301441, 1.0, 0.5, 0.957009960597674, 0.0, 0.0, 0.8096049824067558, 0.7412266588424632, 0.7412266588424632, 0.7671968375053226], 
reward next is 0.2328, 
noisyNet noise sample is [array([-0.4907722], dtype=float32), -0.4683512]. 
=============================================
[2019-04-02 19:40:19,277] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-02 19:40:19,279] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 19:40:19,281] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:40:19,281] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 19:40:19,284] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 19:40:19,287] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:40:19,288] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 19:40:19,290] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:40:19,286] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 19:40:19,291] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:40:19,293] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:40:19,323] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run40
[2019-04-02 19:40:19,346] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run40
[2019-04-02 19:40:19,371] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run40
[2019-04-02 19:40:19,372] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run40
[2019-04-02 19:40:19,372] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run40
[2019-04-02 19:40:50,123] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.382194], dtype=float32), -0.086074725]
[2019-04-02 19:40:50,126] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [22.0, 83.0, 1.0, 2.0, 0.6166700536337074, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 756861.5113790258, 756861.5113790258, 163130.9927022176]
[2019-04-02 19:40:50,126] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 19:40:50,128] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.2950094e-38], sampled 0.6368186765143362
[2019-04-02 19:40:50,129] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 756861.5113790258 W.
[2019-04-02 19:40:51,247] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.382194], dtype=float32), -0.086074725]
[2019-04-02 19:40:51,248] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [28.15, 63.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8672921557120892, 6.9112, 6.9112, 121.9260426156618, 634062.4798198519, 634062.4798198519, 171778.1032937512]
[2019-04-02 19:40:51,253] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 19:40:51,257] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.0000000e+00 1.1188856e-34 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.6120431807895204
[2019-04-02 19:41:20,014] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.382194], dtype=float32), -0.086074725]
[2019-04-02 19:41:20,015] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [23.16666666666667, 86.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7956845270943245, 6.911199999999999, 6.9112, 121.9260426156618, 591210.2384929674, 591210.2384929679, 159691.1193927793]
[2019-04-02 19:41:20,015] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 19:41:20,019] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8060100802187502
[2019-04-02 19:41:21,377] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.382194], dtype=float32), -0.086074725]
[2019-04-02 19:41:21,377] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [31.0, 63.0, 1.0, 2.0, 0.7004428696772872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 798306.9865393486, 798306.9865393486, 176230.1506555831]
[2019-04-02 19:41:21,378] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 19:41:21,380] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1.000000e+00 0.000000e+00 8.745053e-10 0.000000e+00 8.288937e-37], sampled 0.5600200933879069
[2019-04-02 19:41:21,383] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 798306.9865393486 W.
[2019-04-02 19:41:37,069] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.382194], dtype=float32), -0.086074725]
[2019-04-02 19:41:37,072] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation this: [20.5, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.920835011608154, 6.9112, 6.9112, 121.9260426156618, 688189.53876634, 688189.53876634, 170464.4955616623]
[2019-04-02 19:41:37,074] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Observation forecast: []
[2019-04-02 19:41:37,078] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.16949412601635117
[2019-04-02 19:41:37,078] A3C_EVAL-Part3-NA-Bej-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 688189.53876634 W.
[2019-04-02 19:42:26,995] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7827.8716 2531179063.6005 830.0000
[2019-04-02 19:42:27,129] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 8538.6717 2259409836.2404 535.0000
[2019-04-02 19:42:27,157] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 8388.4233 2294281336.9464 695.0000
[2019-04-02 19:42:27,206] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 8341.1830 2340420203.4782 617.0000
[2019-04-02 19:42:27,341] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 8610.9703 2220580598.4841 542.0000
[2019-04-02 19:42:28,355] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 975000, evaluation results [975000.0, 7827.871649928004, 2531179063.600508, 830.0, 8538.671714964097, 2259409836.2404323, 535.0, 8610.970338942027, 2220580598.4841466, 542.0, 8341.182992779966, 2340420203.478191, 617.0, 8388.423332370152, 2294281336.946412, 695.0]
[2019-04-02 19:42:34,471] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.2913796e-02 2.2057744e-15 1.6765516e-11 7.4934726e-15 9.4708622e-01], sum to 1.0000
[2019-04-02 19:42:34,481] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3690
[2019-04-02 19:42:34,490] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.95, 94.0, 1.0, 2.0, 0.3940179526565435, 1.0, 1.0, 0.3940179526565435, 1.0, 1.0, 0.6272894128980679, 6.911200000000001, 6.9112, 121.94756008, 1347689.976450566, 1347689.976450566, 294471.5818601036], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4786200.0000, 
sim time next is 4786800.0000, 
raw observation next is [23.96666666666667, 94.0, 1.0, 2.0, 0.349814683549374, 1.0, 2.0, 0.349814683549374, 1.0, 2.0, 0.5569163688794829, 6.911199999999999, 6.9112, 121.94756008, 1196380.079845517, 1196380.079845517, 275796.7108488994], 
processed observation next is [1.0, 0.391304347826087, 0.4432098765432099, 0.94, 1.0, 1.0, 0.22596986136830238, 1.0, 1.0, 0.22596986136830238, 1.0, 1.0, 0.4461454610993536, -8.881784197001253e-17, 0.0, 0.8096049824067558, 0.4272785999448275, 0.4272785999448275, 0.5303782900940373], 
reward next is 0.4696, 
noisyNet noise sample is [array([0.25956786], dtype=float32), -0.64964634]. 
=============================================
[2019-04-02 19:42:44,535] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 8.9136285e-20 0.0000000e+00 1.1646381e-34], sum to 1.0000
[2019-04-02 19:42:44,547] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1028
[2019-04-02 19:42:44,557] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 799755.530630674 W.
[2019-04-02 19:42:44,562] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.0, 83.16666666666667, 1.0, 2.0, 0.2339044130648988, 1.0, 1.0, 0.2339044130648988, 1.0, 1.0, 0.3723834433342307, 6.9112, 6.9112, 121.94756008, 799755.530630674, 799755.530630674, 231990.4713788722], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4925400.0000, 
sim time next is 4926000.0000, 
raw observation next is [27.0, 82.33333333333334, 1.0, 2.0, 0.3469013285108342, 0.0, 1.0, 0.0, 1.0, 2.0, 0.552278212776778, 6.911200000000001, 6.9112, 121.9260426156618, 790735.1163605674, 790735.1163605669, 208281.5965840122], 
processed observation next is [1.0, 0.0, 0.5555555555555556, 0.8233333333333335, 1.0, 1.0, 0.22250158156051694, 0.0, 0.5, -0.1904761904761905, 1.0, 1.0, 0.44034776597097247, 8.881784197001253e-17, 0.0, 0.8094621288201359, 0.28240539870020265, 0.2824053987002025, 0.40054153189233116], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1147902], dtype=float32), 1.7590082]. 
=============================================
[2019-04-02 19:42:44,580] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[35.39039 ]
 [37.0988  ]
 [38.17955 ]
 [37.74249 ]
 [37.675957]], R is [[34.84863281]
 [34.50014496]
 [34.81233215]
 [35.08676147]
 [35.35355759]].
[2019-04-02 19:42:55,131] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.998586e-01 0.000000e+00 1.413918e-04 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-02 19:42:55,143] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9340
[2019-04-02 19:42:55,153] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 936321.245823401 W.
[2019-04-02 19:42:55,159] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.33333333333334, 81.0, 1.0, 2.0, 0.4107320467997843, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6538987951141512, 6.911199999999999, 6.9112, 121.9260426156618, 936321.245823401, 936321.2458234015, 227442.819482015], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5078400.0000, 
sim time next is 5079000.0000, 
raw observation next is [29.16666666666667, 82.5, 1.0, 2.0, 0.4140639152048087, 1.0, 1.0, 0.4140639152048087, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 121.9260426156618, 943921.384433172, 943921.3844331724, 210088.1712124703], 
processed observation next is [0.0, 0.782608695652174, 0.6358024691358026, 0.825, 1.0, 1.0, 0.3024570419104865, 1.0, 0.5, 0.3024570419104865, 0.0, 0.5, -0.25, -8.881784197001253e-17, 0.0, 0.8094621288201359, 0.33711478015470425, 0.3371147801547044, 0.4040157138701352], 
reward next is 0.5960, 
noisyNet noise sample is [array([-2.2741752], dtype=float32), -0.2025093]. 
=============================================
[2019-04-02 19:42:55,175] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[42.059742]
 [41.38018 ]
 [41.272263]
 [40.36855 ]
 [40.615993]], R is [[42.04666138]
 [42.18880463]
 [42.33162308]
 [42.47735214]
 [42.62937546]].
[2019-04-02 19:43:27,168] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 7.4505022e-23 0.0000000e+00 4.1182766e-32], sum to 1.0000
[2019-04-02 19:43:27,176] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5846
[2019-04-02 19:43:27,185] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 778058.1531752596 W.
[2019-04-02 19:43:27,191] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.51666666666667, 92.83333333333333, 1.0, 2.0, 0.3413426725060668, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5434286513844009, 6.911199999999999, 6.9112, 121.9260426156618, 778058.1531752596, 778058.15317526, 206690.9357190036], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5536200.0000, 
sim time next is 5536800.0000, 
raw observation next is [25.5, 93.0, 1.0, 2.0, 0.6828738307968901, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 778273.0799424663, 778273.0799424663, 172926.9225541675], 
processed observation next is [1.0, 0.08695652173913043, 0.5, 0.93, 1.0, 1.0, 0.6224688461867739, 0.0, 1.0, -0.1904761904761905, 0.0, 0.5, -0.25, 0.0, 0.0, 0.8094621288201359, 0.2779546714080237, 0.2779546714080237, 0.3325517741426298], 
reward next is 0.6674, 
noisyNet noise sample is [array([-1.4523569], dtype=float32), 2.205949]. 
=============================================
[2019-04-02 19:43:28,461] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 6.4878522e-20 1.9354343e-30 2.3321517e-28], sum to 1.0000
[2019-04-02 19:43:28,469] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4949
[2019-04-02 19:43:28,478] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 818902.4847469069 W.
[2019-04-02 19:43:28,486] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.4, 86.0, 1.0, 2.0, 0.7173840861911247, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 818902.4847469069, 818902.4847469069, 179521.3690971751], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5556600.0000, 
sim time next is 5557200.0000, 
raw observation next is [25.4, 85.66666666666667, 1.0, 2.0, 0.3639945877726515, 1.0, 1.0, 0.3639945877726515, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260426156618, 829718.9859840783, 829718.9859840783, 196526.2179273793], 
processed observation next is [1.0, 0.30434782608695654, 0.49629629629629624, 0.8566666666666667, 1.0, 1.0, 0.242850699729347, 1.0, 0.5, 0.242850699729347, 0.0, 1.0, -0.25, 0.0, 0.0, 0.8094621288201359, 0.29632820928002795, 0.29632820928002795, 0.3779350344757294], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.23443559], dtype=float32), 0.28856066]. 
=============================================
[2019-04-02 19:43:34,910] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 1.0742606e-37 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-02 19:43:34,923] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0073
[2019-04-02 19:43:34,924] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.000000e+00 6.411704e-20 4.443780e-36 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-02 19:43:34,929] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.5, 97.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9333860547854497, 6.911199999999999, 6.9112, 121.9260426156618, 678344.4201533688, 678344.4201533693, 181279.4215644526], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5629800.0000, 
sim time next is 5630400.0000, 
raw observation next is [23.5, 97.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9354553728200978, 6.9112, 6.9112, 121.9260426156618, 679859.6843245316, 679859.6843245316, 181561.2445299877], 
processed observation next is [0.0, 0.17391304347826086, 0.42592592592592593, 0.97, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.9193192160251223, 0.0, 0.0, 0.8094621288201359, 0.24280703011590413, 0.24280703011590413, 0.3491562394807456], 
reward next is 0.6508, 
noisyNet noise sample is [array([0.3771487], dtype=float32), 0.07622627]. 
=============================================
[2019-04-02 19:43:34,938] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8692
[2019-04-02 19:43:34,945] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.43333333333333, 85.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8106538939008263, 6.9112, 6.9112, 121.9260426156618, 601418.8216270993, 601418.8216270993, 161992.0843415432], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6050400.0000, 
sim time next is 6051000.0000, 
raw observation next is [23.31666666666666, 86.16666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8102115818987297, 6.9112, 6.9112, 121.9260426156618, 601111.1817791744, 601111.1817791744, 161927.2935096325], 
processed observation next is [1.0, 0.0, 0.4191358024691355, 0.8616666666666666, 0.0, 1.0, -0.1904761904761905, 0.0, 1.0, -0.1904761904761905, 1.0, 1.0, 0.7627644773734121, 0.0, 0.0, 0.8094621288201359, 0.2146825649211337, 0.2146825649211337, 0.3113986413646779], 
reward next is 0.6886, 
noisyNet noise sample is [array([0.71892184], dtype=float32), 1.4981031]. 
=============================================
[2019-04-02 19:43:34,960] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[82.457924]
 [82.96115 ]
 [83.63529 ]
 [84.64422 ]
 [86.02197 ]], R is [[82.03552246]
 [81.90364838]
 [81.77305603]
 [81.6436615 ]
 [81.51527405]].
[2019-04-02 19:43:35,271] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-02 19:43:35,276] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation job starts!
[2019-04-02 19:43:35,278] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:43:35,278] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation job starts!
[2019-04-02 19:43:35,283] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation job starts!
[2019-04-02 19:43:35,285] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:43:35,280] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation job starts!
[2019-04-02 19:43:35,285] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:43:35,287] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation job starts!
[2019-04-02 19:43:35,286] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:43:35,367] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-02 19:43:35,803] EPLUS_ENV_Part3-NA-Bej-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v1-res1/Eplus-env-sub_run41
[2019-04-02 19:43:35,927] EPLUS_ENV_Part3-NA-Bej-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v3-res1/Eplus-env-sub_run41
[2019-04-02 19:43:35,932] EPLUS_ENV_Part3-NA-Bej-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Train-v1-res1/Eplus-env-sub_run41
[2019-04-02 19:43:36,054] EPLUS_ENV_Part3-NA-Bej-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v2-res1/Eplus-env-sub_run41
[2019-04-02 19:43:36,438] EPLUS_ENV_Part3-NA-Bej-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_bej_na/1/Eplus-env-Part3-NA-Bej-Test-v4-res1/Eplus-env-sub_run41
[2019-04-02 19:44:04,070] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.36696458], dtype=float32), -0.026773836]
[2019-04-02 19:44:04,071] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation this: [17.30456145666667, 80.21066252000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4481262004045333, 6.911199999999999, 6.9112, 121.9260426156618, 319963.4939053266, 319963.4939053271, 101879.0683620248]
[2019-04-02 19:44:04,071] A3C_EVAL-Part3-NA-Bej-Test-v4 DEBUG:Observation forecast: []
[2019-04-02 19:44:04,073] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Softmax [1.00000e+00 0.00000e+00 3.75941e-20 0.00000e+00 0.00000e+00], sampled 0.7647160235253546
[2019-04-02 19:44:14,252] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.36696458], dtype=float32), -0.026773836]
[2019-04-02 19:44:14,254] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [21.54492247, 102.60827687, 1.0, 2.0, 0.2425517184135725, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3901144775013298, 6.911199999999999, 6.9112, 121.9260426156618, 577583.1930062879, 577583.1930062884, 179534.5685525723]
[2019-04-02 19:44:14,254] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 19:44:14,258] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [1.3056383e-17 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.4622508665312519
[2019-04-02 19:44:27,510] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.36696458], dtype=float32), -0.026773836]
[2019-04-02 19:44:27,513] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation this: [33.33333333333334, 36.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7789033549022086, 6.911199999999999, 6.9112, 121.9260426156618, 577719.2620113569, 577719.2620113574, 158101.4816901714]
[2019-04-02 19:44:27,513] A3C_EVAL-Part3-NA-Bej-Train-v1 DEBUG:Observation forecast: []
[2019-04-02 19:44:27,518] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.009074336051712262
[2019-04-02 19:44:55,730] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.36696458], dtype=float32), -0.026773836]
[2019-04-02 19:44:55,730] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation this: [28.80295023666667, 91.88024052166666, 1.0, 2.0, 0.9703753739296366, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.962146469664061, 6.9112, 121.9257461438789, 1132283.740570002, 1106194.657995816, 233670.9675983434]
[2019-04-02 19:44:55,733] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Observation forecast: []
[2019-04-02 19:44:55,738] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Softmax [0.99056643 0.         0.00943353 0.         0.        ], sampled 0.7099271567870522
[2019-04-02 19:44:55,740] A3C_EVAL-Part3-NA-Bej-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1132283.740570002 W.
[2019-04-02 19:45:10,128] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.36696458], dtype=float32), -0.026773836]
[2019-04-02 19:45:10,129] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation this: [22.0, 83.0, 1.0, 2.0, 0.845995911077985, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 121.9260096134638, 1037651.908105574, 1037651.908105574, 208829.9859380582]
[2019-04-02 19:45:10,130] A3C_EVAL-Part3-NA-Bej-Test-v1 DEBUG:Observation forecast: []
[2019-04-02 19:45:10,132] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Softmax [0.00201287 0.         0.99798715 0.         0.        ], sampled 0.3697748912564397
[2019-04-02 19:45:44,743] A3C_EVAL-Part3-NA-Bej-Train-v1 INFO:Evaluation: average rewards by now are 7670.1706 2670997372.3641 578.0000
[2019-04-02 19:45:44,780] A3C_EVAL-Part3-NA-Bej-Test-v1 INFO:Evaluation: average rewards by now are 7243.3756 2471797306.3166 336.0000
[2019-04-02 19:45:45,061] A3C_EVAL-Part3-NA-Bej-Test-v4 INFO:Evaluation: average rewards by now are 7672.0115 2495859200.0401 396.0000
[2019-04-02 19:45:45,065] A3C_EVAL-Part3-NA-Bej-Test-v3 INFO:Evaluation: average rewards by now are 7048.3129 2542517985.8214 409.0000
[2019-04-02 19:45:45,158] A3C_EVAL-Part3-NA-Bej-Test-v2 INFO:Evaluation: average rewards by now are 7868.7810 2452988491.0423 284.0000
[2019-04-02 19:45:46,176] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1000000, evaluation results [1000000.0, 7670.170608699544, 2670997372.3641195, 578.0, 7243.375631848576, 2471797306.3166494, 336.0, 7868.780969755134, 2452988491.042251, 284.0, 7048.312930178863, 2542517985.8214474, 409.0, 7672.011484038162, 2495859200.040082, 396.0]
